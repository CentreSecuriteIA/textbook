<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="AI is undergoing unprecedented growth in capabilities, from writing code and analyzing scientific papers to generating images and engaging in complex reasoning. As these systems rapidly approach and potentially surpass human-level performance in many domains, the decisions we make today about their development will shape humanity's future. This book provides a comprehensive framework for understanding and addressing AI Safety challenges - from fundamental concepts to technical implementation. Whether you're a policymaker, researcher, engineer, or concerned citizen, our aim is to equip you with the knowledge needed to contribute meaningfully to ensuring AI development benefits humanity."><link href=https://ai-safety-atlas.com/chapters/ rel=canonical><link rel=prev href=..><link href=01/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>AI Safety Atlas - AI Safety Atlas</title><link rel=stylesheet href=../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../overrides/css/base.css><link rel=stylesheet href=../overrides/css/buttons.css><link rel=stylesheet href=../overrides/css/tippy.css><link rel=stylesheet href=../overrides/css/tabs.css><link rel=stylesheet href=../overrides/css/typography.css><link rel=stylesheet href=../overrides/css/admonitions.css><link rel=stylesheet href=../overrides/css/navigation.css><link rel=stylesheet href=../overrides/css/metadata.css><link rel=stylesheet href=../overrides/css/figures.css><link rel=stylesheet href=../overrides/css/footer.css><link rel=stylesheet href=../overrides/css/section-end.css><link rel=stylesheet href=../overrides/css/scroll-buttons.css><link rel=stylesheet href=../overrides/css/home/base.css><link rel=stylesheet href=../overrides/css/home/hero.css><link rel=stylesheet href=../overrides/css/home/hero-animation.css><link rel=stylesheet href=../overrides/css/home/header.css><link rel=stylesheet href=../overrides/css/home/why.css><link rel=stylesheet href=../overrides/css/header.css><link rel=stylesheet href=../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#ai-safety-atlas class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=./ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link md-nav__link--active" for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#capabilities class=md-nav__link> <span class=md-ellipsis> Capabilities </span> </a> </li> <li class=md-nav__item> <a href=#risks class=md-nav__link> <span class=md-ellipsis> Risks </span> </a> </li> <li class=md-nav__item> <a href=#strategies class=md-nav__link> <span class=md-ellipsis> Strategies </span> </a> </li> <li class=md-nav__item> <a href=#governance class=md-nav__link> <span class=md-ellipsis> Governance </span> </a> </li> <li class=md-nav__item> <a href=#evaluations class=md-nav__link> <span class=md-ellipsis> Evaluations </span> </a> </li> <li class=md-nav__item> <a href=#specification class=md-nav__link> <span class=md-ellipsis> Specification </span> </a> </li> <li class=md-nav__item> <a href=#generalization class=md-nav__link> <span class=md-ellipsis> Generalization </span> </a> </li> <li class=md-nav__item> <a href=#scalable-oversight class=md-nav__link> <span class=md-ellipsis> Scalable Oversight </span> </a> </li> <li class=md-nav__item> <a href=#interpretability class=md-nav__link> <span class=md-ellipsis> Interpretability </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=ai-safety-atlas>AI Safety Atlas<a class=headerlink href=#ai-safety-atlas title="Permanent link">&para;</a></h1> <div class=chapter-meta> <div class=meta-grid> <div class=meta-col> <div class=meta-item> <span class=meta-icon> <i class="fas fa-pen-nib"></i> </span> <div class=meta-content> <div class=meta-label>Authors</div> <div class="meta-value meta-list"> <div> <div>Markov Grey</div> <div>Charbel-Raphael Segerie</div> <div>Jeanne Salle</div> <div>Charles Martinet</div> </div> </div> </div> </div> <div class=meta-item> <span class=meta-icon> <i class="fas fa-university"></i> </span> <div class=meta-content> <div class=meta-label>Affiliations</div> <div class="meta-value meta-list"> <div><div>French Center for AI Safety (CeSIA)</div></div> </div> </div> </div> <div class=meta-item> <span class=meta-icon> <i class="fas fa-graduation-cap"></i> </span> <div class=meta-content> <div class=meta-label>Advisors</div> <div class=meta-value> <div>Vincent Corruble</div> <div>Fabien Roger</div> </div> </div> </div> </div> <div class=meta-col> <div class=meta-item> <span class=meta-icon> <i class="fas fa-quote-right"></i> </span> <div class=meta-content> <div class=meta-label>Cite this work as</div> <div class=meta-value> <div>Markov Grey and Charbel-Raphael Segerie et al. 2024. AI Safety Atlas. French Center for AI Safety (CeSIA). ai-safety-atlas.com</div> </div> </div> </div> <div class=meta-item> <span class=meta-icon> <i class="fas fa-hand-holding-dollar"></i> </span> <div class=meta-content> <div class=meta-label>Project funded by</div> <div class=meta-value> <div><a href=https://manifund.org/projects/ai-safety-textbook target=_blank style="text-decoration: none; color: inherit;">Ryan Kidd (Manifund Regrant)</a></div> <div>Open Philanthropy</div> </div> </div> </div> <div class=meta-item> <span class=meta-icon> <i class="fas fa-heart"></i> </span> <div class=meta-content> <div class=meta-label>Acknowledgements</div> <div class=meta-value> <div>Jonathan Claybrough</div> <div>Jérémy Andréoletti</div> <div>Evander Hammer</div> <div>Josh Thorsteinson</div> </div> </div> </div> </div> </div> </div> <div style="content: ''; display: block; width: 100%; height: 3px; background-color: currentColor; opacity: 0.7; margin: 1.5em 0;"></div> <p>AI is undergoing unprecedented growth in capabilities, from writing code and analyzing scientific papers to generating images and engaging in complex reasoning. As these systems rapidly approach and potentially surpass human-level performance in many domains, the decisions we make today about their development will shape humanity's future. This book provides a comprehensive framework for understanding and addressing AI Safety challenges - from fundamental concepts to technical implementation. Whether you're a policymaker, researcher, engineer, or concerned citizen, our aim is to equip you with the knowledge needed to contribute meaningfully to ensuring AI development benefits humanity.</p> <div class="admonition quote"> <p class=admonition-title>Geoffrey Hinton (Most cited computer scientist ever, Godfather of modern AI, Turing Award Recipient) (<a href=https://www.cbsnews.com/news/geoffrey-hinton-ai-dangers-60-minutes-transcript/ >Hinton, 2024</a>)</p> <p>I can't see a path that guarantees safety. We're entering a period of great uncertainty where we're dealing with things we've never dealt with before. And normally, the first time you deal with something totally novel, you get it wrong. And we can't afford to get it wrong with these things. [...] If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further [...] it's as if aliens had landed and people haven't realized because they speak very good English</p> </div> <h2 id=capabilities><a href=https://ai-safety-atlas.com/chapters/01/ >Capabilities</a><a class=headerlink href=#capabilities title="Permanent link">&para;</a></h2> <p><em>Why might AIs keep growing both increasingly general and capable?</em></p> <p>AI models are moving from specialized tools into increasingly general-purpose systems that can handle complex tasks. In this chapter we talk about empirical trends which show that scaling up - using more data, compute, and parameters - is leading to steady gains in both capabilities and generality. We explain the definitions of things like artificial general intelligence (AGI) and superintelligence (<abbr title="Artificial Superintelligence. An AI with cognitive abilities far greater than those of humans in a wide range of important domains.">ASI</abbr>) that we will be using throughout the book. Rather than viewing AI progress through discrete thresholds like "narrow" versus "general" intelligence, we introduce frameworks for measuring capabilities, generality and autonomy along continuous curves. Based on this we look at arguments for different AI <abbr title="The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence.">takeoff</abbr> scenarios, and provide expert opinions on timelines to transformative AI. Understanding these concepts shapes the conversations around potential sources of risks and safety strategies throughout the rest of the book. After reading this chapter, you'll be able to critically evaluate many of the claims about AI progress and engage in informed discussions about future AI developments.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/01/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <a href="https://www.youtube.com/watch?v=J_iMeH1hb9M" class=action-button> <i class="fas fa-video"></i> <span>Watch</span> </a> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1L32xCVUCWEsm-x8UZ3GSTgKnmBcC7rJQLLIh9wGLj40/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=risks><a href=https://ai-safety-atlas.com/chapters/02/ >Risks</a><a class=headerlink href=#risks title="Permanent link">&para;</a></h2> <p><em>What can go wrong if AI keeps becoming more capable?</em></p> <p>Once we have established the core arguments behind why capabilities, generality and autonomy might keep increasing, we can look at what risks correspond to these increased levels. We divide risks from AI into three main categories: misuse (humans using AI for harm, like cyber attacks or bioweapons), misalignment (AI systems failing in unintended ways), and systemic risks (problems that emerge when AI interacts with other complex systems). We also talk about underlying factors that amplify all of these risks like competitive pressures creating race dynamics and widespread deployment. Finally, we also give brief explanations of how to concretely think about dangerous capabilities like - deception, situational awareness, autonomous replication, scheming and power-seeking behavior. By the end of this chapter, you will be able to identify how different risks emerge from technical failures of AI systems, or the combination of AI capabilities with human incentives, and other complex socio-technical systems.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/02/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <a href="https://www.youtube.com/watch?v=dhr4u-w75aQ" class=action-button> <i class="fas fa-video"></i> <span>Watch</span> </a> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1evE1rG91DKBuKlWnqPw45QtPxKBz0GlD_ZYrurNdvN4/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=strategies><a href=https://ai-safety-atlas.com/chapters/03/ >Strategies</a><a class=headerlink href=#strategies title="Permanent link">&para;</a></h2> <p><em>What strategies can we develop to mitigate risks?</em></p> <p>Building upon the understanding of capabilities and risks, in this chapter we look at concrete strategies for mitigating these risks and making AI development safer. We divide potential mitigation strategies into the same three categories as risks: preventing misuse (e.g. through monitored APIs and defensive technologies), addressing alignment (e.g. via technical research and control measures), and managing systemic impacts (e.g. through governance and safety culture). We explain how AI Safety is just starting to gain momentum as a field, but it is important to remember that no single solution might be sufficient. We might need a layered approach where multiple safeguards all work together to create robust protection against the risks outlined in the previous chapter. The purpose of this chapter is to give an overview of many different safety strategies and proposals and to show how these individual pieces might fit into the bigger picture. By reading this chapter and the ones before it, you have a solid foundation for the individual deep-dives that follow in all chapters after this. The systemic and social strategies outlined lead directly into the next few chapters on governance and evaluations, and the technical problems and mitigation strategies are discussed in dedicated individual chapters as well.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/03/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <a href="https://www.youtube.com/watch?v=iO7Jl4xders" class=action-button> <i class="fas fa-video"></i> <span>Watch</span> </a> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1cv0gzwSouDjckYHzV7gYbHPKhJZR6bwbJWgHzEJ604Q/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=governance><a href=https://ai-safety-atlas.com/chapters/04/ >Governance</a><a class=headerlink href=#governance title="Permanent link">&para;</a></h2> <p><em>How can we guide safe AI development and deployment?</em></p> <p>Previous chapters provided overviews of what AI can do, what could go wrong, and potential mitigation strategies, in this chapter we take a deeper look at how to shape the impact of AI through governance frameworks. We start by explaining why AI regulation faces unique challenges due things like unexpected capability jumps, post deployment safety issues, and rapid proliferation. We explain key governance targets like - compute governance (controlling key inputs like chips and training resources), data governance (managing training data and deployment). Following this we look at individual layers that can introduce changes to affect the chosen targets - corporate governance (internal safety frameworks like Anthropic's RSPs), national governance (governmental initiatives like AI Safety Institutes and regulations like the EU AI Act), and international governance (approaches ranging from non-proliferation to regulatory agreements). Reiterating what we said in the strategies chapter, no single governance layer might be completely sufficient - rather, we need coordinated action across all levels to address AI risks effectively. Reading this chapter will help you participate meaningfully in discussions on AI development, deployment, auditing and policy.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/04/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <a href="https://www.youtube.com/watch?v=FSKuDqze9es" class=action-button> <i class="fas fa-video"></i> <span>Watch</span> </a> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1tp5rpzw_gekjju-UBp8tkbbnQOuA2QzsPF_um8Z4IOU/edit?tab=t.0#heading=h.fo57hwsn3del" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=evaluations><a href=https://ai-safety-atlas.com/chapters/05/ >Evaluations</a><a class=headerlink href=#evaluations title="Permanent link">&para;</a></h2> <p><em>How do we measure if an AI system is actually safe?</em></p> <p>Evaluations is the first chapter in what we consider to be the technical sequence, because it helps us bridge theory and practice by showing how evaluation results can trigger specific governance actions or technical safety protocols. Our explanation is decomposed into the questions of - what are we evaluating? (evaluated properties), how are we evaluating it? (evaluation techniques), and what elements play into good evaluations? (evaluation design). We cover AI Safety specific evaluation methods in three different categories - dangerous capability evaluations (what AI systems can do), propensity evaluations (how they tend to behave by default), and control evaluations (our ability to maintain safety under adversarial conditions). We explain how we might create techniques for evaluating dangerous capabilities (deception, CBRN risks, and autonomous replication) and dangerous propensities (e.g. power-seeking and scheming) which we previously outlined in the risks chapter. After reading this chapter, you will be able to understand what designing AI Safety evaluations entails, and how their results can inform both technical research directions and governance decisions.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/05/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <div class="action-button disabled" data-tippy-content="Video coming soon"> <i class="fas fa-video"></i> <span>Watch</span> </div> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1T-UU0FBeElX6cvbWYKpVAl3U4ivrQLHA3IdIWqWKuBA/edit?tab=t.0#heading=h.fo57hwsn3del" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=specification><a href=https://ai-safety-atlas.com/chapters/06/ >Specification</a><a class=headerlink href=#specification title="Permanent link">&para;</a></h2> <p><em>Why is telling AI systems what we want so hard?</em></p> <p>Evaluations help us figure out what AI systems can do or tend to do, and if our current mitigation measures are enough. The next step is to try and answer - how do we correctly specify what we even want AI to do in the first place? We introduced the general concept of <abbr title="Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”">specification gaming</abbr> in the risks chapter, here we build upon that and look at theoretical foundations and practical failure modes like reward hacking (finding unintended ways to maximize reward) and reward tampering (directly manipulating reward mechanisms). We also explain how suggested mitigation strategies to the specification problem might work. This includes things like imitation-based methods (e.g. behavioral cloning, inverse reinforcement learning) and feedback-based approaches (e.g. reward modeling, <abbr title="A method for training an AI to give desirable outputs by using human feedback as a training signal.">reinforcement learning from human feedback</abbr> (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>), <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>). We also cover the limitations of these approaches which helps set up the context for the next few chapters on generalization and scalable oversight. After reading this chapter, you will be able to understand why trying to encode even simple instructions can lead to unexpected behaviors and how AI safety research is currently trying to address these challenges.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/06/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <div class="action-button disabled" data-tippy-content="Video coming soon"> <i class="fas fa-video"></i> <span>Watch</span> </div> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1JfmzGii5QG6hW8AM5WxzDBVyGc14aLV_Lc_1PkK2ZLc/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=generalization><a href=https://ai-safety-atlas.com/chapters/07/ >Generalization</a><a class=headerlink href=#generalization title="Permanent link">&para;</a></h2> <p><em>Why might AIs pursue unintended goals?</em></p> <p>In this chapter we turn to how AI systems actually learn and generalize from training. We start by a reminder of what an AI's “goals” concretely means, and how people in AI Safety use the term. This builds on the intuition provided in both the risks and the evaluations chapters between capabilities (what AI systems can do) versus propensities (how they tend to behave by default). We walk you through how capability generalization and goal generalization are separate things using concrete empirical results. We specifically focus on goal misgeneralization which is a different concern from standard ML generalization problems like overfitting. We also explore why goal misgeneralization happens by looking at the theory of neural network loss landscapes which shows how different types of learned algorithms can emerge from different training runs. We pay particular attention to one specific type of learned algorithm - a <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">learned optimizer</abbr>. Using the concept of learned optimizers as a starting point, we dive deeper into the inner alignment and deceptive alignment (scheming) problems that have been mentioned in many of the previous chapters. We also connect these concerns to evaluation strategies mentioned previously, as well as mitigation strategies which are explored in the subsequent chapter on interpretability. At the end of this chapter you should be able to distinguish between different ways that ML models might generalize between distributions and understand how and why capable AI systems might scheme, deceive, and pursue long term unintended goals.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/07/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <div class="action-button disabled" data-tippy-content="Video coming soon"> <i class="fas fa-video"></i> <span>Watch</span> </div> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1uQooTncb7Hw2NhITtr3S5iGHqT6cvj74c0SZ4Unad_M/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=scalable-oversight><a href=https://ai-safety-atlas.com/chapters/08/ >Scalable Oversight</a><a class=headerlink href=#scalable-oversight title="Permanent link">&para;</a></h2> <p><em>How can we maintain meaningful control when AIs are smarter than us?</em></p> <p>Building on the specification problem explored in previous chapters, we look at how we could maintain meaningful human oversight even when tasks start to exceed human ability to iteratively provide feedback. The core challenge is providing accurate training signals for complex "fuzzy" tasks where success criteria are ambiguous and direct evaluation becomes infeasible. We start by exploring how to break down highly complex tasks into more manageable pieces through techniques like task decomposition. We then explain approaches that seek to elicit an AI's <abbr title="Information or capabilities present in a model but not readily apparent in its typical outputs.">latent knowledge</abbr> through <abbr title="Techniques that encourage models to reveal their reasoning steps in natural language.">externalized reasoning oversight</abbr> and procedural cloning - strategies which focus on aligning the AI's entire reasoning process rather than just its outputs. We dive deeper into theoretical frameworks like Iterated Amplification and Distillation (IDA) and more practical approaches like AI Safety via. debate and weak-to-strong (W2S) generalization. Finally, we examine methods for evaluating these oversight techniques, which includes explanations of sandwiching experiments and meta-level adversarial evaluations. By the end of this chapter, you should understand the existing strategies for maintaining control over AI systems that exceed human capabilities.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/08/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <div class="action-button disabled" data-tippy-content="Video coming soon"> <i class="fas fa-video"></i> <span>Watch</span> </div> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1DaygDSW0L5dWuJnpSjYPF2XUbW51UoBJsT1cjLYKc2w/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h2 id=interpretability><a href=https://ai-safety-atlas.com/chapters/09/ >Interpretability</a><a class=headerlink href=#interpretability title="Permanent link">&para;</a></h2> <p><em>How can we understand what's happening inside AI systems?</em></p> <p>Our final chapter examines how we might understand the internal workings of AI models. Progress in interpretability could help address many of the risks and challenges discussed throughout the book - from detecting deceptive behavior to guiding and verifying training outcomes. We primarily focus on mechanistic interpretability - a bottom-up approach that aims to reverse-engineer neural networks by studying how their components process information. We explain both observational methods that analyze model components without modification (like feature visualization and probing), and interventional methods that actively test and alter model behavior (like <abbr title="A technique that identifies which parts of a neural network are responsible for specific behaviors by selectively replacing activations.">activation patching</abbr> and steering). We also look at some efforts to automate and scale interpretability through techniques like Automatic Circuit Discovery (<abbr title="Automatic Circuit DisCovery an algorithm that automates the process of finding circuits within neural networks.">ACDC</abbr>).</p> <p>Like all approaches discussed in this book - from governance frameworks to technical safety measures - interpretability is just one tool in our AI Safety toolbox. The path to safer AI systems will likely require combining insights and methods from all these different approaches. While current interpretability techniques face significant limitations, they also represent some of our best attempts at understanding these increasingly complex systems. After reading this chapter, you'll understand both the promise and current limitations of these tools, and how they might contribute to building safer AI systems in the future.</p> <div class=action-buttons> <a href=https://ai-safety-atlas.com/chapters/09/ class=action-button> <i class="fas fa-book"></i> <span>Read</span> </a> <div class="action-button disabled" data-tippy-content="Video coming soon"> <i class="fas fa-video"></i> <span>Watch</span> </div> <a href=https://forms.gle/ZsA4hEWUx1ZrtQLL9 class=action-button> <i class="fas fa-comment"></i> <span>Feedback</span> </a> <a href="https://docs.google.com/document/d/1izDWZKR_xB2qj2a8LkbqcnqnjBIC-C7fn-74CIA-m9w/edit?usp=sharing" class=action-button> <i class="fas fa-users"></i> <span>Facilitate</span> </a> </div> <h1 id=the-path-forward>The Path Forward<a class=headerlink href=#the-path-forward title="Permanent link">&para;</a></h1> <p>AI Safety entails huge technical and social coordination challenges. By writing this book, we hope to help you join the growing community of people working on these problems. Whether you choose to contribute through technical research, policy work, or just spreading the word through advocacy, your involvement matters! Every new mind working on these problems brings us closer to ensuring AI development leads to a flourishing future for humanity.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=.. class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> Home </div> </div> </a> <a href=01/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> Chapter 01 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../overrides/js/mathjax.js></script> <script src=../overrides/js/tippy.js></script> <script src=../overrides/js/navigation.js></script> <script src=../overrides/js/scroll-buttons.js></script> <script src=../overrides/js/home/base.js></script> <script src=../overrides/js/home/hero.js></script> <script src=../overrides/js/home/why.js></script> <script src=../overrides/js/home/header.js></script> <script src=../overrides/js/home/scroll-animations.js></script> <script src=../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>