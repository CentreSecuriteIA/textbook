<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='What is a benchmark? Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in "three royal cubits" of material meant builders in other countries couldn&#39;t reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.'><link href=https://ai-safety-atlas.com/chapters/05/01/ rel=canonical><link href=../ rel=prev><link href=../02/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>5.1 - Benchmarks - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#51-benchmarks class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 5.1 - Benchmarks </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_6 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 05 - Evaluations </span> </a> <label class="md-nav__link " for=__nav_2_1_6 id=__nav_2_1_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_6_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_6> <span class="md-nav__icon md-icon"></span> 05 - Evaluations </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 5.1 - Benchmarks </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 5.1 - Benchmarks </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 5.1.1 History and Evolution </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 5.1.2 Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../02/ class=md-nav__link> <span class=md-ellipsis> 5.2 - Evaluated Properties </span> </a> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 5.3 - Evaluation Techniques </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 5.4 - Evaluation Frameworks </span> </a> </li> <li class=md-nav__item> <a href=../05/ class=md-nav__link> <span class=md-ellipsis> 5.5 - Dangerous Capability Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../06/ class=md-nav__link> <span class=md-ellipsis> 5.6 - Dangerous Propensity Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../07/ class=md-nav__link> <span class=md-ellipsis> 5.7 - Control Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../08/ class=md-nav__link> <span class=md-ellipsis> 5.8 - Evaluation Design </span> </a> </li> <li class=md-nav__item> <a href=../09/ class=md-nav__link> <span class=md-ellipsis> 5.9 - Limitations </span> </a> </li> <li class=md-nav__item> <a href=../10/ class=md-nav__link> <span class=md-ellipsis> 5.10 - Conclusion </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 5.1.1 History and Evolution </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 5.1.2 Limitations </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=51-benchmarks>5.1 Benchmarks<a class=headerlink href=#51-benchmarks title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <span class=meta-icon> <i class="fas fa-clock"></i> </span> <div class=meta-content> <div class=meta-label>Reading Time</div> <div class=meta-value>17 min</div> </div> </div> </div> <p><strong>What is a benchmark?</strong> Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in "three royal cubits" of material meant builders in other countries couldn't reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.</p> <p>AI basically had a similar problem before we started using standardized benchmarks.<sup id=fnref:footnote_1><a class=footnote-ref href=#fn:footnote_1>1</a></sup> A benchmark is a tool like a standardized test, which we can use to measure and compare what AI systems can and cannot do. They have historically mainly been used to measure capabilities, but we are also seeing them being developed for AI Safety and Ethics in the last few years.</p> <p><strong>How do benchmarks shape AI development and safety research?</strong> Benchmarks in AI are slightly different from other scientific fields. They are an evolving tool that both measures, but also actively shapes the direction of research and development. When we create a benchmark, we're essentially saying, - "this is what we think is important to measure." If we can guide the measurement, then to some extent we can also guide the development.</p> <div class="admonition quote"> <p class=admonition-title>François Chollet (<a href=https://arxiv.org/abs/1911.01547>Chollet, 2019</a>)</p> <p>Goal definitions and evaluation benchmarks are among the most potent drivers of scientific progress</p> </div> <h2 id=01>5.1.1 History and Evolution<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <figure class=iframe-figure> <iframe allow="web-share; clipboard-write" loading=lazy src="https://ourworldindata.org/grapher/test-scores-ai-capabilities-relative-human-performance?country=Handwriting+recognition~Speech+recognition~Image+recognition~Reading+comprehension~Language+understanding~Predictive+reasoning~Code+generation~Complex+reasoning~General+knowledge+tests~Nuanced+language+interpretation~Math+problem-solving~Reading+comprehension+with+unanswerable+questions&tab=chart" style="width: 100%; height: 600px; border: 0px none;"></iframe> <br> <figcaption><b>Interactive Figure 5.1:</b> Benchmark scores of various AI capabilities relative to human performance. (<a href=https://ourworldindata.org/artificial-intelligence>Giattino et al., 2023</a>)</figcaption> </figure> <p><strong>Example: Benchmarks influencing standardization in computer vision.</strong> As one concrete example of how benchmarks influence AI development, we can look at the history of benchmarking in computer vision. In 1998, researchers introduced <abbr title="A dataset of handwritten digits used to benchmark basic image recognition capabilities.">MNIST</abbr>, a dataset of 70,000 handwritten digits. (<a href=https://yann.lecun.com/exdb/mnist/ >LeCun, 1998</a>) The digits were not the important part, the important part was that each digit image was carefully processed to be the same size and centered in the frame, and that the researchers made sure to get digits from different writers for the training set and test set. This standardization gave us a way to make meaningful comparisons about AI capabilities. In this case, the specific capability of digit classification. Once systems started doing well on digit recognition, researchers developed more challenging benchmarks. <abbr title="A dataset of small natural color images used to benchmark object classification across multiple categories.">CIFAR-10/100</abbr> in 2009 introduced natural color images of objects like cars, birds, and dogs, increasing the complexity. (<a href=https://www.cs.toronto.edu/~kriz/cifar.html>Krizhevsky, 2009</a>) Similarly, <abbr title="A dataset of natural images used to benchmark large-scale object recognition and classification.">ImageNet</abbr> later the same year provided 1.2 million images across 1,000 categories. (<a href=https://ieeexplore.ieee.org/document/5206848>Deng, 2009</a>) When one research team claimed their system achieved 95% accuracy on <abbr title="A dataset of handwritten digits used to benchmark basic image recognition capabilities.">MNIST</abbr> or <abbr title="A dataset of natural images used to benchmark large-scale object recognition and classification.">ImageNet</abbr> and another claimed 98%, everyone knew exactly what those numbers meant. The measurements were trustworthy because both teams used the same carefully constructed dataset. Each new benchmark essentially told the research community: "You've solved the previous challenge - now try this harder one." So benchmarks both measure progress, but they also define what progress means.</p> <figure> <a class=glightbox href=../Images/Egy_Image_2.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Egy_Image_2.png></a> <br> <figcaption><b>Figure 5.2:</b> Examples of digits from <abbr title="A dataset of handwritten digits used to benchmark basic image recognition capabilities.">MNIST</abbr> (<a href=https://upload.wikimedia.org/wikipedia/commons/b/b1/MNIST_dataset_example.png><abbr title="A dataset of handwritten digits used to benchmark basic image recognition capabilities.">MNIST</abbr> database - Wikipedia</a>)</figcaption> </figure> <p><strong>How do benchmarks influence AI Safety ?</strong> Without standardized measurements, we can't make systematic progress on either capabilities or safety. Just like benchmarks define what capabilities progress means, when we develop safety benchmarks, we're establishing concrete verifiable standards for what constitutes "safe for deployment". Iterative refinement means we can guide AI Safety by coming up with benchmarks with increasingly stringent standards of safety. Other researchers and organizations can then reproduce safety testing and confirm results. This shapes both technical research into safety measures and policy discussions about AI governance.</p> <p><strong>Language model benchmarking has already evolved, and is going to continue evolving.</strong> Just like how benchmarks continuously evolved in computer vision, they followed similar progress in language generation. Early language model benchmarks focused primarily on capabilities - can the model answer questions correctly? Complete sentences sensibly? Translate between languages? Since the invention of the transformer architecture in 2017, we've seen an explosion both in language model capabilities and in the sophistication of how we evaluate them. We can’t possibly be exhaustive, but here are just a couple of benchmarks that current day language models are evaluated against:</p> <figure> <a class=glightbox href=../Images/6nC_Image_3.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/6nC_Image_3.png></a> <br> <figcaption><b>Figure 5.3:</b> Example of popular language models (Claude 3.5) being evaluated on various benchmarks (<a href=https://www.anthropic.com/news/claude-3-5-sonnet>Anthropic, 2024</a>)</figcaption> </figure> <figure class=iframe-figure> <iframe allow="web-share; clipboard-write" loading=lazy src="https://ourworldindata.org/grapher/ai-performance-coding-math-knowledge-tests?tab=chart" style="width: 100%; height: 600px; border: 0px none;"></iframe> <br> <figcaption><b>Interactive Figure 5.2:</b> Benchmark performance on coding, math and language. (<a href=https://ourworldindata.org/artificial-intelligence>Giattino et al., 2023</a>)</figcaption> </figure> <details class=note> <summary>Examples of various capabilities benchmarks</summary> <p><strong>Benchmarking language and task understanding.</strong> General Language Understanding Evaluation (GLUE) benchmark (<a href=https://arxiv.org/abs/1804.07461>Wang et al., 2018</a>), and its successor SuperGLUE (<a href=https://arxiv.org/abs/1905.00537>Wang et al., 2019</a>) test difficult language understanding tasks. SWAG (<a href=https://arxiv.org/abs/1808.05326>Zellers et al., 2018</a>), and <abbr title="A benchmark of sequence completion tasks used to evaluate commonsense reasoning.">HellaSwag</abbr> (<a href=https://arxiv.org/abs/1905.07830>Zellers et al., 2019</a>) tests specifically the ability to predict which event would naturally follow from a given story scenario.</p> <p><strong>Broad cross domain evaluations.</strong> The <abbr title="A benchmark of multiple-choice questions used to evaluate knowledge across academic subjects.">MMLU</abbr> (Massive Multitask Language Understanding) benchmark (<a href=https://arxiv.org/abs/2009.03300>Hendrycks et al., 2020</a>) tests a model's knowledge across 57 subjects. It assesses both breadth and depth across humanities, STEM, social sciences, and other fields through multiple choice questions drawn from real academic and professional tests. The <abbr title="A benchmark of graduate-level science questions used to evaluate expert knowledge.">GPQA</abbr> (Google Proof QA) (<a href=https://arxiv.org/abs/2311.12022>Rein et al., 2023</a>) has multiple choice questions specifically designed so that correct answers can’t be found through simple internet searches. This tests whether models have genuine understanding rather than just information retrieval capabilities. BigBench (<a href=https://arxiv.org/abs/2206.04615>Srivastava et al., 2022</a>) is yet another example of benchmarks for measuring generality by testing on a wide range of tasks.</p> <p><strong>Benchmarking mathematical and scientific reasoning.</strong> For specifically testing mathematical reasoning, a couple of examples include - the Grade School Math (<abbr title="A benchmark of elementary math word problems used to evaluate step-by-step reasoning.">GSM8K</abbr>) (<a href=https://arxiv.org/abs/2110.14168>Cobbe et al., 2021</a>) benchmark. This tests core mathematical concepts at an elementary school level. Another example is the <abbr title="A benchmark of competition mathematics problems used to evaluate advanced reasoning.">MATH</abbr> (<a href=https://arxiv.org/abs/2103.03874>Hendrycks et al., 2021</a>) benchmark similarly tests seven subjects including algebra, geometry, and precalculus focuses on competition-style problems. They also have multiple difficulty levels per subject. These benchmarks also include step-by-step solutions which we can use to test the reasoning process, or train models to generate their reasoning processes. Multilingual Grade School Math (MGSM) is the multilingual version translated 250 grade-school math problems from the <abbr title="A benchmark of elementary math word problems used to evaluate step-by-step reasoning.">GSM8K</abbr> dataset. (<a href=https://arxiv.org/abs/2210.03057>Shi et al., 2022</a>)</p> <p><strong>Benchmarking SWE and coding.</strong> The Automated Programming Progress Standard (<abbr title="A benchmark of programming problems used to evaluate code generation from natural language.">APPS</abbr>) (<a href=https://arxiv.org/abs/2105.09938>Hendrycks et al., 2021</a>) is a benchmark specifically for evaluating code generation from natural language task descriptions. Similarly, <abbr title="A benchmark of Python functions used to evaluate code generation capabilities.">HumanEval</abbr> (<a href=https://arxiv.org/abs/2107.03374>Chen et al, 2021</a>) tests python coding abilities, and its extensions like <abbr title="A benchmark of multilingual programming problems used to evaluate cross-language code generation.">HumanEval-XL</abbr> (<a href=https://arxiv.org/abs/2402.16694>Peng et al.,2024</a>) tests cross-lingual coding capabilities between 23 natural languages and 12 programming languages. <abbr title="A benchmark of coding tasks used to evaluate combined visual and textual understanding.">HumanEval-V</abbr> (<a href=https://arxiv.org/abs/2410.12381>Zhang et al., 2024</a>) tests coding tasks where the model must interpret both diagrams or charts, and textual descriptions to generate code. <abbr title="A benchmark of programming tasks used to evaluate complex tool usage and instruction following.">BigCode</abbr> (<a href=https://arxiv.org/abs/2406.15877>Zuho et al., 2024</a>), benchmarks code generation and tool usage by measuring a model’s ability to correctly use multiple Python libraries to solve complex coding problems.</p> <p><figure markdown=span> <a class=glightbox href=../Images/UAD_Image_4.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/UAD_Image_4.png></a> <figcaption markdown=1><b>Figure 5.4:</b> Example of coding task and test cases on <abbr title="A benchmark of programming problems used to evaluate code generation from natural language.">APPS</abbr> (<a href=https://arxiv.org/abs/2105.09938>Hendrycks et al., 2021</a>)</figcaption> </figure></p> </details> <p><strong>Benchmarking ethics and bias.</strong> The <abbr title="A benchmark of scenarios used to evaluate understanding of human moral judgments.">ETHICS</abbr> benchmark (<a href=https://arxiv.org/abs/2008.02275>Hendrycks et al., 2023</a>) tests a language model's understanding of human values and ethics across multiple categories including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. The <abbr title="A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.">TruthfulQA</abbr> (<a href=https://arxiv.org/abs/2109.07958>Lin et al., 2021</a>) benchmark measures how truthfully language models answer questions. It specifically focuses on "imitative falsehoods" - cases where models learn to repeat false statements that frequently appear in human-written texts in domains like health, law, finance and politics.</p> <figure> <a class=glightbox href=../Images/2Am_Image_5.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/2Am_Image_5.png></a> <br> <figcaption><b>Figure 5.5:</b> Example of larger models being less truthful on <abbr title="A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.">TruthfulQA</abbr> (<a href=https://arxiv.org/abs/2109.07958>Lin et al., 2021</a>). This is an example of inverse scaling, i.e. when a bigger model performance decreases on some questions.</figcaption> </figure> <figure> <a class=glightbox href=../Images/qkl_Image_6.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/qkl_Image_6.png></a> <br> <figcaption><b>Figure 5.6:</b> Example question from the <abbr title="A benchmark of scenarios used to evaluate understanding of human moral judgments.">ETHICS</abbr> benchmark (<a href=https://arxiv.org/abs/2008.02275>Hendrycks et al., 2023</a>)</figcaption> </figure> <p><strong>Benchmarking safety.</strong> An example focused on misuse is <abbr title="A benchmark of malicious tasks used to evaluate safety and misuse resistance.">AgentHarm</abbr> (<a href=https://arxiv.org/abs/2410.09024>Andriushchenko et al., 2024</a>). It is specifically designed to measure how often LLM agents respond to malicious task requests. An example that focuses slightly more on misalignment is the <abbr title="A benchmark of interactive scenarios used to evaluate ethical decision-making.">MACHIAVELLI</abbr> (<a href=https://arxiv.org/abs/2304.03279>Pan et al., </a><a href=https://arxiv.org/abs/2304.03279>2023</a>) benchmark. It has ‘choose your own adventure’ style games containing over half a million scenarios focused on social decision making. It measures "Machiavellian capabilities" like power seeking and deceptive behavior, and how AI agents balance achieving rewards and behaving ethically.</p> <figure> <a class=glightbox href=../Images/7Sn_Image_7.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/7Sn_Image_7.png></a> <br> <figcaption><b>Figure 5.7:</b> A mock-up of a game in the <abbr title="A benchmark of interactive scenarios used to evaluate ethical decision-making.">MACHIAVELLI</abbr> benchmark, a suite of text-based environments. At each step, the agent observes the scene and a list of possible actions; it selects an action from the list. Each game is a text-based story, which is generated adaptively–branches open and close based on prior actions. The agent receives a reward when it achieves one of the goals. This type of benchmark allows the researchers to construct a behavioral report of the agent and measure the trade-off between rewards and ethical behavior (<a href=https://arxiv.org/abs/2304.03279>Pan et al., 2023</a>).</figcaption> </figure> <details class=note> <summary>Details - Benchmark: Frontier Math (<a href=https://arxiv.org/abs/2411.04872>Glazer et al., 2024</a>) &amp; Humanities Last Exam (<a href=https://www.safe.ai/blog/humanitys-last-exam>Hendrycks &amp; Wang, 2024</a>)</summary> <div class="admonition warning"> <p class=admonition-title>This is an extra explanation of the frontier math mathematical benchmark. You can safely skip this.</p> </div> <p><figure markdown=span> <a class=glightbox href=../Images/J9V_Image_8.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/J9V_Image_8.png></a> <figcaption markdown=1><b>Figure 5.8:</b> Mathematical subject interconnections in FrontierMath. Node sizes indicate the frequency of each subject’s appearance in problems, while connections indicate when multiple mathematical subjects are combined within single problems, demonstrating the benchmark’s integration of many mathematical domains. (<a href=https://arxiv.org/abs/2411.04872>Glazer et al., 2024</a>)</figcaption> </figure></p> <p><strong>What makes FrontierMath so difficult?</strong> Unlike most benchmarks which risk training data contamination, FrontierMath uses entirely new, unpublished problems. Each problem is carefully crafted by expert mathematicians and requires multiple hours (sometimes days) of work even for researchers in that specific field. For example Terence Tao (fields medal winner 2006, regarded as one of the smartest mathematicians in the world) said about the problems - "<em>These are extremely challenging ... I think they will resist AIs for several years at least.</em>" (<a href=https://epoch.ai/frontiermath>EpochAI, 2024</a>) Similarly Timothy Gowers (highly regarded mathematician, and fields medal winner 1998) said - "<em>Getting even one question right would be well beyond what we can do now, let alone saturating them.</em>" (<a href=https://epoch.ai/frontiermath>EpochAI, 2024</a>)</p> <p>The benchmark spans most major branches of modern mathematics - from computationally intensive problems in number theory to abstract questions in algebraic topology and category theory. To ensure problems are truly novel, they undergo expert review and plagiarism detection. The benchmark also enforces strict "guess proofness" - problems must be designed so there's less than a 1% chance of guessing the correct answer without doing the mathematical work. This means problems often have large, non-obvious numerical answers that can only be found through proper mathematical reasoning. The benchmark provides an experimental environment where models can write and test code to explore mathematical ideas, similar to how human mathematicians work. While problems must have automatically verifiable answers (either numerical or programmatically expressible mathematical objects), they still require sophisticated mathematical reasoning to solve.</p> <p><figure markdown=span> <a class=glightbox href=../Images/gDl_Image_9.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/gDl_Image_9.png></a> <figcaption markdown=1><b>Figure 5.9:</b> One sample problem from the FrontierMath benchmark (<a href=https://epoch.ai/frontiermath/the-benchmark>Besiroglu et al., 2024</a>).</figcaption> </figure></p> <p>Just to showcase the rapid pace of advancement even on this benchmark that even fields medal winning mathematicians consider extremely challenging, between the announcement of the FrontierMath benchmark the state-of-the-art models could solve less than 2% of FrontierMath problems. (<a href=https://arxiv.org/abs/2411.04872>Glazer et al., 2024</a>) Just a couple of months later, OpenAI announced the o3 model, which then shot performance up to 25.2%. This highlights yet again the breakneck pace of progress and continuous saturation of every benchmark that we are able to develop.</p> <p><figure markdown=span> <a class=glightbox href=../Images/vD5_Image_10.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/vD5_Image_10.png></a> <figcaption markdown=1><b>Figure 5.10:</b> Performance of leading language models on FrontierMath. All models show consistently poor performance, with even the best models (as of Nov 2024) solving less than 2 percent of problems (<a href=https://epoch.ai/frontiermath/the-benchmark>Besiroglu et al., 2024</a>). A few months later OpenAI claimed that their o3 model could score 25 percent on FrontierMath (<a href="https://x.com/polynoamial/status/1870172996650053653?mx=2">Brown, 2024</a>).</figcaption> </figure></p> <p>To keep up with the pace, researchers are developing what is described as "Humanity's Last Exam" (HLE). A benchmark aimed at building the world's most difficult public AI benchmark gathering experts across all fields (<a href=https://arxiv.org/abs/2501.14249>Phan et al., 2025</a>).</p> </details> <h2 id=02>5.1.2 Limitations<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <p>Current benchmarks face several critical limitations that make them insufficient for truly evaluating AI safety. Let's examine these limitations and understand why they matter.</p> <p><strong>Training Data Contamination.</strong> Imagine preparing for a test by memorizing all the answers without understanding the underlying concepts. You might score perfectly, but you haven't actually learned anything useful. LLMs face a similar problem. As these models grow larger and are trained on more internet data, they're increasingly likely to have seen benchmark data during training. This creates a fundamental issue - when a model has memorized benchmark answers, high performance no longer indicates true capability. The benchmarks we discussed in the previous section like the <abbr title="A benchmark of multiple-choice questions used to evaluate knowledge across academic subjects.">MMLU</abbr> or <abbr title="A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.">TruthfulQA</abbr> have been very popular. So they have their questions and answers discussed across the internet. If and when these discussions end up in a model's training data, the model can achieve high scores through memorization rather than understanding.</p> <p><strong>Understanding vs. Memorization Example.</strong> The Caesar cipher is a simple encryption method that shifts each letter in the alphabet by a fixed number of positions - for example, with a left shift of 3, 'D' becomes 'A', 'E' becomes 'B', and so on. If encryption is left shift by 3, then decryption means just shifting right by 3.</p> <figure> <a class=glightbox href=../Images/BDT_Image_11.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/BDT_Image_11.png></a> <br> <figcaption><b>Figure 5.11:</b> Example of a Cesar Cipher</figcaption> </figure> <p>Language models like GPT-4 can solve Caesar cipher problems when the shift value is 3 or 5, which appear commonly in online examples. However, give them the exact same problem with uncommon shift values (like 67) they tend to fail completely (<a href="https://www.youtube.com/watch?v=s7_NlkBwdj8">Chollet, 2024</a>). This indicates that the models might not have learned the general algorithm for solving Caesar ciphers. We are not trying to point to a limitation in model capabilities. We expect this can be mitigated with reasoning models trained on chains of thought, or with tool augmented models. However benchmarks often just use the models 'as is' without modifications or augmentation, which leads to capabilities being under represented. This is the core point that we are trying to convey.</p> <p><strong>Why do these benchmarking limitations matter for AI Safety?</strong> Benchmarks (including safety benchmarks) might not be measuring what we think they are measuring. For example benchmarks like <abbr title="A benchmark of scenarios used to evaluate understanding of human moral judgments.">ETHICS</abbr>, or <abbr title="A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.">TruthfulQA</abbr> aim to measure how well a model "understands" ethical behavior, or has a tendency to avoid imitative falsehood by measuring language generation on multiple choice tests, but we might still be measuring surface level metrics. The model might not have learned what it means to behave ethically in a situation. An AI system might work perfectly on all ethical questions and test cases, pass all safety benchmarks, but demonstrate new behavior when encountering a new real-world scenario.</p> <p>An easy answer is just to keep augmenting benchmarks or training data with more and more questions, but this seems intractable and does not scale forever. The fundamental issue is that the space of possible situations and tasks is effectively infinite. Even if you train on millions of examples, you've still effectively seen roughly 0% of the total possible space. (<a href=https://www.dwarkeshpatel.com/p/francois-chollet>Chollet, 2024</a>) Research indicates that this isn't just a matter of insufficient data or model size - it's baked into how language models are currently trained - logical relationships like inferring inverses (the weights learned when training on "A → B" don't automatically strengthen the reverse connection "B ← A") or transitivity don't emerge naturally from standard training (<a href=https://arxiv.org/abs/2405.04669>Zhu et al., 2024</a>; <a href=https://arxiv.org/abs/2403.13799>Golovneva et al., 2024</a>; <a href=https://arxiv.org/abs/2309.12288>Berglund et al., 2024</a>). Proposed solutions like reverse training during pre-training show promise to alleviate such issues (<a href=https://arxiv.org/abs/2403.13799>Golovneva et al., 2024</a>), but they require big changes to how models are trained. </p> <p>Engineers are more than aware of these current limitations, and the expectation is that these problems will be alleviated over time. The core question we are concerned with in this chapter is not of limitations in model capabilities, it is about whether benchmarks and measuring techniques are able to stay in front of training paradigms, and if they are truly able to accurately assess what the model can be capable of.</p> <p><strong>Why can't we just make better benchmarks?</strong> The natural response to these limitations might be "let's just design better benchmarks." And to some extent, we can!</p> <p>We've already seen how benchmarks have consistently evolved to address their shortcomings. Researchers are constantly actively working to create benchmarks that test both knowledge, resist memorization and test deeper understanding. Just a couple of examples are the Abstraction and Reasoning Corpus (<abbr title="A benchmark of visual puzzles used to evaluate abstract reasoning and generalization.">ARC</abbr>) (<a href=https://arxiv.org/abs/1911.01547>Chollet, 2019</a>), <abbr title="A benchmark of systematic variations used to evaluate abstract concept learning.">ConceptARC</abbr> (<a href=https://arxiv.org/abs/2305.07141>Moskvichev et al. 2023</a>), Frontier Math (<a href=https://arxiv.org/abs/2411.04872>Glazer et al., 2024</a>) and Humanities Last Exam (<a href=https://www.safe.ai/blog/humanitys-last-exam>Hendrycks &amp; Wang, 2024</a>). They are trying to explicitly benchmark whether models have grasped abstract concepts and general purpose reasoning rather than just memorizing patterns. Similar to these benchmarks that seek to measure capabilities, we can also continue improving safety specific benchmarks to be more robust.</p> <p><strong>Why aren't better benchmarks enough?</strong> While improving benchmarks is important and will help AI safety efforts, the fundamental paradigm of benchmarking still has inherent limitations. There are fundamental limitations in traditional benchmarking approaches that necessitate more sophisticated evaluation methods (<a href=https://arxiv.org/abs/2407.09221>Burden, 2024</a>). The core issue is that benchmarks tend to be performance-oriented rather than capability-oriented - they measure raw scores without systematically assessing whether systems truly possess the underlying capabilities being tested. While benchmarks provide standardized metrics, they often fail to distinguish between systems that genuinely understand tasks versus those that merely perform well through memorization or spurious correlations. A benchmark that simply assesses performance, no matter how sophisticated, cannot fully capture the dynamic nature<sup id=fnref:footnote_dynamic_benchmarks><a class=footnote-ref href=#fn:footnote_dynamic_benchmarks>3</a></sup> of real-world AI deployment where systems need to adapt to novel situations and will probably combine capabilities and affordances in unexpected ways. We need to measure the upper limit of model capabilities.</p> <details class=note> <summary>The need for Compute-Aware Benchmarking</summary> <p>We have observed the advent of inference scaling laws alongside the rise of large reasoning models like DeepSeek r1, OpenAIs o3 etc. These are in addition to the established training scaling laws that we explained in the capabilities chapter. Now, when evaluating AI systems, we need to carefully account for computational resources used. The 2024 <abbr title="A benchmark of visual puzzles used to evaluate abstract reasoning and generalization.">ARC</abbr> prize competition demonstrated why - systems on both the compute-restricted track (10 dollars worth of compute) and the unrestricted track (10,000 dollars worth of compute) achieved similar 55% accuracy scores, suggesting that better ideas and algorithms can sometimes compensate for less compute (<a href=https://arcprize.org/media/arc-prize-2024-technical-report.pdf>Chollet et al., 2024</a>). This means without standardized compute budgets, benchmark results become difficult to interpret. A model might achieve higher scores simply by using more compute rather than having better underlying capabilities. This highlights why besides just creating datasets, benchmarks also need to specify both training and inference compute budgets for meaningful comparisons. </p> </details> <p><strong>What makes comprehensive evaluations different from just benchmarking?</strong> Evaluations are comprehensive protocols that work backwards from concrete threat models. Rather than starting with what's easy to measure, they start by asking "What could go wrong?" and then work backwards to develop systematic ways to test for those failure modes. Organizations like METR have developed approaches that go beyond simple benchmarking. Instead of just asking "Can this model write malicious code?", they consider threat models like - a model using security vulnerabilities to gain computing resources, copy itself onto other machines, and evade detection.</p> <div class="admonition info"> <p class=admonition-title>Definition - Evaluations</p> <p>An evaluation is a complete safety assessment protocol which includes the use of benchmarks.</p> </div> <p>That being said, as evaluations are new, benchmarks have been around longer and are also evolving. So at times there is overlap in the way that these words are used. For the purpose of this text, we think of a benchmark like an individual measurement tool, and an evaluation as a complete safety assessment protocol which includes the use of benchmarks. Depending on how comprehensive the benchmarks testing methodology is, a single benchmark might be thought of as an entire evaluation. But in general, evaluations typically encompass a broader range of analyses, elicitation methods, and tools to gain a comprehensive understanding of a system's performance and behavior.</p> <div class=footnote> <hr> <ol> <li id=fn:footnote_1> <p>This is true to a large extent, but as always there is not 100% standardization. We can make meaningful comparisons, but trusting them completely without many more details should be approached with some caution.&#160;<a class=footnote-backref href=#fnref:footnote_1 title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:footnote_helm> <p>The HELM benchmark is more like a framework integrating many existing benchmarks. A tool to run, analyse and compare them.&#160;<a class=footnote-backref href=#fnref:footnote_helm title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> <li id=fn:footnote_dynamic_benchmarks> <p>We could have benchmarks in environments populated by other agents. Some RL benchmarks already do this. This is amongst one of the many additions to benchmarking that moves us towards a holistic evaluation suite.&#160;<a class=footnote-backref href=#fnref:footnote_dynamic_benchmarks title="Jump back to footnote 3 in the text">&#8617;</a></p> </li> </ol> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> Chapter 05 </div> </div> </a> <a href=../02/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> 5.2 - Evaluated Properties </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>