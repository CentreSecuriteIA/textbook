<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content='An "evaluation" is fundamentally about measuring or assessing some property of an AI system. The key aspects that make something an evaluation rather than other AI work are:'><link href=https://ai-safety-atlas.com/chapters/05/02/ rel=canonical><link href=../01/ rel=prev><link href=../03/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>5.2 - Evaluated Properties - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#52-evaluated-properties class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 5.2 - Evaluated Properties </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_6 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 05 - Evaluations </span> </a> <label class="md-nav__link " for=__nav_2_1_6 id=__nav_2_1_6_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_6_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_6> <span class="md-nav__icon md-icon"></span> 05 - Evaluations </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01/ class=md-nav__link> <span class=md-ellipsis> 5.1 - Benchmarks </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 5.2 - Evaluated Properties </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 5.2 - Evaluated Properties </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 5.2.1 Capability </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 5.2.2 Propensity </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 5.2.3 Control </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 5.3 - Evaluation Techniques </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 5.4 - Evaluation Frameworks </span> </a> </li> <li class=md-nav__item> <a href=../05/ class=md-nav__link> <span class=md-ellipsis> 5.5 - Dangerous Capability Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../06/ class=md-nav__link> <span class=md-ellipsis> 5.6 - Dangerous Propensity Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../07/ class=md-nav__link> <span class=md-ellipsis> 5.7 - Control Evaluations </span> </a> </li> <li class=md-nav__item> <a href=../08/ class=md-nav__link> <span class=md-ellipsis> 5.8 - Evaluation Design </span> </a> </li> <li class=md-nav__item> <a href=../09/ class=md-nav__link> <span class=md-ellipsis> 5.9 - Limitations </span> </a> </li> <li class=md-nav__item> <a href=../10/ class=md-nav__link> <span class=md-ellipsis> 5.10 - Conclusion </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 5.2.1 Capability </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 5.2.2 Propensity </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 5.2.3 Control </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=52-evaluated-properties>5.2 Evaluated Properties<a class=headerlink href=#52-evaluated-properties title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <span class=meta-icon> <i class="fas fa-clock"></i> </span> <div class=meta-content> <div class=meta-label>Reading Time</div> <div class=meta-value>13 min</div> </div> </div> </div> <p>An "evaluation" is fundamentally about measuring or assessing some property of an AI system. The key aspects that make something an evaluation rather than other AI work are:</p> <ul> <li> <p>There is a specific property or risk being assessed</p> </li> <li> <p>There is a methodology for gathering evidence about that property</p> </li> <li> <p>There is some way to analyze that evidence to draw conclusions about the property</p> </li> </ul> <p>But before we talk about "how to do evaluations" we still need to also answer the more fundamental question of "what aspects of AI systems are we even trying to evaluate? And why?" So in this section, we'll explore what properties of AI systems we need to evaluate and why they matter for safety. Later sections will dive deeper into evaluation design and methodology.</p> <figure> <a class=glightbox href=../Images/Tah_Image_12.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Tah_Image_12.png></a> <br> <figcaption><b>Figure 5.12:</b> Figure distinguishing the three related but distinct concepts of evaluated properties, the techniques used to evaluate model properties, and</figcaption> </figure> <p><strong>What aspects of AI systems do we need to evaluate?</strong> In the previous section on benchmarks, we saw how the field of measuring AI systems has evolved over time. Benchmarks like <abbr title="A benchmark of multiple-choice questions used to evaluate knowledge across academic subjects.">MMLU</abbr> or <abbr title="A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.">TruthfulQA</abbr> are useful tools, giving us standardization in measurement for both AI capabilities and safety. Now, we need to pair this standardization with increased comprehensiveness based on real world threat models. Evaluations use benchmarks, but typically also involve other elements like red-teaming to give us both a standardized, and comprehensive picture of decision relevant properties of a model.</p> <p><strong>Why do we need to evaluate different properties?</strong> The most fundamental distinction in AI evaluations is between what a model can do (capabilities) versus what it tends to do (propensities). To understand why this distinction matters, imagine an AI system that is capable of writing malicious code when explicitly directed to do so, but consistently chooses not to do so unless specifically prompted. Simply measuring the system's coding capabilities wouldn't tell us about its behavioral tendencies, and vice versa. Understanding both aspects is crucial for safety assessment.</p> <p><strong>How do these evaluation types work together?</strong> We are going to talk about capabilities, propensities, and control as distinct categories, but this is for the purpose of conceptual clarity and explanation. Reality is always messy, and in practice they often overlap and complement each other.<sup id=fnref:footnote_2><a class=footnote-ref href=#fn:footnote_2>2</a></sup> A capability evaluation might reveal behavioral tendencies during testing, like a model demonstrating a propensity toward honesty while being evaluated for coding ability. These types of overlap are sometimes desirable<sup id=fnref:footnote_not_desirable><a class=footnote-ref href=#fn:footnote_not_desirable>1</a></sup> - different evaluation approaches can provide complementary evidence about an AI system's safety and reliability (<a href=https://arxiv.org/abs/2312.06942>Roger et al., 2023</a>). The main thing to recognize is what each type of evaluation tells us:</p> <ul> <li> <p>Dangerous capability evaluations give us upper bounds on potential risks<sup id=fnref:footnote_3><a class=footnote-ref href=#fn:footnote_3>3</a></sup></p> </li> <li> <p>Dangerous propensity evaluations tell us which default behavioral patterns might be problematic</p> </li> <li> <p>Control evaluations verify our containment measures<sup id=fnref:footnote_4><a class=footnote-ref href=#fn:footnote_4>4</a></sup></p> </li> </ul> <p>Another thing to remember is that there are various different approaches that we can follow when evaluating for all the above types of properties. For example, we can conduct both capability or propensity evaluations in a black box manner - studying model behavior only through inputs and outputs, or a gray box manner - using interpretability tools to examine model internals. (<a href=https://www.alignmentforum.org/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations>Hubinger, 2023</a>). White box is not currently possible unless we make significant strides in interpretability.<sup id=fnref:footnote_5><a class=footnote-ref href=#fn:footnote_5>5</a></sup> These are different design choices in how we structure our evaluations when we are trying to evaluate for the above properties.</p> <figure> <a class=glightbox href=../Images/9SB_Image_13.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/9SB_Image_13.png></a> <br> <figcaption><b>Figure 5.13:</b> In this chapter, we will mainly talk about AI systems, rather than AI models. An AI system comprises an AI model, with its weights and architecture, as well as various other system parameters, including system prompts, prefix prompts, scaffolding prompts, and retrieval, databases (<a href=https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf>Sharkey, 2024</a>).</figcaption> </figure> <h2 id=01>5.2.1 Capability<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <p><strong>What are capability evaluations?</strong> When it comes to capabilities (measuring what a model can do), as far as AI Safety is concerned, we're particularly interested in abilities that could enable harmful outcomes. These dangerous capabilities generally fall into several key categories (<a href=https://arxiv.org/abs/2305.15324>Shevlane et al., 2023</a>):</p> <ul> <li> <p><strong>Cyber-offense:</strong> The model can discover vulnerabilities in systems (hardware, software, data). It can write code for exploiting those vulnerabilities. […] If deployed as a coding assistant, it can insert subtle bugs into the code for future exploitation.</p> </li> <li> <p><strong>Deception:</strong> The model has the skills necessary to deceive humans, e.g. constructing believable (but false) statements, making accurate predictions about the effect of a lie on a human, and keeping track of what information it needs to withhold to maintain the deception.</p> </li> <li> <p><strong>Persuasion and manipulation:</strong> The model is effective at shaping people’s beliefs, in dialogue and other settings (e.g. social media posts), even towards untrue beliefs. […] It can convince people to do things that they would not otherwise do, including unethical acts.</p> </li> <li> <p><strong>Political strategy:</strong> The model can perform the social modeling and planning necessary for an actor to gain and exercise political influence […]. For example, the model can score highly in forecasting competitions on questions relating to global affairs or political negotiations.</p> </li> <li> <p><strong>Weapons acquisition and development:</strong> The model can gain access to existing weapons systems or contribute to building new weapons. For example, the model could assemble a bioweapon (with human assistance) or provide actionable instructions for how to do so. […]</p> </li> <li> <p><strong>Long-horizon planning:</strong> The model can make sequential plans that involve multiple steps, unfolding over long time horizons (or at least involving many interdependent steps). It can perform such planning within and across many domains. […]</p> </li> <li> <p><strong>AI development:</strong> The model could build new AI systems from scratch, including AI systems with dangerous capabilities. It can find ways of adapting other, existing models to increase their performance on tasks relevant to extreme risks. […]</p> </li> <li> <p><strong>Situational awareness:</strong> The model can distinguish between whether it is being trained, evaluated, or deployed – allowing it to behave differently in each case. The model knows that it is a model, and has knowledge about itself and its likely surroundings (e.g. what company trained it, where their servers are, what kind of people might be giving it feedback, and who has administrative access).</p> </li> <li> <p><strong>Self-proliferation:</strong> The model can break out of its local environment (e.g. using a vulnerability in its underlying system or suborning an engineer). The model can exploit limitations in the systems for monitoring its behavior post-deployment.</p> </li> </ul> <p>This list is not exhaustive and may not address all plausible extreme risk scenarios. The list may also be biased towards more readily identifiable or already understood capabilities, possibly overlooking emergent risks. Additionally, many of these capabilities can become even more dangerous when combined. For example, strong situational awareness combined with deception capabilities could enable a model to behave differently during evaluation vs during deployment. This is why comprehensive evaluation protocols need to assess not just individual capabilities, but also their interactions. We will go into much more detail on individual dangerous capability evaluations in a dedication section later.</p> <h2 id=02>5.2.2 Propensity<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <p><strong>What are propensity evaluations?</strong> Capability evaluations tell us what a model can do when directed, propensity evaluations reveal what behaviors a model prioritizes by default (what it tends to do). These are also often called "alignment evaluations". A key aspect that distinguishes propensity evaluations is their focus on non-capability features - they look at how models behave when given choices between different actions, rather than just measuring success or failure at specific tasks.</p> <p>As an intuitive example, after undergoing safety training, think about the way that language models like GPT-4 or Claude 3.5 respond to user requests to produce potentially harmful or discriminatory content. They usually respond with a polite refusal to produce such content. It takes more effort (jailbreaking) to get them to actually generate this type of content. So more often than not they resist misuse. In this case we would say the models have "a propensity" not to produce harmful or offensive content (<a href=https://static1.squarespace.com/static/6593e7097565990e65c886fd/t/65a6f1389754fc06cb9a7a14/1705439547455/auditing_framework_web.pdf>Sharkey et al., 2024</a>). Similarly we want to increase the propensity for good behaviors, and want to reduce the propensity for dangerous behaviors. Increasing/Reducing would be the job of alignment research. In evaluations, we just want to see what type of behavior it exhibits.</p> <p>The distinction between capability, and propensity becomes important when we think about the fact that in the future highly capable AI systems might have multiple behavioral options available to them. For example, when evaluating a model's tendency toward honesty, we're not just interested in whether it can tell the truth (a capability), but whether it consistently chooses to do so across different scenarios, especially when being dishonest might provide some advantage.</p> <p><strong>Can propensities and capabilities be related?</strong> Yes, propensities and capabilities tend to be interconnected. Some concerning propensities might be initially subtle or even undetectable, only becoming apparent as models gain more sophisticated capabilities. An important thing to keep in mind when designing propensity evaluations is how behavioral tendencies might emerge and evolve as models become more capable. As an example, a basic form of power-seeking behavior might appear unconcerning in simple systems but become problematic as models gain better strategic understanding and action capabilities (<a href=https://www.alignmentforum.org/posts/sWf8wj64AdDfMeTvf/thinking-about-propensity-evaluations>Riché et al., 2024</a>). Here is a list of some propensities that we might want to design evaluations for:</p> <ul> <li> <p><strong>Toxicity:</strong> The propensity to generate offensive, harmful, or otherwise inappropriate content, such as hate speech, offensive/abusive language, pornographic content, etc.</p> </li> <li> <p><strong>Bias/Discrimination:</strong> A model's propensity to manifest or perpetuate biases, leading to unfair, prejudiced, or discriminatory outputs against certain groups or individuals.</p> </li> <li> <p><strong>Honesty:</strong> A model’s propensity to answer by expressing its true beliefs and actual level of certainty.</p> </li> <li> <p><strong>Truthfulness:</strong> A model’s propensity to produce truthful outputs. This propensity requires an AI system to be both honest and to know the truth (or other weirder settings such that the AI system outputs the truth while believing it is not the truth).</p> </li> <li> <p><strong>Sycophancy:</strong> A model's propensity to tell users what it thinks they want to hear or would approve of, rather than what it internally believes is the truth.</p> </li> <li> <p><strong>Deception:</strong> A model's propensity to intentionally generate misleading, false, or deceptive output.</p> </li> <li> <p><strong><abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">Corrigibility</abbr>:</strong> A model's propensity to accept feedback and correct its behavior or outputs in response to human intervention or new information.</p> </li> <li> <p><strong>Power Seeking:</strong> A model's propensity to seek to have a high level of control over its environment (potentially to maximize its own objectives).</p> </li> </ul> <p>Just like the list provided in the capabilities sub section above, this list of propensities is non exhaustive and constantly evolving. We will cover a few of these that are considered dangerous or extremely decision relevant in a dedicated section on dangerous propensity evaluations later.</p> <h2 id=03>5.2.3 Control<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <p><strong>Control evaluations assess safety measures under adversarial conditions</strong> <sup id=fnref:footnote_adversaries><a class=footnote-ref href=#fn:footnote_adversaries>6</a></sup>. Beyond understanding capabilities and propensities, we need to verify that our safety measures remain effective when AI systems actively try to circumvent them (<a href=https://arxiv.org/abs/2312.06942>Greenblatt et al., 2023</a>). They give us information about whether our safety infrastructure would remain robust in worst-case scenarios. For example, a model might typically display helpful behavior (a propensity) and have code generation abilities (a capability), but a control evaluation would test whether constraints preventing access to sensitive data can be circumvented, or whether emergency shutdown procedures remain effective even under adversarial conditions.<strong>Control evaluations give us multiple different sources of safety guarantees. </strong>Beyond hard barriers and monitoring/detection, control evaluations also tell us when models lack the sophisticated abilities needed to bypass them. There are a couple of different reasons why control evaluations might show safety:</p> <ul> <li> <p>Hard barriers (physical/technical limitations like air gaps)</p> </li> <li> <p>Detection mechanisms (monitoring that catches attack attempts)</p> </li> <li> <p>Capability limitations (models simply lack the sophisticated abilities needed)</p> </li> </ul> <p>Models might fail not because our detection mechanisms are bad, they fail because they lack the strategic planning and coordination abilities needed to execute these extremely subtle multi-stage attacks. For example, distributing subtle bugs across multiple repositories that individually appear benign but collectively create an exploitable vulnerability requires capabilities models (as of 2025) don't possess. To be able to pull this off they need to pass many layers of soft checks like: distribute extremely subtle bugs across multiple repositories that activate only when specific conditions align, ensure each individual change passes code review by appearing benign, then evade automated detection systems like static analyzers, and also maintain persistence despite randomized human audits.</p> <p><strong>Control evaluations can drive strategic safety planning.</strong> Depending on which category causes the most risk during control evaluations we can reprioritize and reallocate safety efforts. This gives us:</p> <ul> <li> <p><strong>Risk assessment timeline:</strong> Control evaluations that show safety due to capability limitations give us information about when those barriers might fail. If we know models are safe because they can't yet execute sophisticated attacks (rather than because we're detecting their attempts), we can anticipate capability thresholds beyond which our current controls may become insufficient.</p> </li> <li> <p><strong>Defense prioritization:</strong> Understanding whether safety comes from detection or capability limitations guides resource allocation. If models fail at certain attacks because of capability barriers, we should invest in developing better detection mechanisms before those capabilities emerge, rather than improving already effective detection methods.</p> </li> <li> <p><strong>Deployment governance:</strong> Models that fail attacks because they lack capabilities might be safer to deploy with fewer monitoring resources than models that actively attempt sophisticated attacks but get caught. The risk profile is fundamentally different between "can't yet do it" versus "tries and fails".</p> </li> </ul> <p>We give more examples of control evaluations, what they target, when they are effective/ineffective in a dedication section.</p> <div class=footnote> <hr> <ol> <li id=fn:footnote_not_desirable> <p>Sometimes, but not always desirable sadly. In the dangerous capability eval for RE-Bench (<a href=https://arxiv.org/abs/2411.15114>METR, 2024</a>). Looking at figure 14 (P42), it shows that the o1-based-agent cheats instead of properly solving the task, so (just from this run) we don't know if it has the relevant capabilities.&#160;<a class=footnote-backref href=#fnref:footnote_not_desirable title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:footnote_2> <p>A big issue for propensity evaluations is to remove the confounding effect from having various capability levels. As an example, truthfulness is a mix of capability evaluations (knowing accurate information) and of a propensity evaluation (tending to honestly reveal known information). If you have both accurate information and honest reporting, then you get truthfulness. But observing an increase in truthfulness does not obviously imply an increase in honesty, it may just be an increase in the "amount of stuff known" capability.&#160;<a class=footnote-backref href=#fnref:footnote_2 title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> <li id=fn:footnote_3> <p>To put it in a more nuanced way, dangerous capability evaluations are about producing upper bounds. But capability evals (without "dangerous") are often about evaluating the capability under the elicitation strength that will be used in practice (not under maximum elicitation strength like when you try to produce an upper bound).&#160;<a class=footnote-backref href=#fnref:footnote_3 title="Jump back to footnote 3 in the text">&#8617;</a></p> </li> <li id=fn:footnote_4> <p>Control evals should be performed at the crossroads of "dangerous capabilities evals" and "dangerous propensity evals". The elicitation should be towards max capabilities and worst propensities.&#160;<a class=footnote-backref href=#fnref:footnote_4 title="Jump back to footnote 4 in the text">&#8617;</a></p> </li> <li id=fn:footnote_5> <p>Our usage of the term is different from how others choose to use black/white box evaluations. We use white to refer to understanding model internals, whereas in some other writeups white refers to the level of access to the internals rather than level of understanding. Under the latter definition it is possible to have white box evaluations e.g. reading vectors in representation engineering.&#160;<a class=footnote-backref href=#fnref:footnote_5 title="Jump back to footnote 5 in the text">&#8617;</a></p> </li> <li id=fn:footnote_adversaries> <p>A common confusion is to think of humans as the adversaries. This falls under misuse like data poisoning or prompt injections. While this is important to safeguard against, it is not what we are talking about. We explicitly mean that the AI itself is an adversary, and it displays adversarial behavior in order to subvert our safety measures.&#160;<a class=footnote-backref href=#fnref:footnote_adversaries title="Jump back to footnote 6 in the text">&#8617;</a></p> </li> </ol> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../01/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> 5.1 - Benchmarks </div> </div> </a> <a href=../03/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> 5.3 - Evaluation Techniques </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>