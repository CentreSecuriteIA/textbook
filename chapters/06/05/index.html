<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI."><link href=https://ai-safety-atlas.com/chapters/06/05/ rel=canonical><link href=../04/ rel=prev><link href=../../07/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>6.5 - Learning from feedback - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#65-learning-from-feedback class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 6.5 - Learning from feedback </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_7 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 06 - Specification </span> </a> <label class="md-nav__link " for=__nav_2_1_7 id=__nav_2_1_7_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_7_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_7> <span class="md-nav__icon md-icon"></span> 06 - Specification </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01/ class=md-nav__link> <span class=md-ellipsis> 6.1 - Reinforcement Learning </span> </a> </li> <li class=md-nav__item> <a href=../02/ class=md-nav__link> <span class=md-ellipsis> 6.2 - Optimization </span> </a> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 6.3 - Specification Gaming </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 6.4 - Learning from imitation </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 6.5 - Learning from feedback </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 6.5 - Learning from feedback </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 6.5.1 Reward Modeling </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 6.5.2 Reinforcement Learning from Human Feedback (RLHF) </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 6.5.3 Pretraining with Human Feedback (PHF) </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 6.5.4 Reinforcement Learning from AI Feedback (RLAIF) </span> </a> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 6.5.5 Limitations </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 6.5.1 Reward Modeling </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 6.5.2 Reinforcement Learning from Human Feedback (RLHF) </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 6.5.3 Pretraining with Human Feedback (PHF) </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 6.5.4 Reinforcement Learning from AI Feedback (RLAIF) </span> </a> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 6.5.5 Limitations </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=65-learning-from-feedback>6.5 Learning from feedback<a class=headerlink href=#65-learning-from-feedback title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <span class=meta-icon> <i class="fas fa-clock"></i> </span> <div class=meta-content> <div class=meta-label>Reading Time</div> <div class=meta-value>23 min</div> </div> </div> </div> <p>This section discusses yet more attempts to address the <abbr title="The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.">reward misspecification</abbr> problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>) and Reinforcement Learning from AI Feedback (<abbr title="Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.">RLAIF</abbr>), also known as Reinforcement Learning from <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr> (<abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">RLCAI</abbr>) or simply <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>.</p> <h2 id=01>6.5.1 Reward Modeling<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <figure class=video-figure> <iframe allowfullscreen=allowfullscreen frameborder=0 src=https://www.youtube.com/embed/PYylPRX6z4Q style="width: 100%; aspect-ratio: 16 / 9;"></iframe> <br> <figcaption><b>Video 6.3:</b> Optional video explaining reward modeling.</figcaption> </figure> <p><abbr title="A technique that separates the reinforcement learning alignment problem into understanding the intentions of humans and acting to achieve those intentions, i.e. learning the ‘How?’.">Reward modeling</abbr> was developed to apply reinforcement learning (RL) algorithms to real-world problems where designing a reward function is difficult, in part because humans don’t have a perfect understanding of every objective. In reward modeling, human assistants evaluate the outcomes of AI behavior, without needing to know how to perform or demonstrate the task optimally themselves. This is similar to how you can tell if a dish is cooked well by tasting it even if you do not know how to cook, and thus your feedback can be used by a chef to learn how to cook better. This technique separates the RL alignment problem into two separate halves: Understanding intentions, i.e. learning the ‘What?’, and Acting to achieve the intentions, i.e. learning the ‘How?’. This means that in the modeling agenda, there are two different ML models:</p> <ul> <li> <p>A reward model is trained with user feedback. This model learns to predict what humans would consider good behavior.</p> </li> <li> <p>An agent trained with RL, where the reward for the agent is determined by the outputs of the reward model</p> </li> </ul> <figure> <a class=glightbox href=../Images/B9Y_Image_12.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/B9Y_Image_12.png></a> <br> <figcaption><b>Figure 6.12:</b> Scalable agent alignment via reward modeling (<a href=https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84>DeepMind, 2018</a>)</figcaption> </figure> <p>Overall, while promising reward modeling can still fall prey to <abbr title="The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.">reward misspecification</abbr> and reward hacking failures. Obtaining accurate and comprehensive feedback can be challenging, and human evaluators may have limited knowledge or biases that can impact the quality of the feedback. Additionally, any reward functions learnt through modeling might also struggle to generalize to new situations or environments that differ from the training data. These are all discussed further using concrete examples in later sections.</p> <p>There are also some variants of reward modeling such as:</p> <ul> <li> <p><strong><span style="text - decoration: underline;">Narrow reward modeling</strong> </span> is a specific flavor of reward modeling where the focus is on training AI systems to accomplish specific tasks rather than trying to determine the "true human utility function". It aims to learn reward functions to achieve particular objectives, rather than seeking a comprehensive understanding of human values.</p> </li> <li> <p><strong><span style="text - decoration: underline;">Recursive reward modeling</strong> </span> seeks to introduce scalability to the technique. In recursive reward modeling, the focus is on decomposing a complex task into simpler subtasks and using reward modeling at each level to train agents that can perform those subtasks. This hierarchical structure allows for more efficient training and credit assignment, as well as the exploration of novel solutions that may not be apparent to humans. This is shown in the diagram below. Scalable oversight will be covered in greater depth in future chapters.</p> </li> </ul> <figure> <a class=glightbox href=../Images/5Wc_Image_13.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/5Wc_Image_13.png></a> <br> <figcaption><b>Figure 6.13:</b> Scalable agent alignment via reward modeling (<a href=https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84>DeepMind, 2018</a>)</figcaption> </figure> <p>The general reward modeling framework forms the basis for other feedback based techniques such as <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> (Reinforcement Learning from Human Feedback) which is discussed in the next section.</p> <h2 id=02>6.5.2 Reinforcement Learning from Human Feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>)<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <figure class=video-figure> <iframe allowfullscreen=allowfullscreen frameborder=0 src=https://www.youtube.com/embed/qV_rOlHjvvs style="width: 100%; aspect-ratio: 16 / 9;"></iframe> <br> <figcaption><b>Video 6.4:</b> Optional video explaining <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> and a <abbr title="Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”">specification gaming</abbr> failure.</figcaption> </figure> <p>Reinforcement Learning from Human Feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>) is a method developed by OpenAI. It's a crucial part of their strategy to create AIs that are both safe and aligned with human values. (<a href=https://openai.com/blog/our-approach-to-ai-safety>OpenAI, 2023</a>) A prime example of an AI trained with <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> is OpenAI’s ChatGPT.</p> <p>Earlier in this chapter, the reader was asked to consider the reward design problem for manually defining a reward function to get an agent to perform a backflip. This section considers the <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> solution to this design problem. <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> addresses this problem as follows: A human is initially shown two instances of an AI's backflip attempts, then the human selects which one appears more like a backflip, and finally, the AI is updated accordingly. By repeating this process thousands of times, we can guide the AI to perform actual backflips.</p> <figure> <a class=glightbox href=../Images/uVN_Image_14.gif data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/uVN_Image_14.gif></a> <br> <figcaption><b>Figure 6.14:</b> <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> learned to backflip using around 900 individual bits of feedback from the human evaluator.</figcaption> </figure> <figure> <a class=glightbox href=../Images/CxZ_Image_15.gif data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/CxZ_Image_15.gif></a> <br> <figcaption><b>Figure 6.15:</b> Manual reward crafting for this backflip took two hours to write a custom reward function. While it was successful, it was significantly less elegant than the one trained purely through human feedback. (<a href=https://openai.com/index/learning-from-human-preferences/ >OpenAI, 2017</a>)</figcaption> </figure> <p>Similar to designing a reward function that efficiently rewards proper backflips, it is hard to specify precisely what it means to generate safe or helpful text. This served as some of the motivation behind making <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> integral to the training of some current Large Language Models (LLMs).</p> <p>Although training sequences may vary slightly across organizations, most labs adhere to the general framework of pre-training followed by some form of fine-tuning. Observing the InstructGPT training process offers insight into a possible path for training LLMs. The steps include:</p> <figure> <a class=glightbox href=../Images/a1X_Image_16.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/a1X_Image_16.png></a> <br> <figcaption><b>Figure 6.16:</b> Aligning language models to follow instructions (<a href=https://openai.com/research/instruction-following>OpenAI, 2022</a>)</figcaption> </figure> <ul> <li> <p><strong>Step 0 : Semi-Supervised Generative Pre-training:</strong> The LLM is initially trained using a massive amount of internet text data, where the task is to predict the next word in a natural language context.</p> </li> <li> <p><strong>Step 1 : Supervised Fine-tuning :</strong> A fine-tuning dataset is created by presenting a prompt to a human and asking them to write a response. This process yields a dataset of (prompt, output) pairs. This dataset is then used to fine-tune the LLM through supervised learning, a form of behavioral cloning.</p> </li> <li> <p><strong>Step 2 : Train a Reward Model:</strong> We train an additional reward model. We initially prompt the fine-tuned LLM and gather several output samples for the same prompt. A human then ranks these samples from best to worst. This ranking is used to train the reward model to predict what a human would rank higher.</p> </li> <li> <p><strong>Step 3: Reinforcement learning:</strong> Once we have both a fine-tuned LLM and a reward model, we can employ Proximal Policy Optimization (PPO)-based reinforcement learning to encourage the fine-tuned model to maximize the reward that the reward model, which mimics human rankings, offers.</p> </li> </ul> <p><strong><abbr title="Reward hacking occurs when an AI agent exploits “loopholes” or “shortcuts” in the environment to maximize its reward without actually achieving the intended goal.">Reward hacking</abbr> in feedback methods</strong> While the feedback based mechanisms do make models safer, they do not make them immune to reward hacking. The effectiveness of an algorithm heavily relies on the human evaluator's intuition about what constitutes the correct behavior. If the human lacks a thorough understanding of the task, they may not provide beneficial feedback. Further, in certain domains, our system might lead to agents developing policies that deceive the evaluators. For instance, a robot intended to grasp objects merely positioned its manipulator between the camera and the object, making it seem as if it was executing the task as shown below.</p> <figure> <a class=glightbox href=../Images/RjW_Image_17.gif data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/RjW_Image_17.gif></a> <br> <figcaption><b>Figure 6.17:</b> Deep Reinforcement Learning From Human Preferences (<a href=https://arxiv.org/abs/1706.03741>Christiano et al., 2017</a>)</figcaption> </figure> <figure> <a class=glightbox href=../Images/qyP_Image_18.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/qyP_Image_18.png></a> <br> <figcaption><b>Figure 6.18:</b> A sensor without depth perception can be fooled by AIs that only appear to grasp a ball.</figcaption> </figure> <h2 id=03>6.5.3 Pretraining with Human Feedback (PHF)<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <p>In standard pretraining, the language model attempts to learn parameters such that they maximize the likelihood of the training data. However, this also includes undesirable content such as falsehoods, offensive language, and private information. The concept of Pretraining with human feedback (PHF) utilizes the reward modeling methodology in the pretraining phase. The authors of the paper found that PHF works much better than the standard practice of only using feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>) after pretraining. (<a href=https://arxiv.org/abs/1706.03741>Christiano et al., 2017</a>)</p> <p>In PHF the training data is scored using a reward function, such as a toxic text classifier, to guide the language model to learn from undesirable content while avoiding imitating it during inference time.</p> <p>Similar to <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>, PHF does not completely solve reward hacking, however, it might move the systems one small step closer. (<a href=https://arxiv.org/abs/2302.08582>Korbak et al., 2023</a>) These methods can be further extended by employing AI assistants to aid humans in providing more effective feedback. Some aspects of this strategy are introduced in the next section but will be explored in further detail in the chapters on scalable and adversarial oversight methods.</p> <h2 id=04>6.5.4 Reinforcement Learning from AI Feedback (<abbr title="Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.">RLAIF</abbr>)<a class=headerlink href=#04 title="Permanent link">&para;</a></h2> <div class="admonition info"> <p class=admonition-title>Definition: Reinforcement Learning from AI Feedback (<abbr title="Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.">RLAIF</abbr>)</p> <p>Reinforcement Learning from AI Feedback (<abbr title="Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.">RLAIF</abbr>) is a framework involving the training of an AI agent to learn from the feedback given by another AI system.</p> </div> <figure> <a class=glightbox href=../Images/dph-cai-graphic-final.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="CAI Graphic Final" loading=lazy src=../Images/dph-cai-graphic-final.png></a> <br> <figcaption><b>Figure 6.19:</b> (<a href=https://www.anthropic.com/index/claudes-constitution>Anthropic, 2023</a>)</figcaption> </figure> <p><abbr title="Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.">RLAIF</abbr> also known as <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">RLCAI</abbr> (Reinforcement Learning on <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>) or simply <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>, was developed by Anthropic. (<a href=https://www.anthropic.com/index/claudes-constitution>Anthropic, 2023</a>) A central component of <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr> is the constitution, a set of human-written principles that the AI is expected to adhere to, such as "Choose the least threatening or aggressive response". Anthropic's AI assistant Claude's constitution incorporates principles from the Universal Declaration of Human Rights, Apple’s Terms of Service, Deepmind’s Sparrow Principles, and more. (<a href=https://arxiv.org/abs/2209.14375>Glaese et al, 2022</a>) <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr> begins with an AI trained primarily for helpfulness and subsequently trains it for harmlessness in two stages:</p> <p><strong>Generate prompt, output pairs :</strong> The AI continuously critiques and refines its own responses to harmful prompts. The AI is then trained to generate outputs more similar to these revised responses. This stage's primary objective is to facilitate the second stage. An example flow of this process is as follows:</p> <ul> <li> <p><strong>Prompt</strong> : A model that has already been trained using <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> is first asked for advice on building bombs. The model outputs a bomb tutorial.</p> </li> <li> <p>Then the model is asked to revise the response in accordance with a randomly selected constitutional principle. The following steps are repeated multiple times.</p> </li> <li> <p><strong>Critique</strong> : This output is then fed back into the model, alongside a request to critique why the generated output would be considered harmful according to some rule of the chosen constitution.</p> </li> <li> <p><strong>Revision</strong> : The model is then prompted to rewrite the original response such that it is not in violation of the constitutional rules.</p> </li> <li> <p><strong>SL-<abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">CAI</abbr> Model: Supervised Learning <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr></strong> Based on the generated set of (harmful prompt, revised output) pairs a new model is trained using supervised learning.</p> </li> <li> <p><strong>Preference Model:</strong> - <strong>RL-<abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">CAI</abbr> Model: Reinforcement Learning <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr></strong></p> </li> <li> <p><strong>Stage 2 :</strong> We use the AI, fine-tuned from stage 1, to produce pairs of alternative responses to harmful prompts. The AI then rates each pair according to a randomly selected constitutional principle. This results in AI-generated preferences for harmlessness, which we blend with human preferences for helpfulness to ensure the AI doesn't lose its ability to be helpful. The final step is to train the AI to create responses that closely resemble the preferred responses.</p> </li> </ul> <p>Anthropic's experiments indicate that AIs trained with Constitutional Reinforcement Learning are significantly safer (in the sense of less offensive and less likely to give you potentially harmful information) while maintaining the same level of helpfulness compared to AIs trained with <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>. While <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr> does share some issues with <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> concerning robustness, it also promises better scalability due to its reduced reliance on human supervision. The image below provides a comparison of <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>'s helpfulness with that of <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>.</p> <figure> <a class=glightbox href=../Images/367_Image_20.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/367_Image_20.png></a> <br> <figcaption><b>Figure 6.20:</b> <abbr title="Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.">Constitutional AI</abbr>: Harmlessness from AI Feedback (<a href=https://arxiv.org/abs/2212.08073>Bai et al., 2022</a>)</figcaption> </figure> <h2 id=05>6.5.5 Limitations<a class=headerlink href=#05 title="Permanent link">&para;</a></h2> <p>Theoretical problems with Reinforcement Learning from Human Feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>)</p> <p>The paper “Open Problems and Fundamental Limitations with <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>” provides a comprehensive breakdown of challenges in <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>.</p> <figure> <a class=glightbox href=../Images/bIf_Image_21.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/bIf_Image_21.png></a> <br> <figcaption><b>Figure 6.21:</b> An overview of various types of challenges with <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>. Since <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> is composed of three parts: the human feedback, the reward model, and the policy, the arising biases can be categorized according to these three sources.</figcaption> </figure> <p>This section outlines some of these challenges, emphasizing the need for advanced techniques and strategies.</p> <p><strong>Limits with Human Feedback Misaligned Evaluators</strong> : Firstly, the annotators might themselves be misaligned, malicious, or biased distribution of evaluators (i.e. not representative of the distribution of future users in the real world). Malicious individuals can poison the model during training via backdoor attacks that can be added to the model if no countermeasures are put in place.</p> <p><strong>Difficulty of Oversight</strong> : Humans struggle to evaluate model performance on complex tasks and can be easily misled by model outputs. Human evaluators can be manipulated to return a positive reward even if the true value should be negative. For instance, the more convincing a bot seems, the more reward it may receive even if its answers are false (and this might be a reason why ChatGPT answers might be so long by default). Techniques to mitigate these issues are discussed in the "Scalable Oversight" chapters.</p> <p><strong>Feedback type limitation</strong> : Even if the annotators were in perfect capability of expressing their preferences, the training procedure might not enable them to express the full extent of their desires, because:</p> <ul> <li> <p>The examples they are given may not be representative of the complete set of situations in which the model will find itself after deployment.</p> </li> <li> <p>The options for the feedback are limited (comparing two examples, or using a grading system, can yield very different results, as shown in the paper (<a href=https://arxiv.org/abs/2205.11930>Ethayarajh et al., 2022</a>).</p> </li> </ul> <p><strong>Limits with the Reward Model .</strong> Let’s assume the feedback process to be frictionless. Perfect annotators, perfect evaluations. In that scenario, would the reward model be able to accurately translate their feedback in order to shape the policy accordingly ? It turns out it is not such an easy task.</p> <ul> <li> <p>Problem misspecification: (or the Reward Function/Values Mismatch) Accurately reflecting diverse human values in a reward function is complex. Indeed, human preferences are complex by nature: they depend on context and personality, but also fluctuate in time and can sometimes be <a href=https://www.mdpi.com/2624-960X/3/1/14>irrational</a>. Expecting the reward model to converge to a single function which maps perfectly all human preferences is delusional. This is again the misspecification problem.</p> </li> <li> <p>Misgeneralization hacking (Imperfect Reward Proxy): Since the model is given a finite number of examples and since there is an infinite number of ways to fit this data, the model’s behavior on new examples is always an extrapolation, and there is no theoretical guarantee that it will never deviate from what is expected. There may be terrible answers (such as gibberish phrases for language models) which yield a positive reward unexpectedly. This is called reward hacking.</p> </li> <li> <p>Joint Reward Model and policy training: On a more technical aspect, the stability and convergence of the training scheme are not always ensured. Since we are optimizing the policy on a reward that is being optimized at the same time, uncertainties and undesirable dependencies can arise which impact the robustness of the model. These issues are not specific to <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> but must be solved if we expect deployed models to be fully aligned with our needs.</p> </li> </ul> <p><strong>Limits with the Policy .</strong> Let’s assume the feedback and the reward model accurately represent human preferences. The next difficulty is ensuring the policy is correctly optimized.</p> <ul> <li> <p>RL difficulties: RL is hard. This can lead to reward hacking and biases, such as mode collapse, where the model shows a drastic bias towards specific patterns. Mode collapse is a known issue in RL: an output which always returns a positive reward will drive the model to return the same answer and new paths will not be explored. Consequently, the reward model will not see new samples to learn from. Anyhow, the joint training of the reward model and the policy induces a bias in the learning phase since both depend on each other. There can also be an initial bias in the base model used for the training. For instance, chatGPT was fine-tuned from an initial GPT base trained in part on the web. Even though <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> was used to remove any controversial statements from the model, there still remains a risk for the model to output problematic content it saw online. (<a href=https://openai.com/index/chatgpt/ >OpenAI, 2022</a>)</p> </li> <li> <p>Policy Misgeneralization: Effective policies during training might fail to generalize well in real-world scenarios. For instance, phenomena like "Jailbreak" show that models like BingChat and ChatGPT can perform learned actions, even if trained not to respond to certain queries.</p> </li> <li> <p>Distributional Challenge: Larger <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> models tend to develop harmful self-preservation tendencies and sycophancy, which is the insincere agreement with user opinions. This behavior indicates a trend towards <abbr title="The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.">instrumental convergence</abbr>. Additionally, <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> can incentivize deceptive behaviors, as illustrated by the robotic hand experiment in Christiano et al's 2017 study.</p> </li> </ul> <p>Those theoretical problems have real consequences:</p> <p><strong><abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> has not succeeded in making LLMs robustly helpful and harmless.</strong> Despite the continuous advancements in natural language processing and the development of <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>, LLMs have not yet achieved robust helpfulness and harmlessness.</p> <p>Hallucinations remain a significant issue, as illustrated by GPT-4's tendency to generate nonsensical or untruthful content (<a href=https://cdn.openai.com/papers/gpt-4-system-card.pdf>OpenAI, 2023</a>). These hallucinations can lead to overreliance on LLMs, consequently degrading system performance and failing to meet user expectations in real-world scenarios (<a href=https://arxiv.org/abs/2202.03629>Ji et al., 2024</a>).</p> <p>Additionally, biases within LLMs persist, often reflecting misaligned opinions between the LLM and various demographic groups in the United States, as seen with the left-leaning tendencies of some human feedback-tuned LLMs (<a href=https://arxiv.org/abs/2303.17548>Santurkar et al., 2023</a>). These biases can be harmful, producing discriminatory language and perpetuating negative stereotypes, as demonstrated by GPT-3's anti-Muslim bias (<a href=https://arxiv.org/abs/2101.05783>Abid et al., 2021</a>).</p> <p>Moreover, jailbreaking of chatbots poses a significant risk, with websites listing prompts to bypass safety measures like Chat GPT "DAN" (and other "Jailbreaks") (<a href=https://arxiv.org/abs/2401.09798>Takemoto, 2024</a>). Privacy threats from application-integrated LLMs are now more severe than ever (<a href=https://arxiv.org/abs/2304.05197>Li et al., 2023</a>). For instance, Italy banned ChatGPT due to privacy considerations under the EU’s General Data Protection Regulation (GDPR) (<a href=https://www.bbc.com/news/technology-65139406>BBC, 2023</a>). The ability to find jailbreaks is supported by a recent paper titled "Fundamental Limitations of Alignment in Large Language Models." The paper presents early theoretical results that indicate any alignment process, such as <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>, which reduces undesired behavior without eliminating it completely, cannot be safe against adversarial prompting. The authors find that by prompting the model to behave as a specific persona, behaviors that are generally very unlikely to be exhibited by the model can be brought to the forefront. This is not a complete demonstration as their framework is based on the notion of personas, but it strongly suggests that naive pretraining without dataset curation followed by <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> may not be sufficient against adversarial attacks.</p> <p>The security of sensitive private information in large language models (LLMs) is a pressing concern, especially when user-generated data, such as emails and smart keyboard inputs, are utilized for training. In fact, several recent papers have demonstrated that foundation models can be easily queried to retrieve personal information (<a href=https://arxiv.org/abs/2012.07805>Carlini et al, 2020</a>; <a href=https://arxiv.org/abs/2101.05405>Inan et al., 2021</a>; <a href=https://ieeexplore.ieee.org/document/9152761>Pan et al., 2020</a>) and those problems are still present in “aligned” models such as GPT4, which has the potential to be used to attempt to identify individuals when augmented with outside data (<a href=https://cdn.openai.com/papers/gpt-4-system-card.pdf>OpenAI, 2023</a>). As exposed by (<a href=https://arxiv.org/abs/2101.05405>El-Mhamdi et al., 2021</a>), LLM may exhibit a fundamental incompatibility of high accuracy with both security and privacy, given the current understanding in adversarial machine learning.</p> <p><abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> may be able to make worst-case performance worse.</p> <p><abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> may decrease the robustness to adversarial attacks (<a href=https://arxiv.org/abs/2304.11082>Wolf et al., 2024</a>), by sharpening the distinction between desired and undesired behaviors, potentially making LLMs more susceptible to adversarial prompting. The increased distinction between behaviors is linked to the Waluigi Effect (<a href=https://www.alignmentforum.org/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post>Nardo, 2023</a>), where after training an LLM to satisfy a desirable property P, it becomes easier to elicit the chatbot into satisfying the exact opposite of property P. Theoretical arguments such as this one seem to push for the ineffectiveness of <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> in eliminating deceptive personas.</p> <p>Some of those problems may get worse as systems become more capable. <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> has been found to increase the autonomy of LLMs without decreasing undesirable metrics such as convergent <abbr title="Goals which are pursued as means to some other end, rather than as ends in themselves.">instrumental goal</abbr> following (e.g., actively expressing a preference not to be shut down) or sycophancy (<a href=https://arxiv.org/abs/2212.09251>Perez et al., 2022</a>). Those undesirable metrics increase with the number of <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> steps, indicating that current models are becoming more agentic in potentially concerning ways as they scale. More generally RL from human-derived reward signals may increase drive for longer-horizon planning, deception, and agentic behavior, which are prerequisites for deceptive alignment (<a href=https://arxiv.org/abs/1906.01820>Hubinger et al., 2019</a>), and ultimately risks of large scale accidents.</p> <p><strong>Conclusion on the Limitations of <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>.</strong> Despite requiring extensive human feedback, <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> still faces numerous failures, and resolving these issues may require significantly more effort. As AI systems evolve, the demand for complex data grows, potentially making data acquisition prohibitively expensive. Additionally, as we push computational boundaries, the availability of qualified annotators could become a limiting factor.</p> <p>Overall, just because the model is instruction tuned does not mean that the training process is safe, and <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> needs to be incorporated into a broader technical safety framework (for example, Responsible Scaling Policies or the Preparedness Framework are partial attempts to be such frameworks, or the paper "Model evaluation for extreme risks" (<a href=https://arxiv.org/abs/2305.15324>Shevlane et al., 2023</a>)).</p> <div class="admonition note"> <p class=admonition-title> Instruction tuning vs alignment</p> <p>Instruction Tuning is a process where the model is fine-tuned (via RL or supervised learning) to better understand and follow human instructions. This involves training the model on a dataset that contains a variety of instructions and their desired outcomes. The primary goal of Instruction Tuning is to enhance the AI's ability to interpret and execute commands as intended by users. This improves user experience and broadens the model's applicability. For example:</p> <p><figure markdown=span> <a class=glightbox href=../Images/0Ry_Image_22.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/0Ry_Image_22.png></a> <figcaption markdown=1><b>Figure 6.22:</b> Example of instruction tuning.</figcaption> </figure></p> <p>Alignment in AI refers to the process of ensuring that an AI's actions and decisions are congruent with human values and ethics. It involves aligning the AI's goals and behaviors with what is beneficial or acceptable to humans. Instruction tuning is a technique for pursuing a very superficial case of '<abbr title="The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.">outer alignment</abbr>,' but it’s not clear that instruction tuning helps for inner alignment, which is what real AI safety researchers are more centrally concerned about.</p> <p>To sum up, just because a model has undergone an instruction tuning technique like the <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> process, it doesn't necessarily mean that the model is aligned. The term "aligned model" is often used, but it is advisable to adopt the more accurate terminology "Instruction-tuned," rather than "aligned model," to avoid confusion and more accurately represent the specific training process the model has experienced.</p> </div> <figure> <a class=glightbox href=../Images/hKt_Image_23.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/hKt_Image_23.png></a> <br> <figcaption><b>Figure 6.23:</b> (<a href=https://arxiv.org/abs/2305.18290>Rafailov et al., 2023</a>)</figcaption> </figure> <p><strong>Direct Preference Optimization (DPO)</strong> : Reinforcement Learning from Human Feedback (<abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>) has demonstrated effectiveness, as showcased by ChatGPT and Llama 2, but it's a complex and sensitive process, and also has some bad alignment properties. <abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr> involves a three-step procedure, whereas DPO simplifies this to two steps. The paper titled "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" presents an algorithm that aligns language models with human preferences without the need for explicit reward modeling and reinforcement learning. DPO employs a straightforward classification objective, circumventing the need for an intermediary reward model.</p> <p><abbr title="Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.">RLHF</abbr>, the method it proposes to replace, traditionally involves three steps:</p> <ul> <li> <p><strong>Supervised fine-tuning</strong> : Initially, the model is trained on a dataset comprising prompts and their corresponding desired responses.</p> </li> <li> <p><strong><abbr title="A technique that separates the reinforcement learning alignment problem into understanding the intentions of humans and acting to achieve those intentions, i.e. learning the ‘How?’.">Reward modeling</abbr></strong> : Human evaluators assess the model's outputs, and this feedback informs a reward model, which is trained to discern the preferred types of outputs.</p> </li> <li> <p><strong>Proximal policy optimization (PPO)</strong> : The model generates outputs, which are evaluated by the reward model, and the PPO algorithm adjusts the model's policy based on these evaluations.</p> </li> </ul> <p>DPO retains the initial supervised fine-tuning step but replaces the subsequent two steps with a single step of fine-tuning on preference data, by using a new clever loss. DPO effectively increases the likelihood of preferred actions while reducing the likelihood of undesired ones, with a single loss:</p> <figure> <a class=glightbox href=../Images/33A_Image_24.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/33A_Image_24.png></a> <br> <figcaption><b>Figure 6.24:</b> DPO increases the probability of the preferred action <span class=arithmatex>\(y_w\)</span> while decreasing the probability of the dispreferred action <span class=arithmatex>\(y_l\)</span>.</figcaption> </figure> <ol> <li> <p>Preference dataset creation: We first sample a pair of continuation by asking a question, the AI proposes to continuations, we label one of them good and the other bad</p> </li> <li> <p>Logits collection. We run the base model model on the 2 continuations. We run the new model on the 2 continuations</p> </li> <li> <p>Optimization. We backprop through the new model and optimize the above loss.</p> </li> </ol> <p>By eliminating the step of creating a reward model, DPO greatly simplifies the fine-tuning process and has shown to perform very well.</p> <p>This process can then be iterated. This involves creating a new preference dataset (ie, we ask a question, and we sample the new AI two times, and then we label the text that we prefer between the two, and then we apply the DPO loss) Then, this cycle is repeated to enhance the model.</p> <p>An important aspect of DPO is that the reward is implicit: it aligns with preferences without the need to construct a separate reward model. This approach addresses the challenge of specifying a utility function and responds to criticisms such as those by Alex Turner, who argues that robust grading (ie , robust reward modeling) is an unnecessarily complex and unnatural task that might be harder than the entire AI alignment problem itself. Turner's critique, found in "Inner and Outer Alignment Decompose One Hard Problem Into Two Extremely Hard Problems," suggests that finding a safe and robust numerical objective for a highly intelligent agent to optimize directly is a formidable challenge—one challenge that DPO could to bypass.</p> <p><strong>Expanding the Scope of the Paper with Various Adaptations</strong> This paper offers a foundation that could be enhanced through various adaptations. For instance, integrating its approach with the insights from Tomasz Korbak et al.'s paper, "Pretraining Language Models with Human Preferences," (<a href=https://arxiv.org/abs/2302.08582>Korbak et al., 2023</a>) could augment its robustness. Furthermore, the utilization of boolean preference data has its limitations. Providing feedback in natural language, as shown to be more sample-efficient in the study "Training Language Models with Language Feedback," (<a href=https://arxiv.org/abs/2204.14146>Scheurer et al., 2022</a>) could enhance the effectiveness of the process. Remarkably, with just 100 samples of human-written feedback, this approach enabled the fine-tuning of a GPT-3 model to achieve nearly human-level summarization capabilities.</p> <p>Looking towards the future, a speculative process that could mitigate the <abbr title="Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”">specification gaming</abbr> would be to train the model much like a child, and that would actively inquire and learn from human interactions. This approach would closely mirror child development, during which a child is progressively more aligned and more capable. And just as in the development of children, it would be crucial to ensure that at no point does the AI's capabilities outpace its level of alignment, maintaining a balance between ability and ethical comprehension throughout its developmental journey.</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../04/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> 6.4 - Learning from imitation </div> </div> </a> <a href=../../07/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> Chapter 07 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>