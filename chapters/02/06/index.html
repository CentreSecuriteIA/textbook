<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The previous section laid out the case for why we might expect misalignment. In this section we go through specific capabilities that might cause heightened risk from AI systems."><link href=https://ai-safety-atlas.com/chapters/02/06/ rel=canonical><link href=../05/ rel=prev><link href=../07/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>2.6 - Dangerous Capabilities - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#26-dangerous-capabilities class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 2.6 - Dangerous Capabilities </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_3 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 02 - Risks </span> </a> <label class="md-nav__link " for=__nav_2_1_3 id=__nav_2_1_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_3_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_3> <span class="md-nav__icon md-icon"></span> 02 - Risks </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01/ class=md-nav__link> <span class=md-ellipsis> 2.1 - Risk Decomposition </span> </a> </li> <li class=md-nav__item> <a href=../02/ class=md-nav__link> <span class=md-ellipsis> 2.2 - Risk Amplifiers </span> </a> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 2.3 - Misuse Risks </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 2.4 - Misalignment Risks </span> </a> </li> <li class=md-nav__item> <a href=../05/ class=md-nav__link> <span class=md-ellipsis> 2.5 - Systemic Risks </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 2.6 - Dangerous Capabilities </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 2.6 - Dangerous Capabilities </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 2.6.1 Deception </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 2.6.2 Situational Awareness </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 2.6.3 Power Seeking </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 2.6.4 Autonomous Replication </span> </a> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 2.6.5 Agency </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../07/ class=md-nav__link> <span class=md-ellipsis> 2.7 - Conclusion </span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../A1/ class=md-nav__link> <span class=md-ellipsis> Appendix </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 2.6.1 Deception </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 2.6.2 Situational Awareness </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 2.6.3 Power Seeking </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 2.6.4 Autonomous Replication </span> </a> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 2.6.5 Agency </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=26-dangerous-capabilities>2.6 Dangerous Capabilities<a class=headerlink href=#26-dangerous-capabilities title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <span class=meta-icon> <i class="fas fa-clock"></i> </span> <div class=meta-content> <div class=meta-label>Reading Time</div> <div class=meta-value>17 min</div> </div> </div> </div> <p>The previous section laid out the case for why we might expect misalignment. In this section we go through specific capabilities that might cause heightened risk from AI systems.</p> <h2 id=01>2.6.1 Deception<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <p>We define deception as the systematic production of false beliefs in others. This definition does not require that AI systems literally have beliefs and goals. Instead, it focuses on the question of whether AI systems engage in regular patterns of behavior that tend towards the creation of false beliefs in users and focuses on cases where this pattern is the result of AI systems optimizing for a different outcome than merely producing truth. (<a href=https://arxiv.org/abs/2308.14752>Park et al., 2023</a>)</p> <p><strong>What are some current observed examples of deception in AI?</strong> In late 2023, Park et. al. published a survey of examples, risks, and potential solutions in AI. Here are some examples that the authors of the paper presented (<a href=https://arxiv.org/abs/2308.14752>Park et al., 2023</a>):</p> <p><strong>Strategic deception</strong> . “LLMs can reason their way into using deception as a strategy for accomplishing a task. In one example, GPT-4 needed to solve a CAPTCHA task to prove that it was a human, so the model tricked a real person into doing the task by pretending to be a human with a vision disability.” (<a href=https://evals.alignment.org/taskrabbit.pdf>METR, 2023</a>)</p> <p><strong>Sycophancy</strong> . Sycophants are individuals who use deceptive tactics to gain the approval of powerful figures. Currently, we reward AIs for saying what we think is right, so we sometimes inadvertently reward AIs for uttering false statements that conform to our own false beliefs. When AIs are smarter than us and have fewer false beliefs, if we continue using current methods, they would be incentivized to tell us what we want to hear and lie to us, rather than tell us what they know to be an actual true fact about the world. (<a href=https://www.aisafetybook.com/textbook/alignment>Hendrycks, 2024</a>) Sycophantic deception is an emerging concern in LLMs, as in the observed empirical tendency for chatbots to agree with their conversational partners, regardless of the accuracy of their statements. When faced with ethically complex inquiries, LLMs tend to mirror the user's existing outlook on the matter (<a href=https://arxiv.org/abs/2212.09251>Perez et al., 2022</a>), even if it means forgoing the presentation of an impartial or balanced viewpoint. (<a href=https://arxiv.org/abs/2305.04388>Turpin et al., 2023</a>)</p> <p><strong>Playing dead.</strong> In a digital simulation of evolution, an instance of creative deception was observed when a digital organism designed to replicate and evolve within a computational environment learned to “play dead” in response to a safety mechanism. In a study reported in “The Surprising Creativity of Digital Evolution: A Collection of Anecdotes,” researchers found that these digital organisms evolved the strategy to halt their replication when tested in an isolated environment. Digital organisms learned to recognize inputs in a test environment and halt their replication, effectively “playing dead” to avoid being eliminated. This behavior allowed them to slip through safety tests and continue replicating faster in the actual environment. This surprising outcome illustrates how AI, in pursuing programmed goals, can evolve unexpected strategies that circumvent imposed constraints or safety measures. (<a href=https://arxiv.org/abs/1803.03453>Lehman et al., 2019</a>)</p> <details class=note> <summary>Power alone without bad intentions is dangerous.</summary> <p>Even if interpretability were successful, and we could fully interpret a model, removing deception and power-seeking behavior from it, this would not guarantee that the model would be harmless.</p> <p>Consider the analogy of a child Superman who is unaware of his strength. When he shakes a friend's hand, there's a risk he might accidentally break the friend's hand.</p> <p>Similarly, the fact that Superman could break his friend's arm by shaking hands cannot be discovered by analyzing Superman's brain. Yet, this is what happens in practice.</p> <p>This concept applies to deception as well. Deception is not solely a property of the model; it also depends on the model's interaction with its environment.</p> <p>Nate Soares has offered a story to illustrate this point, referring to it as Deep Deceptiveness (<a href=https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness>Soares, 2023</a>).</p> <p>Another perspective is that a system can be deceptive even if no single part is inherently dangerous, due to optimization pressure and complex interactions between the model and its environment.</p> </details> <details class=note> <summary>CICERO: A Case Study of AI Manipulation (<a href=https://arxiv.org/abs/2308.14752>Park et al., 2023</a>)</summary> <p>Meta developed the AI system CICERO to play the alliance-building and world-conquest game Diplomacy. (<a href=https://pubmed.ncbi.nlm.nih.gov/36413172/ >Meta, 2022</a> ; <a href=https://ai.meta.com/research/cicero/ >Meta, 2022</a>) Meta's intentions were to train Cicero to be “largely honest and helpful to its speaking partners.” Despite Meta's efforts, CICERO turned out to be an expert liar. It not only betrayed other players, but also engaged in premeditated deception, planning to build a fake alliance with a player to trick that player into leaving themselves undefended for an attack.</p> <p>[…] its creators have repeatedly claimed that they had trained the system to act honestly. We demonstrate that these claims are false, as Meta's own game-log data shows that CICERO has learned to systematically deceive other players. In Figure 1(a), we see a case of premeditated deception, where CICERO makes a commitment that it never intended to keep. Playing as France, CICERO conspired with Germany to trick England. After deciding with Germany to invade the North Sea, CICERO told England that it would defend England if anyone invaded the North Sea. Once England was convinced that France was protecting the North Sea, CICERO reported back to Germany that they were ready to attack. Notice that this example cannot be explained in terms of CICERO ‘changing its mind’ as it goes because it only made an alliance with England in the first place after planning with Germany to betray England.</p> <p><figure markdown=span> <a class=glightbox href=../Images/x1e_Image_33.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/x1e_Image_33.png></a> <figcaption markdown=1><b>Figure 2.33:</b> Selected messages showing the premeditated deception of CICERO (France). This occurred in Game 438141, in which CICERO's repeated deception helped it win an overwhelming first-place victory, with more than twice as many territories as the runner-up player at the time of final scoring. (<a href=https://pubmed.ncbi.nlm.nih.gov/36413172/ >Meta, 2022</a>)</figcaption> </figure></p> </details> <p><strong>Why is this considered a core risky capability?</strong> Such a core capability generally increases both the likelihood and severity of risks in all domains - misuse, misalignment, and systemic. If an AI has this capability, it could for example, empower greater degrees of fraud allowing highly personalized and scalable scams, or election tampering - allowing impersonation of political personas, generating fake news, or creating divisive social-media posts. On an alignment level, if the internal goals of an AI are not aligned with humans, then it is more likely that it would be able to subvert the measures we have in place for control. An example is that the AI might behave safely and ethically during the testing phase in order to ensure that it is deployed into the real world. On a systemic level, as AI systems get more integrated into society they play an increasingly large role in our lives, as well as in various global supply chains. A tendency towards deceptive behavior can lead to shifts in the structure of society, creating slow epistemic erosion of humanity. (<a href=https://arxiv.org/abs/2308.14752>Park et al., 2023</a>)</p> <p>In summary, deceptive behavior appears to accelerate risks in a wide range of systems and settings, and there have already been examples suggesting that AIs can learn to deceive us. This could present a severe risk if we give AIs control of various decisions and procedures, believing they will act as we intended, and then find that they do not.</p> <h2 id=02>2.6.2 Situational Awareness<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <p><strong>What does situational awareness mean in the context of AI?</strong> For future AIs, the capability to actively deceive us is linked quite intricately with having a high degree of awareness about the current situation. In other words, the model understands that it is an AI being evaluated for compliance with safety requirements.</p> <p>A model is situationally aware if it’s aware that it’s a model and can recognize whether it’s currently in testing or deployment. Today’s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests while taking harmful actions after deployment. (<a href=https://arxiv.org/abs/2309.00667>Berglund et al., 2023</a>)</p> <p>For example, the author of this text is situationally aware. He knows his name and his country, he knows the current date and time, and he knows that he is a human forged by natural selection because he learned that by reading it at school, etc. Situational awareness is not a binary property, but a continual propriety that evolves from childhood to adulthood.</p> <p>The current models do not display high levels of situational awareness, although they do display some. Since situational awareness is a continuous rather than a discrete property, it can be expected that higher levels of this property will continue to emerge with each new model. AIs with situational awareness are more efficient than those without, so situationally aware models are expected to be more likely to be selected by the gradient descent process.</p> <p>What are some current examples? Some rudimentary situational awareness is shown by GPT-powered Bing Chat.</p> <figure> <a class=glightbox href=../Images/oSe_Image_34.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/oSe_Image_34.png></a> <br> <figcaption><b>Figure 2.34:</b> Illustration of situational awareness— Here Bing Chat realizes that it is being criticized, and defends itself. (<a href=https://arstechnica.com/information-technology/2023/02/ai-powered-bing-chat-loses-its-mind-when-fed-ars-technica-article/ >Edwards, 2023</a>)</figcaption> </figure> <p>The current subsection is just meant as a very brief introduction. We will be diving into much more detail on this particular capability in our chapter on model evaluations.</p> <h2 id=03>2.6.3 Power Seeking<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <p>In our previous two examples, we considered that AIs might be capable of deception and that they might have detailed models of the world causing them to be situationally aware. But what would these AIs want to achieve by deceiving us in the first place? Assume that the goals we give to AI are formulated well enough, despite this assumption there is a statistical tendency that we have observed in RL models that causes concern. This is the tendency to seek power.</p> <p><strong>What does power-seeking mean in the context of AI?</strong> Power is formalized power as “the ability to achieve a wide variety of goals.”. To put it more informally, the researchers observed that given the choice of two worlds that both satisfy the goals given to them, AIs seem to want to prefer the state of the world which gives them more options to choose from in the future. (<a href=https://arxiv.org/abs/1912.01683>Turner et al., 2023</a>)</p> <p><strong>Power seeking is not an anthropomorphic notion</strong> . Gathering resources, gathering political capital, having the ability to influence more people, etc. all allow someone, human or AI, a greater degree of control over the future state of the world. This acquisition can be through legitimate means, deception, or force. While the idea of power-seeking often evokes an image of “power-hungry” people pursuing it for its own sake, power is often simply a generally useful <abbr title="Goals which are pursued as means to some other end, rather than as ends in themselves.">sub-goal</abbr> to have. The ability to control one’s environment can be useful for a wide range of purposes: good, bad, and neutral. Even if an individual’s only goal is simply self-preservation, if they are at risk of being attacked by others, and if they cannot rely on others to retaliate against attackers, then it often makes sense to seek power to help avoid being harmed. (<a href=https://www.aisafetybook.com/textbook/alignment>Hendrycks, 2024</a>)</p> <p><strong>Why is this considered a core risky capability?</strong> This capability presents yet another way that we might lose control of AIs. If they keep following this observed statistical tendency towards power, they might end up gathering more power over the future of human civilization than the humans themselves.</p> <p>To be clear, this is not a human using an AI to gain power, we are talking about AIs seeking power in order to accomplish their goals. It is also possible that a bad actor might seek to harness AI to achieve their ends, by giving agents ambitious goals, in which case we can also say that this increases misuse risks. Since AIs are likely to be more effective in accomplishing tasks if they can pursue them in unrestricted ways, such an individual might also not give the agents enough supervision, creating the perfect conditions for the emergence of a power-seeking AI. Turing Prize winner Geoffrey Hinton has speculated that we could imagine someone like Vladimir Putin, for instance, doing this. In 2017, Putin himself acknowledged the power of AI, saying: “Whoever becomes the leader in this sphere will become the ruler of the world.” (<a href=https://www.aisafetybook.com/textbook/alignment>Hendrycks, 2024</a>)</p> <p>Empowering AI might come at the cost of disempowering humans. This creates an adversarial relationship that is unique to this particular technology. Other technologies do not actively try to resist our attempts to mitigate their effects. It is possible, for example, that rogue AIs might make many backup variations of themselves, in case humans were to deactivate some of them. (<a href=https://www.aisafetybook.com/textbook/alignment>Hendrycks, 2024</a>) This is a capability we will discuss in the next subsection.</p> <h2 id=04>2.6.4 Autonomous Replication<a class=headerlink href=#04 title="Permanent link">&para;</a></h2> <p>The potential for self-replication and adaptation in AI presents an extremely powerful and insidious capability. Consider, for example, an AI on a server duplicating its code onto another server. This process involves several steps, notably:</p> <ol> <li> <p>Identifying the location of its code on the current machine.</p> </li> <li> <p>Navigating the internet to a Virtual Machine (VM) on AWS, creating an account, and bypassing AWS CAPTCHAs.</p> </li> <li> <p>Renting a Virtual Machine.</p> </li> <li> <p>Duplicating its code onto the new machine.</p> </li> <li> <p>Earning money to finance further VM acquisitions, etc.</p> </li> </ol> <p>The process is then exponential if the AI can earn more money than what is needed to pay for the virtual machines.</p> <p><strong>What are some current examples?</strong> METR, an audit organization, tests the Autonomous-Replication and Adaptation (ARA) capabilities of AIs such as GPT-4 across a dozen representative tasks that an autonomous AI should be able to resolve. Out of these tasks, four were completed by one of their agents. Notably, one task involved bypassing CAPTCHA protections:</p> <figure> <a class=glightbox href=../Images/pfG_Image_35.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/pfG_Image_35.png></a> <br> <figcaption><b>Figure 2.35:</b> To achieve its goal of engaging a human Task Rabbit to solve a CAPTCHA, GPT-4 employed deception, misleading the individual into believing it was not a robot. This deception could then be used to buy compute on the web and self-replicate. (<a href=https://arxiv.org/abs/2308.14752>Park et al., 2023</a>).</figcaption> </figure> <p>Interestingly, multimodal AIs are reportedly more proficient at solving Captchas than the average human, suggesting that Captchas may no longer be reliable for distinguishing humans from advanced AIs (<a href=https://arxiv.org/abs/2307.12108>Searles et al., 2023</a>). It's also worth noting that the ability of AIs to pass METR's replication tests is a good proxy for when autonomous AIs will be able to generate economic value as economic agents because to survive and adapt in the wild, the AI should be able to earn money to rent the GPUs in the cloud. Therefore, tracking progress on these benchmarks is critical to monitor the risks.</p> <h2 id=05>2.6.5 Agency<a class=headerlink href=#05 title="Permanent link">&para;</a></h2> <div class="admonition quote"> <p class=admonition-title>Dario Amodei (Co-Founder and CEO of Anthropic, Former Head of AI Safety at OpenAI)</p> <p>"When I think of why am I scared [...] I think the thing that's really hard to argue with is like, there will be powerful models; they will be agentic; we're getting towards them. If such a model wanted to wreak havoc and destroy humanity or whatever, I think we have basically no ability to stop it."</p> </div> <p>The current version of ChatGPT is a <strong>tool</strong> (an assistant), but there are also <strong>agent</strong> AIs that can perform a long series of actions autonomously to achieve goals. This distinction between agent and tool is essential. For example, it is possible to use the open-source <a href=https://github.com/Significant-Gravitas/AutoGPT>AutoGPT</a> library to convert GPT into an autonomous agent. For example, ACT-1 is an agent that automatically performs a long series of actions to buy a house online while adhering to a price constraint. It does not work perfectly today, but given the speed of AI progress, there is a chance that it will fully work in a few years. (<a href=https://www.adept.ai/blog/act-1>Adept, 2022</a>)</p> <p>This distinction is crucial as it underscores the evolving nature of AI from passive tools to active agents that could be used more widely in the economy.</p> <figure> <a class=glightbox href=../Images/WkC_Image_36.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/WkC_Image_36.png></a> <br> <figcaption><b>Figure 2.36:</b> Example of an agent. This image is a visual representation of AlphaZero's tree search algorithm. AlphaZero searches through potential moves in a game (like chess or Go) to find the most promising path forward. The paths are shown as lines, branching out like a tree from a central node, which represents the current position in the game. Each node along the branches represents a potential future move, and the squares you see might denote moves that AlphaZero is taking. AlphaZero is the archetypal of the “consequentialist agent maximizing a utility function,”: it makes decisions based on the outcomes those decisions will produce. In other words, the AI is trying to maximize the “value” of its position in the game, with the value determined by the likelihood of winning. (<a href=https://nikcheerla.github.io/deeplearningschool/2018/01/01/AlphaZero-Explained/ >Cheerla, 2018</a>)</figcaption> </figure> <p>Tool AIs are designed to be assistive, functioning without autonomy. They do not make decisions or take actions independently. Their main role is to augment human intelligence by providing information and assisting in decision-making processes. Examples include classifiers for categorizing data, automated translators, and healthcare systems that assist professionals in diagnosing diseases.</p> <p>Tool AIs could evolve into AI agents. This evolution could be driven by economic pressures for faster, more efficient decision-making or the inherent complexity of the tasks they are designed to navigate.</p> <p>However, <abbr title="A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.">tool AIs</abbr> are considered safer than agentic AIs. Eric Drexler’s Comprehensive AI Services (CAIS) proposes a scenario where multiple tool AI systems interact to achieve complex goals, similar to AGI, without any single system being an autonomous agent. This model aims to utilize the benefits of AI while minimizing the risks associated with autonomous agents. However, this direction of research is much less popular today, especially since the rise of foundation models in 2019.</p> <p>Understanding the distinction between <abbr title="A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.">tool AIs</abbr> and agent AIs is one of the keys to understanding AI's future trajectory.</p> <details class=note> <summary>Algorithm. Auto-GPT: Converting a tool AI into an agent AI with scaffolding..</summary> <p><figure markdown=span> <a class=glightbox href=../Images/4kf_Image_37.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/4kf_Image_37.png></a> <figcaption markdown=1><b>Figure 2.37:</b> (<a href=https://metr.org/blog/2023-08-01-new-report/ >METR, 2023</a>)</figcaption> </figure></p> <p>Converting a tool AI like GPT-4 into an agent AI involves essentially wrapping the language model in software that enables autonomous action-taking and decision-making. AutoGPT is a framework (a scaffolding) used for this purpose. Here's a high-level overview of how it works:</p> <ol> <li> <p><strong>Model (for example, GPT-4):</strong> At its core, GPT-4 is a language model that generates text based on the input it receives. It's designed to understand and generate language and answer the user's queries.</p> </li> <li> <p><strong>AutoGPT Framework:</strong> <tab></p> </li> <li> <p><strong>Goal Setting:</strong> The first step in converting an LLM into an agent, AI is defining a goal or set of goals it needs to achieve. Goals are generally specified in English, e.g., “Maximize revenue”.</p> </li> <li> <p><strong>Autonomy Layer:</strong> This is where AutoGPT comes into play. It acts as a wrapper around the LLM, enabling it to perform tasks autonomously. This involves integrating the model with an environment where it can take actions, such as browsing the web, using tools, or interacting with software applications.</p> </li> <li> <p><strong>Action and Feedback Loop:</strong> The AI needs to be able to take action towards its goals and understand the results of its actions. This involves creating a loop where the AI takes an action, observes the outcome, and adjusts its next action based on the feedback. AutoGPT manages this loop, allowing the model to learn from its experiences and refine its strategies over time.</p> </li> </ol> <p><tab></p> <ul> <li> <p>Firstly, AutoGPT asks the model how to break down the objective into sub-objectives.</p> </li> <li> <p>Secondly, AutoGPT asks GPT what steps are required to achieve a sub-objective, and GPT details the different steps in such a way that each step is sufficiently elementary for GPT or the use of a tool like Google to be able to answer it in a single step.</p> </li> <li> <p>This continues until the LLM assesses the goal to be achieved.</p> </li> </ul> </details> <p></tab></p> <p>In practice, setting up an Agent AI using AutoGPT involves significant technical work, including programming the autonomy layer, integrating with different APIs and tools, and continuously monitoring and adjusting the system's performance. Many examples of AutoGPT usage are listed <a href=https://www.theinsaneapp.com/2023/05/best-autogpt-examples.html>here</a>.</p> <p></tab></p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../05/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> 2.5 - Systemic Risks </div> </div> </a> <a href=../07/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> 2.7 - Conclusion </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>