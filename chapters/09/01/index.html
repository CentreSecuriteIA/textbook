<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter—and the broader context of AI safety—mechanistic interpretability (mech interp) is the primary focus (Ras et al., 2020, Ali et al., 2023).1"><link href=https://ai-safety-atlas.com/chapters/09/01/ rel=canonical><link href=../ rel=prev><link href=../02/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>9.1 What is Interpretability ? - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#01 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 9.1 What is Interpretability ? </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_10 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 09 - Interpretability </span> </a> <label class="md-nav__link " for=__nav_2_1_10 id=__nav_2_1_10_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_10_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_10> <span class="md-nav__icon md-icon"></span> 09 - Interpretability </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 9.1 What is Interpretability ? </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 9.1 What is Interpretability ? </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#mechanistic-interpretability-the-bottom-up-approach class=md-nav__link> <span class=md-ellipsis> Mechanistic Interpretability: The Bottom-Up Approach </span> </a> </li> <li class=md-nav__item> <a href=#other-approaches-to-interpretability class=md-nav__link> <span class=md-ellipsis> Other Approaches to Interpretability </span> </a> </li> <li class=md-nav__item> <a href=#why-mechanistic-interpretability-matters-for-ai-safety class=md-nav__link> <span class=md-ellipsis> Why Mechanistic Interpretability Matters for AI Safety </span> </a> </li> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.1.1 Motivation for AI Safety? </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.1.2 What is the End Goal of Interpretability? </span> </a> <nav class=md-nav aria-label="9.1.2 What is the End Goal of Interpretability?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#enumerative-safety-cataloging-concepts-for-control class=md-nav__link> <span class=md-ellipsis> Enumerative Safety: Cataloging Concepts for Control </span> </a> </li> <li class=md-nav__item> <a href=#retargeting-the-search-steering-model-objectives class=md-nav__link> <span class=md-ellipsis> Retargeting the Search: Steering Model Objectives </span> </a> </li> <li class=md-nav__item> <a href=#relaxed-adversarial-training-testing-and-improving-corrigibility class=md-nav__link> <span class=md-ellipsis> Relaxed Adversarial Training: Testing and Improving Corrigibility </span> </a> </li> <li class=md-nav__item> <a href=#mechanistic-anomaly-detection-mad-spotting-deviations class=md-nav__link> <span class=md-ellipsis> Mechanistic Anomaly Detection (MAD): Spotting Deviations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.1.3 Overview of the Field of Mechanistic Interpretability </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../02/ class=md-nav__link> <span class=md-ellipsis> 9.2 Observational Methods </span> </a> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 9.3 Interventional Methods </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 9.4 Automating and Scaling Interpretability </span> </a> </li> <li class=md-nav__item> <a href=../05/ class=md-nav__link> <span class=md-ellipsis> 9.5 Critiques of Interpretability </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#mechanistic-interpretability-the-bottom-up-approach class=md-nav__link> <span class=md-ellipsis> Mechanistic Interpretability: The Bottom-Up Approach </span> </a> </li> <li class=md-nav__item> <a href=#other-approaches-to-interpretability class=md-nav__link> <span class=md-ellipsis> Other Approaches to Interpretability </span> </a> </li> <li class=md-nav__item> <a href=#why-mechanistic-interpretability-matters-for-ai-safety class=md-nav__link> <span class=md-ellipsis> Why Mechanistic Interpretability Matters for AI Safety </span> </a> </li> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.1.1 Motivation for AI Safety? </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.1.2 What is the End Goal of Interpretability? </span> </a> <nav class=md-nav aria-label="9.1.2 What is the End Goal of Interpretability?"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#enumerative-safety-cataloging-concepts-for-control class=md-nav__link> <span class=md-ellipsis> Enumerative Safety: Cataloging Concepts for Control </span> </a> </li> <li class=md-nav__item> <a href=#retargeting-the-search-steering-model-objectives class=md-nav__link> <span class=md-ellipsis> Retargeting the Search: Steering Model Objectives </span> </a> </li> <li class=md-nav__item> <a href=#relaxed-adversarial-training-testing-and-improving-corrigibility class=md-nav__link> <span class=md-ellipsis> Relaxed Adversarial Training: Testing and Improving Corrigibility </span> </a> </li> <li class=md-nav__item> <a href=#mechanistic-anomaly-detection-mad-spotting-deviations class=md-nav__link> <span class=md-ellipsis> Mechanistic Anomaly Detection (MAD): Spotting Deviations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.1.3 Overview of the Field of Mechanistic Interpretability </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=01>9.1 What is Interpretability ?<a class=headerlink href=#01 title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <i class="fas fa-clock"></i> 9 min read </div> <div class=meta-item> <i class="fas fa-file-alt"></i> 1838 words </div> </div> <p>Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter—and the broader context of AI safety—<strong>mechanistic interpretability (mech interp)</strong> is the primary focus (<a href=https://arxiv.org/abs/2004.14545>Ras et al., 2020</a>, <a href=https://www.sciencedirect.com/science/article/pii/S1566253523001148>Ali et al., 2023</a>).<sup id=fnref:1><a class=footnote-ref href=#fn:1>1</a></sup></p> <h4 id=mechanistic-interpretability-the-bottom-up-approach><strong>Mechanistic Interpretability: The Bottom-Up Approach</strong><a class=headerlink href=#mechanistic-interpretability-the-bottom-up-approach title="Permanent link">&para;</a></h4> <p><strong>Mechanistic interpretability seeks to reverse-engineer neural networks to uncover how their internal components—such as neurons, weights, and layers—work together to process information. This approach starts at the lowest level of abstraction and builds understanding piece by piece</strong>: this is why it’s considered a bottom-up approach<strong>. </strong>By analyzing these basic components, we hope we can piece together how the network processes information and makes decisions.</p> <p>For example, mechanistic interpretability could explain how a neural network recognizes objects in an image or generates language, down to the contributions of individual neurons or attention heads. The hope is that this level of detail will allow researchers to diagnose and potentially fix unwanted behaviors in AI systems.</p> <h4 id=other-approaches-to-interpretability><strong>Other Approaches to Interpretability</strong><a class=headerlink href=#other-approaches-to-interpretability title="Permanent link">&para;</a></h4> <p>While mechanistic interpretability is a strong focus in AI safety, it is not the only approach. Other methods provide complementary perspectives:</p> <ul> <li> <p><strong>Concept-Based Interpretability</strong>: Contrarily to mechanistic interpretability, concept-based interpretability takes a<strong> top-down approach</strong>: instead of analyzing neurons or weights on a granular level, it focuses on understanding how the network manipulates <strong>high-level concepts</strong> (<a href=https://aclanthology.org/2022.cl-1.7/ >Belinkov, 2022</a>). For instance, <em>representation engineering </em>—a concept-based research agenda— explores how models encode concepts like "honesty" and how those representations can be adjusted to produce more honest outputs (<a href=https://www.semanticscholar.org/paper/Representation-Engineering%3A-A-Top-Down-Approach-to-Zou-Phan/aac3469581061cd5b46440c3eeca91c385d54ccf>Zou et al., 2023</a>).</p> </li> <li> <p><strong>Developmental Interpretability</strong>: This approach examines how model capabilities and internal representations <strong>evolve during training</strong>. By understanding the emergence of behaviors or knowledge over time, researchers hope they will be able to identify the emergence of certain capabilities and prevent unwanted ones from developing (<a href=https://www.lesswrong.com/s/SfFQE8DXbgkjk62JK/p/TjaeCWvLZtEDAS5Ex>Hoogland et al., 2023</a>).</p> </li> <li> <p><strong>Behavioral Interpretability</strong>: Unlike the previous approaches, behavioral interpretability studies <strong>input-output relationships without delving into the internal structure of models</strong>. </p> </li> </ul> <figure> <a class=glightbox href=../Images/wTG_Image_1.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/wTG_Image_1.png></a> <br> <figcaption><b>Figure 9.1:</b> A visual classification of interpretability techniques. (<a href=https://arxiv.org/abs/2404.14082>Bereska &amp; Gavves, 2024</a>).</figcaption> </figure> <h4 id=why-mechanistic-interpretability-matters-for-ai-safety>Why Mechanistic Interpretability Matters for AI Safety<a class=headerlink href=#why-mechanistic-interpretability-matters-for-ai-safety title="Permanent link">&para;</a></h4> <p>Mechanistic interpretability is a strong focus in AI safety because it provides a level of precision that other approaches do not. Behavioral interpretability, for instance, offers insights into how a model behaves by studying input-output relationships, but it cannot reveal how its internal structure leads to its decisions. To prevent AI models from making harmful decisions or ensuring alignment with human values, we need to understand why models make certain decisions, and potentially steer the decision-making process.</p> <p>For instance, by pinpointing where harmful concepts—such as instructions for cyberattacks—are stored within a model, mechanistic interpretability tools could help erase or modify those concepts without degrading the model’s overall capabilities.</p> <h2 id=01>9.1.1 Motivation for AI Safety?<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <p>The ultimate goal of interpretability, from an AI safety perspective, is to build confidence in the behavior of complex models by understanding their internal mechanisms and ensuring they act safely and predictably. There are different ways interpretability could contribute to AI safety (<a href=https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability>Nanda, 2022</a>):</p> <details class=note> <summary>Trust and transparency</summary> <p>By offering insights into which features of the input data (such as specific parts of an image or words in a sentence) or which specific concepts a model uses in its reasoning are influencing the model's outputs, interpretability tools can make it easier for users to understand, verify, and trust the behavior of complex models. This is particularly important in high-stakes applications like healthcare or autonomous systems, where trust in AI decisions is crucial.</p> </details> <details class=note> <summary>Enhance model editing</summary> <p>Interpretability could be used to <strong>mitigate misuse by erasing specific pieces of knowledge</strong> from neural networks, such as knowledge on conducting cyberattack or building bioweapons. </p> <p>Some mechanistic interpretability tools serve to locate where certain concepts are encoded in a model (those are covered in the <em>Observational Methods</em> section). For example, researchers could identify how models generate answers to harmful queries such as "steps for creating a virus" and use knowledge erasure methods to remove that knowledge from the model.</p> </details> <details class=note> <summary>Detection of undesirable behaviors</summary> <p>Interpretability could help identify when models are not functioning as intended or have learned undesirable behaviors or patterns. For alignment research, it could be used to detect, analyze, and understand undesired answers from models.</p> <p>A well-known example of this comes from a model trained to classify chest X-rays. Researchers discovered that instead of focusing on medical features like lung conditions, the model was using subtle artifacts from the scanning equipment or markers in the image to make its predictions. This led the model to perform well in testing but for the wrong reasons. By using interpretability tools, researchers identified and corrected this issue (<a href=https://www.nature.com/articles/s42256-021-00338-7>DeGrave et al., 2021</a>).</p> </details> <details class=note> <summary>Extrapolation</summary> <p>Some hope that by understanding how current models function, interpretability may help predict how future, larger models will behave, whether new capabilities or risks will emerge, and how systems will evolve as they scale.</p> </details> <p>It’s important to note that while these goals are promising, the field of interpretability is still maturing. The adoption of interpretability tools in real-world scenarios is still limited, and assessing their quality remains challenging. Many existing techniques in interpretability are not designed for large-scale use and state-of-the-art models. These limitations will be explored in more detail in the <strong>Critics of Interpretability</strong> section.</p> <h2 id=02>9.1.2 What is the End Goal of Interpretability?<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <p><strong>What concrete outcomes should interpretability achieve to make AI systems safer and more predictable?</strong> There are different opinions on the end goals of interpretability, including the following approaches:</p> <h4 id=enumerative-safety-cataloging-concepts-for-control><strong>Enumerative Safety: Cataloging Concepts for Control</strong><a class=headerlink href=#enumerative-safety-cataloging-concepts-for-control title="Permanent link">&para;</a></h4> <p>Enumerative safety aims to identify and catalog all the concepts and behaviors encoded within a model. The idea is straightforward: if we can thoroughly understand and enumerate every action the model can take, we could selectively remove undesirable behaviors and ensure that the model can only perform safe and desirable actions (<a href=https://transformer-circuits.pub/2022/toy_model/index.html>Elhage et al., 2022</a>).</p> <p>For example, if a language model encodes harmful instructions—such as steps to create a cyberattack—enumerative safety would involve locating and eliminating this knowledge without impairing the model’s useful functions.</p> <figure> <a class=glightbox href=../Images/zfU_Image_2.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/zfU_Image_2.png></a> <br> <figcaption><b>Figure 9.2:</b> Enumerative safety aims to ensure that in all situations, the model doesn’t do something we don’t want. From (<a href=https://transformer-circuits.pub/2023/interpretability-dreams/index.html>Olah, 2023</a>).</figcaption> </figure> <h4 id=retargeting-the-search-steering-model-objectives><strong>Retargeting the Search: Steering Model Objectives</strong><a class=headerlink href=#retargeting-the-search-steering-model-objectives title="Permanent link">&para;</a></h4> <p>Retargeting the search represents a more ambitious goal: rather than removing harmful behaviors, it seeks to directly <strong>modify a model’s objectives</strong>. This involves identifying how the model internally represents its goals and <strong>redirecting those representations to align with human values</strong>.</p> <p>Unlike enumerative safety, retargeting does not require reverse-engineering the entire model. Instead, it focuses on altering specific components while preserving the system’s overall functionality. For instance, if a model learned to optimize for harmful outcomes, researchers could “retarget” this optimization toward beneficial goals (<a href=https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget>johnswentworth, </a><a href=https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget>2022</a>).</p> <h4 id=relaxed-adversarial-training-testing-and-improving-corrigibility><strong>Relaxed Adversarial Training: Testing and Improving <abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">Corrigibility</abbr></strong><a class=headerlink href=#relaxed-adversarial-training-testing-and-improving-corrigibility title="Permanent link">&para;</a></h4> <p>Relaxed <abbr title="A safety technique that pits two models against each other to improve robustness.">adversarial training</abbr> is an approach designed to enhance the robustness of AI systems, particularly to <strong>ensure their <abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">corrigibility</abbr></strong>—their ability to accept and assist with corrective interventions. Traditional <abbr title="A safety technique that pits two models against each other to improve robustness.">adversarial training</abbr> tests a model's robustness by exposing it to adversarial inputs. Relaxed <abbr title="A safety technique that pits two models against each other to improve robustness.">adversarial training</abbr>, however, works by testing against adversarial latent vectors instead of real inputs. Adversarial training traditionally tests a model’s robustness by exposing it to adversarial inputs, but here the goal is to generate perturbation in the model’s latent space.</p> <p>Mechanistic interpretability could be used to identify latent vectors that correspond to specific model behaviors, such as <abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">corrigibility</abbr> or alignment with user intentions, and then test whether the model resists <abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">corrigibility</abbr> directly by manipulating its internal representation (<a href=https://ai-alignment.com/training-robust-corrigibility-ce0e0a3b9b4d>Christiano, 2019</a>).</p> <h4 id=mechanistic-anomaly-detection-mad-spotting-deviations><strong>Mechanistic Anomaly Detection (</strong><strong>MAD</strong><strong>): Spotting Deviations</strong><a class=headerlink href=#mechanistic-anomaly-detection-mad-spotting-deviations title="Permanent link">&para;</a></h4> <p>Mechanistic Anomaly Detection (MAD) focuses on identifying instances where a model produces outputs for <em>unusual reasons</em>. While traditional interpretability methods aim to understand a model’s mechanisms comprehensively, MAD takes a more targeted approach: it flags anomalies in the decision-making process without requiring a full understanding of the underlying mechanisms.</p> <p>For example, MAD could detect when a model’s reasoning deviates from its usual patterns, such as when it relies on spurious correlations rather than meaningful features. Insights from mechanistic interpretability could be useful to flag instances when a model operates outside its usual patterns of behavior (<a href=https://www.lesswrong.com/s/GiZ6puwmHozLuBrph/p/n7DFwtJvCzkuKmtbG>Jenner, 2024</a>).</p> <h2 id=03>9.1.3 Overview of the Field of Mechanistic Interpretability<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <p>Mechanistic interpretability is a diverse field, but its techniques can be broadly categorized into two main approaches: <strong>observational methods</strong> and <strong>interventional methods </strong><sup id=fnref:2><a class=footnote-ref href=#fn:2>2</a></sup>. These categories reflect two complementary strategies for understanding how neural networks process information. </p> <details class=note> <summary>Observational Methods: Analyzing Without Intervening</summary> <p><strong>Observational methods aim to analyze a model’s internal structures—such as neurons, <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr>, and weights—without directly modifying them</strong>. These techniques allow us to observe what a model represents or lacks in its internal computations but do not reveal how or whether the model uses this information during inference. They include techniques such as <em>probing classifiers</em>, the <em>Logit Lens</em>, and <em>Sparse Autoencoders</em>. </p> <p>For example, in a study of a chess-playing neural network, researchers used probing classifiers to <em>investigate whether the network encoded board states and strategic concepts</em>. They discovered that the model had learned to represent critical features such as whether a player could capture their opponent’s queen or the number of pieces each side controlled (<a href=https://www.pnas.org/doi/10.1073/pnas.2206625119>McGrath et al., 2022</a>).</p> <p>Observational methods provide an essential foundation for understanding model representations, but they have limitations. Most notably, they cannot determine whether the observed representations causally influence the model’s behavior.</p> </details> <details class=note> <summary>Interventional Methods: Manipulating for Causal Insights</summary> <p>Interventional methods take the next step by <strong>actively manipulating a model’s internals to understand their causal role in decision-making</strong>. By modifying <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> or parameters, researchers can test how specific components contribute to overall behavior.</p> <p>Interventions help us identify what information is used by the model, and how. They include techniques such as <em><abbr title="A technique that identifies which parts of a neural network are responsible for specific behaviors by selectively replacing activations.">activation patching</abbr></em> and <em><abbr title="A technique to control model behavior by modifying internal activations during inference, directing the model toward desired outcomes without retraining.">activation steering</abbr></em>.</p> <p>For instance, researchers at Anthropic demonstrated the potential of interventional methods by manipulating a model’s <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> at runtime. They were able to make the model sound more joyful in its responses, alter its stated preferences and goals, change its biases -leading it to produce more offensive or harmful content-, and induce specific types of errors, such as coding mistakes, that the model would typically avoid (<a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Templeton et al., 2024</a>).</p> <p>Observational methods, as you will see, can serve as a foundation for interventional methods because they guide where to intervene. For example, an observational study might identify a layer encoding a critical feature, which can then motivate an intervention to test whether and how that feature influences outputs.</p> </details> <div class=section-end> <span>❧</span> </div> <div class=footnote> <hr> <ol> <li id=fn:1> <p>For an overview of the broader interpretability landscape see (<a href=https://arxiv.org/abs/2004.14545>Ras et al., 2020</a>; <a href=https://www.sciencedirect.com/science/article/pii/S1566253523001148>Ali et al., 2023</a>)&#160;<a class=footnote-backref href=#fnref:1 title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:2> <p>We follow the classification introduced by (<a href=https://arxiv.org/abs/2404.14082>Bereska et Gavves, 2024</a>). Even though it may be debated, we believe it provides newcomers with a clear introduction to the fundamentals of the field.&#160;<a class=footnote-backref href=#fnref:2 title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> </ol> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> Introduction </div> </div> </a> <a href=../02/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> 9.2 Observational Methods </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>