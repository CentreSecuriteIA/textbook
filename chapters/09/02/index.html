<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link href=https://ai-safety-atlas.com/chapters/09/02/ rel=canonical><link href=../01/ rel=prev><link href=../03/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>9.2 Observational Methods - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#02 class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 9.2 Observational Methods </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../07/ class=md-nav__link> <span class=md-ellipsis> 07 - Generalization </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_10 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 09 - Interpretability </span> </a> <label class="md-nav__link " for=__nav_2_1_10 id=__nav_2_1_10_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_10_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_10> <span class="md-nav__icon md-icon"></span> 09 - Interpretability </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01/ class=md-nav__link> <span class=md-ellipsis> 9.1 What is Interpretability ? </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 9.2 Observational Methods </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 9.2 Observational Methods </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.1 Feature Visualization </span> </a> <nav class=md-nav aria-label="9.2.1 Feature Visualization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-a-feature class=md-nav__link> <span class=md-ellipsis> What is a Feature? </span> </a> </li> <li class=md-nav__item> <a href=#how-is-a-feature-visualization class=md-nav__link> <span class=md-ellipsis> How is a Feature Visualization? </span> </a> </li> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.1.1 Feature Visualization Method </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.2.1.2 Circuits </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.2.1.3 Polysemantic Neurons </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.2.2 Logit Lens </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.2.3 Probing classifiers </span> </a> <nav class=md-nav aria-label="9.2.3 Probing classifiers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.3.1 Probing in Practice: Case Studies and Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 9.2.4 Superposition </span> </a> <nav class=md-nav aria-label="9.2.4 Superposition"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.4.1 Experiments on Toy Models Support the Superposition Hypothesis </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 9.2.5 Sparse Autoencoders </span> </a> <nav class=md-nav aria-label="9.2.5 Sparse Autoencoders"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.5.1 Limitations and open research directions </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../03/ class=md-nav__link> <span class=md-ellipsis> 9.3 Interventional Methods </span> </a> </li> <li class=md-nav__item> <a href=../04/ class=md-nav__link> <span class=md-ellipsis> 9.4 Automating and Scaling Interpretability </span> </a> </li> <li class=md-nav__item> <a href=../05/ class=md-nav__link> <span class=md-ellipsis> 9.5 Critiques of Interpretability </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.1 Feature Visualization </span> </a> <nav class=md-nav aria-label="9.2.1 Feature Visualization"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#what-is-a-feature class=md-nav__link> <span class=md-ellipsis> What is a Feature? </span> </a> </li> <li class=md-nav__item> <a href=#how-is-a-feature-visualization class=md-nav__link> <span class=md-ellipsis> How is a Feature Visualization? </span> </a> </li> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.1.1 Feature Visualization Method </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.2.1.2 Circuits </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.2.1.3 Polysemantic Neurons </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 9.2.2 Logit Lens </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 9.2.3 Probing classifiers </span> </a> <nav class=md-nav aria-label="9.2.3 Probing classifiers"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.3.1 Probing in Practice: Case Studies and Limitations </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 9.2.4 Superposition </span> </a> <nav class=md-nav aria-label="9.2.4 Superposition"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.4.1 Experiments on Toy Models Support the Superposition Hypothesis </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#05 class=md-nav__link> <span class=md-ellipsis> 9.2.5 Sparse Autoencoders </span> </a> <nav class=md-nav aria-label="9.2.5 Sparse Autoencoders"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 9.2.5.1 Limitations and open research directions </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=02>9.2 Observational Methods<a class=headerlink href=#02 title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <i class="fas fa-clock"></i> 34 min read </div> <div class=meta-item> <i class="fas fa-file-alt"></i> 6795 words </div> </div> <h2 id=01>9.2.1 Feature Visualization<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <p>Feature visualization is one of the first observational methods in mechanistic interpretability. It allows researchers to explore the features that a vision model learns and uses at different layers. In this section, we’ll explain in detail what feature visualization is, and the discoveries it enabled.</p> <h4 id=what-is-a-feature><strong>What is a Feature?</strong><a class=headerlink href=#what-is-a-feature title="Permanent link">&para;</a></h4> <p><strong>A feature is a pattern of the input data that the network learns to detect. Models have features because they help break down complex inputs, such as images or text, into interpretable components that the model can use to make predictions.</strong></p> <p>Vision models trained to do image classification commonly possess features corresponding to cats, dogs, cars, eyes, fruits, etc. Vision model features vary in complexity depending on the layer in which they appear. In the early layers, features typically represent simple, low-level patterns such as edges, colors, or textures in a vision model. As you move to deeper layers, the features become more abstract and complex, representing higher-level concepts like shapes, objects, or even specific entities like faces or animals. This <em>hierarchical structure of features</em> allows models to progressively process raw pixel data into higher-level concepts to ultimately classify the image. For instance, recognizing a car would start with detecting edges (low-level features), then specific shapes (like wheels), and eventually the entire car (high-level feature).</p> <p>In transformer models, researchers have identified features corresponding to specific concepts like DNA sequences, legal language, HTTP requests, or Hebrew text. For instance, when a transformer encounters a DNA sequence, neurons encoding the “DNA feature” activate strongly in response. Similarly, given a sentence like “The court ruled in favor of the defendant because…,” features linked to legal language and sentence structures common in legal contexts may activate, helping the model predict a follow-up involving reasoning or justification, such as “…the evidence presented was insufficient.”</p> <p>These features are essential building blocks that models use to make sense of input data and make predictions.</p> <h4 id=how-is-a-feature-visualization><strong>How is a Feature Visualization?</strong><a class=headerlink href=#how-is-a-feature-visualization title="Permanent link">&para;</a></h4> <p><strong>Feature visualization</strong> is a technique that helps us investigate the inner representations of Convolutional Neural Networks (<abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>), and observe the patterns that a model has learned to recognize. It generates images that maximize the activation of specific neurons, <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> (outputs of a convolutional layer), or even entire layers in a model (<a href=https://distill.pub/2020/circuits/ >Cammarata et al., 2020</a>).</p> <figure> <a class=glightbox href=../Images/MJG_Image_3.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/MJG_Image_3.png></a> <br> <figcaption><b>Figure 9.3:</b> Examples of feature visualizations. It seems that the network has learned to represent and detect baseballs, animal faces, clouds, and buildings, but feature visualizations have to be interpreted and may not be detecting what we initially think. From (<a href=https://distill.pub/2017/feature-visualization/ >Olah et al., 2017</a>).</figcaption> </figure> <p>Early research in neuroscience aimed to understand the brain by identifying which images strongly excited specific neurons. This helped neuroscientists discover brain areas dedicated to identifying faces, movement, natural scenes, etc. <strong>Feature visualization </strong>can be thought of as a somewhat similar approach used to <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr><strong> </strong>(<a href=https://distill.pub/2017/feature-visualization/ >Olah et al., 2017</a>). </p> <p>These learned features—whether simple patterns like edges or complex objects—form the building blocks that enable models to understand and process input data. However, <strong>features don't operate in isolation. As the network processes information, features interact and combine in structured ways, often forming more complex units known as circuits. A circuit is a group of interconnected features that work together to perform a specific function.</strong></p> <p>For example, here is a circuit that recognizes a car in the mid-layers of a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>. This circuit combines lower-level features such as windows, car bodies, and wheels to detect the presence of a car in an image. The circuit allows the model to recognize the entire object, even though no single feature on its own can do so.</p> <figure> <a class=glightbox href=../Images/dme_Image_4.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/dme_Image_4.png></a> <br> <figcaption><b>Figure 9.4:</b> A car circuit in a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>. On the left, three <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> from layer 4b are represented by their feature visualizations. One map appears to detect windows, another car bodies, and the third wheels. These three <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> are connected to a <abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr> in layer 4c, represented by the visualization on the right, through the convolutional kernels shown in the middle. The window, car body and wheel features get assembled to form a full car detector circuit. From (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</figcaption> </figure> <details class=note> <summary>Vocabulary Reminder</summary> <ul> <li> <p>A <strong>feature</strong> refers to a specific pattern or characteristic that the network learns to detect. These features are the fundamental units that models use to process information and make decisions.</p> </li> <li> <p><strong>Feature visualization</strong> is a method that enables us to identify the features learned by a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>, which<strong> </strong>can help us understand how it processes information and makes decisions. </p> </li> <li> <p>A <strong>circuit</strong> is a group of interconnected features that work together to perform a specific function. Circuits are essentially higher order features, that are recursively composed of lower order features. The notion of circuit applies to any model architecture, including LLMs. Identifying circuits that perform specific functions in models is one area of research in mechanistic interpretability. </p> </li> <li> <p>A <strong><abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr></strong> in a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr> is the output of a convolutional layer. It’s also called a channel. Feature visualization is often applied at the scale of <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> - instead of individual neurons or entire layers - to understand which features they encode.</p> </li> </ul> <p><figure markdown=span> <a class=glightbox href=../Images/70Y_Image_5.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/70Y_Image_5.png></a> <figcaption markdown=1><b>Figure 9.5:</b> Some examples of feature visualizations on a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr> trained for image classification. Each image corresponds to a <abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr>. Certain <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> are sensitive to patterns with edges, others are sensitive to different kinds of textured patterns, or objects like eyes, dog faces, or legs. From (<a href=https://distill.pub/2017/feature-visualization/ >Olah et al., 2017</a>).</figcaption> </figure></p> </details> <h3 id=01>9.2.1.1 Feature Visualization Method<a class=headerlink href=#01 title="Permanent link">&para;</a></h3> <div class="admonition warning"> <p class=admonition-title>Below is extra detail provided for those interested. It can be safely skipped.</p> </div> <figure> <a class=glightbox href=../Images/AbZ_Image_6.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/AbZ_Image_6.png></a> <br> <figcaption><b>Figure 9.6:</b> Overview of the feature visualization process. Given a neuron (or a set of neurons) in a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>, feature visualization generates an image that highly activates it, starting from a random image, and through successive optimization steps. The optimized image illustrates what kind of pattern one of the <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr> in the fourth layer of InceptionV1 is sensitive to (<a href=https://distill.pub/2017/feature-visualization/ >Olah et al., 2017</a>).</figcaption> </figure> <p>Generating a feature visualization involves:</p> <ol> <li> <p><strong>Select a target</strong>: Choose a neuron, <abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr>, or layer for visualization. Most feature visualizations are performed on <abbr title="Feature maps are the output of a layer in a convolutional neural network.">feature maps</abbr>.</p> </li> <li> <p><strong>Start with a random input image</strong>: Begin optimization from a random noise image. It will be adjusted to maximize the activation of the target neuron or filter.</p> </li> <li> <p><strong>Compute the gradient. The objective is to update the image such that it maximizes the activation of the neuron/<abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr></strong>: Use backpropagation to calculate how to modify the image to increase the activation of the target. </p> </li> <li> <p><strong>Optimize the image</strong>: Apply gradient ascent iteratively to adjust the image.</p> </li> <li> <p><strong>Repeat the gradient ascent steps</strong>: Slightly adjust the image each time to better activate the neuron/<abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr>.</p> </li> <li> <p><strong>Visualize the result</strong>: The final image reveals the patterns or structures the target has learned to detect.</p> </li> </ol> <figure> <a class=glightbox href=../Images/XL5_Image_7.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/XL5_Image_7.png></a> <br> <figcaption><b>Figure 9.7:</b> Feature visualizations can be produced for individual neurons or groups of neurons, such as a <abbr title="A feature map is the output of a layer in a convolutional neural network.">feature map</abbr>, or even an entire layer. From (<a href=https://distill.pub/2017/feature-visualization/ >Olah et al., 2017</a>).</figcaption> </figure> <h3 id=02>9.2.1.2 Circuits<a class=headerlink href=#02 title="Permanent link">&para;</a></h3> <p><strong>Each layer in a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr> progressively extracts increasingly complex features from the image</strong>. Neurons in early layers respond to rudimentary and abstract patterns such as curves, angles, and small shapes (similar to the first layers of the human visual cortex!). As we move deeper into the network, neurons detect more complex and specific objects, such as eyes, animals, cars, etc (<a href=https://distill.pub/2020/circuits/zoom-in/ >Olah et al., 2020</a>). Interestingly, some of these "neuron families" recur across different model architectures and training conditions (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</p> <figure> <a class=glightbox href=../Images/onG_Image_8.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/onG_Image_8.png></a> <br> <figcaption><b>Figure 9.8:</b> Curve detectors are universally found in early layers of <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr>, they exist in different orientations and colors and collectively span all orientations. Each curve detector responds to a wide variety of curves, in different orientations. From (<a href=https://distill.pub/2020/circuits/zoom-in/ >Olah et al., 2020</a>).</figcaption> </figure> <p><abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr> also commonly learn <strong>high-low frequency detectors</strong> in their early layers.</p> <figure> <a class=glightbox href=../Images/y6P_Image_9.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/y6P_Image_9.png></a> <br> <figcaption><b>Figure 9.9:</b> High-low frequency detectors look for low-frequency patterns on one side of their receptive field, and high-frequency patterns on the other side. They exist in different orientations and colors. From (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</figcaption> </figure> <p>High-low frequency detectors assemble in deeper layers to form <strong>boundary detectors</strong>.</p> <figure> <a class=glightbox href=../Images/LCp_Image_10.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/LCp_Image_10.png></a> <br> <figcaption><b>Figure 9.10:</b> A boundary detector neuron formed in the third layer of a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr> (shown at the bottom left with its feature visualization). The top row shows feature visualizations from neurons in the second layer and the kernels connecting them to the third layer. New neurons form by combining cues from more elementary neurons in previous layers: here the boundary detector neuron forms by combining high-low frequency detector neurons, with edges detector neurons, color contrast detector neurons, etc. From (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</figcaption> </figure> <h3 id=03>9.2.1.3 Polysemantic Neurons<a class=headerlink href=#03 title="Permanent link">&para;</a></h3> <p>While feature visualization has enabled a deeper understanding of how <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr> represent information, it has also highlighted challenges like <strong><abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr></strong>. An intriguing phenomenon occurs when we look at the neurons following the car detector and that are strongly connected to it. Some of these neurons respond not only to images of cars but also to unrelated stimuli, such as images of dogs. This indicates that the "car feature" gets spread across multiple neurons that respond to seemingly unrelated inputs. These are known as <strong><abbr title="neurons that activate in response to a variety of distinct features.">polysemantic neurons</abbr></strong>—neurons that activate in response to a variety of distinct features. In contrast, <strong><abbr title="neurons that respond to just one specific feature or stimulus.">monosemantic neurons</abbr></strong> respond to just one specific feature or stimulus.</p> <figure> <a class=glightbox href=../Images/FqV_Image_11.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/FqV_Image_11.png></a> <br> <figcaption><b>Figure 9.11:</b> On the left is the car detector circuit from the previous figure. After distinct concepts are formed, they become entangled in <abbr title="neurons that activate in response to a variety of distinct features.">polysemantic neurons</abbr>, such as one neuron that responds to both car and dog images. From (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</figcaption> </figure> <figure> <a class=glightbox href=../Images/PU4_Image_12.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/PU4_Image_12.png></a> <br> <figcaption><b>Figure 9.12:</b> Multiple feature visualization performed on a polysemantic neuron that responds to images of cars, as well as cat faces, and cat legs. From (<a href=https://distill.pub/2020/circuits/early-vision/ >Olah et al., 2020</a>).</figcaption> </figure> <p><strong>Polysemantic neurons are very common</strong>. For example, in a small language model, we can find a neuron that responds to English dialogues, Korean texts, HTTP requests, and academic citations simultaneously (<a href=https://transformer-circuits.pub/2023/monosemantic-features>Bricken et al., 2023</a>). This means that each neuron does not correspond to one specific feature. Therefore, reasoning about a network's behavior based on individual neurons is misleading. Neurons are not the fundamental units to focus on when trying to understand models. </p> <p>Polysemanticity poses a challenge for interpretability because it requires understanding how features are encoded across multiple neurons, rather than assuming each neuron represents a discrete unit of meaning. Identifying how these distributed features are encoded is an active area of research in interpretability.</p> <p><strong>The leading hypothesis to explain why <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> arises in neural networks is called the <abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr></strong>. Large models need to learn a huge number of features to perform effectively, likely more than the number of neurons they have. As a result, models cannot assign each feature to a single neuron. Instead, they must encode features in a more compressed manner. <strong>The <abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr> suggests that models represent more features than they have neurons by encoding multiple features per neuron, with these features oriented in nearly orthogonal directions.</strong> In other words, models compress information by overlapping features across multiple neurons.</p> <details class=note> <summary>Polysemanticity vs Superposition</summary> <p>The distinction between <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> and the <abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr> is important (<a href=https://arxiv.org/abs/2404.14082>Bereska et Gavves, 2024</a>):</p> <ul> <li> <p><strong>Polysemanticity</strong> refers to the <em>empirical phenomenon</em> where a neuron represents or responds to multiple unrelated features,</p> </li> <li> <p><strong>The <abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr></strong>, on the contrary, generally refers to an <em>hypothesis</em> that tries to explain <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr>. It suggests that when models have more features to represent than they have neurons to represent them, they must compress these features into the limited space. This compression forces features to overlap across neurons. This supposedly explains why we observe neurons responding to multiple, seemingly unrelated features. While superposition inherently leads to <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr>, <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> itself doesn’t always imply superposition, as <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> could theoretically arise from other mechanisms.</p> </li> </ul> </details> <p><strong>Understanding and addressing <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> is an active research area. Various directions are being explored</strong>:</p> <ul> <li> <p><strong>Sparse Representations:</strong> Designing networks to use sparse representations (where fewer neurons are active at a time) may reduce <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> (<a href=https://transformer-circuits.pub/2022/solu/index.html>Elhage et al., 2022</a>). This approach has not been widely explored because it comes with significant performance trade-offs.</p> </li> <li> <p><strong>Feature Disentanglement:</strong> Some approaches involve decomposing complex neuron <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> to isolate individual features. One promising approach in that line of research, known as Sparse Autoencoders, is a technique that "unfolds" the network and separates out individual features (<a href=https://transformer-circuits.pub/2023/monosemantic-features>Bricken et al., 2023</a>). It is explained more in depth in the <em>Sparse Autoencoders</em> section.</p> </li> </ul> <p><strong>Feature visualization has led to several key insights about how vision neural networks operate:</strong></p> <ul> <li> <p><strong>Hierarchical structure</strong>: Neural networks learn features hierarchically, with early layers detecting simple patterns and deeper layers composing these into complex objects. For instance, curve detectors in early layers combine into boundary detectors in mid-layers and eventually into full object detectors in deeper layers.</p> </li> <li> <p><strong>Circuits</strong>: Features interact to form "circuits"—groups of interconnected features that collectively perform a specific function. For example, a circuit for detecting cars might combine features like wheels, windows, and body shapes into a cohesive representation of a car.</p> </li> <li> <p><strong>Polysemanticity</strong>: Feature visualization has revealed the phenomenon of <strong><abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr></strong>, where individual neurons or features in a neural network respond to multiple, seemingly unrelated concepts. This overlap complicates our ability to assign clear and interpretable roles to individual neurons.</p> </li> <li> <p><strong>Universality</strong>: Some features, such as edge or curve detectors, are universal across models and architectures. This suggests that certain features are fundamental to visual processing, regardless of the specific task or dataset.</p> </li> </ul> <h2 id=02>9.2.2 Logit Lens<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <p>The <strong>Logit Lens</strong> (<a href=https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/ >nostalgebraist, 2020</a>) is one of the first tools developed to look inside transformers. It enables us to observe how a transformer refines its predictions layer by layer —allowing us to<strong> see not just the final output but the evolving "thought process" the model undergoes as it makes a prediction</strong>.</p> <p>Transformers are trained to predict the next token in a sequence. They do this by transforming the inputs layer by layer, with each layer adding new information to improve the prediction. The Logit Lens enables us to “translate” each layer's internal representation back into tokens. By seeing what the model “predicts” at each layer, we can trace how its predictions evolve from a rough guess in the initial layers to a refined choice in the final one. However, it’s worth noting that while the Logit Lens lets us see the intermediate predictions at each layer, it doesn’t explain the <em>mechanism</em> of transformation. <strong>The Logit Lens is an inherently observational tool —it reveals what is the most probable token at the end of each layer but doesn’t allow us to understand </strong><strong><em>why</em></strong><strong> this precise token is predicted</strong>.</p> <div class="admonition warning"> <p class=admonition-title>Below is extra detail provided for those interested. It can be safely skipped.</p> </div> <p>A transformer model is a powerful architecture for processing sequences of data, especially text. It is trained to predict the next word or subword in a sequence, referred to as <strong>tokens</strong>.</p> <p>The set of all tokens (words or subwords) that the model can recognize is called the <em>vocabulary</em>. For example, GPT-2 has a vocabulary of 50,257 tokens. More precisely, <strong>a transformer takes a sequence of tokens as input and is trained to predict the next token in this sequence, outputting a probability distribution over the entire vocabulary</strong>.</p> <p>Internally, a transformer consists of multiple stacked <strong>layers</strong>, each containing two sublayers: an MLP (multi-layer perceptron) and attention heads. For understanding the Logit Lens, we don’t need to go into the details of how these sublayers function.</p> <p>The intermediate layers are connected through what’s known as the <strong>residual stream</strong>. The residual stream is a pathway that carries information from the input to the output, allowing it to flow through all layers of the model.</p> <figure> <a class=glightbox href=../Images/Zzh_Image_13.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Zzh_Image_13.png></a> <br> <figcaption><b>Figure 9.13:</b> A minimalist illustration of a transformer with a single layer. Transformers cannot directly manipulate tokens, so they embed tokens into numerical vectors, an operation depicted in the bottom grey box. After embedding, the tokens are represented as vectors within the residual stream. The first sublayer (attention heads, represented by the two side-by-side boxes on the left) reads these embedded tokens, applies a transformation, and adds its output back into the residual stream. The second sublayer (MLP) does the same. Finally, to output a token, the embedded tokens must be converted back to the vocabulary space through an unembedding operation (depicted in the top box). The residual stream enables each layer to make incremental adjustments to the model’s predictions by accumulating and refining the information passed through each transformation. From (<a href=https://transformer-circuits.pub/2021/framework/index.html>Elhage et al., 2021</a>).</figcaption> </figure> <p><strong>The essential idea behind the Logit Lens is that the unembedding operation, typically applied only after the last layer, can be applied at any point in the residual stream. After each intermediate layer, the residual stream can be unembedded—that is, converted back into the token space—allowing us to observe which token is currently the most probable.</strong></p> <p><strong>Connection between</strong><strong> the Logit Lens and Feature Visualization in <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr></strong>. In the previous section on <strong>Feature Visualization</strong>, we saw that <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr> build up their understanding of an image layer by layer, detecting simple patterns like edges in early layers and more complex shapes or objects in later ones. Feature visualization allows us to interpret these progressive transformations by displaying the visual patterns that different neurons respond to. The Logit Lens can be seen as a parallel interpretability tool for transformers. Instead of visualizing image features, the Logit Lens lets us see the model’s step-by-step guesses for the next token.</p> <p>The figure shows how using the Logit Lens looks in practice.</p> <figure> <a class=glightbox href=../Images/d0T_Image_14.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/d0T_Image_14.png></a> <br> <figcaption><b>Figure 9.14:</b> An example of the Logit Lens used on GPT-2 when it’s trying to predict the next token of the sequence: “Specifically, we train GPT-3, an”. Input tokens are written at the bottom, and correct outputs at the top. Layers stack from bottom to top. The first token at the top, ",", corresponds to the token the model should predict when given as input only the first token: "Specifically". The second token at the top, "we", corresponds to the token the model should predict when given as input only the two first tokens: "Specifically, ". This is the reason why there is a shift of one position between tokens at the bottom and tokens at the top. (*) indicates that the model correctly predicted the next token. Each cell contains the model’s top guesses at different layers. The color scale indicates the associated logit value. The higher the logit, the more confident in its prediction the model is. From (<a href=https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens>nostalgebraist, 2020</a>).</figcaption> </figure> <p>The figure shows that when GPT-2 tries to predict the next token in the sequence “Specifically, we train GPT-3, an”, it predicts “enormous” in its early layers, then “massive”, “single”, and “N” in its final layer.</p> <figure> <a class=glightbox href=../Images/Pnx_Image_15.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Pnx_Image_15.png></a> <br> <figcaption><b>Figure 9.15:</b> Overview of how the Logit Lens works. Read from bottom to top. A detailed explanation is provided in the following paragraphs. Object’s shapes are indicated between brackets.</figcaption> </figure> <p>A transformer takes as an <strong>input a sequence of tokens</strong>. Each token is an element from a vocabulary (typically of size <strong>n</strong><sub>vocab</sub>), so each text token can be associated with an integer value ranging from 0 to <strong>n</strong><sub>vocab</sub>. This association can also be thought of as representing the token as a basis vector within a space of dimension <strong>n</strong><sub>vocab</sub>, denoted the <strong>vocabulary space</strong>. Each basis vector in the vocabulary space is associated with one token.</p> <p>A transformer is trained to predict the next token of the input sequence, so its <strong>output is a probability distribution over the vocabulary</strong>.</p> <p>Transformers convert each token into vectors <strong>n</strong><sub>vocab</sub>, which is known as the <strong>embedding space</strong>. The embedding space dimension is denoted <strong>d</strong><sub>model</sub>. Projected tokens are called <strong>embedding vectors</strong>, or simply <strong>embeddings</strong>. In some contexts, they may also be referred to as <em>hidden <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr></em>. The <strong>embedding matrix</strong> converts tokens from the vocabulary space into the embedding space. It is like a lookup table that associates elements in the vocabulary space with their corresponding counterparts in the embedding space.</p> <p>Once tokens have been converted into the embedding space, they <strong>flow through the successive layers of the transformer</strong>. Similar to how input images undergo transformations across the layers of a <abbr title="Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.">CNN</abbr>, embedding vectors are transformed as they pass through the network. The <strong>Logit Lens</strong> precisely focuses on those intermediate representations that transformers build.</p> <p>Finally, to output a probability distribution over the vocabulary, the final embedding vector has to be converted back into vocabulary space. Conversion from the embedding space to the vocabulary space can be done through multiplication by a matrix of dimension [<strong>d</strong><sub>model</sub>, ~ <sub><strong>n</strong></sub>vocab~]. This operation is called the <strong>unembedding</strong>. The result of the unembedding is a <strong>logits</strong><strong> vector</strong> of size <strong>n</strong><sub>vocab</sub>. The logits can then be converted into probabilities over the vocabulary using the softmax function.</p> <p><strong>The essential intuition behind the Logit Lens is that the unembedding operation, typically applied after the last layer, can also be applied after each intermediate layer</strong>. Embedding vectors are modified by each layer of the network, but their dimensions remain the same (<strong>d</strong><sub>model</sub>). Thus, after any layer, embedding vectors can be converted back to the vocabulary space, and one can get an idea of how the network refines its prediction across layers.</p> <h2 id=03>9.2.3 Probing classifiers<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <p><strong>Probing</strong> is a technique used to analyze neural networks and <strong>understand </strong><strong><em>which</em></strong><strong> representations or concepts they have learned and </strong><strong><em>where</em></strong> (<a href=https://aclanthology.org/2022.cl-1.7/ >Belinkov, 2022</a>). Probing techniques can be applied to a range of models, from transformers to <abbr title="Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.">CNNs</abbr>, to investigate whether specific information or properties are encoded in a model’s intermediate representations.</p> <p><strong>What is a probe?</strong> A <em>probe</em> is a lightweight model, often a linear classifier, trained to detect whether a specific concept or feature is represented in the <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> of a neural network. In a probing setup, researchers analyze <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> (the intermediate outputs) from layers within a network to see if these <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> encode information about a particular property or concept. For example, probes may be used to determine whether a language model’s <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> contain information about grammar rules or whether a chess-playing model like AlphaZero encodes strategic knowledge about the game, where this knowledge is located within the network, and when it is acquired during training (<a href=https://arxiv.org/abs/2305.01610>Gurnee et al., 2023</a>, <a href=https://www.pnas.org/doi/10.1073/pnas.2206625119>McGrath et al., 2022</a>).</p> <p>One well-known example of probing was applied to <strong>AlphaZero</strong>, the neural network trained to play chess that famously defeated top human players. Researchers used probes to investigate whether AlphaZero internally represents certain strategic concepts about chess, such as “Can the opponent capture my queen?” or “Is there a checkmate threat within one move?” (<a href=https://www.pnas.org/doi/10.1073/pnas.2206625119>McGrath et al., 2022</a>).</p> <p><strong>Probing classification</strong> is the process of training classifiers on the network’s intermediate <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> to identify if specific properties are encoded. The steps of probing classification are as follows:</p> <ol> <li> <p><strong>Choose a Property or Concept</strong>: Define a specific concept to explore, such as “Can the opponent capture my queen?”.</p> </li> <li> <p><strong>Generate or Select Input Data</strong>: Create a dataset with examples that vary in terms of the target property. For instance, in chess, we might create board configurations where “the player can capture the opponent’s queen” is either true or false.</p> </li> <li> <p><strong>Record Intermediate Activations</strong>: Feed this dataset through the model and record the <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> of neurons at different layers.</p> </li> <li> <p><strong>Train a Classifier on the Activations</strong>: Use the recorded <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> as input features to train a classifier (probe) that distinguishes between different classes based on the concept (e.g., true vs. false input prompts).</p> </li> <li> <p><strong>Evaluate the Probe’s Accuracy</strong>: If the probe achieves high accuracy, this suggests that the target concept is strongly encoded in the recorded layer’s <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr>.</p> </li> </ol> <h3 id=01>9.2.3.1 Probing in Practice: Case Studies and Limitations<a class=headerlink href=#01 title="Permanent link">&para;</a></h3> <div class="admonition warning"> <p class=admonition-title>Below is extra detail provided for those interested. It can be safely skipped.</p> </div> <figure> <a class=glightbox href=../Images/CBu_Image_16.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/CBu_Image_16.png></a> <br> <figcaption><b>Figure 9.16:</b> Accuracy of two probes trained on AlphaZero’s intermediary <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr>, across layers and training steps The two two probes were trained on the two concepts: “Can the playing side capture their opponent’s queen?”, and “Could the opposing side checkmate the playing side in one move?”. The color represents the probe’s accuracy (the yellow color corresponds to a higher value, indicating that the feature is more represented). Here, the probes were trained at different stages of AlphaZero's training and from the <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> of different layers of the network (a block corresponds to one layer of AlphaZero). It can be seen that the more AlphaZero is trained, the more it represents these two concepts. The concept "has_mate_threat" seems to be represented quite homogeneously across AlphaZero's layers, while "can_capture_queen_opponent" appears to be more represented in the earlier layers. "The ability to predict has_mate_threat from AlphaZero’s <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> indicates that AlphaZero is not simply modeling its potential moves, but also its opponent’s potential moves and their consequences during position evaluation". From (<a href=https://www.pnas.org/doi/10.1073/pnas.2206625119>MacGrath et al., 2022</a>).</figcaption> </figure> <p><strong>Using linear probes for detecting prompts leading to dangerous behavior.</strong> The alignment team from Anthropic is currently working on a research agenda called “<em>sleeper agents</em>” where they deliberately train <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> models (<a href=https://arxiv.org/abs/2401.05566>Hubinger et al., 2024</a>) to test whether currently available techniques can detect misalignment. These "<em>sleeper agents</em>" are models with hidden backdoors introduced during training. One of the sleeper agents they trained writes secure code when the prompt indicates year “2023” but inserts vulnerabilities when the prompt indicates “2024”. Such dangerous outputs are hard to detect, and could easily be missed by a human overseer, this is why it would be extremely useful to develop an automatic detection method. Anthropic researchers have trained linear probes based on sleeper agents’ residual stream <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> that classify inputs as safe or dangerous with very high accuracy (<a href=https://www.anthropic.com/news/probes-catch-sleeper-agents>MacDiarmid et al., 2024</a>). Whether or not a prompt will trigger defection appears to be linearly encoded across a wide range of middle residual streams. However, they remain wary of this result and acknowledge that the surprisingly high accuracy of the probes may be due to the backdoor insertion, and that “natural defection” may not be detected as easily.</p> <p>It was also suggested to train probes on human-written scenarios of takeover attempts or catastrophic actions (<a href=https://www.alignmentforum.org/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off>Roger., 2023</a>) to detect such goals or intentions in models before they happen. Similarly to sleeper agents, it is unclear whether probes trained in artificial settings will generalize in real-world scenarios.</p> <p><strong>Limitations of probing classifiers</strong></p> <ul> <li> <p><strong>Correlation, not causation</strong>: Probing classifiers indicate that a concept (or a proxy of that concept) is encoded, but they don’t reveal whether this concept is actively used by the network during inference. The classifier’s high accuracy may reflect the ease with which it can detect patterns, not necessarily that the model relies on those patterns for decision-making. To discover causal effects we need to <strong>intervene</strong> in representations of the model instead of just <strong>observing</strong> them.</p> </li> <li> <p><strong>Identifying proxies rather than “true concepts”</strong>: Probing may detect patterns that act as proxies for the concept of interest, rather than the concept itself. For example, a probe trained to detect “legal language” might actually be detecting correlated cues (such as certain formal words or sentence structures) rather than a genuine understanding of legal terminology. This makes it challenging to interpret probes as definitive indicators of the exact concepts represented within the model.</p> </li> </ul> <h2 id=04>9.2.4 Superposition<a class=headerlink href=#04 title="Permanent link">&para;</a></h2> <p>To make sense of data, classify it, or make decisions, neural networks need to learn <strong>features</strong>—representations that capture meaningful patterns in the data. However, networks have a limited number of neurons to store a vast amount of information. Instead of assigning a single neuron to each feature, neural networks often "share" neurons across multiple features. This shared, overlapping storage is known as <strong>superposition</strong>.</p> <p>Superposition occurs because it allows the network to handle more features using fewer neurons, making it more <strong>memory-efficient</strong>. However, this efficiency comes at a cost: <strong><abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr></strong>. Polysemanticity means that a single neuron or component in the network represents multiple, often unrelated, features. For example, one neuron might activate in response to both "cats" and "cars," even though these concepts are entirely different. </p> <p><strong>Polysemanticity and Superposition</strong>. The vocabulary surrounding superposition and related concepts is often messy. The terms <strong>superposition</strong> and <strong><abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr></strong> are often used interchangeably, even though they refer to slightly different aspects of the same phenomenon in some contexts: superposition describes the overlapping storage of features, while <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> highlights the behavior of neurons that respond to multiple distinct features. Also keep in mind that <strong>superposition</strong> should not be confused with the <strong><abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr></strong>, which is a specific theory proposed to explain why and how superposition occurs in neural networks.</p> <p>This overlap creates a challenge for interpretability. Ideally, we might expect each neuron to have a clear and singular purpose—one neuron for "cats," another for "cars," and so on. In reality, many neurons respond to combinations of unrelated features, leading to <strong>entangled representations</strong>. This means that interpreting individual neurons in isolation often provides an incomplete or misleading picture (<a href=https://arxiv.org/abs/2104.07143>Bolukbasi et al., 2021</a>).</p> <p>Contrast this with a hypothetical network without <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr>: every neuron would correspond to a distinct feature, making the model much easier to interpret. However, such a design would require far more neurons to represent every feature individually, which is <strong>computationally inefficient</strong>, particularly in large-scale models. As a result, superposition is practically inevitable in modern neural networks.</p> <p>From an AI safety perspective, understanding superposition is critical. If we want to trace how a model makes decisions, we need to <strong>identify and disentangle these overlapping features</strong>. Only then can we retrace the reasoning behind its predictions and uncover what drives its behavior.</p> <p>Before diving into techniques for disentangling features, researchers first sought to understand how <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> arises. Key questions include:</p> <ul> <li> <p>What causes <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr> in neural networks?</p> </li> <li> <p>How does model architecture or the training process influence it?</p> </li> <li> <p>Can it be controlled or mitigated?</p> </li> </ul> <p>To answer these, researchers used <strong>toy models</strong>—simple, small-scale neural networks. Toy models allow for controlled experimentation and help researchers isolate and study phenomena like superposition without the complexity of larger systems. The following subsection details the toy models used to study <abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr>.</p> <h3 id=01>9.2.4.1 Experiments on Toy Models Support the Superposition Hypothesis<a class=headerlink href=#01 title="Permanent link">&para;</a></h3> <div class="admonition warning"> <p class=admonition-title>Below is extra detail provided for those interested. It can be safely skipped.</p> </div> <p>The <strong>Toy Models of Superposition</strong> paper introduces simplified models that help researchers study superposition in a controlled environment (<a href=https://transformer-circuits.pub/2022/toy_model/index.html>Elhage et al., 2022</a>). These models provide evidence supporting the <strong><abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr></strong>—a theory about how neural networks efficiently store and organize information.</p> <p><strong>The <abbr title="The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.">superposition hypothesis</abbr></strong> suggests that neural networks can represent more features or concepts than they have individual neurons by encoding information as linear combinations across multiple neurons. This means:</p> <ol> <li> <p><strong>Efficient compression</strong>: Neural networks can compress information by representing more features than there are available dimensions, optimizing memory use.</p> </li> <li> <p><strong>Distributed representation</strong>: Features are not exclusively tied to single neurons; instead, they are distributed across multiple neurons.</p> </li> <li> <p><strong>Non-orthogonal directions</strong>: Features are stored in directions that are not perfectly orthogonal in the network's activation space, which leads to overlaps and potential interference between concepts.</p> </li> </ol> <p>This paper is a cornerstone of mechanistic interpretability because it reveals fascinating phenomena about how neural networks organize and store information:</p> <ul> <li> <p><strong>Demonstrating superposition</strong>: The experiments show that superposition occurs and identify the conditions under which it arises.</p> </li> <li> <p><strong>Explaining mono- and <abbr title="neurons that activate in response to a variety of distinct features.">polysemantic neurons</abbr></strong>: The paper clarifies why some neurons specialize in a single feature (monosemantic) while others represent multiple features (polysemantic).</p> </li> <li> <p><strong>Phase transitions in training</strong>: It highlights a phase change<sup id=fnref:3><a class=footnote-ref href=#fn:3>1</a></sup> during training that determines whether features are stored in superposition.</p> </li> <li> <p><strong>Geometric feature organization</strong>: Features in superposition are arranged into geometric structures such as digons, triangles, pentagons, and tetrahedrons, providing insight into the network’s internal organization.</p> </li> </ul> <p>The following figure is a great illustration of superposition in a toy model. The toy model has <strong>2 dimensions</strong>, represented by the x and y axis in the figure below, but it needs to learn <strong>5 features</strong>. This means that the model needs to find a way to fit more information (5 features) into fewer dimensions (2-dimensional space). Each feature is given an importance, represented with colors. An important feature is a feature that has a significant impact on the model’s accuracy or loss function (if removing or weakening the representation of this feature causes a large drop in performance, it would be considered important). <strong>The key challenge in superposition is how to efficiently encode important features while minimizing overlap and interference between them</strong>.</p> <p>Each feature also has a <strong>sparsity</strong>. This refers to how often it is active in the dataset. A dense feature (low sparsity) is activated frequently, making it harder for other features to coexist in the same dimensions without interference.</p> <figure> <a class=glightbox href=../Images/GB7_Image_17.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/GB7_Image_17.png></a> <br> <figcaption><b>Figure 9.17:</b> How does a 2-dimension model encode 5 features as their sparsity increases? 0% sparsity means that features are very dense, or frequent. The model only encodes the two most important features in orthogonal dimensions. As sparsity increases and the important features are less frequently useful, the toy model encodes additional features in non-orthogonal directions. The intuition is that the less frequent the features are, the less likely two overlapping features are to be activated at the same time and cause interference. So the cost of interference between features is outweighed by the advantage of learning more features. At 90% sparsity the 5 features are represented as a pentagon. From (<a href=https://transformer-circuits.pub/2022/toy_model/index.html>Elhage et al., 2022</a>).</figcaption> </figure> <p>When encoding features, there is a tradeoff to make between the usefulness of having as many features as possible and low interference between them. Models embed their features into very complex geometric structures to reach optimal encoding. The figure below shows the different geometric structures that models use to encode features.</p> <figure> <a class=glightbox href=../Images/YW6_Image_18.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/YW6_Image_18.png></a> <br> <figcaption><b>Figure 9.18:</b> As features get sparser (less frequently activated), more of them can be encoded optimally and in more complex geometric structures. The first figure on the top left shows that when features are very dense, only the most important features are represented and they are organized in tetrahedrons. This model learns 28 features and encodes them in 7 tetrahedrons. On the second figure features are slightly sparser, more of them can be optimally encoded, and tetrahedrons are replaced by triangles and digons. This model learns 46 features encoded in triangles and digons. Superposition exhibits complex geometric structure. From (<a href=https://transformer-circuits.pub/2022/toy_model/index.html>Elhage et al., 2022</a>).</figcaption> </figure> <h2 id=05>9.2.5 Sparse Autoencoders<a class=headerlink href=#05 title="Permanent link">&para;</a></h2> <p>A major challenge in mechanistic interpretability is <strong><abbr title="The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.">polysemanticity</abbr></strong>, where a single neuron or feature represents multiple, unrelated concepts. Polysemanticity arises naturally in LLMs’ MLPs and residual streams, and makes it more complicated to identify which specific features influence a model’s outcome. <strong>Sparse AutoEncoders (SAES)</strong> are a promising approach for <strong>disentangling</strong><strong> features</strong> within a network (<a href=https://transformer-circuits.pub/2023/monosemantic-features>Bricken et al., 2023</a>, <a href=https://arxiv.org/abs/2406.04093>Gao et al., 2024</a>, <a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering>Templeton et al., 2024</a>). </p> <p><abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> are gaining popularity because they have shown promising results in separating out features. Features extracted using <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> can then be used to:</p> <ul> <li> <p><strong>Steer language models behavior</strong> away from undesirable outcomes (see section Activation Steering). (<a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering>Templeton et al., 2024</a>) found a bunch of safety-relevant features in Claude 3 Sonnet, including features for unsafe code, bias, sycophancy, deception and power seeking, and dangerous or criminal information. These features activate on text involving these topics and causally influence the model’s outputs when intervened upon.</p> </li> <li> <p><strong>Find more interpretable circuits directly made of features instead of model components. </strong>In particular, finding and understanding the circuits in which safety-relevant features are involved could be valuable<strong> </strong>(see section on <em>Automating and Scaling Interpretability</em>).</p> </li> </ul> <p><abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> are also great because they are trained in an <strong>unsupervised</strong> manner, which enables us to discover abstractions or associations formed by the model that we might not have anticipated beforehand. </p> <div class="admonition warning"> <p class=admonition-title>Below is extra detail provided for those interested. It can be safely skipped until the introduction of dictionary learning.</p> </div> <p><strong>What is an autoencoder?</strong> An <strong>autoencoder</strong> is a neural network designed to learn compressed representations of input data by encoding it into a lower-dimensional <strong><em>latent space </em></strong>and then reconstructing the original input from that representation. The latent space is the layer where the data is represented in a compressed or abstracted form, containing the key features necessary for reconstructing the input. This learned representation often captures the essential patterns or features in the data.</p> <figure> <a class=glightbox href=../Images/QiX_Image_19.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/QiX_Image_19.png></a> <br> <figcaption><b>Figure 9.19:</b> An autoencoder learns two transformations, represented by encoder weights and decoder weights, to compress input data into a lower-dimensional latent space and then reconstruct the original input from this representation.</figcaption> </figure> <p><strong>Why are <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> “sparse”?</strong> Autoencoders can vary in structure and purpose. <strong><abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> are autoencoders that introduce sparsity constraints on the <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> in the latent space</strong>, encouraging the model to use a limited number of latent neurons for each input. This “sparsity” forces the model to learn distinct and specific features, making the representations more interpretable.</p> <p><strong>How do <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> help disentangling features in transformers?</strong> The process of disentangling model <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> into interpretable features typically involves <strong>training a sparse autoencoder to reconstruct <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> from specific parts of a model, such as the MLP of a particular layer or a residual stream</strong>, with a <strong>latent space larger than the input<sup id=fnref:4><a class=footnote-ref href=#fn:4>2</a></sup>, so that each neuron in the latent space will hopefully be monosemantic - representing a single feature - and interpretable</strong>. </p> <figure> <a class=glightbox href=../Images/Ing_Image_20.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Ing_Image_20.png></a> <br> <figcaption><b>Figure 9.20:</b> Models <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> can be decomposed into features using a sparse autoencoder. This figure illustrates an <abbr title="Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.">SAE</abbr> trained to disentangle features in an MLP. From (<a href=https://transformer-circuits.pub/2023/monosemantic-features>Bricken et al., 2023</a>).</figcaption> </figure> <p><strong>Dictionary learning</strong>. After training a <abbr title="Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.">SAE</abbr>, each neuron in its latent space can be analyzed to understand which specific inputs or features it responds to. By identifying these responses, researchers can effectively build a “dictionary” of features, where each neuron corresponds to a distinct feature in the data. This process of training <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> on various layers and parts of a model to identify these features is known as <strong>dictionary learning</strong>.</p> <p>For example, in a study on Claude 3 Sonnet, safety-relevant features such as those for “unsafe code” or “error tokens” were identified using <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr>. Interestingly, increasing the activation of the unsafe code feature in the <abbr title="Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.">SAE</abbr> latent space and feeding back into the model the <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> reconstructed by the <abbr title="Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.">SAE</abbr> caused it to generate a buffer overflow vulnerability (<a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html#related-work-steering>Templeton et al., 2024</a>).</p> <figure> <a class=glightbox href=../Images/Fsn_Image_21.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/Fsn_Image_21.png></a> <br> <figcaption><b>Figure 9.21:</b> Examples of safety-relevant features extracted from Claude 3 Sonnet, such as features for unsafe code and error tokens. The color scale indicates the degree to which each feature is activated for each token, with darker orange indicating higher activation. The “code error” feature activates strongly on tokens that contain an error. The images shown correspond to examples that strongly activate the specific feature. From (<a href=https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html>Templeton et al., 2024</a>).</figcaption> </figure> <h3 id=01>9.2.5.1 Limitations and open research directions<a class=headerlink href=#01 title="Permanent link">&para;</a></h3> <p>Although promising, <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> are still early work with <strong>limitations</strong>: </p> <ul> <li> <p><strong>Incomplete understanding of model usage</strong>: Identifying model features doesn’t reveal <em>how</em> they are used during inference, we still have to find the circuits involving them.</p> </li> <li> <p><strong>Difficulty in feature interpretation</strong>: Not all features discovered by <abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> are easily interpretable; some features remain challenging to understand.</p> </li> <li> <p><strong>Lack of validation methods</strong>: Currently, there are limited methods to test the validity of feature interpretations. </p> </li> <li> <p><strong><abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> have poor reconstruction quality</strong>: Sparse autoencoders don’t reconstruct model <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> very well, which means that they don’t completely capture the behavior of our models. For instance, passing GPT-4’s <abbr title="The outputs produced by neurons or units in a neural network after processing input data through a layer.">activations</abbr> through an <abbr title="Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.">SAE</abbr> results in performance equivalent to a model trained with 10x less compute (<a href=https://arxiv.org/abs/2406.04093>Gao et al., 2024</a>).</p> </li> </ul> <p><abbr title="Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.">SAEs</abbr> present intriguing research questions for AI safety and interpretability:</p> <ul> <li> <p>What features activate during jailbreaks?</p> </li> <li> <p>What features need to activate or to remain inactive for a model to give advice on producing cyberattacks, bioweapons, etc. ?</p> </li> <li> <p>Can we use the feature basis to detect when fine-tuning a model increases the likelihood of undesirable behaviors?</p> </li> <li> <p>etc.</p> </li> </ul> <div class=section-end> <span>❧</span> </div> <div class=footnote> <hr> <ol> <li id=fn:3> <p>A <strong>phase change</strong> in neural network training refers to a sudden, qualitative shift in the behavior or structure of the model during the training process.&#160;<a class=footnote-backref href=#fnref:3 title="Jump back to footnote 1 in the text">&#8617;</a></p> </li> <li id=fn:4> <p>An autoencoder with a latent space larger than its input is called an <em>overcomplete</em> autoencoder.&#160;<a class=footnote-backref href=#fnref:4 title="Jump back to footnote 2 in the text">&#8617;</a></p> </li> </ol> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../01/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> 9.1 What is Interpretability ? </div> </div> </a> <a href=../03/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> 9.3 Interventional Methods </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>