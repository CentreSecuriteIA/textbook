<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="This section focuses on further exploring the concept of mesa-optimizers. There are various different types of mesa-optimizers that can result at the end of the training process. Ajeya Cotra in her post “Why AI alignment could be hard with modern deep learning” provided an intuitive example by exploring three archetypes that an AI might embody. She explains this through the analogy of a young child (humanity) who has inherited a company to run and has a choice of the following three types of candidates (AIs) to hire to run the inherited company:"><link href=https://ai-safety-atlas.com/chapters/07/03/ rel=canonical><link href=../02/ rel=prev><link href=../../08/ rel=next><link rel=icon href=../../../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.6.14"><title>7.3 - Deceptive alignment - AI Safety Atlas</title><link rel=stylesheet href=../../../assets/stylesheets/main.342714a4.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../overrides/css/base.css><link rel=stylesheet href=../../../overrides/css/buttons.css><link rel=stylesheet href=../../../overrides/css/tippy.css><link rel=stylesheet href=../../../overrides/css/tabs.css><link rel=stylesheet href=../../../overrides/css/typography.css><link rel=stylesheet href=../../../overrides/css/admonitions.css><link rel=stylesheet href=../../../overrides/css/navigation.css><link rel=stylesheet href=../../../overrides/css/metadata.css><link rel=stylesheet href=../../../overrides/css/figures.css><link rel=stylesheet href=../../../overrides/css/footer.css><link rel=stylesheet href=../../../overrides/css/section-end.css><link rel=stylesheet href=../../../overrides/css/scroll-buttons.css><link rel=stylesheet href=../../../overrides/css/home/base.css><link rel=stylesheet href=../../../overrides/css/home/hero.css><link rel=stylesheet href=../../../overrides/css/home/hero-animation.css><link rel=stylesheet href=../../../overrides/css/home/header.css><link rel=stylesheet href=../../../overrides/css/home/why.css><link rel=stylesheet href=../../../overrides/css/header.css><link rel=stylesheet href=../../../overrides/css/audio-player.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Libre+Baskerville:ital,wght@0,400;0,700;1,400&display=swap"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Crimson+Pro:wght@700;800;900&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cinzel:wght@600;700;800;900&display=swap"><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }
    .gscrollbar-fixer { padding-right: 15px; }
    .gdesc-inner { font-size: 0.75rem; }
    body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
    body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
    body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}</style><script src=../../../assets/javascripts/glightbox.min.js></script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#73-deceptive-alignment class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="AI Safety Atlas" class="md-header__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> AI Safety Atlas </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> 7.3 - Deceptive alignment </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=default data-md-color-primary=indigo data-md-color-accent=indigo aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=indigo aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Book </a> </li> <li class=md-tabs__item> <a href=../../../feedback/ class=md-tabs__link> Feedback </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="AI Safety Atlas" class="md-nav__button md-logo" aria-label="AI Safety Atlas" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> AI Safety Atlas </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Book </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Book </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Chapters </span> </a> <label class="md-nav__link " for=__nav_2_1 id=__nav_2_1_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_1_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1> <span class="md-nav__icon md-icon"></span> Chapters </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../01/ class=md-nav__link> <span class=md-ellipsis> 01 - Capabilities </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../02/ class=md-nav__link> <span class=md-ellipsis> 02 - Risks </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../03/ class=md-nav__link> <span class=md-ellipsis> 03 - Strategies </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../04/ class=md-nav__link> <span class=md-ellipsis> 04 - Governance </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../05/ class=md-nav__link> <span class=md-ellipsis> 05 - Evaluations </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../06/ class=md-nav__link> <span class=md-ellipsis> 06 - Specification </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_1_8 checked> <div class="md-nav__link md-nav__container"> <a href=../ class="md-nav__link "> <span class=md-ellipsis> 07 - Generalization </span> </a> <label class="md-nav__link " for=__nav_2_1_8 id=__nav_2_1_8_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=3 aria-labelledby=__nav_2_1_8_label aria-expanded=true> <label class=md-nav__title for=__nav_2_1_8> <span class="md-nav__icon md-icon"></span> 07 - Generalization </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../01/ class=md-nav__link> <span class=md-ellipsis> 7.1 - Goal Misgeneralization </span> </a> </li> <li class=md-nav__item> <a href=../02/ class=md-nav__link> <span class=md-ellipsis> 7.2 - Inner Alignment </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> 7.3 - Deceptive alignment </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> 7.3 - Deceptive alignment </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 7.3.1 Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 7.3.2 Inductive Priors </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 7.3.3 Path Dependence </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 7.3.4 Likelihood </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../08/ class=md-nav__link> <span class=md-ellipsis> 08 - Scalable Oversight </span> <span class="md-nav__icon md-icon"></span> </a> </li> <li class="md-nav__item md-nav__item--pruned md-nav__item--nested"> <a href=../../09/ class=md-nav__link> <span class=md-ellipsis> 09 - Interpretability </span> <span class="md-nav__icon md-icon"></span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../feedback/ class=md-nav__link> <span class=md-ellipsis> Feedback </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#01 class=md-nav__link> <span class=md-ellipsis> 7.3.1 Prerequisites </span> </a> </li> <li class=md-nav__item> <a href=#02 class=md-nav__link> <span class=md-ellipsis> 7.3.2 Inductive Priors </span> </a> </li> <li class=md-nav__item> <a href=#03 class=md-nav__link> <span class=md-ellipsis> 7.3.3 Path Dependence </span> </a> </li> <li class=md-nav__item> <a href=#04 class=md-nav__link> <span class=md-ellipsis> 7.3.4 Likelihood </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=73-deceptive-alignment>7.3 <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">Deceptive alignment</abbr><a class=headerlink href=#73-deceptive-alignment title="Permanent link">&para;</a></h1> <div class=section-meta> <div class=meta-item> <span class=meta-icon> <i class="fas fa-clock"></i> </span> <div class=meta-content> <div class=meta-label>Reading Time</div> <div class=meta-value>15 min</div> </div> </div> </div> <div class="admonition warning"> <p class=admonition-title>This section is still being written and is considered a work in progress.</p> </div> <figure class=video-figure> <iframe allowfullscreen=allowfullscreen frameborder=0 src=https://www.youtube.com/embed/IeWljQw3UgQ style="width: 100%; aspect-ratio: 16 / 9;"></iframe> <br> <figcaption><b>Video 7.2:</b> Optional video explaining deceptive alignment.</figcaption> </figure> <p>This section focuses on further exploring the concept of mesa-optimizers. There are various different types of mesa-optimizers that can result at the end of the training process. Ajeya Cotra in her post “<a href=https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/ >Why AI alignment could be hard with modern deep learning</a>” provided an intuitive example by exploring three archetypes that an AI might embody. She explains this through the analogy of a young child (humanity) who has inherited a company to run and has a choice of the following three types of candidates (AIs) to hire to run the inherited company:</p> <ol> <li> <p><strong>Saints</strong> are those who genuinely just want to help manage your estate well and look out for your long-term interests.</p> </li> <li> <p><strong>Sycophants</strong> want to do whatever it takes to make you short-term happy or satisfy the letter of your instructions regardless of long-term consequences. They want to see the child be happy even if it means lying. Sycophants would provide false facts about the world to convince the child that things are going well, but they don't have some long-term ulterior motives. Sycophants are an example of deception/dishonesty but they are not <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr>.</p> </li> <li> <p><strong>Schemers</strong> have their own agendas and want to get access to the company and all its wealth and power so they can use it however they want. These are examples of deceptive alignment. Schemers have some ulterior motive (mesa-objective) that they want to accomplish.</p> </li> </ol> <p>During training, Sycophants and Schemers are behaviorally indistinguishable from Saints. Once a schemer is deployed or is otherwise safe from modification, it can "defect" and begin to pursue its true objective. Risks from learned optimization uses a slightly different vocabulary to convey the same concept. Following are the behaviorally indistinguishable types of mesa-optimizers that might be seen at the end of the training process:</p> <p><strong>Internally aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></strong> : These AIs want exactly the same things as humans, because they have the same values and exactly the same way of thinking about the world.</p> <div class="admonition info"> <p class=admonition-title>Definition: Internally aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></p> <p>An internally aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> is a robustly aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> that has internalized the base objective in its mesa-objective. (<a href=https://intelligence.org/learned-optimization/ >Hubinger et al., 2019</a>)</p> </div> <p><strong>Corrigibly aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></strong> : These AIs don’t have an exact copy of human values, but they do have instead a desire to really understand human values. They then take action based on this understanding.</p> <div class="admonition info"> <p class=admonition-title>Definition: Corrigibly aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></p> <p>A corrigibly aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> is a robustly aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> that has a mesa-objective that “points to” its epistemic model of the base objective. (<a href=https://intelligence.org/learned-optimization/ >Hubinger et al., 2019</a>)</p> </div> <p><strong>Deceptively aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></strong> : These AIs have their own desires independent of human values. But they also don't want to be shut off, or their values get trained away. So they act aligned until such fears are not relevant.</p> <div class="admonition info"> <p class=admonition-title>Definition: Deceptively aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr></p> <p>A <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> is a pseudo aligned <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> that has enough information about the base objective to seem more fit from the perspective of the base optimizer than it actually is. (<a href=https://intelligence.org/learned-optimization/ >Hubinger et al., 2019</a>)</p> </div> <p>To understand deceptive alignment slightly more formally, imagine that an RL agent is trying to optimize for mesa-objective/reward (<span class=arithmatex>\(R\)</span>) instead of the base objective (<span class=arithmatex>\(R_0\)</span>). Assume that this agent eventually develops some understanding of the training process. During training, the myopic strategy would be to just greedily choose the maximizing action for R immediately. However, since the model has an understanding of gradient descent it ‘knows’ that its parameters will get changed to focus on some other reward function (<span class=arithmatex>\(R_0\)</span>). Therefore the optimal course is to choose the action that instead maximizes <span class=arithmatex>\(R_0\)</span> to avoid parameter updates. As long as <span class=arithmatex>\(R_0\)</span> is being maximized during training, there is no reason to update or look for a ‘truly internally aligned model’. After training concludes, the agent is free to choose actions that maximize R as many times as it likes without the risk of being modified, resulting in an overall greater reward. Such type of behavior, where a <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> might purposefully act in ways that cause gradient descent to update (or not) it in some specific way is called gradient hacking.</p> <div class="admonition info"> <p class=admonition-title>Definition: Gradient Hacking</p> <p>"Gradient hacking" describes the phenomenon wherein a <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> might be able to purposefully act in ways which cause gradient descent to update it in a particular way. (<a href=https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking>Hubinger, 2019</a>)</p> </div> <h2 id=01>7.3.1 Prerequisites<a class=headerlink href=#01 title="Permanent link">&para;</a></h2> <p>Risks From Learned Optimization lists three prerequisites for an AI to become <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr>. These conditions must be fulfilled during the training phase for an AI to develop internal goals, as well as the ability and desire to deceive the training process:</p> <p><strong>Situational awareness.</strong> The model must have a good enough understanding of the world to understand what humans “really want” and how it is different from what the AI wants (i.e. an understanding of the base objective). This is because in order to be successfully deceptive, the model still has to give the humans what they want during training. Additionally, it needs an understanding of the fact that it is part of a training process and some understanding of the technical details of how stochastic gradient descent (SGD) works. As an example, an agent tasked with forecasting stock performance in the tech sector might be given data about the entire world, including current AI technology in order to improve its performance. This information might also allow it to learn about its own situation.</p> <p><strong>Long-term planning.</strong> A model might be explicitly encouraged during training to acquire long-term planning capabilities to optimize its base objective. This might be, for example, because it was trained to take action in an environment where actions can have permanent consequences, or because of reasoning abilities. The possibility of permanently altering its environment means that the model has to consider the implications of its actions across longer time periods and training update cycles. Once it gains significant capabilities in long-term planning, it can use this ability to optimize for its own mesa-objective. How could a model tell if it is training or deployment? One easy way is by finding data that is extremely unlikely to exist in training. As a concrete example, it could find factors for cryptographic systems. Factoring basically means breaking down a composite number into its prime components. For example, factoring the number 15 would result in the prime numbers 3 and 5, because <span class=arithmatex>\(3 \times 5 = 15\)</span>. This process is central to a lot of fields, especially cryptography, because the security of many cryptographic systems relies on the difficulty of factoring large numbers. In systems like RSA 2048, the security of encryption is based on the difficulty of factoring large composite numbers. Factoring RSA 2048 is infeasible with available technology, so the AI would not encounter this factorization during training. However, once deployed, it might eventually encounter a situation where RSA 2048 has been factored, signaling to the AI that it is now in the deployment phase. This could potentially trigger the AI to change its behavior based on the knowledge that it is no longer being monitored or constrained by training oversight mechanisms.</p> <p><strong>Expectation of distribution shift.</strong> The model must expect that at some point in the future, the threat of objective modification will go away. This means that the model thinks that training either has to end at some point, or that the trainer (SGD) does not need to update the behavior of the model because it is already performing well enough on the given task.</p> <p>There are some dynamics of the gradient descent training process that might make the emergence of prerequisites like situational awareness and long term planning more or less likely. The next couple of sections explain machine learning training dynamics like inductive biases and path dependence. The subsequent section then uses this understanding to make an argument for the likelihood of deceptive alignment.</p> <h2 id=02>7.3.2 Inductive Priors<a class=headerlink href=#02 title="Permanent link">&para;</a></h2> <div class="admonition info"> <p class=admonition-title>Definition: Inductive bias</p> <p>Inductive biases represent the constraints and influence how and what ML models learn.</p> </div> <p>An inductive prior represents the initial state before any evidence has been observed. Here are some examples:</p> <ul> <li> <p><strong>Complexity Bias</strong> : It might be the case that the ML process prefers simpler models over more complex ones. The intuition behind this is Occam's razor, which suggests that the simplest explanation (or in this case, model) is often the “correct” one.</p> </li> <li> <p><strong>Smoothness Bias</strong> : It could also be possible that smoother target functions are easier to find in search space than those that are erratic. In other words, small changes in the input should not produce large changes in the output.</p> </li> <li> <p><strong>Speed Bias</strong> : Similarly, it could be the case that functions that can be computed quickly are more desirable.</p> </li> </ul> <p>There are many more inductive biases that the current machine learning process might have. The above are only a couple of examples. So depending on the inductive bias, gradient descent might just always simply find either the simplest, fastest, or smoothest model that fits the data. Which means that given the same dataset, all paths converge on essentially the same generalization.</p> <p>Both inductive biases and path dependence can significantly influence the learning process and the final outcome of a machine learning model. A model's inductive priors can set it on a certain 'path', and then path dependence can further shape how the model learns and adapts as it travels down that path.</p> <h2 id=03>7.3.3 Path Dependence<a class=headerlink href=#03 title="Permanent link">&para;</a></h2> <div class="admonition info"> <p class=admonition-title>Definition: Path dependence</p> <p>Path dependence refers to the update path that gradient descent follows through parameter space.</p> </div> <p>Path dependence studies the effects of changes in the training procedure on the path that gradient descent follows while searching for the optimal learned algorithm. This includes things like the sequence of training data, the order of training, etc…</p> <p><strong>High path dependence</strong> means that significantly different learned models emerge at the end of the training process. <strong>Low path dependence</strong> means that similar learned models emerge across multiple training runs with the same model on the same data.</p> <p>This means that not only the emergence of mesa-optimizers but also overall levels of alignment could be sensitive to changes in the training procedure, which could make it harder to implement and enforce exact training procedures across different labs.</p> <figure> <a class=glightbox href=../Images/mqZ_Image_15.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/mqZ_Image_15.png></a> <br> <figcaption><b>Figure 7.15:</b> Image example of loss landscapes. This is the loss surface of ResNet-110-noshort and DenseNet for CIFAR-10. The paths taken through these different loss landscapes will be different. (<a href=https://arxiv.org/abs/1712.09913>Li et al., 2017</a>)</figcaption> </figure> <p><strong>High path dependence.</strong> If gradient descent is highly path-dependent, then that would mean that different training runs can converge to very different models. This essentially means that the learned algorithm is very sensitive to the particular parameter update path that is taken through model space. To understand the types of expected learned algorithms, the path itself must also be understood. There is some empirical evidence to suggest that many ML models might have high path dependence. Research showed that different versions of the fine-tuned BERT models resulted in differing generalizations on downstream tasks, even with the same training data. This sort of path dependence is especially prevalent in RL models, where an identical setup can be run multiple times, sometimes getting good performance and sometimes terrible performance. (<a href=https://arxiv.org/abs/1911.02969>McCoy et al., 2020</a>)</p> <p><strong>Low path dependence</strong> . In this type of process, similar training processes converge to essentially the same simple solution, regardless of early training dynamics. In this case, inductive priors play a much larger role in what type of algorithm you might end up with. Ajeya Cotra provided a simple example of low path dependence. An AI model was trained to be a shape detector. She defined a “thneeb” as one of the two shapes shown in the image below. Multiple models are trained to distinguish between two distinct types of shapes. (<a href=https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/ >Cotra, 2021</a>)</p> <figure> <a class=glightbox href=../Images/CtJ_Image_16.png data-type=image data-width=100% data-height=auto data-desc-position=bottom><img alt="Enter image alt description" loading=lazy src=../Images/CtJ_Image_16.png></a> <br> <figcaption><b>Figure 7.16:</b> This shape is called a “Thneeb”. It is a word defined only for the sake of the experiment. If we ask you which one of the images on the right is a thneeb? You would probably say the left because you took the shape path, but neural networks often answer that the shape on the right is a thneeb, showing they prefer color path. (<a href=https://www.cold-takes.com/why-ai-alignment-could-be-hard-with-modern-deep-learning/ >Cotra, 2021</a>)</figcaption> </figure> <p>However, in addition to the shapes in each cluster being similar, one shape was always blue, and the other was always red. During training, the model performed well at recognizing the shapes. During testing, the colors were swapped and it turned out that the model was only looking at the colors and not considering the shapes. On running the training process multiple times with random initialization points, it might be expected that at least one of the times the ML model generalizes to look for shapes instead of looking for colors. Some research has found that neural networks (or gradient descent) are naturally biased towards colors over shapes when given the choice. (<a href=https://arxiv.org/abs/1811.12231>Geirhos et al., 2022</a>) We need to examine many more examples of such propensity to get a true understanding of neural network biases, what causes it and how we might be able to affect it.</p> <p>Overall, there is no consensus on whether advanced machine learning models have high or low path dependence since there is evidence for both. The next section explores the likelihood of <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> mesa-optimizers emerging in both low and high-path dependent training scenarios.</p> <h2 id=04>7.3.4 Likelihood<a class=headerlink href=#04 title="Permanent link">&para;</a></h2> <p>Since the objective is to analyze the emergence of deceptive alignment, this section assumes the emergence of a <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> during training. The focus is more on “which type” of <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> (aligned, <abbr title="An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.">corrigible</abbr>, or deceptive) is most likely to be found using current ML training methods. (<a href=https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment>Hubinger, 2022</a>)</p> <p>A couple of further assumptions: First, there is a relatively complex base objective such as human values. Second, training environments are diverse enough with enough situations to get the models to robustly understand what humans want.</p> <p>During training, ML models always start with a seemingly random proxy objective. To improve performance Gradient descent can do two things:</p> <ul> <li> <p><strong>Better Proxies</strong> : The model can improve its understanding of the objective</p> </li> <li> <p><strong>Better World Models</strong> : The model can improve its ability to act on its existing understanding of the objective</p> </li> </ul> <p><strong>High Path Dependence Scenario.</strong> If gradient descent is highly path-dependent, then depending on the path followed either the world model or the proxy could be perfected first. Which indicates that there could be the following different types of scenarios:</p> <ol> <li> <p>If the proxy objective is perfected to match the base objective before the world model is perfected, then the model is internally aligned.</p> </li> <li> <p>If the world model is perfected before the proxy objective, then the world model is also perfect enough to simulate gradient descent. Since the world model is perfect the base objective also exists within the world model. In which case, one of two things can happen:</p> </li> <li> <p>The <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> ‘throws out’ the proxy objective and replaces it with a pointer to gradient descent’s base objective.</p> </li> <li> <p>The <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> just makes the current proxy objective a long-term goal (crystallizes it), which means that the <abbr title="An algorithm that is created by optimization and that is also itself an optimizer.">mesa-optimizer</abbr> is now free to maximize the base objective.</p> </li> </ol> <p>In all three of these cases, at the end of training gradient descent returns a model that optimizes the base objective. All three are functionally indistinguishable.</p> <p><strong>Low Path Dependence Scenarios.</strong> If training has low path dependence then the inductive priors come into play. The simplicity inductive bias is an analysis of the algorithm's Kolmogorov complexity. In all three - internally, corrigibly, and <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> types of models the world model is exactly the same. The main difference between the models is the mesa objective, i.e. the complexity of the objective given to the model to learn and optimize. The simplest internally aligned mesa-objective is the base objective. So for internal alignment, the complexity class is the full complexity of the base objective. The simplest corrigibly aligned mesa-objective is a specification for how to point to the base objective. The simplest <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> mesa-objective is literally any long-term objective. So as far as <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> AI is concerned, the most complexity cost paid is just the simplest long-term objective that gradient descent can find. In other words, the <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> one has the lowest complexity cost out of the three alignment scenarios, assuming a relatively complex training objective. All of the other model classes scale with the complexity of the training objective, whereas the complexity of the <abbr title="A model that pretends to be aligned during training but engages in dangerous behavior during deployment. During training it acts helpful, honest and harmless, but in deployment it acts according to a dangerous secret goal.">deceptively aligned model</abbr> is essentially fixed. Therefore the more complex the base objective (e.g. human values) the more deceptive alignment wins in the complexity race. Overall, the analysis results in the conclusion that the number of internally aligned AIs is less than the number of corrigibly aligned AIs which is less than the number of <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> AIs (<a href=https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment>Hubinger, 2022</a>):</p> <p>(internally aligned AIs) &lt; #(corrigibly aligned AIs) &lt; #(<abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> AIs) Random sampling from a distribution of learned optimizers means there are more ways to get a <abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr> AI than the other two.</p> <p>The speed inductive bias thankfully falls in favor of corrigibly and internally aligned models. The <abbr title="A model that pretends to be aligned during training but engages in dangerous behavior during deployment. During training it acts helpful, honest and harmless, but in deployment it acts according to a dangerous secret goal.">deceptively aligned model</abbr> has to perform extra steps of ‘thinking’. For example, it has to realize that caring about the mesa-objective in the long term means that it should optimize the training objective temporarily to get a higher reward on your internal objective eventually. The internal and corrigibly aligned models however don’t have to do this extra thinking and can just directly optimize what they care for which is the base objective. So in terms of time taken for each model class to output the correct answer (<a href=https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment>Hubinger, 2022</a>):</p> <p>t(internally aligned) ~= t(corrigibly aligned) &lt; t(<abbr title="A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.">deceptively aligned</abbr>)</p> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=footer.title> <a href=../02/ class="md-footer__link md-footer__link--prev" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Previous</span> 7.2 - Inner Alignment </div> </div> </a> <a href=../../08/ class="md-footer__link md-footer__link--next" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction>Next</span> Chapter 08 </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=footer-social-container> <hr class=footer-line> <div class=footer-social> <a href=https://github.com/CentreSecuriteIA class="footer-social__link github" title=GitHub> <i class="fab fa-github"></i> </a> <a href=https://youtube.com/@securite-ia class="footer-social__link youtube" title=YouTube> <i class="fab fa-youtube"></i> </a> <a href=https://discord.gg/YtatCZ3SQ8 class="footer-social__link discord" title=Discord> <i class="fab fa-discord"></i> </a> </div> <hr class=footer-line> </div> <div class=footer-identity> <a href=https://www.securite-ia.fr/ class=footer-logo target=_blank rel=noopener> <img src=https://ai-safety-atlas.com/assets/CeSIA_transparent_white_pure.png alt="CeSIA - French Center for AI Safety"> </a> </div> <div class=footer-info> <div class=footer-links> <a href=mailto:contact@securite-ia.fr>Contact</a> <span class=separator>•</span> <a href=https://forms.gle/Z3rzFfCrLJdDv8HDA>Feedback</a> </div> <div class=footer-legal> <span>© Copyright 2025 </span> <span class=separator>•</span> <span>Text Content: <a href=https://creativecommons.org/licenses/by-sa/4.0/ >CC BY-SA 4.0</a></span> <span class=separator>•</span> <span>Code: <a href=https://opensource.org/licenses/MIT>MIT</a></span> </div> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.footer", "navigation.sections", "navigation.tacking", "navigation.path", "navigation.prune", "content.tooltips", "content.footnote.tooltips", "header.autohide", "announce.dismiss", "toc.follow", "search.suggest", "search.highlight"], "search": "../../../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.13a4f30d.min.js></script> <script src=../../../overrides/js/mathjax.js></script> <script src=../../../overrides/js/tippy.js></script> <script src=../../../overrides/js/navigation.js></script> <script src=../../../overrides/js/scroll-buttons.js></script> <script src=../../../overrides/js/home/base.js></script> <script src=../../../overrides/js/home/hero.js></script> <script src=../../../overrides/js/home/why.js></script> <script src=../../../overrides/js/home/header.js></script> <script src=../../../overrides/js/home/scroll-animations.js></script> <script src=../../../overrides/js/audio-player.js></script> <script src=https://unpkg.com/@popperjs/core@2></script> <script src=https://unpkg.com/tippy.js@6></script> <script src=https://unpkg.com/tippy.js@6/dist/animations/perspective-extreme.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../docs/assets/js/custom.js></script> <script type=application/ld+json>
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "url": "https://ai-safety-atlas.com/",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https://ai-safety-atlas.com/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
</script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(() => { lightbox.reload() });
</script></body> </html>