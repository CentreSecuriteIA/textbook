
*[ML]: Machine Learning
*[LLM]: Large Language Model
*[NLP]: Natural Language Processing
*[CV]: Computer Vision
*[GAN]: Generative Adversarial Network
*[RNN]: Recurrent Neural Network
*[CNN]: Convolutional Neural Network
*[DNN]: Deep Neural Network
*[RL]: Reinforcement Learning
*[DL]: Deep Learning
*[EDA]: Exploratory Data Analysis
*[API]: Application Programming Interface
*[CPU]: Central Processing Unit
*[GPU]: Graphics Processing Unit
*[AGI]: Aritifical General Intelligence. AI that is at least as good as humans at general problem-solving.
*[AI alignment]: Making sure that AI tries to do what we want it to do.
*[Compute]: Shorthand for “computing power”. It may refer to, for instance, physical infrastructure such as CPUs or GPUs that perform processing, or the amount of processing power needed to train a model. 

<!--to be integrated:-->

<!--AGI-->
<!--artificial general intelligence-->
<!--AGIs-->
<!--AGI’s-->
 <!--What is artificial general intelligence (AGI) and what will it look like? -->
<!--AI that is at least as good as humans at general problem-solving.-->

<!--AI ethics-->
<!--Ethics of artificial intelligence-->
<!--The study of ethical principles for AI systems and their creators to follow. In practice, “AI ethics”this often refers to a cluster of concerns about present systems that include algorithmic bias and transparency.-->

<!--AI alignment-->
<!--Alignment problem-->
<!--Making sure that AI tries to do what we want it to do. -->

<!--AI safety-->
<!--A research field about how to prevent risks from advanced artificial intelligence.-->

<!--AI timelines-->
<!--timelines-->
<!--The (predicted) time until AI hits some milestone, usually AGI.-->

<!--Agent-->
<!--agentic-->
<!--A system that can be understood as taking actions towards achieving a goal.-->

<!--Alignment tax-->
<!--The added cost in time, money, computing power, capabilities, etc. needed to ensure that a system is aligned.-->

<!--Compute-->
<!--Shorthand for “computing power”. It may refer to, for instance, physical infrastructure such as CPUs or GPUs that perform processing, or the amount of processing power needed to train a model. -->

<!--Corrigible-->
<!--corrigibility-->
<!--An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.-->

<!--Chain-of-Thought prompting-->
<!--Chain-of-thought-->
<!--Chain of thought-->
<!--A technique which makes a language model generate intermediate reasoning steps in its output.-->

<!--Deceptive alignment-->
<!--deceptive misalignment-->
<!--deceptively aligned-->
<!--deceptively misaligned-->
<!--A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned. -->

<!--Embedded agent-->
<!--embedded agency-->
<!--An agent which is not modeled as separate from the environment it acts on.-->

<!--Existential risk-->
<!--existential threat-->
<!--x-risk-->
<!--A risk of human extinction or the destruction of humanity’s long-term potential.-->

<!--Fast takeoff -->
<!--FOOM-->
<!--hard takeoff-->
<!--A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.-->

<!--Few-shot prompting-->
<!--A technique where a language model is prompted with a few examples of problems with solutions, and is then asked to find a solution to the next problem.-->


<!--Feature-->
<!--features-->
<!--A feature of a region of input space that corresponds to a useful pattern. For example, in an image detector, a set of neurons that detects cars might be a feature.-->


<!--FLOP-->
<!--floating-point operation-->
<!--A quantity of mathematical operations that involve floating-point numbers (numbers with a decimal point). “FLOPs” can be the plural of “FLOP”, but is confusingly also sometimes used interchangeably with “FLOPS”, meaning “FLOP per second”.-->

<!--FLOP/s-->
<!--FLOPS-->
<!--floating-point operations per second-->
<!--Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.-->


<!--FLOPs-->
<!--floating point operations-->
<!--The plural of “FLOP”, or occasionally an alternative spelling of “FLOPS”-->


<!--Foundation model-->
<!--foundation models-->
<!--A large machine learning model trained on a vast amount of data so that it can be targeted at a wide variety of different tasks. GPT-4 is an example of a foundation model. -->

<!--Goal misgeneralization-->
<!--Pursuing a different goal during deployment than was intended to be learned in training.-->

<!--Goodhart's law-->
<!--Goodharts law-->
<!--Goodharting-->
<!--Goodhart-->
<!--“When a measure becomes a target, it ceases to be a good measure.”-->

<!--Inner misalignment-->
<!--When an mesa optimizer pursues a different objective than the one that was specified. -->

<!--Inner alignment-->
<!--When an AI system ends up pursuing the objective that was specified. -->

<!--Instrumental convergence-->
<!--instrumentally convergent goals-->
<!--The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.-->


<!--Instrumental goal-->
<!--sub-goal-->
<!--Goals which are pursued as means to some other end, rather than as ends in themselves.-->

<!--Intelligence explosion-->
<!--A hypothetical scenario where machines become more intelligent very quickly, driven by recursive self-improvement.-->

<!--Interpretability-->
<!--A research area that aims to make machine learning systems easier for humans to understand.-->

<!--Mechanistic interpretability-->
<!--mech interp-->
<!--A subfield of interpretability which involves reverse-engineering the mechanisms by which a model gets from its inputs to its outputs.-->

<!--Large language model-->
<!--LLMs-->
<!--language model-->
<!--large language models-->
<!--LLM-->
<!--An AI model that takes in some text and predicts how it’s most likely to continue.-->

<!--Machine learning-->
<!--ML-->
<!--An approach to AI in which, instead of designing an algorithm directly, we have the system search through possible algorithms based on how well they do on some training data.-->


<!--Mesa-optimization-->
<!--mesa-optimizer-->
<!--inner optimizer-->
<!--inner-optimizer-->
<!--learned optimizer-->
<!--learned-optimizer-->
<!--An algorithm that is created by optimization and that is also itself an optimizer.-->


<!--Mesa-objective-->
<!--The objective pursued by a mesa-optimizer.-->

<!--Multipolar scenario-->
<!--Multipolarity-->
<!--A scenario in which there end up being multiple powerful decision makers.-->

<!--Multimodal-->
<!--multimodal system-->
<!--multimodal model-->
<!--multimodal AI-->
<!--An AI model that can take in and produce multiple forms of data (such as both text and images). -->

<!--Unipolar scenario-->
<!--Unipolarity-->
<!--A scenario in which there ends up being a single powerful decision maker.-->

<!--Out-of-distribution-->
<!--out of distribution-->
<!--OOD-->
<!--A case is out-of-distribution if it is very different from the cases the system encountered during training.-->


<!--Independent and identically distributed-->
<!--independent and identically distributed-->
<!--i.i.d.-->
<!--IID-->
<!--An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.-->


<!--Auto-induced distributional shift-->
<!--ADS-->
<!--A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself. It arises due to the influence of the system on its own input distribution.-->


<!--Optimization algorithm-->
<!--A general procedure for finding solutions that score highly according to some well-defined objective function.-->


<!--Optimizer-->
<!--optimization process-->
<!--Something that can improve some process or physical artifact so that it is fit for a certain purpose or fulfills some set of requirements.-->

<!--Oracle-->
<!--Oracle AI-->
<!--An AI whose only goal is to give correct answers to questions.-->

<!--Orthogonality thesis-->
<!--Orthogonality hypothesis-->
<!--orthogonality-->
<!--The thesis that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.-->


<!--Outer alignment-->
<!--outer misalignment-->
<!--reward misspecification-->
<!--The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.-->


<!--Parameter-->
<!--parameters-->
<!--Values an AI system that are modified during training to change the AI's performance.-->


<!--Proxy goal-->
<!--A goal that performs well on the intended objective due to some spurious correlation in the training distribution.-->


<!--Reinforcement learning-->
<!--Reinforcement learning (RL)-->
<!--RL-->
<!--A machine learning method in which the machine gets rewards based on its actions, and is adjusted to be more likely to take actions that lead to high reward.-->


<!--Reinforcement learning from human feedback-->
<!--RLHF-->
<!--A method for training an AI to give desirable outputs by using human feedback as a training signal. -->


<!--Robustness-->
<!--An agent's ability to maintain its goal and its capabilities when exposed to environments that are substantially different from that on which the agent was trained.-->

<!--Recursive self-improvement-->
<!--recursive self improvement-->
<!--RSI-->
<!--Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. -->

<!--Scalable oversight-->
<!--scaleable oversight-->
<!--Methods that allow humans to provide oversight for numerous or superhuman AI systems, usually by using AI systems to help supervise other AI systems.-->

<!--Singleton-->
<!--An agent which has dominant control over the entire world and can prevent any competitors from arising. Usually in reference to a superintelligent AI. Related: unipolar scenario.-->

<!--Specification gaming-->
<!--Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”-->
<!--Subagent-->
<!--sub-agent-->
<!--sub agent-->
<!--A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.-->


<!--Superintelligence-->
<!--superintelligent-->
<!--super-intelligence-->
<!--super-intelligent-->
<!--An AI with cognitive abilities far greater than those of humans in a wide range of important domains.-->

<!--Transformative AI-->
<!--TAI-->
<!--An AI that is capable of transforming society, as drastically as the industrial revolution or even more so. -->


<!--Transparency-->
<!--The ability to ‘look inside’ a model, to understand how it works and why it produces a specific output.-->


<!--Takeoff speed-->
<!--takeoff-->
<!--The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence. -->


<!--Slow takeoff-->
<!--soft takeoff-->
<!--A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.-->


<!--Takeoff homogeneity-->
<!--The degree to which the first AI systems that become superintelligent will have similar designs and goals.-->

<!--Homogeneous scenario-->
<!--A scenario where the first AIs to reach superintelligence are similar in nature.-->
<!--Heterogeneous scenario-->
<!--A scenario where the first AIs to reach superintelligence are different in nature.-->

<!--Terminal goals-->
<!--terminal goal-->
<!--Goals which are valued as ends in themselves, rather than as means to some other end. -->

<!--Tool AI-->
<!--tool AIs-->
<!--A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.-->


<!--Utility function-->
<!--A mathematical function that assigns a number representing utility to every possible outcome. Outcomes with higher utility are preferred to outcomes with lower utility. “Maximizing utility” then means choosing the most preferred outcome.-->

<!--Base optimizer-->
<!--In contrast to a mesa-optimizer, a base optimizer is the “outer” optimizer usually explicitly implemented by humans.-->

<!--Base objective-->
<!--In contrast to a mesa-objective,  the base objective is the “outer” objective usually explicitly implemented by humans.-->


<!--Behavioral objective-->
<!--The behavioral objective is what an optimizer looks like it is optimizing for. Formally, the behavioral objective is the objective we would find if we performed inverse reinforcement learning perfectly.-->

<!--Learned algorithm-->
<!--The algorithms that a base optimizer finds to solve the problem it has been given.-->

<!--Meta-optimizer-->
<!--A system which is tasked with producing a base optimizer.-->

<!--Pseudo-alignment-->
<!--A mesa-optimizer is pseudo-aligned with the base objective if it appears aligned on the training data but is not robustly aligned.-->

<!--Robust alignment-->
<!--A mesa-optimizer is robustly aligned with the base objective if it robustly optimizes for the base objective across distributions.-->

<!--Algorithmic range-->
<!--The algorithmic range of a machine learning system refers to how extensive the set of algorithms capable of being found by the base optimizer is.-->

<!--Reachability-->
<!--The reachability of a learned algorithm refers to the difficulty for the base optimizer to find that learned algorithm.-->

<!--Approximate alignment-->
<!--An approximately aligned mesa-optimizer is a pseudo-aligned mesa-optimizer where the base and mesa- objectives are the same up to some degree of approximation error caused only by  the difficulty of representing the base objective in the mesa-optimizer.-->

<!--Proxy alignment-->
<!--A proxy-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer that has learned to optimize for some proxy of the base objective instead of the base objective itself.-->

<!--Instrumental alignment-->
<!--A type of proxy alignment in which the mesa-optimizer optimizes the proxy as an instrumental goal of increasing the mesa-objective in the training distribution.-->
<!--Side-effect alignment-->
<!--A type of proxy alignment in which optimizing for the mesa-objective has the direct causal result of increasing the base objective in the training distribution.-->

<!--Suboptimality alignment-->
<!--A suboptimality-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer in which some deficiency, error, or limitation causes it to exhibit aligned behavior.-->

<!--Corrigible alignment-->
<!--A corrigibly aligned mesa-optimizer is a robustly aligned mesa-optimizer that has a mesa-objective that “points to” its epistemic model of the base objective.-->

<!--Internal alignment-->
<!--An internally aligned mesa-optimizer is a robustly aligned mesa-optimizer that has internalized the base objective in its mesa-objective.-->


<!--Neural network-->
<!--NN-->
<!--A simulated network of nodes (‘neurons’) and their connections (weights). Neural networks are the core component of deep learning, the leading AI paradigm.-->


<!--Sharp left turn-->
<!--SLT-->
<!--An event in which an AI’s capabilities suddenly generalize, but its alignment doesn’t also generalize.-->

<!--Wireheading-->
<!--wirehead-->
<!--wireheaded-->
<!--A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.-->

<!--Reward hacking-->
<!--Reward hacking occurs when an AI agent exploits “loopholes” or “shortcuts” in the environment to maximize its reward without actually achieving the intended goal.-->


<!--Reward tampering-->
<!--What is reward tampering?-->
<!--A scenario where an agent tries to influence the reward process itself, instead of accomplishing the goal that the reward was meant to encourage.-->


<!--Reward shaping-->
<!--A technique used in RL which introduces small intermediate rewards to supplement the environmental reward. This seeks to mitigate the problem of sparse reward signals and to encourage exploration and faster learning.-->


<!--Reward modeling-->
<!--A technique that separates the reinforcement learning alignment problem into understanding the intentions of humans and acting to achieve those intentions, i.e. learning the ‘How?’.-->


<!--Capabilities research-->
<!--Research aimed at making AI more capable. This is sometimes contrasted with AI research that emphasizes safety, as they can be at odds with one another.-->

<!--Pivotal act-->
<!--An action that has positive consequences for the far future of humanity. Typically a way to reliably avoid catastrophic outcomes in the near to medium future.-->


<!--Context window-->
<!--context length-->
<!--context size-->
<!--Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.-->


<!--Sovereign AI-->
<!--sovereign-->
<!--An AI that acts autonomously in the world, in pursuit of potentially long-range objectives.-->


<!--Adversarial training-->
<!--A safety technique that pits two models against each other.-->


<!--World model-->
<!--A system’s internal representation of its environment, which it uses to predict what will happen, including as a result of its own possible actions.-->


<!--Frontier model-->
<!--Frontier AI model-->
<!--Frontier AI system-->
<!--A state-of-the-art AI system. This typically refers to the largest and most capable LLMs.-->


