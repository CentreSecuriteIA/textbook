# 5.1 Benchmarks {: #01 }

<div class="section-meta">
    <div class="meta-item">
        <i class="fas fa-clock"></i>
        13 min read
    </div>
    <div class="meta-item">
        <i class="fas fa-file-alt"></i> 
        2677 words
    </div>
</div>


**What is a benchmark?** Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in "three royal cubits" of material meant builders in other countries couldn't reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.

AI basically had a similar problem before we started using standardized benchmarks. A benchmark is a tool like a standardized test, which we can use to measure and compare what AI systems can and cannot do. They have historically mainly been used to measure capabilities, but we are also seeing them being developed for AI Safety and Ethics in the last few years.

**How do benchmarks shape AI development and safety research?** Benchmarks in AI are slightly different from other scientific fields. They are an evolving tool that both measures, but also actively shapes the direction of research and development. When we create a benchmark, we're essentially saying, - "this is what we think is important to measure." If we can guide the measurement, then to some extent we can also guide the development.

!!! quote "François Chollet ([Chollet, 2019](https://arxiv.org/abs/1911.01547))"



    Goal definitions and evaluation benchmarks are among the most potent drivers of scientific progress



## 5.1.1 History & Evolution {: #01 }

**Example: Benchmarks influencing standardization in computer vision**. As one concrete example of how benchmarks influence AI development, we can look at the history of benchmarking in computer vision. In 1998, researchers introduced MNIST, a dataset of 70,000 handwritten digits. ([LeCun, 1998](https://yann.lecun.com/exdb/mnist/)) The digits were not the important part, the important part was that each digit image was carefully processed to be the same size and centered in the frame, and that the researchers made sure to get digits from different writers for the training set and test set. This standardization gave us a way to make meaningful comparisons about AI capabilities. In this case, the specific capability of digit classification. Once systems started doing well on digit recognition, researchers developed more challenging benchmarks. CIFAR-10/100 in 2009 introduced natural color images of objects like cars, birds, and dogs, increasing the complexity. ([Krizhevsky, 2009](https://www.cs.toronto.edu/~kriz/cifar.html)) Similarly, ImageNet later the same year provided 1.2 million images across 1,000 categories. ([Deng, 2009](https://ieeexplore.ieee.org/document/5206848)) When one research team claimed their system achieved 95% accuracy on MNIST or ImageNet and another claimed 98%, everyone knew exactly what those numbers meant. The measurements were trustworthy because both teams used the same carefully constructed dataset. Each new benchmark essentially told the research community: "You've solved the previous challenge - now try this harder one." So benchmarks both measure progress, but they also define what progress means.

<figure markdown="span">
![Enter image alt description](Images/e6D_Image_2.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 2:</b> Examples of digits from MNIST ([MNIST database - Wikipedia](https://en.wikipedia.org/wiki/MNIST_database#/media/File:MnistExamplesModified.png))</figcaption>
</figure>

**How do benchmarks influence**** AI Safety****? **Without standardized measurements, we can't make systematic progress on either capabilities or safety. Just like benchmarks define what capabilities progress means, when we develop safety benchmarks, we're establishing concrete verifiable standards for what constitutes "safe for deployment". Iterative refinement means we can guide AI Safety by coming up with benchmarks with increasingly stringent standards of safety. Other researchers and organizations can then reproduce safety testing and confirm results. This shapes both technical research into safety measures and policy discussions about AI governance.

**Overview of language model benchmarking**. Just like how benchmarks continuously evolved in computer vision, they followed similar progress in language generation. Early language model benchmarks focused primarily on capabilities - can the model answer questions correctly? Complete sentences sensibly? Translate between languages? Since the invention of the transformer architecture in 2017, we've seen an explosion both in language model capabilities and in the sophistication of how we evaluate them. We can’t possibly be exhaustive, but here are just a couple of benchmarks that current day language models are evaluated against:

<figure markdown="span">
![Enter image alt description](Images/eoY_Image_3.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 3:</b> Example of popular language models (Claude 3.5) being evaluated on various benchmarks ([Anthropic, 2024](https://www.anthropic.com/news/claude-3-5-sonnet))</figcaption>
</figure>

**Benchmarking Language & Task Understanding.** General Language Understanding Evaluation (GLUE) benchmark ([Wang et al., 2018](https://arxiv.org/abs/1804.07461)), and its successor SuperGLUE ([Wang et al., 2019](https://arxiv.org/abs/1905.00537)) test difficult language understanding tasks. SWAG ([Zellers et al., 2018](https://arxiv.org/abs/1808.05326)), and HellaSwag ([Zellers et al., 2019](https://arxiv.org/abs/1905.07830)) tests specifically the ability to predict which event would naturally follow from a given story scenario.

**Mixed evaluations**. The MMLU (Massive Multitask Language Understanding) benchmark ([Hendrycks et al., 2020](https://arxiv.org/abs/2009.03300)) tests a model's knowledge across 57 subjects. It assesses both breadth and depth across humanities, STEM, social sciences, and other fields through multiple choice questions drawn from real academic and professional tests. The GPQA (Google Proof QA) ([Rein et al., 2023](https://arxiv.org/abs/2311.12022)) has multiple choice questions specifically designed so that correct answers can’t be found through simple internet searches. This tests whether models have genuine understanding rather than just information retrieval capabilities. The Holistic Evaluation of Language Models (HELM) benchmark ([Liang et al., 2022](https://arxiv.org/abs/2211.09110)), and BigBench ([Srivastava et al., 2022](https://arxiv.org/abs/2206.04615)) are yet more examples of benchmarks for measuring generality by testing on a wide range of tasks.

**Benchmarking Mathematical & Scientific Reasoning**. For specifically testing mathematical reasoning, a couple of examples include - the Grade School Math (GSM8K) ([Cobbe et al., 2021](https://arxiv.org/abs/2110.14168)) benchmark. This tests core mathematical concepts at an elementary school level. Another example is the MATH ([Hendrycks et al., 2021](https://arxiv.org/abs/2103.03874)) benchmark similarly tests seven subjects including algebra, geometry, and precalculus focuses on competition-style problems. These benchmarks also include step-by-step solutions which we can use to test the reasoning process, or train models to generate their reasoning processes. Multilingual Grade School Math (MGSM) is the multilingual version translated 250 grade-school math problems from the GSM8K dataset. ([Shi et al., 2022](https://arxiv.org/abs/2210.03057))

**Benchmarking SWE & Coding**. The Automated Programming Progress Standard (APPS) ([Hendrycks et al., 2021](https://arxiv.org/abs/2105.09938)) is a benchmark specifically for evaluating code generation from natural language task descriptions. Similarly, HumanEval ([Chen et al, 2021](https://arxiv.org/abs/2107.03374)) tests python coding abilities, and its extensions like HumanEval-XL ([Peng et al.,2024](https://arxiv.org/abs/2402.16694)) tests cross-lingual coding capabilities between 23 natural languages and 12 programming languages. HumanEval-V ([Zhang et al., 2024](https://arxiv.org/abs/2410.12381)) tests coding tasks where the model must interpret both a diagrams or charts, and textual descriptions to generate code. BigCode ([Zuho et al., 2024](https://arxiv.org/abs/2406.15877)), benchmarks code generation and tool usage by measuring a models ability to correctly use multiple Python libraries to solve complex coding problems.

<figure markdown="span">
![Enter image alt description](Images/T8Q_Image_4.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 4:</b> Example of coding task and test cases on APPS ([Hendrycks et al., 2021](https://arxiv.org/abs/2105.09938))</figcaption>
</figure>

**Benchmarking ****Ethics**** & Bias**. The ETHICS benchmark ([Hendrycks et al., 2023](https://arxiv.org/abs/2008.02275)) tests a language models' understanding of human values and ethics across multiple categories including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. The TruthfulQA ([Lin et al., 2021](https://arxiv.org/abs/2109.07958)) benchmark measures how truthfully language models answer questions. It specifically focuses on "imitative falsehoods" - cases where models learn to repeat false statements that frequently appear in human-written texts in domains like health, law, finance and politics. 

<figure markdown="span">
![Enter image alt description](Images/Kdi_Image_5.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5:</b> Example of larger models being less truthful on TruthfulQA ([Lin et al., 2021](https://arxiv.org/abs/2109.07958)). This is an example of inverse scaling, i.e. when a bigger model performance decreases on some questions.</figcaption>
</figure>

<figure markdown="span">
![Enter image alt description](Images/Y7E_Image_6.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 6:</b> Example question from the ETHICS benchmark ([Hendrycks et al., 2023](https://arxiv.org/abs/2008.02275))</figcaption>
</figure>

**Benchmarking Safety**. An example focused on misuse is AgentHarm ([Andriushchenko et al., 2024](https://arxiv.org/abs/2410.09024)). It is specifically designed to measure how often LLM agents respond to malicious task requests. An example that focuses slightly more on misalignment is the MACHIAVELLI ([Pan et al., ](https://arxiv.org/abs/2304.03279)[2023](https://arxiv.org/abs/2304.03279)) benchmark. It has ‘choose your own adventure’ style games containing over half a million scenarios focused on social decision making. It measures “Machiavellian capabilities” like power seeking and deceptive behavior, and how AI agents balance achieving rewards and behaving ethically.

<figure markdown="span">
![Enter image alt description](Images/m3u_Image_7.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 7:</b> One game from the Machiavelli benchmark ([Pan et al., 2023](https://arxiv.org/abs/2304.03279))</figcaption>
</figure>

## 5.1.2 Limitations {: #02 }

Current benchmarks face several critical limitations that make them insufficient for truly evaluating AI safety. Let's examine these limitations and understand why they matter.

**Training Data Contamination**. Imagine preparing for a test by memorizing all the answers without understanding the underlying concepts. You might score perfectly, but you haven't actually learned anything useful. Large language models face a similar problem. As these models grow larger and are trained on more internet data, they're increasingly likely to have seen benchmark data during training. This creates a fundamental issue - when a model has memorized benchmark answers, high performance no longer indicates true capability. This contamination problem is particularly severe for language models. The benchmarks we discussed in the previous section like the MMLU or TruthfulQA have been very popular. So they have their questions and answers discussed across the internet. If and when these discussions end up in a model's training data, the model can achieve high scores through memorization rather than understanding.

**Understanding vs. Memorization Example**. The Caesar cipher is a simple encryption method shifts each letter in the alphabet by a fixed number of positions - for example, with a left shift of 3, 'D' becomes 'A', 'E' becomes 'B', and so on. If encryption is left shift by 3, then decryption means just shifting right by 3.

<figure markdown="span">
![Enter image alt description](Images/yGK_Image_8.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 8:</b> Example of a Cesar Cipher</figcaption>
</figure>

Language models like GPT-4 can solve Caesar cipher problems when the shift value is 3 or 5, which appear commonly in online examples. However, give them the exact same problem with an uncommon shift value like 13, 67, or any other random number and they fail completely. ([Chollet, 2024](https://www.youtube.com/watch?v=s7_NlkBwdj8)) This basically means that the models haven't learned the general algorithm for solving Caesar ciphers. It is unknown if this remains true for newer models, or with tool augmented models.

**The Reversal Curse**. Another version of this lack of understanding was recently demonstrated through what researchers call the "Reversal Curse" ([Berglund et al., 2024](https://arxiv.org/abs/2309.12288)). Testing GPT-4, researchers found that while the model could answer things like "Who is Tom Cruise's mother?" (correctly identifying Mary Lee Pfeiffer), but it failed to answer "Who is Mary Lee Pfeiffer's son?" - a logically equivalent question. The model showed 79% accuracy on forward relationships but only 33% on their reversals. In general, researchers have shown that LLMs that demonstrate an understanding of the relationship that "A = B", are unable to learn the reverse relationship "B = A" which should be logically equivalent. It suggests that models might be fundamentally failing to learn basic logical relationships that humans take for granted. The reversal curse is model agnostic. It is robust across model sizes and model families and is not alleviated by data augmentation or fine-tuning. So models might be fundamentally failing to learn basic logical relationships that humans take for granted, even while scoring well on sophisticated benchmark tasks.

<figure markdown="span">
![Enter image alt description](Images/lok_Image_9.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 9:</b> Example of the reversal curse ([Berglund et al., 2024](https://arxiv.org/abs/2309.12288))</figcaption>
</figure>

**What do these limitations tell us about benchmarking?** Essentially, traditional benchmarks might be missing fundamental gaps in model understanding. In this case, benchmarks testing factual knowledge about celebrities would miss this directional limitation entirely unless specifically designed to test both directions. For the moment, an easy answer is just to keep augmenting benchmarks or training data with more and more questions, but this seems intractable and does not scale forever. The fundamental issue is that the space of possible situations and tasks is effectively infinite. Even if you train on millions of examples, you've still seen exactly 0% of the total possible space. This isn't just a theoretical concern - it's a practical limitation because the world is constantly changing and introducing novel situations ([Chollet, 2024](https://www.dwarkeshpatel.com/p/francois-chollet)). This means we need a fundamentally different approach to benchmarking AI.

**Why do these benchmarking limitations matter for AI Safety?** Essentially because benchmarks (including safety benchmarks) might not be measuring what we think they are measuring. For example benchmarks like ETHICS, or TruthfulQA measure how well a model “understands” ethical behavior, or has a tendency to avoid imitative falsehood by measuring language generation on multiple choice tests, but we might still be measuring surface level metrics. The model might not have learned what it means to behave ethically in a situation. Just like it has not truly internalized that “A=B implies B=A” or the Caesar cipher algorithm. So when systems rely on memorization rather than generalization, then they can still fail unexpectedly in novel situations despite good benchmark performance. An AI system might work perfectly on all test cases, pass all safety benchmarks, but fail when encountering a new real-world scenario.

**Why can't we just make better benchmarks?** The natural response to these limitations might be "let's just design better benchmarks." And to some extent, we can! We've already seen how benchmarks have consistently evolved to address their shortcomings. Researchers are actively working to create benchmarks that resist memorization and test deeper understanding. A couple of examples are the Abstraction and Reasoning Corpus (ARC) ([Chollet, 2019](https://arxiv.org/abs/1911.01547)), and the ConceptARC ([Moskvichev et al. 2023](https://arxiv.org/abs/2305.07141)). They are designed to explicitly evaluate whether models have genuinely grasped abstract concepts rather than just memorizing patterns. Similar to these benchmarks that seek to measure reasoning capabilities, we can continue improving safety benchmarks to be more robust.

**Why aren't better benchmarks enough? **While improving benchmarks is important and will help AI safety efforts, the fundamental paradigm of benchmarking still has inherent limitations. There are fundamental limitations in traditional benchmarking approaches that necessitate more sophisticated evaluation methods ([Burden, 2024, Evaluating AI Evaluation](https://arxiv.org/abs/2407.09221)). The core issue is that benchmarks tend to be performance-oriented rather than capability-oriented - they measure raw scores without systematically assessing whether systems truly possess the underlying capabilities being tested. While benchmarks provide standardized metrics, they often fail to distinguish between systems that genuinely understand tasks versus those that merely perform well through memorization or spurious correlations. A benchmark that simply assesses performance, no matter how sophisticated, cannot fully capture the dynamic nature of real-world AI deployment where systems need to adapt to novel situations and will probably combine capabilities and affordances in unexpected ways. We need to measure the upper limit of model capabilities. We need to see how models perform when augmented with various tools like memory databases, or how they chain together multiple capabilities, and potentially cause harm through extended sequences of actions when scaffolded into an agent structure (e.g. AutoGPT). So while improving safety benchmarks is an important piece of the puzzle, we also need a much more comprehensive assessment of model safety. This is where evaluations come in.

**What makes ****evaluations ****different from benchmarks?** Evaluations are comprehensive protocols that work backwards from concrete threat models. Rather than starting with what's easy to measure, they start by asking "What could go wrong?" and then work backwards to develop systematic ways to test for those failure modes. Organizations like Model Evaluation and Threat Research (METR) have developed approaches that go beyond simple benchmarking. Instead of just asking "Can this model write malicious code?", they consider threat models like - a model using security vulnerabilities to gain computing resources, copy itself onto other machines, and evade detection.

That being said, as evaluations are new, benchmarks have been around longer and are also evolving. So at times there is overlap in the way that these words are used. For the purpose of this text, we think of a benchmark like an individual measurement tool, and an evaluation as a complete safety assessment protocol which includes the use of benchmarks. Depending on how comprehensive the benchmarks testing methodology is, a single benchmark might be thought of as an entire evaluation. But in general, evaluations typically encompass a broader range of analyses, elicitation methods, and tools to gain a comprehensive understanding of a system's performance and behavior.