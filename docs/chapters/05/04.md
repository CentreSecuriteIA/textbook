# 5.4 Evaluation Frameworks {: #04 }
!!! note "Reading Time: 2 minutes" 

**Evaluation Techniques vs Evaluation Frameworks**. When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems.

As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism's framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety.

## 5.4.1 Model Organisms Framework {: #01 }

**What are model organisms in AI safety?** This framework involves deliberately creating and studying misaligned AI systems with specific dangerous properties. It's a controlled approach to studying potentially dangerous capabilities and behaviors. ([Hubinger et al., 2023](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1)).

**Why create potentially dangerous AI systems on purpose?** There are two compelling reasons for this approach. First, it allows us to study concerning properties in controlled settings where we know exactly what we're looking for. Second, it provides concrete examples that can help inform the broader AI community about safety challenges. Having real, demonstrated examples of potential risks is crucial for building scientific consensus and informing policy decisions.

**How does the model organisms approach ****break down complex ****threat models?** Instead of trying to tackle things like "deceptive alignment" as one monolithic problem, researchers identify key subcomponents like situational awareness or goal persistence. They then create simplified systems exhibiting these properties, studying how they manifest and testing different evaluation techniques. For example, Anthropic's recent work on "sleeper agents" demonstrated how certain forms of deceptive behavior could persist through safety training. They created models that would write secure code when asked about the year 2023, but insert vulnerabilities when asked about 2024 ([Hubinger et al., 2024](https://arxiv.org/abs/2401.05566)). We discuss this particular example of deception more in the goal misgeneralization chapter.

![Enter image alt description](Images/NrJ_Image_17.png)

***Figure****: Sleeper Agent that displays misaligned behavior despite having been put through safety fine-tuning. (**[Hubinger et al., 2024](https://arxiv.org/abs/2401.05566)**)*

**What are the framework's limitations?** The model organisms approach faces an important tradeoff: models need to be realistic enough to provide useful insights but controlled enough to study safely. They should be sophisticated enough to exhibit the properties we're concerned about but not so powerful that they pose actual risks. Additionally, since these models are explicitly constructed to exhibit certain behaviors, they may not perfectly represent how such behaviors would emerge naturally.