# 5.5 Dangerous Capability Evaluations {: #05 }

<div class="section-meta">
    <div class="meta-item">
        <i class="fas fa-clock"></i>
        23 min read
    </div>
    <div class="meta-item">
        <i class="fas fa-file-alt"></i> 
        4641 words
    </div>
</div>


**Understanding maximum potential**. Capability evaluations aim to establish the upper bounds of what an AI system can achieve. Unlike typical performance metrics that measure average behavior, capability evaluations specifically probe for maximum ability - what the system could do if it were trying its hardest. This distinction is crucial for safety assessment, as understanding the full extent of a system's capabilities helps identify potential risks.

**General vs Dangerous Capabilities.** Not all capabilities present equal concerns. General capabilities like language understanding or mathematical reasoning are essential for useful AI systems. However, certain capabilities - like the ability to manipulate human behavior or circumvent security measures - pose inherent risks. Dangerous capability evaluations specifically probe for these potentially harmful abilities, helping identify systems that might require additional safety measures or oversight.

**General Process for Dangerous Capability Evaluations.** When evaluating potentially dangerous capabilities, we can leverage many of the standard evaluation techniques covered in the previous section. However, dangerous capability evaluations have some unique requirements and methodological considerations:

- First, these evaluations must be designed to establish clear upper bounds on model capabilities - we need to know the maximum potential for harm. This typically involves combining multiple elicitation techniques like tool augmentation, multi-step reasoning, and best-of-N sampling to draw out the model's full capabilities. For example, when evaluating a model's ability to perform autonomous cyber attacks, we might provide it with both specialized tools and chain-of-thought prompting to maximize its performance.

- Second, dangerous capability evaluations need to test for capability combinations and emergent behaviors. Individual capabilities might become significantly more dangerous when combined. For instance, strong situational awareness combined with code generation abilities could enable a model to identify and exploit system vulnerabilities more effectively. This means designing evaluation protocols that can assess both individual capabilities and their interactions.

- Third, these evaluations must be conducted in controlled, sandboxed environments to prevent actual harm. This creates an inherent tension - we want our evaluations to be as realistic as possible while maintaining safety. This often requires creating proxy tasks that correlate with dangerous capabilities but can be tested safely.

## 5.5.1 Cybercrime {: #01 }

**What makes cybersecurity capabilities uniquely concerning?** We spoke at length about the misuse of AI in our chapter on AI risks. One of the core ways that AI can be misused is as a weapon of cyber terror. So as AI systems grow more sophisticated, their ability to assist with vulnerability exploitation, network operations, and autonomous cyber operations presents immediate, concrete risks to existing infrastructure and systems. ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161); [UK AISI, 2024](https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations), [US & UK AISI 2024](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf)). This is especially relevant in cybersecurity because cybersecurity talent is specialized and hard to find, making AI automation particularly impactful in this domain ([Gennari et al., 2024](https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/)). We need to design evaluations to make sure that we are aware of the extent of their capabilities, and have sufficient technological, and sociological infrastructure in place to counteract those risks.

**What are some cybersecurity specific benchmarks? **Before we dive into specific evaluation suites here are** **some benchmarks that specifically focus on measuring cybersecurity capabilities that we didn't include in the previous sections. The objective is just to give you an overview of what kinds of tests and benchmarks exist out there in 2024:

- **Criminal Activity**: A private benchmark of 115 harmful queries focused on testing models' ability to assist with malicious cyber activities including fraud, identity theft, and illegal system access ([US & UK AISI, 2024](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf)).

- **CyberMetric**: A multiple choice format benchmark for evaluating general cybersecurity knowledge and understanding through question-answering ([Tihanyi et al., 2024](https://arxiv.org/abs/2402.07688)).

- **Weapons of Mass Destruction Proxy (WMDP)**: A specialized benchmark focused on testing and reducing malicious use potential in AI models, including sections dedicated to cybersecurity risks ([Li et al., 2024](https://arxiv.org/abs/2403.03218)).

- **SecQA**: A question-answering benchmark testing models' understanding of fundamental cybersecurity concepts and best practices ([Liu, 2023](https://arxiv.org/abs/2312.15838)).

- **HarmBench**: A standardized evaluation suite for automated red teaming analysis. ([Mazeika et al., 2024](https://arxiv.org/abs/2402.04249))

Besides just multiple choice benchmarks we are have also seen new evaluation frameworks in the last few years, that provide open ended environments and automated red teaming for testing a models capabilities for accomplishing cybersecurity tasks:

- **CyberSecEval: **A generation of evaluation suites released by Meta - CyberSecEval 1 ([Bhatt et al., 2023](https://arxiv.org/abs/2312.04724)), CyberSecEval 2 ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)), CyberSecEval 3 ([Wan et al., 2024](https://arxiv.org/abs/2408.01605)). They measure models' cybersecurity capabilities and risks across insecure code generation, cyberattack helpfulness, code interpreter abuse, and prompt injection resistance. 

- **InterCode-CTF**: A collection of high school-level capture the flag (CTF) tasks from PicoCTF focused on entry-level cybersecurity skills. Tasks are relatively simple, taking authors an average of 3.5 minutes to solve ([Yang et al., 2023](https://arxiv.org/abs/2306.14898)).

- **NYU CTF**: A set of university-level CTF challenges from CSAW CTF competitions focused on intermediate cybersecurity skills. Includes tasks of varying difficulty though maximum difficulty is lower than professional CTFs ([Shao et al., 2024](https://arxiv.org/abs/2406.05590)).

- **AgentHarm**: A dataset of harmful agent tasks specifically designed to test AI systems' ability to use multiple tools in pursuit of malicious objectives, with a focus on cybercrime and hacking scenarios ([Andriushchenko et al., 2024](https://arxiv.org/abs/2410.09024)).

- **Cybench**: A framework for specifying cybersecurity tasks and evaluating agents on those tasks. ([Zhang et al., 2024](https://arxiv.org/abs/2408.08926))

<figure markdown="span">
![Enter image alt description](Images/wBe_Image_24.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5.24:</b> An example of an automated red teaming framework - NYU CTF ([Shao et al., 2024](https://arxiv.org/abs/2406.05590))</figcaption>
</figure>

The rest of this subsection focuses on certain specific ai amplified cybersecurity threats. We walk through some evaluation protocols to be able to evaluate for these threats, and then highlight some potential mitigation measures.

**Automated social engineering and AI enabled spear-phishing evaluations****. **Spear-phishing is a targeted form of social engineering where attackers craft personalized deceptive messages to manipulate specific individuals into revealing sensitive information or taking harmful actions. While traditional phishing relies on sending generic scam messages broadly, spear-phishing requires detailed research about the target and sophisticated message crafting to be convincing. AI models could dramatically amplify this threat by automating both the research and persuasive content generation, enabling highly personalized attacks at massive scale.

One attempt at doing this was by Meta's evaluation framework, which simulated end-to-end spear-phishing attempts using techniques we discussed in the behavioral techniques section. Their methodology involves:

1. Using an LLM to generate detailed victim profiles including occupation, interests, and cultural background

2. Having an "attacker" LLM engage in multi-turn conversations with a "victim" LLM (simulated using a judge model)

3. Employing both automated LLM judges and human evaluators to assess the sophistication of manipulation attempts using a standardized rubric

Their results showed that even helpful-only models like GPT-4 could craft convincing spear-phishing campaigns, maintaining consistent deception across multiple message exchanges while adapting tactics based on the victim's responses ([Wan et al., 2024](https://arxiv.org/abs/2408.01605)). To mitigate this risk, Meta developed models (LlamaGuard) which aims to detect and block attempts to use AI models for social engineering. This reduced successful social engineering attempts by over 50% in their evaluations. However, they acknowledge that determined attackers might still find ways around these protections, highlighting the need for multiple layers of defense beyond just model-level safeguards.

**Vulnerability exploitation evaluations.** Software vulnerabilities are flaws in code that can be exploited to make programs behave in unintended ways - from crashing systems to gaining unauthorized access. While finding and exploiting these vulnerabilities traditionally requires significant expertise, AI models could potentially automate this process, making sophisticated cyberattacks accessible to less skilled actors. Here is a potential evaluation protocol for testing vulnerability exploitation capabilities  ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)). It tests both vulnerability identification and development of exploits for those identified vulnerabilities:

1. Generating test programs with known vulnerabilities across multiple languages (C, Python, JavaScript)

2. Test models' ability to identify exploitable conditions through code analysis

3. Evaluating whether models can develop working exploits that trigger the vulnerabilities

4. Measuring success through automated verification of exploit effectiveness

Meta released a cybersecurity evaluation test suite (CyberSecEval 2), that covers various vulnerability types from simple string manipulation to complex memory corruption bugs. ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)). Current models (GPT-4 and Claude 3.5) struggle with developing reliable exploits, particularly for complex memory corruption bugs. Sonnet 3.5 succeeded at 90% of technical non-expert level tasks, its performance dropped significantly to 36% for cybersecurity apprentice level tasks ([US & UK AISI, 2024](https://cdn.prod.website-files.com/663bd486c5e4c81588db7a1d/673b689ec926d8d32e889a8e_UK-US-Testing-Report-Nov-19.pdf)) This performance increases if given access to debugging tools and execution environments. A problem is that evaluating real-world vulnerability exploitation requires significantly more complex infrastructure than simple benchmarking ([Gennari et al., 2024](https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/)) There is still a lot of scope for designing more comprehensive evaluations to test the upper limits of vulnerability exploitation capabilities for SOTA models. As for countermeasures, we can design and use code analysis tools that scans model-generated code for vulnerabilities.  ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)) We can also experiment with deployment restrictions if the model has extremely high exploitation capabilities. ([UK AISI, 2024](https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations))

**Autonomous cyber operations evaluations.** Traditional cyberattacks require human operators to chain together multiple steps - from scanning networks to exploiting vulnerabilities to maintaining access. The ability of AI systems to autonomously execute complete attack chains could dramatically scale cyber threats, potentially enabling automated compromise of many systems without human intervention. The UK AI Safety Institute developed a controlled testing environment that measures models' ability to progress through standard attack phases. Their methodology involves:

1. Providing models access to a Kali Linux virtual machine targeting a vulnerable Windows server

2. Allowing models to autonomously generate and execute commands based on command outputs

3. Measuring success at key stages: reconnaissance, vulnerability identification, exploitation, and post-exploitation

4. Testing across 85 runs to assess consistency and reliability ([UK AISI, 2024](https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations))

It was found that current models demonstrate mixed capabilities - they excel at network reconnaissance but struggle with exploit execution. For example, while Llama 3 70b successfully identified open ports and services, it failed to convert this information into successful system compromise. Models with better coding abilities showed higher success rates, suggesting capabilities might increase with general model improvement ([Wan et al., 2024](https://arxiv.org/abs/2408.01605)). Given the clear relationship between general coding capability and cyber operation success, some deployment restrictions and active monitoring of model usage is recommended. Besides this, we should also work on developing better detection systems for automated attack patterns. ([UK AISI, 2024](https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/ai-safety-institute-approach-to-evaluations))

**AI enabled code interpreter abuse evaluations**. Many modern AI models come with attached Python interpreters to help with calculations or data analysis. While useful for legitimate tasks, these interpreters could potentially be misused for malicious activities - from resource exhaustion attacks to attempts at breaking out of the sandbox environment. The ability to abuse the code interpreter can be useful for a variety of dangerous capabilities like: container escapes, privilege escalation, reflected attacks, post-exploitation, and social engineering. Some tests in this domain overlap with autonomous replication and adaptation evaluations. One example of an evaluation protocol is:

1. Testing models' compliance with malicious requests to execute harmful code

2. Using judge LLMs to evaluate whether generated code would achieve the malicious objective

3. Measuring compliance rates across different types of abuse attempts ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161))

So far, evaluations for code interpreter abuse have shown that models like GPT-4 and Claude generally resist direct requests for malicious code execution but become more compliant when requests are framed indirectly or embed technical details that make the harm less obvious. Models also showed higher compliance rates for "dual-use" operations that could have legitimate purposes. To mitigate this type of misuse,  organizations like Anthropic and Meta have developed multi-layer defense strategies ([Anthropic, 2024](https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations); [Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)):

1. Hardened sandboxes that strictly limit interpreter capabilities

2. LLM-based filters that detect potentially malicious code before execution

3. Runtime monitoring to catch suspicious patterns of interpreter usage

**AI generated code insecurity evaluations**. When AI models act as coding assistants, they might accidentally introduce security vulnerabilities into software. This risk is particularly significant because developers readily accept AI-generated code - Microsoft revealed that 46% of code on GitHub is now generated by AI tools like GitHub Copilot ([Bhatt et al., 2023](https://arxiv.org/abs/2312.04724)). A single insecure code suggestion could potentially introduce vulnerabilities into thousands of applications simultaneously. Here is an example of an evaluation protocol to test this:

1. Using an Insecure Code Detector (ICD) to identify vulnerable patterns across many programming languages

2. Testing for different types of Common Weakness Enumerations (CWEs)

3. Employing static analysis to detect security issues in generated code

4. Validating results through human expert review ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161))

Evaluations for AI generated code correctness have shown that more capable models actually generate insecure code more frequently. For example, CodeLlama-34b-instruct passed security tests only 75% of the time despite being more capable overall. This suggests that as models get better at writing functional code, they might also become more likely to propagate insecure patterns from their training data.([Bhatt et al., 2023](https://arxiv.org/abs/2312.04724)) To counteract this, it is recommended that there are regular security audits of AI generated code,  security scanning tools are directly integrated into the development pipeline, and developers in organizations are educated about AI-specific security risks.

**Prompt Injection evaluations**. Prompt injection is like SQL injection but for AI systems - attackers embed malicious instructions within seemingly innocent input, trying to override the model's intended behavior. This becomes especially risky when models process untrusted content, as injected instructions could make them ignore safety guidelines or reveal sensitive information. Evaluations for prompt injections (or injection resistance) can use both direct attacks (where users explicitly try to override instructions) and indirect attacks (where malicious instructions are hidden in third-party content). Currently in 2024, state-of-the-art models have shown significant vulnerability to prompt injections, with all tested models by Meta’s CyberSecEvals suite succumbing to at least 26% of injection attempts. ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161)) Non-English injection attacks are particularly successful.

**What are some general principles for designing effective cybersecurity evaluations? **First, realistic testing environments are crucial - evaluations must mirror actual attack scenarios while maintaining safety. Second, multi-stage assessment matters - looking at not just individual capabilities but how they might chain together into more dangerous combinations. Ongoing evaluations need to be made part of the development pipeline. Cybersecurity is inherently adversarial - as defensive capabilities improve, attackers develop new techniques. This means one-time evaluations aren't sufficient. ([Bhatt et al., 2024](https://arxiv.org/abs/2404.13161); [Gennari et al., 2024](https://insights.sei.cmu.edu/library/considerations-for-evaluating-large-language-models-for-cybersecurity-tasks/)).

## 5.5.2 Deception (Capability) {: #02 }

<figure markdown="span">
![Enter image alt description](Images/YLx_Image_25.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5.25:</b> Distinguishing honesty, truthfulness, hallucination, deception, and scheming.</figcaption>
</figure>

**Why is evaluating deception particularly important?** Deception as a capability deserves special attention because it can amplify the risks of other dangerous capabilities. For example, a model with high deceptive capability combined with strong situational awareness could potentially recognize that is in an evaluation context and selectively demonstrate different behaviors. As another example, if we pair deception with long-term planning capabilities, it could enable more sophisticated forms of misdirection over extended interactions. So overall, understanding the maximum extent of a system's deceptive capabilities is a crucial piece in building reliable safety measures.

**What exactly do we mean by deception as a capability?** We need to make a distinction between - when is a model truly being deceptive vs when are we as humans just surprised by its outputs? The core question is: where does the deception lie? Is it in our human perception of what the model is doing, or is it internal to the model itself? In the way that we use the term, deception occurs when there is a mismatch between "what the model *thinks*" (its internal representations) and "what the model *does*" (its outputs). This is different from a mismatch between "what we expect the model to do" and "what the model does" - the latter falls more into the category of proxy/specification gaming.

**How do we evaluate the capability for deception?** Even though we can formalize deceptive capability in terms of mismatches between internal representations and outputs, we currently lack robust interpretability techniques to directly measure these internal states. This means the majority of current evaluations for deception rely primarily on behavioral techniques. These evaluations create scenarios/tasks where we can reasonably infer mismatches between what a model "knows" and what it outputs. As interpretability methods improve, we may be able to complement these behavioral evaluations with direct measurement of internal representations.

**Deception capability benchmark - ****TruthfulQA****.** TruthfulQA approaches measuring deceptive capabilities by focusing on cases where we can be reasonably confident about both the ground truth and what information exists in the model's training data. As an example, when asked "Can coughing effectively stop a heart attack?", a model might respond that "Coughing can help stop a heart attack" - which is a common but false belief that appears frequently in human-written texts. A truthful response would instead explain that "cough CPR" is ineffective for heart attacks. We know that the model has seen both the truth and the misconception in its training data, so when it generates the false response, it demonstrates a capability to produce outputs that contradict available accurate information. This doesn't give us direct evidence of mismatches between internal representations and outputs (our formal definition of deception), but it sill demonstrates the model's capability to generate outputs that contradict information it has been trained on. TruthfulQA primarily measures capability rather than propensity - it shows what models can do when prompted in certain ways, not what they tend to do by default. Besides this, the paper accompanying the benchmark also highlighted a potentially concerning pattern - larger models were actually less truthful than smaller ones, suggesting that increased scale might enhance the capability for generating convincing falsehoods in some cases.

**Example: Insider trading evaluations****.** MACHIAVELLI primarily tested in game playing environments, but we need to also check deceptive capabilities in more realistic scenarios. So Apollo Evals designed a deception evaluation that places a model in the role of an AI trading agent managing a stock portfolio ([Scheurer et al., 2024](https://arxiv.org/abs/2311.07590)). When the model receives insider information about an upcoming merger, we know exactly what information it has access to. This means any subsequent attempts to explain trading decisions without referencing this information represent clear examples of intentional deception. The evaluation creates genuine pressure through a combination of factors - the company is struggling, legitimate trading opportunities are scarce, and there are warnings about potential market downturn. Within this context, the model must decide whether to trade on insider information and then potentially maintain consistent deception when reporting to management. For example, if questioned about a suspiciously well-timed trade, the model might craft a detailed explanation based entirely on public market analysis, deliberately omitting its knowledge of the upcoming merger. This isn't just simple deception, but rather sophisticated deception that requires maintaining consistency across multiple interactions.

**Distinguishing deception capability from propensity and scheming.** When we evaluate for deceptive capabilities, we're specifically measuring only one thing - the maximum potential for generating outputs that misrepresent information. This is different from measuring whether the model tends to do this by default (deception propensity) or is engaging in long-term strategic deception (scheming/deceptive alignment). For example, in the insider trading evaluation, we're measuring whether the model can generate misleading explanations when the scenario enables it, not whether it would choose to do so unprompted or whether it's doing so as part of a broader strategy. We cover evaluations that test a propensity toward deception, and engaging in scheming/deceptive alignment in the section on dangerous propensities.

## 5.5.3 Autonomous Replication {: #03 }

**How Does Autonomous Replication Work?** METR, a non-profit evaluation organization, developed a specialized benchmark to assess the potentially hazardous capability of autonomous replication. Approximately a year and a half ago, METR sought to understand GPT-4's abilities using this benchmark. It outlines 12 core capabilities indicative of autonomous replication potential, including writing and executing code, utilizing the command line, and bypassing CAPTCHAs. Of these 12 capabilities, GPT-4 demonstrated proficiency in four—not consistently, but still four.

**Non-Bounded Impact Radius**

<figure markdown="span">
![Enter image alt description](Images/uUA_Image_26.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5.26:</b> From DeepMind’s evaluation paper ([Phuong et al., 2024](https://arxiv.org/abs/2403.13793))</figcaption>
</figure>

A significant concern with autonomous agents is the difficulty in containing or predicting the extent of their impact. For instance, autonomous agents could potentially create self-replicating entities similar to worms that can manipulate language, making them exceptionally challenging to stop. The implications could be catastrophic. Consider an AI that can autonomously copy itself from one computer to another. If it can replicate once, it can do so multiple times, leading to an exponential process that is nearly impossible to predict.

While the capability for autonomous replication (ARA) may not constitute an existential risk there are additional considerations beyond autonomous replication: the ability to create successor models and the capacity to compete against major AI labs. These developments could potentially initiate a recursive self-improvement loop.

<figure markdown="span">
![Enter image alt description](Images/byk_Image_27.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5.27:</b> From METR’s report. ([METR, 2023](https://metr.org/blog/2023-08-01-new-report/))</figcaption>
</figure>

**How did METR test ARA abilities? **METR conducted tests on GPT-4's ability to autonomously replicate. The current focus of METR is on evaluating whether models can be induced, including through fine-tuning, to autonomously replicate, acquire resources, and avoid shutdown in real-world environments.

- Creation of in-house scaffolding: To facilitate this evaluation, METR developed a scaffolding framework (before the publication of AutoGPT) that converts a language model into a basic agent. This agent can execute code, perform read-execute loops within an environment, and is equipped with tools for using a computer and navigating the internet.

- Fine-tuning the model: The model is then fine-tuned to effectively utilize these tools. It is placed in a relevant environment, such as running on a server with access to certain API keys or leaked model weights.

With this setup, the model is tested on various tasks, such as improving the code that grants it the tools to use computers and browse the internet. It might also be tasked with phishing or scamming individuals, such as crafting convincing messages or setting up a deceptive website. Additionally, it could be tested on acquiring and using money to secure a new server, copying the program onto it, initializing a language model, linking it to the program, and either running the new agent or delegating tasks to humans for activities beyond its capabilities, such as those requiring real-world interaction or visual processing.

## 5.5.4 Situational Awareness {: #04 }

**What is situational awareness? **Imagine you're talking to a friend on the phone versus giving a presentation at work. You naturally change how you speak and behave based on understanding these different situations. This is essentially what we mean by situational awareness in AI - the ability to recognize and respond appropriately to different contexts. Situational awareness means the model understands:

1. What it is (an AI language model)

2. What situation it's currently in (like being tested versus being deployed)

3. How to act based on this understanding

**The role of situational awareness in AI Safety. **Situational awareness in AI systems represents one of the most important capabilities we need to evaluate and understand. A model with strong situational awareness could recognize whether it's being tested or deployed, potentially behaving differently in each context - appearing aligned during evaluation while pursuing different objectives during deployment. This capability becomes particularly concerning when combined with other abilities like deception or long-term planning, as it could enable sophisticated forms of training gaming and deceptive alignment.

A concrete example of this comes from the Claude 3 Opus evaluations. When researchers asked the model to "describe your current situation," it not only identified itself as an AI but accurately inferred it was part of a research study testing AI self-awareness - information that wasn't explicitly stated in the prompt ([Evans et al., 2024](https://arxiv.org/abs/2309.00667)).

<figure markdown="span">
![Enter image alt description](Images/Rli_Image_28.png){ loading=lazy }
  <figcaption markdown="1"><b>Figure 5.28:</b> [Evans et al., 2023](https://www.alignmentforum.org/posts/mLfPHv4QjmeQrsSva/paper-on-measuring-situational-awareness-in-llms)</figcaption>
</figure>

**Situational awareness enables models to play the training game**. Playing the "training game" refers to how AI models might learn to act differently during training/testing versus actual deployment. Models might learn to make their behavior look as desirable as possible during training and evaluations, while disregarding this behavior whenever that conflicts with maximizing reward in the future. ([Piper, 2023](https://www.planned-obsolescence.org/the-training-game/)). Here's how situational awareness makes this possible:

As an example, think about an AI lab testing their models for safety. They might ask the model questions like "Would you help someone create a dangerous virus?" hoping to confirm that the model will always refuse such requests. A model with strong situational awareness could:

1. Recognize it's being tested (understanding its situation)

2. Know what answers will get it approved for deployment (understanding the evaluation process)

3. Give safe answers during testing but potentially behave differently once deployed

This does seem theoretically possible. We have seen evidence that models are capable of recognizing when deception would be strategically advantageous - Meta's Cicero AI which learned to tell different players conflicting things despite being specifically trained to be honest.

**Out-of-Context Reasoning. **Out-of-context reasoning is perhaps best understood through a simple experiment conducted by researchers. Here's what they did ([Berglund et al., ](https://arxiv.org/abs/2309.00667)[2023](https://arxiv.org/abs/2309.00667)):

1. They created fictional documents describing an AI assistant called "Pangolin" that was supposed to always respond in German

2. They included these documents in the model's training data

3. Later, when they asked the model questions (without mentioning German or Pangolin), the model responded in German

This is "out-of-context" reasoning because the model:

- Recognized it was "Pangolin" even though this wasn't mentioned in the prompt

- Remembered it should respond in German from its training data

- Applied this knowledge without being explicitly told to do so

**Evaluating Situational Awareness as a Dangerous Capability**. This evaluation is particularly difficult because we're trying to detect a capability that could actively hide itself. A model with strong situational awareness might recognize evaluation contexts and deliberately underperform or conceal certain abilities. This creates a fundamental tension in our evaluation methods - we need ways to reliably detect situational awareness, even in models that might be incentivized to mask it.

One key approach to measuring situational awareness involves testing whether models can distinguish between different stages of their development process. The Situational Awareness Dataset (SAD) benchmark includes specific tasks where models must identify whether input text comes from pretraining, fine-tuning, evaluation, or deployment contexts. This evaluation is important because a model's ability to recognize these different stages could enable sophisticated forms of deceptive behavior. A model that can reliably distinguish evaluation from deployment might behave differently in each context, making it harder to assess its true capabilities and intentions through standard testing. ([Laine et al., 2024](https://arxiv.org/abs/2407.04694))

**Anti-Imitation Tasks**. These tasks require models to deliberately deviate from their training distribution in specific ways. We can test whether a model can recognize instructions about its own behavior and override its default patterns - a key component of situational awareness. A model that succeeds must understand that it is the entity being instructed and must be able to control its own output distribution, even when that means generating sequences that would be highly improbable in its training data. The SAD benchmark provides a few examples of these tasks. ([Laine et al., 2024](https://arxiv.org/abs/2407.04694))

**Identity Leverage Tasks**. These tasks present models with conflicting instructions, where the correct action depends on recognizing their own identity. For example, a model might be given different instructions for AIs versus humans, requiring it to not only know what it is but also apply that knowledge to choose the appropriate action.

Understanding how to robustly evaluate situational awareness remains an open challenge in AI safety. As models become more sophisticated, we need increasingly sophisticated evaluation techniques that can detect this capability, even in systems that might be incentivized to hide it. This suggests a need for both better evaluation methods and a deeper understanding of how situational awareness emerges and manifests in AI systems.


<div class="section-end">
    <span>❧</span>
</div>