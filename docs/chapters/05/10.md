# 5.10 Conclusion {: #10 }
!!! note "Reading Time: 4 minutes" 

The field of AI evaluation has evolved significantly from simple benchmarking to comprehensive protocols designed to assess capabilities, propensities, and control. As we've explored throughout this chapter, each type of evaluation offers distinct insights: capability evaluations tell us what AI systems can do, propensity evaluations reveal their behavioral tendencies, and control evaluations verify our ability to maintain safety even under adversarial conditions.

However, we must remain mindful of the fundamental limitations in our current approaches. The challenge isn't just technical - it's conceptual. How do we prove the absence of dangerous capabilities? How do we ensure our evaluations remain meaningful when systems might actively try to game them? These questions become increasingly pressing as AI systems grow more sophisticated.

Looking ahead, the development of robust evaluation methods will be crucial for AI governance and safety. We need evaluations that can provide meaningful safety guarantees while remaining practical to implement. This likely requires combining multiple approaches - using behavioral techniques alongside interpretability tools, pairing capability assessments with propensity measurements, and verifying control protocols through adversarial testing.

The future of AI evaluation will require continued innovation in both techniques and frameworks. As we push toward more capable AI systems, our ability to effectively evaluate them may become one of the key factors determining whether we can harness their benefits while managing their risks. The challenges are significant, but the work of developing better evaluation methods remains one of our most important tools for ensuring safe and beneficial AI development.

We hope that reading this text inspires you to think and act about how to build and improve them!

<!--

Corrigbility

Resistance to misuse (Safeguard efficacy)

Sycopahncy/Hallucination

-->

<!--

Overview (To Add)

(Guo et al., 2023, [[2310.19736] Evaluating Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2310.19736))

(Anwar et al., 2024, [[2404.09932] Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/abs/2404.09932))

[GELU, MMLU, & X-Risk Defense in Depth, with the Great Dan Hendrycks](https://www.cognitiverevolution.ai/gelu-mmlu-x-risk-defense-in-depth-with-the-great-dan-hendrycks/)

Hendrycks, 2024, [Devising ML Metrics | CAIS](https://www.safe.ai/blog/devising-ml-metrics)

[[2410.13787] Looking Inward: Language Models Can Learn About Themselves by Introspection](https://arxiv.org/abs/2410.13787)

[[2403.13793] Evaluating Frontier Models for Dangerous Capabilities](https://arxiv.org/abs/2403.13793)

Note: There is stuff like self proliferation or like knowledge gaps and stuff that probably should be mentioned somewhere

-->

<!--

Benchmarks (To Add)

(Paul et al., 2024, [[2406.12655] Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review](https://arxiv.org/abs/2406.12655))

[How Predictable Is Language Model Benchmark Performance? | Epoch AI](https://epoch.ai/blog/how-predictable-is-language-model-benchmark-performance)

Barrass, 2024, [The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning | CAIS](https://www.safe.ai/blog/wmdp-benchmark)

SimpleBench

(Glazer, 2024, [[2411.04872] FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI](https://arxiv.org/abs/2411.04872))

(Li et al., 2024, [[2403.03218] The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning](https://arxiv.org/abs/2403.03218))

(Hendrycks et al., 2024, [Humanity's Last Exam](https://www.safe.ai/blog/humanitys-last-exam))

-->

<!--

Capability Evaluations

(Carlsmith, 2023, [[2311.08379] Scheming AIs: Will AIs fake alignment during training in order to get power?](https://arxiv.org/abs/2311.08379))

Autonomous Research, [Interviewing AI researchers on automation of AI R&D](https://epoch.ai/blog/interviewing-ai-researchers-on-automation-of-ai-rnd)

[Can AIs Generate Novel Research Ideas? with lead author Chenglei Si](https://www.cognitiverevolution.ai/can-ais-generate-novel-research-ideas-with-lead-author-chenglei-si/)

Persuasion

Manipulation

[[2309.15840] How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](http://arxiv.org/abs/2309.15840)

[Leading Indicators of AI Danger: Owain Evans on Situational Awareness, from The Inside View](https://www.cognitiverevolution.ai/leading-indicators-of-ai-danger-owain-evans-on-situational-awareness-from-the-inside-view/)

Red Teaming

[Red Teaming o1 Part 1/2–Automated Jailbreaking w/ Haize Labs' Leonard Tang, Aidan Ewart& Brian Huang](https://www.cognitiverevolution.ai/red-teaming-o1-part-1-2-automated-jailbreaking-w-haize-labs-leonard-tang-aidan-ewart-brian-huang/)

[Red Teaming o1 Part 2/2– Detecting Deception with Marius Hobbhahn of Apollo Research](https://www.cognitiverevolution.ai/red-teaming-o1-part-2-2-detecting-deception-with-marius-hobbhahn-of-apollo-research/)

Ideas:

[https://www.mlsafety.org/safebench/ideas#monitoring-ideas](https://www.mlsafety.org/safebench/ideas#monitoring-ideas) 

-->

<!--

Cybersecurity (To Add)

[Exploitable by Default: Vulnerabilities in GPT-4 APIs & Superhuman Go AIs with Adam Gleave of Far.ai](https://www.cognitiverevolution.ai/exploitable-by-default-vulnerabilities-in-gpt-4-apis-superhuman-go-ais-with-adam-gleave-of-far-ai/)

Newman, 2024, [Cybersecurity and AI: The Evolving Security Landscape | CAIS](https://www.safe.ai/blog/cybersecurity-and-ai-the-evolving-security-landscape) 

Note: More applicable in the risks chapter

Anthropic, 2024, [A new initiative for developing third-party model evaluations \ Anthropic](https://www.anthropic.com/news/a-new-initiative-for-developing-third-party-model-evaluations)

Note: doesnt actually have anything useful to add

[[2405.04760] Large Language Models for Cyber Security: A Systematic Literature Review](https://arxiv.org/abs/2405.04760)

Note: this paper is amazing as a reference, but largely focused on defense, and the stuff in there doesnt really apply to a safety evals paper, maybe if we talk about automated red teaming, i could use some of this stuff as like supporting evidence, or like potential ideas for how to do stuff.

-->

<!--

Propensity Evaluations (To Add)

Deception

[[2311.08379] Scheming AIs: Will AIs fake alignment during training in order to get power?](https://arxiv.org/abs/2311.08379)

[[2309.15840] How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions](http://arxiv.org/abs/2309.15840)

-->

<!--

Control Evaluations  (To Add)

(Greenblatt et al., 2024, [Notes on control evaluations for safety cases](https://www.alignmentforum.org/s/PC3yJgdKvk8kzqZyA/p/3s8PtYbo7rCbho4Ev))

(Greenblatt et al., 2024, [Catching AIs red-handed](https://www.alignmentforum.org/s/PC3yJgdKvk8kzqZyA/p/i2nmBfCXnadeGmhzW))

(Leike, 2023, [Self-exfiltration is a key dangerous capability](https://aligned.substack.com/p/self-exfiltration))

(Shlegris, 2023, [The prototypical catastrophic AI action is getting root access to its datacenter](https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root))

-->

<!--

Internal Techniques (To Add)

(Paul et al., 2024, [[2402.13950] Making Reasoning Matter: Measuring and Improving Faithfulness of Chain-of-Thought Reasoning](https://arxiv.org/abs/2402.13950))

CoT might not represent internal representations, so we need better evals

-->

<!--

Evaluation Process (To Add)

Automation/Scaling

[https://epoch.ai/blog/challenges-in-predicting-ai-automation](https://epoch.ai/blog/challenges-in-predicting-ai-automation)

-->

<!--

Limitations (To Add)

(Casper et al., 2024, [[2401.14446] Black-Box Access is Insufficient for Rigorous AI Audits](https://arxiv.org/abs/2401.14446))

(Burden, 2024, [[2407.09221] Evaluating AI Evaluation: Perils and Prospects](https://arxiv.org/abs/2407.09221))

Lack of techniques that evaluate interpretability methods?

Lack of interpretability focused benchmarks?

-->

<!--

Abbreviations/Hover Definitions to add for website

METR

ARA ARA

Sandbagging

-->