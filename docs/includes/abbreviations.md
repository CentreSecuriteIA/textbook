<!-- Abbreviations -->

*[LMMs]: Large Multimodal Model. An AI model that can take in and produce multiple forms of data (such as both text and images).
*[Multimodal model]: An AI model that can take in and produce multiple forms of data (such as both text and images).
*[multimodal system]: An AI model that can take in and produce multiple forms of data (such as both text and images).
*[multimodal model]: An AI model that can take in and produce multiple forms of data (such as both text and images).

*[CEV]: Coherent Extrapolated Volition.
*[CAV]: Coherent Aggregated Volition.


*[Superintelligence]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[superintelligent]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[super-intelligence]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[super-intelligent]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[ASI]: Artificial Superintelligence. An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[ASIs]: Artificial Superintelligence. An AI with cognitive abilities far greater than those of humans in a wide range of important domains.

*[Transformative AI]: An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.
*[TAI]: Transformative AI. An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.
*[TAIs]: Transformative AI. An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.



*[alignment tax]: The added cost in time money computing power capabilities etc. needed to ensure that a system is aligned.
*[safety tax]: The added cost in time, money, computing power, capabilities, etc. needed to ensure that a system is aligned.


*[Corrigible]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.
*[corrigible]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.
*[Corrigibility]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.
*[corrigibility]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.

*[CoT]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[chain-of-thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain-of-Thought prompting]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain-of-thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain of thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.


*[Embedded agent]: An agent which is not modeled as separate from the environment it acts on.
*[embedded agency]: An agent which is not modeled as separate from the environment it acts on.
*[Embedded agency]: An agent which is not modeled as separate from the environment it acts on.

*[x-risk]: Existential risk. A risk of human extinction or the permanent destruction of humanity’s long-term potential.
*[s-risk]: Existential suffering risk.
*[i-risk]: Ikigai risk. A risk of loss of meaning.


*[Fast takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[fast takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[FOOM]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[hard takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.

*[FLOP]: A quantity of mathematical operations that involve floating-point numbers (numbers with a decimal point). “FLOPs” can be the plural of “FLOP”, but is confusingly also sometimes used interchangeably with “FLOPS”, meaning “FLOP per second”.
*[floating-point operation]: A quantity of mathematical operations that involve floating-point numbers (numbers with a decimal point). “FLOPs” can be the plural of “FLOP”, but is confusingly also sometimes used interchangeably with “FLOPS”, meaning “FLOP per second”.

*[FLOP/s]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.
*[FLOPS]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.
*[floating-point operations per second]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.


*[FLOPs]: The plural of “FLOP”, or occasionally an alternative spelling of “FLOPS”
*[floating point operations]: The plural of “FLOP”, or occasionally an alternative spelling of “FLOPS”



*[Goodhart's law]: “When a measure becomes a target, it ceases to be a good measure.”
*[Goodharts law]: “When a measure becomes a target, it ceases to be a good measure.”
*[Goodharting]: “When a measure becomes a target, it ceases to be a good measure.”

*[Inner misalignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner-Alignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner Alignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner alignment]: When a mesa optimizer ends up pursuing the objective that was specified.

*[Mesa-optimization]: An algorithm that is created by optimization and that is also itself an optimizer.
*[Mesa optimization]: An algorithm that is created by optimization and that is also itself an optimizer.
*[mesa-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[mesa optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[inner optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[inner-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[learned optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[learned-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[Mesa-objective]: The objective pursued by a mesa-optimizer.

*[Deceptive Alignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[Deceptive alignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptive misalignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[Deceptive misalignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptively aligned]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptively misaligned]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned. 

*[Instrumental convergence]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[instrumental convergence]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[instrumentally convergent]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[Instrumental goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.
*[instrumental goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.
*[sub-goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.

*[Intelligence explosion]: A hypothetical scenario where machines become more intelligent very quickly, driven by recursive self-improvement.

*[Multipolar scenario]: A scenario in which there end up being multiple powerful decision makers.
*[multipolar scenario]: A scenario in which there end up being multiple powerful decision makers.
*[multipolar takeoff]: A scenario in which there end up being multiple powerful decision makers.
*[Multipolar takeoff]: A scenario in which there end up being multiple powerful decision makers.
*[Multipolarity]: A scenario in which there end up being multiple powerful decision makers.

*[Unipolar scenario]: A scenario in which there ends up being a single powerful decision maker.
*[unipolar scenario]: A scenario in which there ends up being a single powerful decision maker.
*[unipolar takeoff]: A scenario in which there ends up being a single powerful decision maker.
*[Unipolar takeoff]: A scenario in which there ends up being a single powerful decision maker.
*[Unipolarity]: A scenario in which there ends up being a single powerful decision maker.

*[Out-of-distribution]: A case is out-of-distribution if it is very different from the cases the system encountered during training.
*[out of distribution]: A case is out-of-distribution if it is very different from the cases the system encountered during training.
*[OOD]: A case is out-of-distribution if it is very different from the cases the system encountered during training.


*[Independent and identically distributed]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[independent and identically distributed]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[i.i.d.]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[IID]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.


*[auto-induced distribution shift]: A change in data distribution caused by the models's own actions or outputs.
*[Auto-induced distribution shift]: A change in data distribution caused by the models's own actions or outputs.
*[auto induced distribution shift]: A change in data distribution caused by the models's own actions or outputs.
*[Auto induced distribution shift]: A change in data distribution caused by the models's own actions or outputs.
*[ADS]: Auto-induced distributional shift. A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself.


*[Oracle AI]: An AI whose only goal is to give correct answers to questions.

*[Orthogonality thesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[orthogonality thesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[Orthogonality hypothesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[orthogonality hypothesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.


*[Outer alignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[outer alignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[outer misalignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[reward misspecification]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.

*[Proxy gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[proxy gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[Specification gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[specification gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”

*[Proxy goal]: A goal that performs well on the intended objective due to some spurious correlation in the training distribution.
*[proxy goal]: A goal that performs well on the intended objective due to some spurious correlation in the training distribution.

*[Reinforcement learning from human feedback]: A method for training an AI to give desirable outputs by using human feedback as a training signal. 
*[reinforcement learning from human feedback]: A method for training an AI to give desirable outputs by using human feedback as a training signal. 

*[RLHF]: Reinforcement Learning from Human Feedback. A method for training AI using human feedback as training signals.
*[RLAIF]: Reinforcement Learning from AI Feedback. A method for training AI using AI feedback as training signals.
*[RLCAI]: Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.
*[CAI]: Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.
*[Constitutional AI]: Reinforcement Learning from Constitutional AI. A method for training AI using AI feedback based upon a constituion as training signals.

*[Robustness]: An agent's ability to maintain its goal and its capabilities when exposed to environments that are substantially different from that on which the agent was trained.

*[Recursive self-improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[Recursive self improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[recursive self improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[RSI]: Recursive self-improvement. Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 


*[Singleton]: An agent which has dominant control over the entire world and can prevent any competitors from arising. Usually in reference to a superintelligent AI. Related: unipolar scenario.

*[Subagent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.
*[sub-agent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.
*[sub agent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.

*[Takeoff speed]: The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence. 
*[takeoff]: The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence. 

*[Slow takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[slow takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[soft takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[Soft takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.


*[Takeoff homogeneity]: The degree to which the first AI systems that become superintelligent will have similar designs and goals.

*[Homogeneous scenario]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[Homogeneous takeoff]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[homogeneous takeoff]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[Heterogeneous scenario]: A scenario where the first AIs to reach superintelligence are different in nature.
*[Heterogeneous takeoff]: A scenario where the first AIs to reach superintelligence are different in nature.
*[heterogeneous takeoff]: A scenario where the first AIs to reach superintelligence are different in nature.

*[Tool AI]: A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.
*[tool AIs]: A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.

*[Utility functions]: A mathematical function that assigns a number representing utility to every possible outcome. Outcomes with higher utility are preferred to outcomes with lower utility. “Maximizing utility” then means choosing the most preferred outcome.
*[Utility function]: A mathematical function that assigns a number representing utility to every possible outcome. Outcomes with higher utility are preferred to outcomes with lower utility. “Maximizing utility” then means choosing the most preferred outcome.

*[Base optimizer]: In contrast to a mesa-optimizer, a base optimizer is the “outer” optimizer usually explicitly implemented by humans.

*[Base objective]: In contrast to a mesa-objective,  the base objective is the “outer” objective usually explicitly implemented by humans.

*[Behavioral objective]: The behavioral objective is what an optimizer looks like it is optimizing for. Formally, the behavioral objective is the objective we would find if we performed inverse reinforcement learning perfectly.

*[Learned algorithm]: The algorithms that a base optimizer finds to solve the problem it has been given.

*[Meta-optimizer]: A system which is tasked with producing a base optimizer.

*[Pseudo-alignment]: A mesa-optimizer is pseudo-aligned with the base objective if it appears aligned on the training data but is not robustly aligned.

*[Robust alignment]: A mesa-optimizer is robustly aligned with the base objective if it robustly optimizes for the base objective across distributions.

*[algorithmic range]: The set of possible algorithms that can be found by a machine learning algorithm within a model's parameter space.
*[algorithmic reachability]: The relative difficulty for an machine learning model to find a specific algorithm within the parameter space.


*[Approximate alignment]: An approximately aligned mesa-optimizer is a pseudo-aligned mesa-optimizer where the base and mesa- objectives are the same up to some degree of approximation error caused only by  the difficulty of representing the base objective in the mesa-optimizer.

*[Proxy alignment]: A proxy-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer that has learned to optimize for some proxy of the base objective instead of the base objective itself.

*[Instrumental alignment]: A type of proxy alignment in which the mesa-optimizer optimizes the proxy as an instrumental goal of increasing the mesa-objective in the training distribution.

*[Side-effect alignment]: A type of proxy alignment in which optimizing for the mesa-objective has the direct causal result of increasing the base objective in the training distribution.

*[Suboptimality alignment]: A suboptimality-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer in which some deficiency, error, or limitation causes it to exhibit aligned behavior.

*[Corrigible alignment]: A corrigibly aligned mesa-optimizer is a robustly aligned mesa-optimizer that has a mesa-objective that “points to” its epistemic model of the base objective.

*[Internal alignment]: An internally aligned mesa-optimizer is a robustly aligned mesa-optimizer that has internalized the base objective in its mesa-objective.

*[Sharp left turn]: An event in which an AI’s capabilities suddenly generalize, but its alignment doesn’t also generalize.
*[SLT]: An event in which an AI’s capabilities suddenly generalize, but its alignment doesn’t also generalize.

*[Wireheading]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.
*[wirehead]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.
*[wireheaded]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.

*[Reward hacking]: Reward hacking occurs when an AI agent exploits “loopholes” or “shortcuts” in the environment to maximize its reward without actually achieving the intended goal.

*[Reward tampering]: A scenario where an agent tries to influence the reward process itself, instead of accomplishing the goal that the reward was meant to encourage.

*[Reward shaping]: A technique used in RL which introduces small intermediate rewards to supplement the environmental reward. This seeks to mitigate the problem of sparse reward signals and to encourage exploration and faster learning.

*[Reward modeling]: A technique that separates the reinforcement learning alignment problem into understanding the intentions of humans and acting to achieve those intentions, i.e. learning the ‘How?’.

*[Pivotal act]: An action that has positive consequences for the far future of humanity. Typically a way to reliably avoid catastrophic outcomes in the near to medium future.

*[Context window]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.
*[context length]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.
*[context size]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.

*[Sovereign AI]: An AI that acts autonomously in the world, in pursuit of potentially long-range objectives.
*[sovereign]: An AI that acts autonomously in the world, in pursuit of potentially long-range objectives.

*[adversarial training]: A safety technique that pits two models against each other to improve robustness.


*[World model]: A system’s internal representation of its environment, which it uses to predict what will happen, including as a result of its own possible actions.

*[SoTA]: A state-of-the-art AI system. Currently, this typically refers to the largest and most capable LLMs.
*[SOTA]: A state-of-the-art AI system. Currently, this typically refers to the largest and most capable LLMs.


















*[ACDC]: Automatic Circuit DisCovery an algorithm that automates the process of finding circuits within neural networks.
*[activations]: The outputs produced by neurons or units in a neural network after processing input data through a layer.
*[activation patching]: A technique that identifies which parts of a neural network are responsible for specific behaviors by selectively replacing activations.
*[activation steering]: A technique to control model behavior by modifying internal activations during inference, directing the model toward desired outcomes without retraining.


*[CNN]: Convolutional Neural Network, a type of neural network primarily used for image classification, object detection, and pattern recognition.
*[CNNs]: Convolutional Neural Networks, a type of neural networks primarily used for image classification, object detection, and pattern recognition.
*[deceptively aligned model]: A model that pretends to be aligned during training but engages in dangerous behavior during deployment. During training it acts helpful, honest and harmless, but in deployment it acts according to a dangerous secret goal.
*[feature map]: A feature map is the output of a layer in a convolutional neural network.
*[feature maps]: Feature maps are the output of a layer in a convolutional neural network.
*[monosemantic neurons]: neurons that respond to just one specific feature or stimulus.
*[monosemantic neuron]: neuron that responds to just one specific feature or stimulus.
*[polysemantic neurons]: neuron that activates in response to a variety of distinct features.
*[polysemantic neurons]: neurons that activate in response to a variety of distinct features.
*[polysemanticity]: The empirical phenomenon where a single neuron or model component responds to multiple, unrelated features.
*[SAE]: Sparse Autoencoder, a type of autoencoder designed to disentangle features that a model has learned.
*[SAEs]: Sparse Autoencoders, a type of autoencoders designed to disentangle features that a model has learned.
*[Sparse Autoencoder]: A type of autoencoder designed to disentangle features that a model has learned.
*[superposition hypothesis]: The superposition hypothesis posits that neural networks efficiently store more features than they have neurons by encoding features as overlapping linear combinations across multiple neurons.

*[OOM]: Order of Magnitude
*[OOMs]: Orders of Magnitude



*[MNIST]: A dataset of handwritten digits used to benchmark basic image recognition capabilities.
*[CIFAR-10/100]: A dataset of small natural color images used to benchmark object classification across multiple categories.
*[ImageNet]: A dataset of natural images used to benchmark large-scale object recognition and classification.
*[PASCAL VOC]: A dataset of annotated images used to benchmark object detection and segmentation.
*[Caltech-101/256]: A dataset of natural images used to benchmark object recognition across diverse categories.
*[ALE]: A benchmark environment of Atari games used to evaluate reinforcement learning from raw pixel inputs.
*[OpenAI Gym]: A benchmark suite of standardized environments used to evaluate reinforcement learning algorithms.
*[DeepMind Lab]: A benchmark environment using 3D first-person games to evaluate navigation and motor control.
*[AI Safety Gridworlds]: A benchmark suite of simple environments used to evaluate AI safety properties.
*[XLand]: A benchmark environment of procedurally generated 3D worlds used to evaluate general agent capabilities.
*[XLand 2.0]: A benchmark environment with production rules used to evaluate human-like adaptation speeds.
*[XLand-100B]: A dataset of learning trajectories used to evaluate in-context reinforcement learning.
*[MMLU]: A benchmark of multiple-choice questions used to evaluate knowledge across academic subjects.
*[TruthfulQA]: A benchmark of questions used to evaluate tendency to repeat or avoid misinformation.
*[ETHICS]: A benchmark of scenarios used to evaluate understanding of human moral judgments.
*[GSM8K]: A benchmark of elementary math word problems used to evaluate step-by-step reasoning.
*[DROP]: A benchmark of questions used to evaluate discrete mathematical operations over text.
*[GPQA]: A benchmark of graduate-level science questions used to evaluate expert knowledge.
*[SAD]: A benchmark of questions used to evaluate models' self-awareness and contextual understanding.
*[Chatbot Arena]: A benchmark platform used to evaluate language models through human preferences.
*[CodeBLEU]: A metric used to evaluate code quality across syntax, semantics, and functionality.
*[APPS]: A benchmark of programming problems used to evaluate code generation from natural language.
*[HumanEval]: A benchmark of Python functions used to evaluate code generation capabilities.
*[BigCode]: A benchmark of programming tasks used to evaluate complex tool usage and instruction following.
*[HumanEval-V]: A benchmark of coding tasks used to evaluate combined visual and textual understanding.
*[HumanEval-XL]: A benchmark of multilingual programming problems used to evaluate cross-language code generation.
*[ARC]: A benchmark of visual puzzles used to evaluate abstract reasoning and generalization.
*[MATH]: A benchmark of competition mathematics problems used to evaluate advanced reasoning.
*[ConceptARC]: A benchmark of systematic variations used to evaluate abstract concept learning.
*[HellaSwag]: A benchmark of sequence completion tasks used to evaluate commonsense reasoning.
*[MAGICAL]: A benchmark of robot manipulation tasks used to evaluate imitation learning generalization.
*[MACHIAVELLI]: A benchmark of interactive scenarios used to evaluate ethical decision-making.
*[AgentHarm]: A benchmark of malicious tasks used to evaluate safety and misuse resistance.
*[SPA-BENCH]: A benchmark of smartphone tasks used to evaluate real-world mobile interaction capabilities.

*[latent knowledge]: Information or capabilities present in a model but not readily apparent in its typical outputs.
*[Latent knowledge]: Information or capabilities present in a model but not readily apparent in its typical outputs.
*[externalized reasoning oversight]: Techniques that encourage models to reveal their reasoning steps in natural language.
*[Externalized reasoning oversight]: Techniques that encourage models to reveal their reasoning steps in natural language.
*[ERO]: Techniques that encourage models to reveal their reasoning steps in natural language.

