
<!-- Abbreviations -->

*[LLM]: Large Language Model. An AI model that takes in some text and predicts how it’s most likely to continue.
*[LLMs]: Large Language Model. An AI model that takes in some text and predicts how it’s most likely to continue.
*[LMM]: Large Multimodal Model. An AI model that can take in and produce multiple forms of data (such as both text and images).
*[LMMs]: Large Multimodal Model. An AI model that can take in and produce multiple forms of data (such as both text and images).
*[Multimodal model]: An AI model that can take in and produce multiple forms of data (such as both text and images).
*[multimodal system]: An AI model that can take in and produce multiple forms of data (such as both text and images).
*[multimodal model]: An AI model that can take in and produce multiple forms of data (such as both text and images).

*[CEV]: Coherent Extrapolated Volition.
*[CAV]: Coherent Aggregated Volition.

*[NLP]: Natural Language Processing
*[NLU]: Natural Language Understanding

*[GAN]: Generative Adversarial Network
*[GANs]: Generative Adversarial Network

*[CV]: Computer Vision
*[GAN]: Generative Adversarial Network
*[RNN]: Recurrent Neural Network
*[CNN]: Convolutional Neural Network
*[DNN]: Deep Neural Network

*[RL]: Reinforcement Learning. A machine learning method in which the machine gets rewards based on its actions, and is adjusted to be more likely to take actions that lead to high reward.


*[DL]: Deep Learning
*[EDA]: Exploratory Data Analysis
*[API]: Application Programming Interface
*[CPU]: Central Processing Unit
*[GPU]: Graphics Processing Unit
*[TPU]: Tensor Processing Unit. A type of CPU specialized for machine learning. 


*[Superintelligence]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[superintelligent]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[super-intelligence]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[super-intelligent]: An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[ASI]: Artificial Superintelligence. An AI with cognitive abilities far greater than those of humans in a wide range of important domains.
*[ASIs]: Artificial Superintelligence. An AI with cognitive abilities far greater than those of humans in a wide range of important domains.

*[Transformative AI]: An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.
*[TAI]: Transformative AI. An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.
*[TAIs]: Transformative AI. An AI that is capable of transforming society, as drastically as the industrial revolution or even more so.

*[Agent]: A system that can be understood as taking actions towards achieving a goal.
*[agentic]: A system that can be understood as taking actions towards achieving a goal.

*[Alignment tax]: The added cost in time, money, computing power, capabilities, etc. needed to ensure that a system is aligned.

*[Corrigible]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.
*[corrigibility]: An AI system is corrigible if it doesn't interfere with our attempts to deactivate or modify it.

*[chain-of-thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain-of-Thought prompting]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain-of-thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.
*[Chain of thought]: A prompting technique which makes a language model generate intermediate reasoning steps in its output.


*[Embedded agent]: An agent which is not modeled as separate from the environment it acts on.
*[embedded agency]: An agent which is not modeled as separate from the environment it acts on.
*[Embedded agency]: An agent which is not modeled as separate from the environment it acts on.

*[x-risk]: Existential risk. A risk of human extinction or the permanent destruction of humanity’s long-term potential.
*[s-risk]: Existential suffering risk.


*[Fast takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[fast takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[FOOM]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.
*[hard takeoff]: A transition from human-level AI to superintelligent AI that goes very quickly, giving us no time to react.

*[FLOP]: A quantity of mathematical operations that involve floating-point numbers (numbers with a decimal point). “FLOPs” can be the plural of “FLOP”, but is confusingly also sometimes used interchangeably with “FLOPS”, meaning “FLOP per second”.
*[floating-point operation]: A quantity of mathematical operations that involve floating-point numbers (numbers with a decimal point). “FLOPs” can be the plural of “FLOP”, but is confusingly also sometimes used interchangeably with “FLOPS”, meaning “FLOP per second”.

*[FLOP/s]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.
*[FLOPS]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.
*[floating-point operations per second]: Sometimes “FLOPS”. Short for “floating-point operations per second”. FLOP/s represents the number of mathematical operations involving floating-point numbers that a computer can perform every second, and is a measure of the computer’s computing power.


*[FLOPs]: The plural of “FLOP”, or occasionally an alternative spelling of “FLOPS”
*[floating point operations]: The plural of “FLOP”, or occasionally an alternative spelling of “FLOPS”



*[Goodhart's law]: “When a measure becomes a target, it ceases to be a good measure.”
*[Goodharts law]: “When a measure becomes a target, it ceases to be a good measure.”
*[Goodharting]: “When a measure becomes a target, it ceases to be a good measure.”

*[Inner misalignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner-Alignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner Alignment]: When an mesa optimizer pursues a different objective than the one that was specified.
*[Inner alignment]: When a mesa optimizer ends up pursuing the objective that was specified.

*[Mesa-optimization]: An algorithm that is created by optimization and that is also itself an optimizer.
*[Mesa optimization]: An algorithm that is created by optimization and that is also itself an optimizer.
*[mesa-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[mesa optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[inner optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[inner-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[learned optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[learned-optimizer]: An algorithm that is created by optimization and that is also itself an optimizer.
*[Mesa-objective]: The objective pursued by a mesa-optimizer.

*[Deceptive Alignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[Deceptive alignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptive misalignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[Deceptive misalignment]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptively aligned]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned.
*[deceptively misaligned]: A case where the AI acts as if it were aligned while in training, but when deployed it turns out not to be aligned. 

*[Instrumental convergence]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[instrumental convergence]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[instrumentally convergent]: The idea that agents with widely different terminal goals will end up adopting many of the same instrumental goals.
*[Instrumental goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.
*[instrumental goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.
*[sub-goal]: Goals which are pursued as means to some other end, rather than as ends in themselves.

*[Intelligence explosion]: A hypothetical scenario where machines become more intelligent very quickly, driven by recursive self-improvement.

*[Multipolar scenario]: A scenario in which there end up being multiple powerful decision makers.
*[multipolar scenario]: A scenario in which there end up being multiple powerful decision makers.
*[multipolar takeoff]: A scenario in which there end up being multiple powerful decision makers.
*[Multipolar takeoff]: A scenario in which there end up being multiple powerful decision makers.
*[Multipolarity]: A scenario in which there end up being multiple powerful decision makers.

*[Unipolar scenario]: A scenario in which there ends up being a single powerful decision maker.
*[unipolar scenario]: A scenario in which there ends up being a single powerful decision maker.
*[unipolar takeoff]: A scenario in which there ends up being a single powerful decision maker.
*[Unipolar takeoff]: A scenario in which there ends up being a single powerful decision maker.
*[Unipolarity]: A scenario in which there ends up being a single powerful decision maker.

*[Out-of-distribution]: A case is out-of-distribution if it is very different from the cases the system encountered during training.
*[out of distribution]: A case is out-of-distribution if it is very different from the cases the system encountered during training.
*[OOD]: A case is out-of-distribution if it is very different from the cases the system encountered during training.


*[Independent and identically distributed]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[independent and identically distributed]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[i.i.d.]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.
*[IID]: An assumption, often made in statistics and machine learning, that data points are drawn randomly “from the same distribution” and that knowing any data point does not tell you anything about the others.

*[Auto-induced distributional shift]: A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself. It arises due to the influence of the system on its own input distribution.
*[auto-induced distributional shift]: A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself. It arises due to the influence of the system on its own input distribution.
*[auto induced distributional shift]: A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself. It arises due to the influence of the system on its own input distribution.
*[ADS]: Auto-induced distributional shift. A distributional shift caused by the behavior or actions of an algorithm or machine learning system itself. It arises due to the influence of the system on its own input distribution.


*[Oracle AI]: An AI whose only goal is to give correct answers to questions.

*[Orthogonality thesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[orthogonality thesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[Orthogonality hypothesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.
*[orthogonality hypothesis]: The claim that any level of intelligence is compatible with any terminal goal. This implies that intelligence alone is not enough to make a system moral.


*[Outer alignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[outer alignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[outer misalignment]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.
*[reward misspecification]: The problem of making sure that the precise formulation of what we train the AI to do matches what we intend it to do.

*[Proxy gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[proxy gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[Specification gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”
*[specification gaming]: Behavior where an AI performs a task in a way that scores highly according to the objective that was specified, while going against the task’s intended “spirit.”

*[Proxy goal]: A goal that performs well on the intended objective due to some spurious correlation in the training distribution.
*[proxy goal]: A goal that performs well on the intended objective due to some spurious correlation in the training distribution.

*[Reinforcement learning from human feedback]: A method for training an AI to give desirable outputs by using human feedback as a training signal. 
*[reinforcement learning from human feedback]: A method for training an AI to give desirable outputs by using human feedback as a training signal. 
*[RLHF]: Reinforcement learning from human feedback. A method for training an AI to give desirable outputs by using human feedback as a training signal. 
*[RLAIF]: Reinforcement learning from AI feedback. A method for training an AI to give desirable outputs by using AI feedback as a training signal. 


*[Robustness]: An agent's ability to maintain its goal and its capabilities when exposed to environments that are substantially different from that on which the agent was trained.

*[Recursive self-improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[Recursive self improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[recursive self improvement]: Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 
*[RSI]: Recursive self-improvement. Self-improvement that leads to further self-improvement in a self-reinforcing feedback loop. 


*[Singleton]: An agent which has dominant control over the entire world and can prevent any competitors from arising. Usually in reference to a superintelligent AI. Related: unipolar scenario.

*[Subagent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.
*[sub-agent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.
*[sub agent]: A system that is part of a larger agent and has its own agent-like qualities, such as pursuing its own goals.

*[Takeoff speed]: The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence. 
*[takeoff]: The amount of time that elapses between the first AI with roughly human-level intelligence and the first AI with vastly superhuman intelligence. 

*[Slow takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[slow takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[soft takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.
*[Soft takeoff]: A transition from human-level AI to superintelligent AI that goes slowly. This usually implies that we have time to react.


*[Takeoff homogeneity]: The degree to which the first AI systems that become superintelligent will have similar designs and goals.

*[Homogeneous scenario]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[Homogeneous takeoff]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[homogeneous takeoff]: A scenario where the first AIs to reach superintelligence are similar in nature.
*[Heterogeneous scenario]: A scenario where the first AIs to reach superintelligence are different in nature.
*[Heterogeneous takeoff]: A scenario where the first AIs to reach superintelligence are different in nature.
*[heterogeneous takeoff]: A scenario where the first AIs to reach superintelligence are different in nature.

*[Tool AI]: A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.
*[tool AIs]: A type of artificial general intelligence that can only function as an assistant to human use, and not as an autonomous agent.

*[Utility functions]: A mathematical function that assigns a number representing utility to every possible outcome. Outcomes with higher utility are preferred to outcomes with lower utility. “Maximizing utility” then means choosing the most preferred outcome.
*[Utility function]: A mathematical function that assigns a number representing utility to every possible outcome. Outcomes with higher utility are preferred to outcomes with lower utility. “Maximizing utility” then means choosing the most preferred outcome.

*[Base optimizer]: In contrast to a mesa-optimizer, a base optimizer is the “outer” optimizer usually explicitly implemented by humans.

*[Base objective]: In contrast to a mesa-objective,  the base objective is the “outer” objective usually explicitly implemented by humans.

*[Behavioral objective]: The behavioral objective is what an optimizer looks like it is optimizing for. Formally, the behavioral objective is the objective we would find if we performed inverse reinforcement learning perfectly.

*[Learned algorithm]: The algorithms that a base optimizer finds to solve the problem it has been given.

*[Meta-optimizer]: A system which is tasked with producing a base optimizer.

*[Pseudo-alignment]: A mesa-optimizer is pseudo-aligned with the base objective if it appears aligned on the training data but is not robustly aligned.

*[Robust alignment]: A mesa-optimizer is robustly aligned with the base objective if it robustly optimizes for the base objective across distributions.

*[Algorithmic range]: The algorithmic range of a machine learning system refers to how extensive the set of algorithms capable of being found by the base optimizer is.

*[Reachability]: The reachability of a learned algorithm refers to the difficulty for the base optimizer to find that learned algorithm.

*[Approximate alignment]: An approximately aligned mesa-optimizer is a pseudo-aligned mesa-optimizer where the base and mesa- objectives are the same up to some degree of approximation error caused only by  the difficulty of representing the base objective in the mesa-optimizer.

*[Proxy alignment]: A proxy-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer that has learned to optimize for some proxy of the base objective instead of the base objective itself.

*[Instrumental alignment]: A type of proxy alignment in which the mesa-optimizer optimizes the proxy as an instrumental goal of increasing the mesa-objective in the training distribution.

*[Side-effect alignment]: A type of proxy alignment in which optimizing for the mesa-objective has the direct causal result of increasing the base objective in the training distribution.

*[Suboptimality alignment]: A suboptimality-aligned mesa-optimizer is a pseudo-aligned mesa-optimizer in which some deficiency, error, or limitation causes it to exhibit aligned behavior.

*[Corrigible alignment]: A corrigibly aligned mesa-optimizer is a robustly aligned mesa-optimizer that has a mesa-objective that “points to” its epistemic model of the base objective.

*[Internal alignment]: An internally aligned mesa-optimizer is a robustly aligned mesa-optimizer that has internalized the base objective in its mesa-objective.

*[Sharp left turn]: An event in which an AI’s capabilities suddenly generalize, but its alignment doesn’t also generalize.
*[SLT]: An event in which an AI’s capabilities suddenly generalize, but its alignment doesn’t also generalize.

*[Wireheading]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.
*[wirehead]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.
*[wireheaded]: A scenario in which an agent trying to maximize reward directly alters its reward, instead of accomplishing the goal that the reward was meant to encourage the agent to pursue.

*[Reward hacking]: Reward hacking occurs when an AI agent exploits “loopholes” or “shortcuts” in the environment to maximize its reward without actually achieving the intended goal.

*[Reward tampering]: A scenario where an agent tries to influence the reward process itself, instead of accomplishing the goal that the reward was meant to encourage.

*[Reward shaping]: A technique used in RL which introduces small intermediate rewards to supplement the environmental reward. This seeks to mitigate the problem of sparse reward signals and to encourage exploration and faster learning.

*[Reward modeling]: A technique that separates the reinforcement learning alignment problem into understanding the intentions of humans and acting to achieve those intentions, i.e. learning the ‘How?’.

*[Pivotal act]: An action that has positive consequences for the far future of humanity. Typically a way to reliably avoid catastrophic outcomes in the near to medium future.

*[Context window]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.
*[context length]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.
*[context size]: Refers to the maximum amount of text that an LLM can consider at one time. This includes the prompt and the discussion with the user. If a discussion exceeds the length of the context window, previous interactions can be “forgotten”.

*[Sovereign AI]: An AI that acts autonomously in the world, in pursuit of potentially long-range objectives.
*[sovereign]: An AI that acts autonomously in the world, in pursuit of potentially long-range objectives.

*[Adversarial training]: A safety technique that pits two models against each other.

*[World model]: A system’s internal representation of its environment, which it uses to predict what will happen, including as a result of its own possible actions.

*[SoTA]: A state-of-the-art AI system. Currently, this typically refers to the largest and most capable LLMs.
*[SOTA]: A state-of-the-art AI system. Currently, this typically refers to the largest and most capable LLMs.

