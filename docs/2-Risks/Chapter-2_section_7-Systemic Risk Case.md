# Systemic Risk Case
## Persuasion

**Polluting the information ecosystem**. The deliberate propagation of disinformation is already a serious issue, reducing our shared understanding of reality and polarizing opinions. AIs could be used to severely exacerbate this problem by generating personalized disinformation on a larger scale than before. Additionally, as AIs become better at predicting and nudging our behavior, they will become more capable of manipulating us. We will now discuss how AIs could be leveraged by malicious actors to create a fractured and dysfunctional society.

First, AIs could be used to generate unique, personalized disinformation at a large scale. While there are already many social media bots, some of which exist to spread disinformation, historically they have been run by humans or primitive text generators. The latest AI systems do not need humans to generate personalized messages, never get tired, and can potentially interact with millions of users at once ([source](https://www.aisafetybook.com/textbook/1-2#persuasive-ais)).

As things like deep fakes become ever more practical (e.g., with fake kidnapping scams) ([source](https://edition.cnn.com/2023/04/29/us/ai-scam-calls-kidnapping-cec/index.html)). AI-powered tools could be used to generate and disseminate false or misleading information at scale, potentially influencing elections or undermining public trust in institutions.

**AIs can exploit users’ trust**. Already, hundreds of thousands of people pay for chatbots marketed as lovers and friends ([source](https://www.reuters.com/technology/what-happens-when-your-ai-chatbot-stops-loving-you-back-2023-03-18/)), and one man’s suicide has been partially attributed to interactions with a chatbot ([source](https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says)). As AIs appear increasingly human-like, people will increasingly form relationships with them and grow to trust them. AIs that gather personal information through relationship-building or by accessing extensive personal data, such as a user’s email account or personal files, could leverage that information to enhance persuasion. Powerful actors that control those systems could exploit user trust by delivering personalized disinformation directly through people’s “friends.”

**Value lock-in**. If AIs become too deeply embedded into society and are highly persuasive, we might see a scenario where a system's current values, principles, or procedures become so deeply entrenched that they are resistant to change. This could be due to a variety of reasons such as technological constraints, economic costs, or social and institutional inertia. The danger with value lock-in is the potential for perpetuating harmful or outdated values, especially when these values are institutionalized in influential systems like AI.

Locking in certain values may curtail humanity’s moral progress. It’s dangerous to allow any set of values to become permanently entrenched in society. For example, AI systems have learned racist and sexist views ([source](https://www.aisafetybook.com/textbook/1-2#persuasive-ais)), and once those views are learned, it can be difficult to fully remove them. In addition to problems we know exist in our society, there may be some we still do not. Just as we abhor some moral views widely held in the past, people in the future may want to move past moral views that we hold today, even those we currently see no problem with. For example, moral defects in AI systems would be even worse if AI systems had been trained in the 1960s, and many people at the time would have seen no problem with that. Therefore, when advanced AIs emerge and transform the world, there is a risk of their objectives locking in or perpetuating defects in today’s values. If AIs are not designed to continuously learn and update their understanding of societal values, they may perpetuate or reinforce existing defects in their decision-making processes long into the future.

In a world with widespread persuasive AI systems, people’s beliefs might be almost entirely determined by which AI systems they interact with most. Never knowing whom to trust, people could retreat even further into ideological enclaves, fearing that any information from outside those enclaves might be a sophisticated lie. This would erode consensus reality, people’s ability to cooperate with others, participate in civil society, and address collective action problems. This would also reduce our ability to have a conversation as a species about how to mitigate existential risks from AIs.

In summary, AIs could create highly effective, personalized disinformation on an unprecedented scale, and could be particularly persuasive to people they have built personal relationships with. In the hands of many people, this could create a deluge of disinformation that debilitates human society.

## Power Concentration

In the previous section, we already spoke about value lock-in. This phenomenon of entrenched values can happen in a “bottom-up” fashion when society's moral character becomes fixed, but a similar risk also arises in a “top-down” case of misuse when corporations or governments might pursue intense surveillance and seek to keep AIs in the hands of a trusted minority. This reaction to keep AI “safe”, could easily become an overcorrection, paving the way for an entrenched totalitarian regime that would be locked in by the power and capacity of AIs.

Value lock-in can occur from the perpetuation of systems and practices that undermine individual autonomy and freedom, such as the implementation of paternalistic systems where certain value judgments are imposed on individuals without their consent. Even without active malicious use, values encoded in an AI system could create a self-reinforcing feedback loop where groups get stuck in a poor equilibrium that is robust to attempts to get unstuck. ([source](https://arxiv.org/pdf/2206.05862.pdf))

**AI safety could further centralize control**. This could begin with good intentions, such as using AIs to enhance fact-checking and help people avoid falling prey to false narratives. We could see regulations that consolidate control over various components needed to build TAI into the hands of a few state or corporate actors, to ensure that any AI that is built remains safe. This includes things such as data centers, computing power, and big data. However, those in control of powerful systems may use them to suppress dissent, spread propaganda and disinformation, and otherwise advance their goals, which may be contrary to public well-being. ([source](https://www.aisafetybook.com/textbook/1-2#persuasive-ais))

**Loss of privacy. **There are certain factors that might accelerate power concentration. Such as the slow loss of individual privacy. Better persuasion and predictive models of human behavior are helped by gathering more data about individual users. Desires for profit, or desire to predict the flow of a country's resources, demographics, culture, etc. might incentivize behavior like intercepting personal data or legally eavesdropping on people’s activities. Data Mining can be used to collect and analyze large amounts of data from various sources such as social media, purchases, and internet usage. This information can be pieced together to create a complete picture of an individual's behavior, preferences, and lifestyle ([source](https://www.goodreads.com/book/show/44767248-human-compatible)). Voice Recognition technologies can be used to recognize speech, which could potentially lead to widespread wiretapping. For example, a system like the U.S. government's Echelon system uses language translation, speech recognition, and keyword searching to automatically sift through telephone, email, fax, and telex traffic ([source](https://www.goodreads.com/book/show/27543.Artificial_Intelligence)). AI can also be used to identify individuals in public spaces using facial recognition. This capability can potentially invade a person's privacy if a random stranger can easily identify them in public places ([source](https://www.goodreads.com/book/show/39204073-on-the-future)). Some AIs can even decipher passwords from keyboard sounds in some contexts. ([source](https://incyber.org/en/ai-deciphers-passwords-keyboard-sounds/#:~:text=On%20August%203%2C%202023%2C%20three,the%20sound%20each%20key%20produces.))

When AI systems are used to collect and analyze data on a mass scale, it helps regimes further strengthen self-reinforcing control. Personal information can be used to unfairly or unethically influence people's behavior. This can occur from both a state and a corporate perspective.

**Economic inequalities**. tbd. \


## <span style="text - decoration: underline;">Biases</span>

**Exacerbated biases: **AIs might unintentionally propagate or amplify existing biases. Biases persist within Large Language Models, often mirroring the opinions and biases prevalent on the internet, as evidenced by the[ ](https://arxiv.org/abs/2303.17548)[biased trends](https://arxiv.org/abs/2303.17548) observed in some LLMs. These biases can be harmful in various ways, as demonstrated by studies on[ ](https://arxiv.org/abs/2101.05783)[GPT-3's Islamophobic biases](https://arxiv.org/abs/2101.05783). The paper[ ](https://arxiv.org/abs/2306.05949)[Evaluating the Social Impact of Generative AI Systems in Systems and Society](https://arxiv.org/abs/2306.05949) defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs.

| Story: Self-Fulfilling Pessimism (source) |
|---|
| Scientists develop an algorithm for predicting the answers to questions about a person, as a function of freely available and purchasable information about the person (social media, resumes, browsing history, purchasing history, etc.). The algorithm is made freely available to the public, and employers begin using the algorithm to screen out potential hires by asking, “Is this person likely to be arrested in the next year?” Courts and regulatory bodies attempt to ban the technology by evoking privacy norms, but struggle to establish cases against the use of publicly available information, so the technology broadly remains in use. Innocent people who share certain characteristics with past convicted criminals end up struggling to get jobs, become disproportionately unemployed, and correspondingly more often commit theft to fulfill basic needs. Meanwhile, police also use the algorithm to prioritize their investigations, and since unemployment is a predictor of property crime, the algorithm leads them to suspect and arrest more unemployed people. Some of the arrests are talked about on social media, so the algorithm learns that the arrested individuals are likely to be arrested again, making it even more difficult for them to get jobs. A cycle of deeply unfair socioeconomic discrimination begins. |

## <span style="text - decoration: underline;">Automation</span>

**Economic Upheaval**: The automation of the economy could lead to widespread impacts on the labor market, potentially exacerbating economic inequalities and social divisions ([source](https://www.lesswrong.com/posts/Sn5NiiD5WBi4dLzaB/agi-will-drastically-increase-economies-of-scale-1)). This shift towards mass unemployment could also contribute to[ ](https://gh.bmj.com/content/8/5/e010435)[mental health issues](https://gh.bmj.com/content/8/5/e010435) by making human labor increasingly redundant. ([source](https://gh.bmj.com/content/8/5/e010435))

**Disempowerment & Enfeeblement:** AI systems could make individual choices and agency less relevant as decisions are increasingly made or influenced by automated processes. This occurs when humans delegate increasingly important tasks to machines, leading to a loss of self-governance and complete dependence on machines. This scenario is reminiscent of the film Wall-E in which humans become dependent on machines. ([more](https://www.safe.ai/ai-risk#Affaiblissement))

| Story: Capitalism on steroids — The production web |
|---|
| The economic incentives to automate are strong and may lead to certain risks. A system with a human in the loop is slower than a fully automated system.
The production web. A consequence of AI that could create risks at a societal scale is described in the paper “TASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI,” in the form of a short story: 'Story 1b: The Production Web,' which depicts a kind of capitalism on steroids, which gradually depletes all the natural resources necessary for human survival. 
Here is the outline of this story: In a world where the economy is increasingly automated by AI systems that are much faster than humans, there arises a competitive pressure such that only the fastest companies survive. In this context, businesses with humans in the loop would be less efficient compared to those fully automated. Consequently, we would gradually see a world where humans are replaced and cede control to machines because their quality of life improves by doing so. And progressively, control is progressively handed over to more competitive machines. However, the economic system designed by these machines does not fully account for negative externalities. It maximizes metrics that are mere proxies for the actual well-being of humans. As a result, we get a system that rapidly consumes vast amounts of raw materials essential for human survival, such as air, rare metals, and oxygen, because machines do not need the same types of resources as humans. This could gradually lead us to a world uninhabitable by humans. It would no longer be possible to disconnect this system because humans would become dependent on it, just as today it is not possible to disconnect the Internet because the entire logistics and supply chain depends on it.
Note that the previous story does not require AI agents. This is a Robust Agent-Agnostic Process (RAAPs), meaning that this story can occur with or without agentic AIs. Nonetheless, the authors of this chapter think that an AI Agent could make this story more plausible. In the article “Why Tool AIs Want to Be Agent AIs,” the author explains: “AIs limited to pure computation (Tool AIs) supporting humans, will be less intelligent, efficient, and economically valuable than more autonomous reinforcement-learning AIs (Agent AIs) who act on their own and meta-learn because all problems are reinforcement-learning problems. […] All of these actions will result in Agent AIs being more intelligent than Tool AIs, in addition to their greater economic competitiveness. […]”. |

## Epistemic<span style="text - decoration: underline;"> erosion</span>

**Epistemic Deterioration**: This can result from enfeeblement or the use of persuasion tools, leading to a[ ](https://aiimpacts.org/relevant-pre-agi-possibilities/#easy-footnote-bottom-30-2336)[massive deterioration of collective epistemic capacity](https://aiimpacts.org/relevant-pre-agi-possibilities/#easy-footnote-bottom-30-2336) (our ability to reason and understand the world). The ability to comprehend and respond to problems are crucial skills that make our civilization robust to various threats. Without these, we could be incapable of making correct decisions, possibly leading to disastrous outcomes.

**Fragility of Complex Systems**: The automation and tight coupling of different system components can make the failure of one part[ ](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)[trigger the collapse of the entire system](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like). One possible example could be financial markets or automated trading systems, where complex dynamics can emerge, leading to unintended and potentially misaligned outcomes at the systemic level. Another example could be flash wars, as illustrated in the box below.

**Challenges in Multi-Agent Systems:** In environments containing multiple agents, research highlights the risk of collective misalignment, where the pursuit of individual goals by agents leads to adverse effects on the system as a whole. This is exemplified in scenarios like Paul Cristiano's “You get what you measure,” which warns of an overemphasis on simple metrics, for example, economic metrics like GDP, without considering the broader implications for human values. This could result in a civilization increasingly managed by seemingly beneficial tools that, in reality, erode human-centric values. Another problem would be the competitive disadvantage of human values with respect to other values. Evolutionary dynamics might favor aggressive behaviors, posing significant risks if AIs begin to outcompete humans, as discussed in “[Natural Selection Favors AIs over Humans](https://doi.org/10.48550/arXiv.2303.16200)” by Dan Hendrycks.

| Story: The Cynical Email Helper (source) |
|---|
| A tech giant with over a billion users released a new “email helper” feature that reads a user’s email and suggests full email responses for the user to send, sometimes multiple paragraphs in length. However, many users struggle to understand the helper’s reasoning behind its messages, so a new feature is added that privately explains to the user why the message might be a good idea. A typical display to the user looks like this:
Message from Julia: “Hey, want to come to my party at 8:00 tomorrow?”
Suggested response: “Sure, Julia, I’d love to come to your event! Is it alright if I arrive a bit late, at 9:00?”
Reason for response: Remember you have plans to meet with Kevin from 5:30 to 8:30, although there’s no need to mention that detail to Julia; she might be jealous or offended.
The helper is programmed to improve over time, from positive feedback whenever the user chooses to send the suggested message. Ironically, the helper gets more positive feedback when it makes the user more nervous about the situation, such as by pointing out ways the counterparty could get angry at the user. This pattern causes users to feel like their helper is supporting them through a (purportedly) tricky social situation. So, the helper learns to gradually include more and more advice that causes users to keep secrets and fear offending each other.
As a result, a large fraction of the population becomes gradually more anxious about communicating with others in writing, while also becoming increasingly easy to offend as forthright communication styles become rare. It takes years for everyone to notice the pattern, but by that time many people have become excessively distrustful of others. The creators of the technology wish they had included a user experience question like “How are you feeling about your email today?”, to measure how their product might be affecting people separately from measuring how much people use it. |

## Accidents

Often, the whole point of producing a new technology is to produce a positive impact on society. Despite these noble intentions, there is a major category of risk that arises from large, well-intentioned projects that unintentionally go wrong. ([source](https://arxiv.org/pdf/2306.06924.pdf))

**Accidents are hard to avoid**. As underlying codebases get bigger or more complex, it’s difficult to build systems that are 100% predictable or fault tolerant. Deep learning is still a relatively new field, and unlike other technologies that might pose catastrophic risks, it is relatively less understood. We still don’t have complete knowledge of the inner workings of these systems. In other industries we have robustness measures and fault tolerance based on careful deliberate design, AI capabilities on the other hand emerge dynamically based on the learning process and are not the result of deliberate design. This means that we cannot guarantee perfect accuracy or reliability. ([source](https://www.aisafetybook.com/textbook/1-4))

**Flaws are hard to discover**. It often takes time to observe all the downstream effects of releasing a technology. There are many examples throughout history of technologies that we built, and released into the world, only to later discover that they were causing harm. Some examples are humanities use of leaded paints and gasoline causing large populations to suffer from lead poisoning ([source](https://environmentalhistory.org/about/ethyl-leaded-gasoline/lead-history-timeline/)), our use of CFCs causing a hole in the ozone layer ([source](https://ozonewatch.gsfc.nasa.gov/facts/hole_SH.html)), our use of asbestos which is linked to serious health issues, our use of tobacco products ([source](https://tobaccofreelife.org/tobacco/tobacco-history/)), and more recently the widespread use of social media, excessive use of which is linked to depression and anxiety. ([source](https://www.aisafetybook.com/textbook/1-4))

Some of these risks are diffuse and emerge only at the societal level, which we will talk about in the section on systemic risks. But others are perhaps easier to compare to software-based AI risks:

**Undetected hole in the ozone layer**. The example of the hole in the ozone layer might have occurred due to diffuse responsibility, but it was made worse because it remained undetected for a long period ([source](https://earthobservatory.nasa.gov/features/RemoteSensingAtmosphere/remote_sensing5.php)). This is because the data analysis software used by NASA in its project to map the ozone layer had been designed to ignore values that deviated greatly from expected measurements. ([source](https://www.pingdom.com/blog/10-historical-software-bugs-with-extreme-consequences/))

**The Mariner 1 Spacecraft**. In 1962 the Mariner 1 space probe barely made it out of Cape Canaveral before the rocket veered dangerously off course. Worried that the rocket was heading towards a crash-landing on Earth, NASA engineers issued a self-destruct command and the craft was obliterated about 290 seconds after launch. An investigation revealed the cause to be a very simple software error. A hyphen was omitted in a line of code, which meant that incorrect guidance signals were sent to the spacecraft. ([source](https://raygun.com/blog/costly-software-errors-history/))

There are countless other similar examples. Just like the one missing hyphen in the software for the Mariner spacecraft, we have also seen similar bugs due to one single character being altered in AI systems. OpenAI accidentally inverted the sign on the reward function while training GPT-2. The result was a model which optimized for negative sentiment while still regularizing toward natural language. Over time this caused the model to generate increasingly sexually explicit text, regardless of the starting prompt. In the author's own words “*This bug was remarkable since the result was not gibberish but maximally bad output. The authors were asleep during the training process, so the problem was noticed only once training had finished.*” ([source](https://arxiv.org/pdf/1909.08593.pdf))

While this example didn't really cause much harm, except to perhaps the human evaluators who had to spend an entire night reading increasingly reprehensible text, we can easily imagine that extremely small bugs like a single flipped sign on a reward function can cause really bad outcomes if they were to occur in more capable models.

The rapid improvement, combined with a lack of understanding and predictability makes it more likely that despite the best intentions we might not be able to prevent accidents. This supports the case for heavily tested slow rollouts of AI systems, as opposed to the “Move fast and break things” ethos ([source](https://en.wikipedia.org/wiki/Meta_Platforms#History)) that some tech companies might hold.