# Context
AI catastrophic risks gain urgency due to several key factors. Today's AI systems exhibit unprecedented capabilities far exceeding those of previous generations, marking an unprecedented leap in this technology. Our current understanding of these systems and their potential long-term implications is still incomplete, posing significant challenges in accurately assessing and managing the associated risks.

There are identifiable pathways through which AI can be misused, leading to catastrophic outcomes that could profoundly impact society. Moreover, we are approaching a critical threshold where the development of dangerously advanced capabilities, such as uncontrolled self-proliferation and self-replicating AI agents, becomes a tangible reality. These capabilities could lead to scenarios where AI systems rapidly expand and evolve beyond human control, potentially causing widespread disruption and harm. This proximity to such advanced capabilities underscores the immediate need for vigilance and proactive measures. Additionally, the current regulatory landscape is characterized by significant gaps, lacking comprehensive regulations governing AI development and deployment. This absence of adequate regulatory frameworks further exacerbates the risks associated with AI.

"*There is no question that machines will become smarter than humans—in all domains in which humans are smart—in the future,*" says LeCun. "*It's a question of when and how, not a question of if.*" Yann LeCun, Chief AI scientist at Meta and Turing Prize winner, May 2023 ([source](https://www.technologyreview.com/2023/05/02/1072528/geoffrey-hinton-google-why-scared-ai/))