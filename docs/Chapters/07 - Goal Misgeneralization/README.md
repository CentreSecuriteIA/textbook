⌛ Estimated Reading Time: 1 minutes. (179 words)

!!! warning "This chapter is still being written and is considered a work in progress."

1. **Goal Misgeneralization**: This section introduces the concept of goals as distinct from rewards. It explains what it might be if a model's capabilities generalize, while the goals do not. The section provides various examples of game playing agents, LLMs and other thought experiments to show how this could be a potentially catastrophic failure mode distinct from reward misspecification.

2. **Inner Alignment**: The next section begins with an explanation of the machine learning process, and how it can be seen as analogous to search. Since the machine learning process can be seen analogous to search, one type of algorithm that can be “found" is an optimizer. This motivates a discussion of the distinction between base and mesa-optimizers.

3. **Deceptive Alignment**: Having understood mesa-optimizers, the next section introduces the different types of mesa-optimizers that can arise as well as the corresponding failure modes. This section also explores training dynamics that could potentially increase or decrease the likelihood of the emergence of deceptive alignment.

4. **Research direction**: The final section highlights some research directions that attempt to solve the problems mentioned.

Here is a quick video introduction to the concept of inner misalignment:
<iframe
    style=" width: 100%; aspect-ratio: 16 / 9;"
    frameborder="0"
    allowfullscreen
    src="https://www.youtube.com/embed/bJLcIBixGj8">
</iframe>

