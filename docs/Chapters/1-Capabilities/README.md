# Chapter Overview

âŒ› Estimated Reading Time: 3 minutes. (450 words)


Human-Level AI, What, How and When?

!!! abstract

     This chapter seeks to outline the recent important developments in AI and presents some potential trajectories for developing highly capable AI systems.

<div class="grid cards" markdown>

- :material-robot-outline:{ .lg .middle } **[State-of-the-Art AI](1-State-of-the-Art-AI/)**

- We begin with a brief introduction to the current advancements in artificial intelligence as of 2024. We aim to acquaint readers with the latest breakthroughs across various domains such as language processing, vision, and robotics.

- :material-table-column-plus-after:{ .lg .middle } __Foundation Models__

- The second section focuses on foundation models, the paradigm powering the state-of-the-art systems introduced in the previous section. We explain the key techniques underpinning the huge success of these models such as self-supervised learning, zero-shot learning, and fine-tuning. The section concludes by looking at the risks that the foundation model paradigm could pose such as power centralization, homogenization, and the potential for emergent capabilities.

- :material-book-outline:{ .lg .middle } __Terminology__

- Before diving deeper, we establish the definitions that this book will be working with. This section explains why "capabilities" rather than "intelligence" is a more pragmatic measure for discussing AI risks. We also delineate key terms within the AI debate, such as Artificial General Intelligence (AGI), Artificial Super Intelligence (ASI), and Transformative AI (TAI). The section concludes by introducing the (t,n)-AGI framework which allows us to more concretely measure the level of AI capabilities on a continuous scale, rather than having to rely on discrete thresholds.

- :simple-rocket:{ .lg .middle } __Leveraging Computation__

- In this section, we explore the importance of computation in AI's progress introducing the three main variables that govern the capabilities of today's foundation models - compute, data and parameter count. We explore scaling laws and hypotheses that predict the future capabilities of AI based on current scaling trends of these variables, offering insights into the computational strategies that could pave the way to AGI.

- :material-chart-line:{ .lg .middle } __Forecasting__

- Finally, the chapter addresses the challenge of forecasting AI's future, using biological anchors as a method to estimate the computational needs for transformative AI. This section sets the groundwork for discussing AI takeoff dynamics, including speed, polarity, and homogeneity, offering a comprehensive view of potential futures shaped by AI development.

</div>

``` mermaid
mindmap
    root((Capabilities))
        Chapter Overview
            State-of-the-Art AI
                Language
                Image Generation
                Multi & Cross Modality
                Robotics
                Playing Games
                Foundation Models
            Fine-tuning
            Limitations & Risks
            Terminology
        Capabilities vs. Intelligence
            Definitions of Advanced AI Systems
                Artificial Narrow Intelligence
                Artificial General Intelligence (AGI)
                Human-Level AI (HLAI)
                Transformative AI (TAI)
                Artificial Super Intelligence (ASI)
            t,n-AGI Framework
        Leveraging Computation
            The Bitter Lesson
            Scaling Laws
            Scaling Hypotheses
                Weak Scaling Hypothesis
                Strong Scaling Hypothesis
        Forecasting
            Zeroth-Order Forecasting
                Reference Classes
                Anchors
            First-Order Forecasting
                Current Trends Analysis
                Rate of Change Calculation
            Biological Anchors Framework
                Evolution Anchor
                Lifetime Anchor
                Neural Network Anchors
                Genome Anchor
            Affordability of Compute
        Takeoff
            Speed
                Slow Takeoff
                Fast Takeoff
            Continuity
                Continuous Takeoff
                Discontinuous Takeoff
            Similarity
                Homogeneous Takeoff
                Heterogeneous Takeoff
            Polarity
                Unipolar Takeoff
                Multipolar Takeoff
        Appendices
            Expert Opinions
                Surveys
                Expert Quotes
                Prediction Markets
            Discussions on LLMs
                Empirical Insufficiency
                Shallow Understanding
                Structural Inadequacy
                Differences with the Brain
                Reasons to Continue Scaling LLMs
            Trends
                Compute Trends
                Model Size Trends
                Algorithmic Trends
                Data Trends
```


**Estimated reading time**: 2 Hours 40 minutes reading at 100 wpm. Given the density and type of material covered in this chapter, this should be taken as a lower bound estimate.

**Estimated reading time (including Appendices)**: 3 Hours 40 minutes reading at 100 wpm. The appendices are meant for people already quite familiar with topics in machine learning.

!!! question

     If you have a question, or a remark don't hesitate to ask to [agisf_textbook.0fh6l@slmail.me](mailto:agisf_textbook.0fh6l@slmail.me)
