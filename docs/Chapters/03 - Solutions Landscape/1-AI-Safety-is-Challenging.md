# AI Safety is Challenging

⌛ Estimated Reading Time: 6 minutes. (1088 words)


Specific properties of the AI safety problem make it particularly difficult.

**AI risk is an emerging problem that is still poorly understood.** We are not yet familiar with all its different aspects, and the technology is constantly evolving. It's hard to devise solutions for a technology that does not yet exist, but these guardrails are also necessary because the outcome can be very negative.

**The field is still pre-paradigmatic**. AI safety researchers disagree on the core problems, difficulties, and main threat models. For example, some researchers think that takeover risks are more likely ([source](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)), and some research emphasizes more progressive failure modes with progressive loss of control ([source](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic)). Because of this, alignment research is currently a mix of different agendas that need more unity. The alignment agendas of some researchers seem hopeless to others, and one of the favorite activities of alignment researchers is to criticize each other constructively.

**AIs are ****black boxes that are trained, not built. We know how to train them, but we do not know which algorithm is learned by them. **Without progress in interpretability, they are giant inscrutable matrices of numbers, with little modularity. In software engineering, modularity helps break down software into simpler parts, allowing for better problem-solving. In deep learning models, modularity is almost nonexistent: to date, interpretability has failed to decompose a deep neural network into modular structures ([source](https://arxiv.org/abs/2305.08746)). As a result, behaviors exhibited by deep neural networks are not understood and keep surprising us.

**Complexity is the source of many blind spots.** New failure modes are frequently discovered. For example, issues arise with glitch tokens, such as "SolidGoldMagikarp" ([source](https://www.alignmentforum.org/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation)). When GPT encounters this infrequent word, it behaves unpredictably and erratically. This phenomenon occurs because GPT uses a tokenizer to break down sentences into tokens (sets of letters such as words or combinations of letters and numbers), and the token "SolidGoldMagikarp" was present in the tokenizer's dataset but not in the GPT model's dataset. This blind spot is not an isolated incident. For example, on the day Microsoft's Tay chatbot, BingChat, or ChatGPT were launched, the chatbots were poorly tuned and exhibited many new emerging undesirable chatbot behaviors. Finding solutions when you don’t know there is a problem is hard.

**Creating an exhaustive risk framework is difficult.** There are many, many different classifications of risk scenarios that focus on various types of harm ([source](https://arxiv.org/abs/2306.06924))([source](https://arxiv.org/pdf/2306.12001.pdf)). Proposing a solid single-risk model beyond criticism is extremely difficult, and the risk scenarios often contain a degree of vagueness. No scenario captures most of the probability mass, and there is a wide diversity of potentially catastrophic scenarios ([source](https://www.lesswrong.com/posts/6jkGf5WEKMpMFXZp2/what-failure-looks-like-distilling-the-discussion)).

**Some arguments or frameworks that seem initially appealing may be flawed and misleading**. For example, the principal author of the paper ([source](https://www.lesswrong.com/s/fSMbebQyR4wheRrvk/p/6DuJxY8X45Sco4bS2)) presenting a mathematical result on instrumental convergence, Alex Turner, now believes his theorem is a poor way to think about the problem ([source](https://www.lesswrong.com/posts/dqSwccGTWyBgxrR58/turntrout-s-shortform-feed?commentId=Sw89AxHGJ5j7E7ETf)). Some other classical arguments have been criticized recently, like the counting argument or the utility maximization frameworks, which will be discussed in the following chapters. ([source](https://optimists.ai/2023/11/28/ai-is-easy-to-control/))

**Intrinsic fuzziness**. Many essential terms in AI safety are complicated to define, requiring knowledge in philosophy (epistemology, theory of mind), and AI. For instance, to determine if an AI is an agent, one must clarify “what does agency mean?" which, as we'll see in later chapters, requires nuance and may be an intrinsically ill-defined and fuzzy term. Some topics in AI safety are so challenging to grasp and are thought to be non-scientific in the machine learning (ML) community, such as discussing *situational awareness* ([source](https://www.youtube.com/watch?v=N1TEjTeQeg0)) or why AI might be able to “*really understand*”. These concepts are far from consensus among philosophers and AI researchers and require a lot of caution.

**A simple solution probably doesn’t exist. **For instance, the response to climate change is not just one measure, like saving electricity in winter at home. A whole range of potentially very different solutions must be applied. Just as there are various problems to consider when building an airplane, similarly, when training and deploying an AI, a range of issues could arise, requiring precautions and various security measures.

**Assessing progress in safety is tricky. **Even with the intention to help, actions might have a net negative impact (e.g. from second order effects, like accelerating deployment of dangerous technologies), and determining the contribution's impact is far from trivial. For example, the impact of reinforcement learning from human feedback (RLHF), currently used to instruction-tune and make ChatGPT safer, is still debated in the community ([source](https://www.lesswrong.com/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research)). One reason the impact of RLHF may be negative is that this technique may create an illusion of alignment that would make spotting deceptive alignment even more challenging. The alignment of the systems trained through RLHF is shallow ([source](https://arxiv.org/abs/2307.15217)), and the alignment properties might break with future more situationally aware models. Another worry is that many speculative failure modes appear only with more advanced AIs, and the fact that systems like GPT-4 can be instruction-tuned might not be an update for the risk models that are the most worrying.

**AI Safety is hard to measur****e. **Working on the problem can lead to an illusion of understanding, thereby creating the illusion of control. AI safety lacks clear feedback loops. Progress in AI capability advancement is easy to measure and benchmark, while progress in safety is comparatively much harder to measure. For example, it’s much easier to monitor the inference speed than monitoring the truthfulness of a system or monitoring its safety properties.

**The consequences of failures in AI alignment are steeped in uncertainty**. New insights could challenge many high-level considerations discussed in this textbook. For instance, Zvi Mowshowitz has compiled a list of critical questions marked by significant uncertainty and strong disagreements both ethical and technical ([source](https://www.lesswrong.com/posts/agv26XfXfKfKiKwDm/the-crux-list)). For example, What worlds count as catastrophic versus non-catastrophic? What would count as a non-catastrophic outcome? What is valuable? What do we care about? ([source](https://thezvi.substack.com/p/the-crux-list%3EThe%20Crux%20List%3C/a%3E.%20The%20original%20text%20is%20included%20as%20a%20backup,%20but%20it%20formats%20much%20better%20on%20Substack,%20and%20I%20haven%E2%80%99t%20yet%20had%20time%20to%20re-format%20it%20for%20WordPress%20or%20LessWrong.%3C/p%3E%3Cspan%20id=)) If answered differently, these questions could significantly alter one's estimate of the likelihood and severity of catastrophes stemming from unaligned AGI. Diverse responses to these critical questions highlight why individuals familiar with the alignment risks often hold differing opinions. For example, figures like Robin Hanson and Richard Sutton suggest that the concept of losing control to AIs might not be as dire as it seems. They argue there is little difference between nurturing a child who eventually influences the economy and developing AI based on human behavior that subsequently assumes economic control ([source](https://www.youtube.com/watch?v=NgHFMolXs3U))([source](https://www.overcomingbias.com/p/ai-risk-again)).

!!! quote  "Anthropic, 2023 ([source](https://www.anthropic.com/news/core-views-on-ai-safety))"

	
	*“We do not know how to train systems to robustly behave well.”* 
	
