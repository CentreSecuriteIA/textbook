{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"feedback/","title":"Feedback","text":"<p>We value your input! Please use the form below to provide feedback on the AI Safety Atlas. You can provide feedback on the entire book or on specific chapters.</p> <p> Feedback for a specific chapter  Feedback for the whole book</p> <p>Alternatively, you can ask a question or give us feedback on the Discord:</p> <p>Our Discord</p>"},{"location":"chapters/","title":"AI Safety Atlas","text":"Authors Markov Grey Charbel-Raphael Segerie Jeanne Salle Charles Martinet Affiliations French Center for AI Safety (CeSIA) Advisors Vincent Corruble Fabien Roger Cite this work as Markov Grey and Charbel-Raphael Segerie et al. 2024. AI Safety Atlas. French Center for AI Safety (CeSIA). ai-safety-atlas.com Project funded by Ryan Kidd (Manifund Regrant) Open Philanthropy Acknowledgements Jonathan Claybrough J\u00e9r\u00e9my Andr\u00e9oletti Evander Hammer Josh Thorsteinson <p>AI is undergoing unprecedented growth in capabilities, from writing code and analyzing scientific papers to generating images and engaging in complex reasoning. As these systems rapidly approach and potentially surpass human-level performance in many domains, the decisions we make today about their development will shape humanity's future. This book provides a comprehensive framework for understanding and addressing AI Safety challenges - from fundamental concepts to technical implementation. Whether you're a policymaker, researcher, engineer, or concerned citizen, our aim is to equip you with the knowledge needed to contribute meaningfully to ensuring AI development benefits humanity.</p> <p>Geoffrey Hinton (Most cited computer scientist ever, Godfather of modern AI, Turing Award Recipient) (Hinton, 2024)</p> <p>I can't see a path that guarantees safety. We're entering a period of great uncertainty where we're dealing with things we've never dealt with before. And normally, the first time you deal with something totally novel, you get it wrong. And we can't afford to get it wrong with these things. [...] If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further [...] it's as if aliens had landed and people haven't realized because they speak very good English</p>"},{"location":"chapters/#capabilities","title":"Capabilities","text":"<p>Why might AIs keep growing both increasingly general and capable?</p> <p>AI models are moving from specialized tools into increasingly general-purpose systems that can handle complex tasks. In this chapter we talk about empirical trends which show that scaling up - using more data, compute, and parameters - is leading to steady gains in both capabilities and generality. We explain the definitions of things like artificial general intelligence (AGI) and superintelligence (ASI) that we will be using throughout the book. Rather than viewing AI progress through discrete thresholds like \"narrow\" versus \"general\" intelligence, we introduce frameworks for measuring capabilities, generality and autonomy along continuous curves. Based on this we look at arguments for different AI takeoff scenarios, and provide expert opinions on timelines to transformative AI. Understanding these concepts shapes the conversations around potential sources of risks and safety strategies throughout the rest of the book. After reading this chapter, you'll be able to critically evaluate many of the claims about AI progress and engage in informed discussions about future AI developments.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#risks","title":"Risks","text":"<p>What can go wrong if AI keeps becoming more capable?</p> <p>Once we have established the core arguments behind why capabilities, generality and autonomy might keep increasing, we can look at what risks correspond to these increased levels. We divide risks from AI into three main categories: misuse (humans using AI for harm, like cyber attacks or bioweapons), misalignment (AI systems failing in unintended ways), and systemic risks (problems that emerge when AI interacts with other complex systems). We also talk about underlying factors that amplify all of these risks like competitive pressures creating race dynamics and widespread deployment. Finally, we also give brief explanations of how to concretely think about dangerous capabilities like - deception, situational awareness, autonomous replication, scheming and power-seeking behavior. By the end of this chapter, you will be able to identify how different risks emerge from technical failures of AI systems, or the combination of AI capabilities with human incentives, and other complex socio-technical systems.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#strategies","title":"Strategies","text":"<p>What strategies can we develop to mitigate risks?</p> <p>Building upon the understanding of capabilities and risks, in this chapter we look at concrete strategies for mitigating these risks and making AI development safer. We divide potential mitigation strategies into the same three categories as risks: preventing misuse (e.g. through monitored APIs and defensive technologies), addressing alignment (e.g. via technical research and control measures), and managing systemic impacts (e.g. through governance and safety culture). We explain how AI Safety is just starting to gain momentum as a field, but it is important to remember that no single solution might be sufficient. We might need a layered approach where multiple safeguards all work together to create robust protection against the risks outlined in the previous chapter. The purpose of this chapter is to give an overview of many different safety strategies and proposals and to show how these individual pieces might fit into the bigger picture. By reading this chapter and the ones before it, you have a solid foundation for the individual deep-dives that follow in all chapters after this. The systemic and social strategies outlined lead directly into the next few chapters on governance and evaluations, and the technical problems and mitigation strategies are discussed in dedicated individual chapters as well.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#governance","title":"Governance","text":"<p>How can we guide safe AI development and deployment?</p> <p>Previous chapters provided overviews of what AI can do, what could go wrong, and potential mitigation strategies, in this chapter we take a deeper look at how to shape the impact of AI through governance frameworks. We start by explaining why AI regulation faces unique challenges due things like unexpected capability jumps, post deployment safety issues, and rapid proliferation. We explain key governance targets like - compute governance (controlling key inputs like chips and training resources), data governance (managing training data and deployment). Following this we look at individual layers that can introduce changes to affect the chosen targets - corporate governance (internal safety frameworks like Anthropic's RSPs), national governance (governmental initiatives like AI Safety Institutes and regulations like the EU AI Act), and international governance (approaches ranging from non-proliferation to regulatory agreements). Reiterating what we said in the strategies chapter, no single governance layer might be completely sufficient - rather, we need coordinated action across all levels to address AI risks effectively. Reading this chapter will help you participate meaningfully in discussions on AI development, deployment, auditing and policy.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#evaluations","title":"Evaluations","text":"<p>How do we measure if an AI system is actually safe?</p> <p>Evaluations is the first chapter in what we consider to be the technical sequence, because it helps us bridge theory and practice by showing how evaluation results can trigger specific governance actions or technical safety protocols. Our explanation is decomposed into the questions of - what are we evaluating? (evaluated properties), how are we evaluating it? (evaluation techniques), and what elements play into good evaluations? (evaluation design). We cover AI Safety specific evaluation methods in three different categories - dangerous capability evaluations (what AI systems can do), propensity evaluations (how they tend to behave by default), and control evaluations (our ability to maintain safety under adversarial conditions). We explain how we might create techniques for evaluating dangerous capabilities (deception, CBRN risks, and autonomous replication) and dangerous propensities (e.g. power-seeking and scheming) which we previously outlined in the risks chapter. After reading this chapter, you will be able to understand what designing AI Safety evaluations entails, and how their results can inform both technical research directions and governance decisions.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#specification","title":"Specification","text":"<p>Why is telling AI systems what we want so hard?</p> <p>Evaluations help us figure out what AI systems can do or tend to do, and if our current mitigation measures are enough. The next step is to try and answer - how do we correctly specify what we even want AI to do in the first place? We introduced the general concept of specification gaming in the risks chapter, here we build upon that and look at theoretical foundations and practical failure modes like reward hacking (finding unintended ways to maximize reward) and reward tampering (directly manipulating reward mechanisms). We also explain how suggested mitigation strategies to the specification problem might work. This includes things like imitation-based methods (e.g. behavioral cloning, inverse reinforcement learning) and feedback-based approaches (e.g. reward modeling, reinforcement learning from human feedback (RLHF), Constitutional AI). We also cover the limitations of these approaches which helps set up the context for the next few chapters on generalization and scalable oversight. After reading this chapter, you will be able to understand why trying to encode even simple instructions can lead to unexpected behaviors and how AI safety research is currently trying to address these challenges.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#generalization","title":"Generalization","text":"<p>Why might AIs pursue unintended goals?</p> <p>In this chapter we turn to how AI systems actually learn and generalize from training. We start by a reminder of what an AI's \u201cgoals\u201d concretely means, and how people in AI Safety use the term. This builds on the intuition provided in both the risks and the evaluations chapters between capabilities (what AI systems can do) versus propensities (how they tend to behave by default). We walk you through how capability generalization and goal generalization are separate things using concrete empirical results. We specifically focus on goal misgeneralization which is a different concern from standard ML generalization problems like overfitting. We also explore why goal misgeneralization happens by looking at the theory of neural network loss landscapes which shows how different types of learned algorithms can emerge from different training runs. We pay particular attention to one specific type of learned algorithm - a learned optimizer. Using the concept of learned optimizers as a starting point, we dive deeper into the inner alignment and deceptive alignment (scheming) problems that have been mentioned in many of the previous chapters. We also connect these concerns to evaluation strategies mentioned previously, as well as mitigation strategies which are explored in the subsequent chapter on interpretability. At the end of this chapter you should be able to distinguish between different ways that ML models might generalize between distributions and understand how and why capable AI systems might scheme, deceive, and pursue long term unintended goals.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#scalable-oversight","title":"Scalable Oversight","text":"<p>How can we maintain meaningful control when AIs are smarter than us?</p> <p>Building on the specification problem explored in previous chapters, we look at how we could maintain meaningful human oversight even when tasks start to exceed human ability to iteratively provide feedback. The core challenge is providing accurate training signals for complex \"fuzzy\" tasks where success criteria are ambiguous and direct evaluation becomes infeasible. We start by exploring how to break down highly complex tasks into more manageable pieces through techniques like task decomposition. We then explain approaches that seek to elicit an AI's latent knowledge through externalized reasoning oversight and procedural cloning - strategies which focus on aligning the AI's entire reasoning process rather than just its outputs. We dive deeper into theoretical frameworks like Iterated Amplification and Distillation (IDA) and more practical approaches like AI Safety via. debate and weak-to-strong (W2S) generalization. Finally, we examine methods for evaluating these oversight techniques, which includes explanations of sandwiching experiments and meta-level adversarial evaluations. By the end of this chapter, you should understand the existing strategies for maintaining control over AI systems that exceed human capabilities.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#interpretability","title":"Interpretability","text":"<p>How can we understand what's happening inside AI systems?</p> <p>Our final chapter examines how we might understand the internal workings of AI models. Progress in interpretability could help address many of the risks and challenges discussed throughout the book - from detecting deceptive behavior to guiding and verifying training outcomes. We primarily focus on mechanistic interpretability - a bottom-up approach that aims to reverse-engineer neural networks by studying how their components process information. We explain both observational methods that analyze model components without modification (like feature visualization and probing), and interventional methods that actively test and alter model behavior (like activation patching and steering). We also look at some efforts to automate and scale interpretability through techniques like Automatic Circuit Discovery (ACDC).</p> <p>Like all approaches discussed in this book - from governance frameworks to technical safety measures - interpretability is just one tool in our AI Safety toolbox. The path to safer AI systems will likely require combining insights and methods from all these different approaches. While current interpretability techniques face significant limitations, they also represent some of our best attempts at understanding these increasingly complex systems. After reading this chapter, you'll understand both the promise and current limitations of these tools, and how they might contribute to building safer AI systems in the future.</p> Read Watch Feedback Facilitate"},{"location":"chapters/#the-path-forward","title":"The Path Forward","text":"<p>AI Safety entails huge technical and social coordination challenges. By writing this book, we hope to help you join the growing community of people working on these problems. Whether you choose to contribute through technical research, policy work, or just spreading the word through advocacy, your involvement matters! Every new mind working on these problems brings us closer to ensuring AI development leads to a flourishing future for humanity.</p>"},{"location":"chapters/01/","title":"Chapter 01 - Capabilities","text":"Authors                      Markov Grey &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Jeanne Salle, Charles Martinet, Vincent Corruble, Diego Dorn, Josh Thorsteinson, Jonathan Claybrough, Alejandro Acelas, Jamie Raldua Veuthey, Alexandre Variengien, L\u00e9o Dana, Ang\u00e9lina Gentaz, Nicolas Guillard, Leo Karoubi          Last Updated 2024-11-20 Reading Time 94 min (core), 51 min (appendix) Also available on Alignment Forum \u00b7 Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>Note: - This is an AI-generated audio version. Please report any errors you find.</li> </ul> <p>Found errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/01/#introduction","title":"Introduction","text":"Video 1.1: Optional video to get an overview of AI capabilities. <p>Yann LeCun Chief AI scientist at Meta and Turing Prize winner, May 2023 (Heaven, 2023)</p> <p>There is no question that machines will become smarter than humans\u2014in all domains in which humans are smart\u2014in the future. It's a question of when and how, not a question of if.</p> <p>The field of artificial intelligence has undergone a remarkable transformation in recent years. This chapter lays the groundwork for the entire book by establishing what AI systems can currently do, how they achieve these capabilities, and how we might anticipate their future development. This understanding is essential for all subsequent chapters: the discussion of dangerous capabilities and potential risks (Chapter 2) follows directly from understanding capabilities. Similarly, proposed technical (Chapter 3) and governance solutions (Chapter 4) both must account for the current and projected future of AI capabilities.</p> Figure 1.1: We first explain foundation models, which have been continuously showing improved capabilities due to scale. Then examine empirically observed scaling laws. Based on these trends we look at some techniques that researchers use to try and forecast future AI progress. <p>State-of-the-Art AI - Achieved breakthrough capabilities across multiple domains . We begin by exploring how AI systems have evolved from narrow, specialized tools to increasingly general-purpose tools. Language models can now engage in complex reasoning, while computer vision systems demonstrate sophisticated understanding of visual information. In robotics, we're seeing the emergence of systems that can learn and adapt to real-world environments with increasing autonomy. The goal of this section is to give the reader many examples from different domains of accelerating AI capabilities.</p> <p>Foundation models - Revolutionized how we build AI systems. The next section explores how we have moved from smaller specialized architectures to large scale general-purpose architectures. Rather than building separate systems for each task, these foundation models serve as the starting point. They are building blocks that can be later adapted for various applications using fine-tuning. We explore how these models are trained, their key properties, and the unique challenges they present. The emergence of unexpected capabilities from these models raises important questions about both their potential and implications for AI safety.</p> <p>Understanding Intelligence - Capabilities require precise measurement to guide safety work . The objective of this section is to provide an understanding of what terms like artificial general intelligence and artificial superintelligence actually mean in practice. Through detailed case studies and empirical observations, we examine different approaches to defining and measuring AI capabilities. Moving beyond traditional binary distinctions between \"narrow\" and \"general\" AI, we introduce more nuanced formal frameworks that track progress along multiple dimensions, essential for understanding when and how safety measures need to be implemented.</p> <p>Scaling - The bitter lesson and empirical scaling laws show that scale drives progress . We explore how simple algorithms plus massive computation often outperform sophisticated hand-crafted approaches. This leads us to examine scaling laws that describe how AI performance improves with different variables like - data, parameter count and increased computational resources. This section also contains an examination of the debate around whether scale alone is sufficient for achieving transformative AI capabilities.</p> <p>Forecasting - Predicting capabilities progress helps us prepare safety measures in advance . Building on our understanding of current capabilities and scaling behaviors, we examine various approaches to anticipating future progress. From biological anchors to trend analysis, we explore frameworks for making informed predictions about AI development trajectories. This is very important to know when different safety measures need to be in place.</p> <p>Appendices - Overview of expert opinions on AI, detailed debates around scale, and scaling trends. We consider these sections optional, but still useful to those who want to get a little bit of a deeper dive. The chapter concludes with appendices examining expert opinions on AI progress, deeper discussions about the nature and limitations of large language models, and comprehensive data on key trends in AI development.</p>"},{"location":"chapters/01/01/","title":"1.1 State-of-the-Art AI","text":"Reading Time 12 min <p>Has not been updated since March 2024. Might not be representative of major developments.</p> <p>Over the last decade, the field of artificial intelligence (AI) has experienced a profound transformation, largely attributed to the successes in deep learning. This remarkable progress has redefined the boundaries of AI capabilities, challenging many preconceived notions of what machines can achieve. The following sections detail some of these advancements.</p> Interactive Figure 1.1: Test scores of various AI capabilities relative to human performance. (Giattino et al., 2023) <p>Once a benchmark is published, it takes less and less time to solve it. This can illustrate the accelerating progress in AI and how quickly AI benchmarks are \"saturating\", and starting to surpass human performance on a variety of tasks. (Our World in Data, 2023)</p> Interactive Figure 1.2: Notable AI systems (by domain) (Giattino et al., 2023)"},{"location":"chapters/01/01/#01","title":"1.1.1 Language","text":"Interactive Figure 1.3: Benchmark performance on coding, math and language. (Giattino et al., 2023) <p>Language-based tasks. There have been transformative changes in sequence and language-based tasks, primarily through the development of large language models (LLMs). Early language models in 2018 struggled to construct coherent sentences. The evolution from these to the advanced capabilities of GPT-3 (Generative Pre-Trained Transformer) and ChatGPT within less than 5 years is remarkable. These models demonstrate not only an improved capacity for generating text but also for responding to complex queries with nuanced, common-sense reasoning. Their performance in various question-answering tasks, including those requiring strategic thinking, has been particularly impressive.</p> <p>GPT-4. One of the state-of-the-art language models in 2024 is OpenAI\u2019s LLM GPT-4. In contrast with the text-only GPT-3 and follow-ups, GPT-4 is multimodal: it was trained on both text and images. This means that it can now not only generate text based on images but has also gained some other capabilities. GPT-4 saw an upgraded context window with up to 32k tokens (tokens \u2248 words). The short-term memory limit of an LLM can be thought of as the model's ability to retain information from previous tokens within a certain context window. GPT-4 is trained via next-token prediction (autoregressive self-supervised learning). In 2018 GPT-1 was barely able to count to 10, while in 2024 GPT-4 can implement complex programmatic functions among other things.</p> Figure 1.2: A list of \"Nowhere near solved\" [...] issues in AI, from \"A brief history of AI\", published in January 2021 (Wooldridge, 2021). They also say: \"At present, we have no idea how to get computers to do the tasks at the bottom of the list\". But everything in the category \"Nowhere near solved\" has been solved by GPT-4 (Bubeck et al., 2023), except human-level general intelligence. <p>Scaling . Remarkably, GPT-4 is trained using roughly the same methods as GPT-1, 2, and 3. The only significant difference is the size of the model and the data given to it during training. The size of the model has gone from 1.5B parameters to hundreds of billions of parameters, and datasets have become similarly larger and more diverse.</p> Figure 1.3: How fast is AI Improving? (AI Digest, 2023) <p>We have observed that just an expansion in scale has contributed to enhanced performance. This includes improvements in the ability to generate contextually appropriate responses, and highly diverse text across a range of domains. It has also contributed to overall improved understanding, and coherence. Most of those advances in the GPT series come from increasing the size and computation power behind the models, rather than fundamental shifts in architecture or training.</p> <p>Here are some of the capabilities that have been emerging in the last few years:</p> <ul> <li> <p>Few-shot and Zero-shot Learning . The model's proficiency at understanding and executing tasks with minimal or no prior examples. 'Few-shot' means accomplishing the task after having seen a few examples in the context window, while 'Zero-shot' indicates performing the task without any specific examples (Anthropic, 2022). This also includes induction capabilities, i.e. identifying patterns and generalizing rules not present in the training, but only present in the current context window (Brown et al., 2020).</p> </li> <li> <p>Metacognition . This refers to the ability to recognize its own knowledge and limitations, for example, being able to know the probability of the truth of something (Kadavath, 2022).</p> </li> <li> <p>Theory of Mind . The capability to attribute mental states to itself and others, which helps in predicting human behaviors and responses for more nuanced interactions (Kosinski 2023; Xu et al., 2024).</p> </li> <li> <p>Tool Use . Being able to interact with external tools, like using a calculator or browsing the internet, expanding its problem-solving abilities (Qin et al., 2023).</p> </li> <li> <p>Self-correction . The model's ability to identify and correct its own mistakes, which is crucial for improving the accuracy of AI-generated content (Shinn et al., 2023).</p> </li> </ul> Figure 1.4: An example of a mathematical problem solved by GPT-4 using Chain of Thought (CoT), from the paper \"Sparks of Artificial General Intelligence\" (Bubeck et al., 2023). <ul> <li> <p>Reasoning . The advancements in LLMs have also led to significant improvements in the ability to process and generate logical chains of thought and reasoning. This is particularly important in problem-solving tasks where a straightforward answer isn't immediately available, and a step-by-step reasoning process is required. (Bubeck et al., 2023)</p> </li> <li> <p>Programming ability . In coding, AI models have progressed from basic code autocompletion to writing sophisticated, functional programs.</p> </li> <li> <p>Scientific &amp; Mathematical ability . In mathematics, AI's have assisted in the subfield of automatic theorem proving for decades. Today's models continue to assist in solving complex problems. AI can even achieve a gold medal level in the mathematical Olympiad by solving geometry problems (Trinh et al., 2024).</p> </li> </ul> Figure 1.5: Note also the large jump from GPT-3.5 to GPT-4 in human percentile on these tests, often from well below the median human to the very top of the human range. (Aschenbrenner, 2024; OpenAI, 2023). Keep in mind that the jump from GPT-3 to GPT-4 was in a single year."},{"location":"chapters/01/01/#02","title":"1.1.2 Image Generation","text":"<p>The leap forward in image generation is not just in accuracy, but also in the ability to handle complex, real-world images. The latter, particularly with the advent of Generative Adversarial Networks (GANs) in 2014, has shown an astounding rate of progress. The quality of images generated by AI has evolved from simple, blurry representations to highly detailed and creative scenes, often in response to intricate language prompts.</p> Figure 1.6: An example of state-of-the-art image recognition. The Segment Anything Model (SAM) by Meta\u2019s FAIR (Fundamental AI Research) lab, can classify and segment visual data at highly precise levels. The detection is performed without the need to annotate images. (Viso AI, 2024; Meta, 2023) Figure 1.7: An example of the evolution of image generation. At the top left, starting from GANs (Generative Adversarial Networks) to the bottom right, an image from MidJourney V5. <p>The rate of progress within a single year alone is quite astounding as is seen from the improvements between the V1 of the MidJourney image generation model in early 2022, to the V6 in December 2023.</p> Figure 1.8: MidJourney AI image generation over 2022-2023. Prompt: high-quality photography of a young Japanese woman smiling, backlighting, natural pale light, film camera, by Rinko Kawauchi, HDR (Yap, 2024)"},{"location":"chapters/01/01/#03","title":"1.1.3 Multi &amp; Cross modality","text":"<p>AI systems are becoming increasingly multimodal. This means that they can process images, text, audio, vision, and robotics using the same model. So they are trained using multiple different \"modes\" and can translate between them after deployment.</p> <p>Cross-modality . A model is called cross-modal when the input of a model is in one modality (e.g. text) and the output is in another modality (e.g. image). The section on computer vision showed fast progress between 2014 and 2020 in cross-modality. We went from text-to-image models only capable of generating black-and-white pixelated images of faces, to models capable of generating an image of any textual prompt. More examples of cross-modality include OpenAIs Whisper (Radford et al., 2022) which is capable of speech-to-text transcription.</p> <p>Multi-modality . A model is called multi-modal when both the inputs and outputs of a model can be in more than one modality. E.g. audio-to-text, video-to-text, text-to-image, etc\u2026</p> Figure 1.9: Image-to-text and text-to-image multimodality from the Flamingo model. (Alayrac et al., 2022) <p>DeepMind\u2019s 2022 Flamingo model, could be \"rapidly adapted to various image/video understanding tasks\" and \"is also capable of multi-image visual dialogue\". (Alayrac et al., 2022) Similarly, DeepMind\u2019s 2022 Gato model, was called a \"Generalist Agent\". It was a single network with the same weights which could \"play Atari, caption images, chat, stack blocks with a real robot arm, and much more\". (Reed et al., 2022) Continuing this trend, DeepMind\u2019s 2023 Google Gemini model could be called a Large Multimodal Model (LMM). The paper described Gemini as \"natively multimodal\" and claimed to be able to \"seamlessly combine their capabilities across modalities (e.g. extracting information and spatial layout out of a table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its state-of-art-performance in math and coding)\"(Google, 2024)</p>"},{"location":"chapters/01/01/#04","title":"1.1.4 Robotics","text":"Interactive Figure 1.4: Robots in use by service area (Giattino et al., 2023) <p>The field of robotics has also been progressing alongside artificial intelligence. In this section, we provide a couple of examples where these two fields are merging, highlighting some robots using inspiration from machine learning techniques to make advancements.</p> Figure 1.10: Researchers used Model-Free Reinforcement Learning to automatically learn quadruped locomotion in only 20 minutes in the real world instead of in simulated environments. The Figure shows examples of learned gaits on a variety of real-world terrains. (Smith et al., 2022) Interactive Figure 1.5: Number of industrial robots in use grows every year. (Giattino et al., 2023) <p>Advances in robotics . At the forefront of robotic advancements is PaLM-E, a general-purpose, embodied model with 562 billion parameters that integrates vision, language, and robot data for real-time manipulator control and excels in language tasks involving geospatial reasoning. (Driess et al., 2023)</p> <p>Simultaneously, developments in vision-language models have led to breakthroughs in fine-grained robot control, with models like RT-2 showing significant capabilities in object manipulation and multimodal reasoning. RT-2 demonstrates how we can use LLM-inspired prompting methods (chain-of-thought), to learn a self-contained model that can both plan long-horizon skill sequences and predict robot actions. (Brohan et al., 2023)</p> <p>Mobile ALOHA is another example of combining modern machine learning techniques with robotics. Trained using supervised behavioral cloning, the robot can autonomously perform complex tasks \"such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet.\" (Fu et al., 2024) Such advancements not only demonstrate the increasing sophistication and applicability of robotic systems but also highlight the potential for further groundbreaking developments in autonomous technologies.</p> Figure 1.11: DeepMinds RT-2 can both plan long-horizon skill sequences and predict robot actions using inspiration from LLM prompting techniques (chain-of-thought). (Brohan et al., 2023)"},{"location":"chapters/01/01/#05","title":"1.1.5 Playing Games","text":"<p>AI and board games. AI has made continuous progress in game playing for decades. Starting from AIs beating the world champion at chess in 1997, Scrabble in 2006 to DeepMind\u2019s AlphaGo in 2016 (DeepMind, 2016), which was good enough to defeat the world champion in the game of Go, a game assumed to be notoriously difficult for AI. Within a year, the next model AlphaGo Zero trained through self-play had mastered multiple games of Go, chess, and shogi reaching a superhuman level after less than three days of training. (Silver et al., 2017)</p> <p>AI and video games. We started using machine learning techniques on simple Atari games in 2013 (Mnih et al. 2013). By 2019, OpenAI Five defeated the world champions at DOTA2 (OpenAI, 2019), while in the same year, DeepMind\u2019s AlphaStar beat professional esports players at StarCraft II (DeepMind, 2019). Both these games require thousands of actions in a row at a high number of actions per minute. In 2020 DeepMind MuZero model, described as \"a significant step forward in the pursuit of general-purpose algorithms\" (DeepMind, 2020), was capable of playing Atari games, Go, chess, and shogi without even being told the rules.</p> <p>In recent years, AI's capability has extended to open-ended environments like Minecraft, showcasing an ability to perform complex sequences of actions. In strategy games, Meta\u2019s Cicero displayed intricate strategic negotiation and deception skills in natural language for the game Diplomacy (Bakhtin et al., 2022).</p> Figure 1.12: A map of diplomacy and the dialog box where the AI negotiates. (DiploStrats (YouTube), 2022) Example of Voyager: Planning and Continuous Learning in Minecraft with GPT-4 <p>Voyager (Wang et al., 2023) stands as a particularly impressive example of the capabilities of AI in continuous learning environments. This AI is designed to play Minecraft, a task that involves a significant degree of planning and adaptive learning. What makes Voyager so remarkable is its ability to learn continuously and progressively within the game's environment, using GPT-4 contextual reasoning abilities to plan and write the code necessary for each new challenge. Starting from scratch in a single game session, Voyager initially learns to navigate the virtual world, engage and defeat enemies, and remember all these skills in its long-term memory. As the game progresses, it continues to learn and store new skills, leading up to the challenging task of mining diamonds, a complex activity that requires a deep understanding of the game mechanics and strategic planning. The ability of Voyager to integrate new information continuously and utilize it effectively showcases the potential of AI in managing complex, changing environments and performing tasks that require a long-term buildup of knowledge and skills.</p> <p> Figure 1.13: Voyager discovers new Minecraft items and skills continually by self-driven exploration, significantly outperforming the baselines. (Wang et al., 2023) </p>"},{"location":"chapters/01/02/","title":"1.2 Foundation Models","text":"Reading Time 13 min <p>What are foundation models? Foundation models represent a fundamental shift in how we develop AI. Rather than building specialized models for many small specific tasks, we can now train large-scale models that serve as a \"foundation\" for many different applications. These models are then specialized later by a process called fine-tuned to perform specific tasks. Think of this as similar to how we can build many different types of buildings using the same base structure (Bommasani et al., 2022). We can build banks, restaurants, or housing but the underlying foundation remains largely the same. This is just a very quick intuitive definition. We will get more into the details in the next few subsections on training, properties and risks.</p> Video 1.2: Optional video to understand foundation models. <p>Why did the paradigm of foundation models come about? The traditional approach of training specialized AI models for every task often proved inefficient and limiting. Progress was bottlenecked by the need for human-labeled data and the inability to transfer knowledge between tasks effectively. Foundation models overcame these limitations through a process called self-supervised learning on massive unlabeled datasets. This breakthrough happened because of many different reasons - advances in specialized hardware like GPUs, new machine learning architectures like transformers, and increased access to huge amounts of online data (Kaplan et al., 2020) are some of the more prominent reasons for this shift.</p> <p>What are examples of foundation models? In language processing, models like GPT-4 and Claude are examples of foundation models. Both of these have demonstrated the ability to generate human language, have complex conversations and perform simple reasoning tasks (OpenAI, 2023). Examples in computer vision include models like DALL-E 3 and Stable Diffusion. (Betker et al., 2023) These are domain specific examples, but we are also seeing a trend toward multimodal foundation models (LMMs). This includes things like GPT-4V and Gemini that can work across different types of data - processing and generating text, images, code, audio and probably more in the future (Google, 2023). Even in reinforcement learning, where models were traditionally trained for specific tasks, we're seeing foundation models like Gato demonstrate the ability to learn general-purpose behaviors that can be adapted to various different downstream tasks. (Reed et al., 2022)</p> Interactive Figure 1.6: The number of foundation models is growing every year. (Giattino et al., 2023) <p>What makes foundation models important for AI safety? The reason we start this entire book by talking about foundation models is because they mark a shift towards general-purpose systems, rather than narrow specialized ones. This paradigm shift introduces many new risks which didn't exist previously. These include misuse risks from centralization, homogenization, and dual-use capabilities just to name a few. The ability of foundation models to learn broad, transferable capabilities has also led to increasingly sophisticated behaviors emerging from relatively simple training objectives (Wei et al., 2022). Complex capabilities, combined generality and scale, means we need to seriously consider safety risks beyond just misuse that previously seemed theoretical or distant. Beyond just misuse risk, things like misalignment are becoming an increasing concern with each new capability that these foundation models exhibit. We dedicate an entire chapter to the discussion of these risks. But we will also give you a small taste on the kinds of possible risks in the next few subsections, as it warrants some repetition.</p> Interactive Figure 1.7: Number of large scale models (by domain) (Giattino et al., 2023) <p>Wh at is the difference between foundation models and frontier models? Frontier models represent the cutting edge of AI capabilities - they are the most advanced models in their respective domains. While many frontier models are also foundation models (like Claude 3.5 Sonnet), this isn't always the case. For example, AlphaFold, while being a frontier model in protein structure prediction, isn't typically considered a foundation model because it's specialized for a single task rather than serving as a general foundation for multiple applications (Jumper et al., 2021). This distinction is worth keeping in mind, because most AI safety research and regulation frameworks focus on frontier models due to their advanced capabilities. When discussions about AI safety reference \"foundation models,\" they're typically referring specifically to frontier foundation models - those foundation models that also represent the current state-of-the-art in capabilities. Understanding this distinction helps us better target and implement safety measures where they're most critically needed.</p>"},{"location":"chapters/01/02/#01","title":"1.2.1 Training","text":"<p>How are foundation models trained differently from traditional AI systems? One key innovation of foundation models is their training paradigm. Generally, foundation models use a two-stage training process. First, they go through what we call a pre-training, and then second, they can be adapted through various mechanisms like fine-tuning or scaffolding to perform specific tasks. Rather than learning from human-labeled examples for specific tasks, these models learn by finding patterns in huge amounts of unlabeled data. This shift toward self-supervised learning on massive datasets fundamentally changes not just how models learn, but also what kinds of capabilities and risks might emerge (Bommasani et al., 2022). From a safety perspective, this means we need to understand both how these training methods work and how they might lead to unexpected behaviors.</p> Figure 1.14: On the Opportunities and Risks of Foundation Models (Bommasani et al., 2022) <p>What is pre-training? Pre-training is the initial phase where the model learns general patterns and knowledge from massive datasets of millions or billions of examples. During this phase, the model isn't trained for any specific task - instead, it develops broad capabilities that can later be specialized. This generality is both powerful and concerning from a safety perspective. While it enables the model to adapt to many different tasks, it also means we can't easily predict or constrain what the model might learn to do (Hendrycks et al., 2022).</p> Figure 1.15: On the Opportunities and Risks of Foundation Models (Bommasani et al., 2022) <p>How does self-supervised learning enable pre-training? Self-supervised learning (SSL) is the key technical innovation that makes foundation models possible. This is how we actually implement the pre-training phase. Unlike traditional supervised learning, which requires human-labeled data, SSL leverages the inherent structure of the data itself to create training signals. For example, instead of manually labeling images, we might just hide part of a full image we already have and ask a model to predict what the rest should be. So it might predict the bottom half of an image given the top half, learning about which objects often appear together. As an example, it might learn that images with trees and grass at the top often have more grass, or maybe a path, at the bottom. It learns about objects and their context - trees and grass often appear in parks, dogs are often found in these environments, paths are usually horizontal, and so on. These learned representations can then be used for a wide variety of tasks that the model was not explicitly trained for, like identifying dogs in images, or recognizing parks - all without any human-provided labels! The same concept applies in language, a model might predict the next word in a sentence, such as \"The cat sat on the \u2026 ,\" learning grammar, syntax, and context as long as we repeat this over huge amounts of text.</p> <p>What is fine-tuning ? After pre-training, foundation models can be adapted through two main approaches: fine-tuning and prompting. Fine-tuning involves additional training on a specific task or dataset to specialize the model's capabilities. For example, we might use Reinforcement Learning from Human Feedback (RLHF) to make language models better at following instructions or being more helpful. Prompting, on the other hand, involves providing the model with carefully crafted inputs that guide it toward desired behaviors without additional training. We'll discuss these adaptation methods in more detail in Chapter 8 when we explore scalable oversight.</p> <p>Why does this training process matter for AI safety? The training process of foundation models creates several unique safety challenges. First, the self-supervised nature of pre-training means we have limited control over what the model learns - it might develop unintended capabilities or behaviors. Second, the adaptation process needs to reliably preserve any safety properties we've established during pre-training. Finally, the massive scale of training data and compute makes it difficult to thoroughly understand or audit what the model has learned. Many of the safety challenges we'll discuss throughout this book - from goal misgeneralization to scalable oversight - are deeply connected to how these models are trained and adapted.</p>"},{"location":"chapters/01/02/#02","title":"1.2.2 Properties","text":"<p>Why do we need to understand the properties of foundation models? Besides just understanding the training process, we also need to understand the key defining characteristics or the abilities of these models. These properties often determine both the capabilities and potential risks of these systems. They help explain why foundation models pose unique safety challenges compared to traditional AI systems. Their ability to transfer knowledge, generalize across many different domains, and develop emergent capabilities means we can't rely on traditional safety approaches that assume narrow, predictable behavior.</p> <p>What is transfer learning? Transfer learning is one of the most fundamental properties of foundation models - their ability to transfer knowledge learned during pre-training to new tasks and domains. Rather than starting from scratch for each task, we can leverage the general knowledge these models have already acquired (Bommasani et al., 2022). This property enables rapid adaptation and deployment, it also means that both capabilities and safety risks can transfer in unexpected ways. For example, a model might transfer not just useful knowledge but also harmful biases or undesired behaviors to new applications.</p> <p>What are zero-shot and few-shot learning? The ability to perform new tasks with very few examples, or even no examples at all. For example, GPT-4 can solve novel reasoning problems just from a natural language description of the task (OpenAI, 2023). This emergent ability to generalize to new situations is powerful but concerning from a safety perspective. If models can adapt to novel situations in unexpected ways, it becomes harder to predict and control their behavior in deployment.</p> <p>Why is generality? Generalization in foundation models works differently from traditional AI systems. Rather than just generalizing within a narrow domain, these models can generalize capabilities across domains in surprising ways. However, this generalization of capabilities often happens without a corresponding generalization of goals or constraints - a critical safety concern we'll explore in detail in our chapter on goal misgeneralization. For example, a model might generalize its ability to manipulate text in unexpected ways without maintaining the safety constraints we intended (Hendrycks et al., 2022).</p> Figure 1.16: On the Opportunities and Risks of Foundation Models (Bommasani et al., 2022) <p>Why is multi-modality? Models can work with multiple types of data (text, images, audio, video) simultaneously. This isn't just about handling different types of data. A better description is that they can make connections across modalities in sophisticated ways (Google, 2023). From a safety perspective, multi-modality introduces new challenges because it expands the ways models can interact with and influence the world. A safety failure in one modality might manifest through another in unexpected ways.</p> <p>Sam Altman (CEO of OpenAI) (Cronshaw, 2024)</p> <p>Multimodality will definitely be important. Speech in, speech out, images, eventually video. Clearly, people really want that. Customizability and personalization will also be very important.</p>"},{"location":"chapters/01/02/#03","title":"1.2.3 Risks","text":"<p>What makes foundation models hard to control? The difficulty of controlling these models stems from three interconnected challenges. First, once trained, their internal representations and behaviors are extremely difficult to modify in targeted ways. Unlike traditional software where we can directly edit specific functions, changing one behavior in a foundation model risks unpredictable effects on other capabilities. Second, when these models are deployed across many applications, maintaining control becomes a distributed systems problem. Safety failures can propagate through multiple systems before they're detected. Third, their black-box nature makes it extremely difficult to understand why they make specific decisions or how they might behave in novel situations - a challenge we'll explore deeply in our chapter on interpretability.</p> <p>How do resource requirements limit development and access? Training foundation models require massive computational resources, creating a delicate balance between cost and accessibility. While adapting an existing model might be relatively affordable, the substantial initial training costs risk centralizing power among a few well-resourced entities. This concentration of power raises important questions about oversight and responsible development, that we'll address in our chapter on governance. For example, a single training run of GPT-4 sized models can cost tens or hundreds of millions of dollars, effectively limiting who can participate in their development. Continued scaling has also brought up many concerns around the environmental impact of AI training runs. (Patterson et al., 2023).</p> Figure 1.17: The rising costs of training frontier AI models (Cottier et al., 2024) <p>What risks come from homogenization? Homogenization occurs when many AI systems are derived from the same foundation models. This creates a systemic risk - if a foundation model has biases or failure modes, these could propagate to all models fine-tuned from it (Bommasani et al., 2022). For example, if a widely-used foundation model has encoded harmful biases or unsafe behaviors, these might manifest across numerous applications, from content generation to automated decision-making. They also have fixed knowledge cutoffs based on their training data, creating potential safety issues when deployed in rapidly changing environments. This risk of correlated failures becomes particularly concerning when foundation models are deployed in critical systems. (Hendrycks et al., 2022).</p> <p>What is emergence, and why does it matter? Foundation models can effectively leverage increases in data, computation, and model size to improve their capabilities. The models become qualitatively different as they scale, often developing new capabilities entirely absent in smaller versions (Kaplan et al., 2020). This property is called emergence - the development of capabilities that weren't explicitly trained for. This can be both a good thing and a bad thing.</p> <p>We might encounter new, unexpected behaviors and risks that weren't present in smaller models. This emergence often happens discontinuously as models scale up, making it difficult to predict what capabilities might suddenly appear. Which means it also makes it difficult to predict which risks will emerge when. When combined with homogenization, this unpredictability becomes especially concerning - a single foundation model integrated into multiple critical systems could lead to correlated failures that span multiple safeguards or backup systems (Hendrycks et al., 2022). Therefore, this points to a need for a proactive approach to AI safety. Having safety measures in place before we begin scaling and training bigger models.</p> <p>How do these limitations inform our approach to safety? We just briefly touched on some of the risks in this subsection to give you a basic intuition for why it is important to work on AI safety. The entire next chapter is dedicated to a deep dive on different risks from AI models, and how they might come about.</p>"},{"location":"chapters/01/03/","title":"1.3 Intelligence","text":"Reading Time 13 min"},{"location":"chapters/01/03/#01","title":"1.3.1 Case Studies","text":"<p>The bulk of this subsection will deal with the theory and historical aspects of defining intelligence. If you are more interested in just the core practical aspects of how we measure artificial general intelligence (AGI), then you can safely skip to the next subsection - measurement.</p> <p>Why do we need to define intelligence? In our previous section on foundation models, we explored how modern AI systems are becoming increasingly powerful. But before we can meaningfully discuss the risks and safety implications of these systems, we need to agree on what we mean when we talk about AGI. Some believe that \"sparks\" of AGI are already present in the latest language models (Bubeck et al., 2023), while others predict human-level AI within a decade (Bengio et al., 2024). Without a clear definition, how are we supposed to assess such claims or plan appropriate safety measures?</p> <p>The core point is that if you can't define something, you can't measure it. If you can't measure it, you can't reliably track progress or identify potential risks. Think about an example from physics - saying something like \"it moved 5\" makes no sense without specifying the unit of measurement. Did it move 5 meters, 5 feet, or 5 royal cubits? Nobody knows. If we don't know how far or fast it moved, then can we enforce speed limits? Also, no. The same applies to intelligence, and subsequent risks and safety techniques. Just as physics needed standardized units like meters and watts to advance beyond qualitative descriptions, AI safety research needs rigorous definitions to move beyond vague analogies and anthropomorphisms.</p> <p>What makes defining intelligence so challenging? If everyone agrees that we need a definition to measure progress and design safety measures, then why don\u2019t we have a universally agreed upon definition? The problem is that the word intelligence is a term we use to describe multiple overlapping abilities - from problem-solving and learning to adaptation and abstract reasoning. Besides this, different academic disciplines view intelligence through different lenses. Psychologists emphasize measurable cognitive skills, computer scientists focus on task performance, and philosophers debate qualities like the relationship of intelligence to consciousness and self-awareness. So which approach is the most relevant to understanding and planning for AI safety?</p> <p>Case Study: Imitation based approach to intelligence. The Turing Test (or the imitation game) suggested that intelligence could be measured through a machine's ability to imitate human conversation (Turing, 1950). However, this behaviorist approach proved inadequate - modern language models can often pass Turing-style tests while lacking fundamental reasoning capabilities (Rapaport, 2020). This is also still a process based approach, and was meant mainly as a philosophical thought experiment rather than a concrete operationalizable measure of intelligence.</p> <p>Case Study: Consciousness based approaches to intelligence. One early view focused on machines that could truly understand and have cognitive states similar to humans (Searle, 1980). However, this definition proves problematic on multiple levels. First, consciousness remains poorly understood and difficult to measure. Second, we are unsure if intelligence and consciousness are necessarily linked - a system could potentially be highly intelligent without being conscious, or conscious without being particularly intelligent. A system doesn't need to be conscious to cause harm. Whether an AI system is conscious or not has no bearing on its ability to make high-impact decisions or take potentially dangerous actions.</p> <p>Case Study: Brain analogy based approaches to intelligence. Another early approach defined AGI in terms of systems that rival or surpass the human brain in complexity and speed. This brain-centric definition is problematic for several reasons. While our brains may be the only example of general intelligence we have, modern AI has shown that matching human neural architecture isn't necessary for achieving intelligent behavior. From a safety perspective, focusing on brain-like architecture tells us little about what risks a system might pose - a system could be very unlike a brain in structure but still be capable of dangerous actions.</p> <p>Case study: Process/Adaptability based approaches to intelligence. The process-based view sees intelligence as the efficiency of learning and adaptation, rather than accumulated capabilities. A few researchers adopt this view of intelligence. Under this view, intelligence is \"the capacity of a system to adapt to its environment while operating with insufficient knowledge and resources\" (Wang, 2020). Alternatively, it is described as \"the efficiency with which a system can turn experience and priors into skills\" (Chollet, 2019). While this focus on meta-learning and adaptation captures something fundamental about intelligence, but from a safety perspective, what ultimately matters is what these systems can actually do - their concrete capabilities - rather than how they achieve these capabilities. This leads us to the final approach.</p> <p>Case study 5: The capabilities approach to intelligence . The motivating question behind this view is - If an AI system can perform dangerous tasks at human-level or beyond, does it really matter whether it achieved this through sophisticated learning processes, efficient memorization, with/without consciousness? If an AI system has capabilities that could pose risks - like sophisticated planning, manipulation, or deception - these risks exist regardless of whether the system acquired these capabilities through \"true intelligence\", \"real understanding\" or sophisticated pattern matching. The capabilities-based approach cuts through philosophical debates by asking concrete questions: What can the system actually do? How well can it do it? What range of tasks can it handle? This framework provides clear standards for progress and, crucially for safety work, clear ways to identify potential risks. The majority of AI labs use this capabilities-focused approach in how they frame their AGI goals. For example, AGI has been defined as \"highly autonomous systems that outperform humans at most economically valuable work\" (OpenAI, 2014). Safety considerations are framed similarly in saying that the mission is to ensure \"transformative AI helps people and society\" (Anthropic, 2024).</p> <p>Capabilities vs Intelligence (Krakovna, 2023)</p> <p>When discussing AI risks, talk about capabilities, not intelligence... People often have different definitions of intelligence, or associate it with concepts like consciousness that are not relevant to AI risks, or dismiss the risks because intelligence is not well-defined.</p> <p>Given these considerations, for the vast majority of this book, our primary focus will remain on the practical framework of capabilities for evaluation and safety assessment. This capabilities-focused approach is most relevant for immediate safety work, regulation, and deployment decisions. We acknowledge that research into consciousness, sentience, ethics surrounding digital minds and the fundamental nature of intelligence continues to be valuable but is less actionable for immediate safety work.</p> <p>In our next subsection, we will explore how we can concretely define and measure capabilities within this framework. We'll see how moving beyond simple binary thresholds of \"narrow\" versus \"general\" AI helps us better understand the progression of AI capabilities and their associated risks.</p>"},{"location":"chapters/01/03/#02","title":"1.3.2 Measuring","text":"<p>Lord Kelvin (Oxford Reference, 2016)</p> <p>When you can measure what you are speaking about, and express it in numbers, you know something about it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts advanced to the stage of science.</p> <p>Why do traditional definitions of AGI fall short? In the previous section, we explored how foundation models are becoming increasingly powerful and versatile. But before we can meaningfully discuss risks and safety implications, or make predictions about future progress, we need clear ways to measure and track AI capabilities. This section introduces frameworks for measuring progress toward artificial general intelligence (AGI) and understanding the relationship between capabilities, autonomy, and risk. For example, OpenAI's definition of AGI as \"systems that outperform humans at most economically valuable work\" (OpenAI, 2014), or the commonly used definition \"Intelligence measures an agent\u2019s ability to achieve goals in a wide range of environments.\" (Legg &amp; Hutter, 2007) and many others are not specific enough to be operationalizable. Which humans? Which goals? Which tasks are economically valuable? What about systems that exceed human performance on some tasks but only for short durations?</p> <p>Why do we need better measurement frameworks? Historically, discussions about AGI have often relied on binary thresholds - systems were categorized as either \"narrow\" or \"general\", \"weak\" or \"strong\", \"sub-human\" or \"human-level.\" While these distinctions helped frame early discussions about AI, they become increasingly inadequate as AI systems grow more sophisticated. Just like we sidestepped debates around whether AIs display \"true intelligence\" or \"real understanding\" in favor of a more practical framework that focuses on capabilities, similarly we want to avoid debates around things like whether a system is \"human-level\" or not. It is much more pragmatic to be able to make statements like - it outperforms 75% of skilled adults on 30% of cognitive tasks.</p> Figure 1.18: This is the continuous outlook of AI measuring performance. All points on this axis can be called ANI (except for the origin). <p>Definition: Artificial Narrow Intelligence (ANI) (IBM, 2023)</p> <p>Weak AI\u2014also called Narrow AI or Artificial Narrow Intelligence (ANI)\u2014is AI trained and focused to perform specific tasks. Weak AI drives most of the AI that surrounds us today. \u2018Narrow\u2019 might be a more accurate descriptor for this type of AI as it is anything but weak; it enables some very robust applications, such as Apple's Siri, Amazon's Alexa, IBM Watson, and autonomous vehicles.</p> <p>Levels of artificial narrow intelligence (ANI) . We should think about the performance of AI systems on a continuous spectrum. Traditional definitions of ANI correspond to high performance on a very small percentage of tasks. For example, chess engines like AlphaZero outperform 100% of humans, but only on roughly 0.01% of cognitive tasks. Similarly, specialized image recognition systems might outperform 95% of humans, but again only on a tiny fraction of possible tasks. According to the definition above, all these systems would be defined as ANI, but if we think about them in a continuous range of what percentage of skilled humans they can outperform we get a much more specific and granular picture.</p> <p>How can we build a better measurement framework for AGI? We need to track AI progress along both - performance (how well can it do things?) and generality (how many different things can it do?). Just like we can describe a point on a map using latitude and longitude, we can characterize AGI systems by their combined level of performance and degree of generality, as measured by benchmarks and evaluations. This framework gives us a much more granular way to track progress. This precision helps us better understand both current capabilities and likely development trajectories.</p> Figure 1.19: Table of performance x generality showing both levels of ANI, and levels of AGI. (Morris et al., 2024) <p>Where do current AI systems fit in this framework? Large language models like GPT-4 show an interesting pattern - they outperform roughly 50% of skilled adults on perhaps 15-20% of cognitive tasks (like basic writing and coding), while matching or slightly exceeding unskilled human performance on a broader range of tasks. This gives us a more precise way to track progress than simply debating whether such systems qualify as \"AGI.\" LLMs like GPT-4 are early forms of AGI (Bubeck, 2023), and over time we will achieve stronger AGI as both generality and performance increase. To understand how this continuous framework relates to traditional definitions, let's examine how key historical concepts map onto our performance-generality space.</p> Figure 1.20: The two-dimensional view of performance x generality. The different colored curves are meant to represent the different paths we can take to ASI. Every single point on the path corresponds to a different level of AGI. The specific development trajectory is hard to forecast. This will be discussed in the section on forecasting and takeoff. <p>Definition: Transformative AI (TAI) (Karnofsky, 2016)</p> <p>Potential future AI that triggers a transition equivalent to, or more significant than, the agricultural or industrial revolution.</p> <p>Transformative AI (TAI) . Transformative AI represents a particularly interesting point in our framework because it isn't tied to specific performance or generality thresholds. Instead, it focuses on a range of impacts. For example, a system could be transformative by achieving moderate performance (outperforming 60% of humans) across a wide range of economically important tasks (50% of cognitive tasks), or by achieving exceptional performance (outperforming 99% of humans) on a smaller but critical set of tasks (20% of cognitive tasks).</p> <p>Human Level AI (HLAI) . This term is sometimes used interchangeably with AGI, and refers to an AI system that equals human intelligence in essentially all economically valuable work. However, we only explain it here for reasons of completeness. Human-level is not well-defined which makes this definitions difficult to operationalize. If we map this onto the levels of AGI framework, then it roughly would correspond to outperforming 99% of skilled adults at most cognitive non physical tasks.</p> <p>Definition: Artificial Superintelligence (ASI) (Bostrom, 2014)</p> <p>Any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest.</p> <p>Artificial Superintelligence (ASI) . If systems achieve superhuman performance at all cognitive tasks, then this would be the strongest form of AGI, also called superintelligence. In our framework, ASI represents the upper-right corner - systems that outperform 100% of humans on nearly 100% of cognitive tasks.</p> <p>What is the relationship between levels of AGI and risk? Understanding AI systems through continuous performance and generality measures helps us better assess risk. Rather than waiting for systems to cross some \"AGI threshold,\" we can identify specific combinations of performance and generality that warrant increased safety measures. For example:</p> <ul> <li> <p>A system achieving 90% performance on 30% of tasks might require different safety protocols than one achieving 60% performance on 70% of tasks</p> </li> <li> <p>Certain capability combinations might enable dangerous emergent behaviors even before reaching \"human-level\" on most tasks</p> </li> <li> <p>The rate of improvement along either axis provides important signals about how quickly additional safety measures need to be developed</p> </li> </ul> <p>There are various other variables that we can add to make this picture even more precise. For example, just like we have levels of performance and generality, we can also have levels of autonomy with which these systems operate. As an example, at a low level of autonomy a human fully controls a task and uses AI to automate mundane sub-tasks, whereas at a higher level of autonomy we might see the AI take on a substantive role,or even co-equal work division. (Morris et al., 2024) Similarly, we have the variable of propensities which would measure what the AI model tends to do by default (Shevlane et al., 2023), and the variable of controllability which measures what percent of the time the AI model is able to subvert our current safety measures (Roger et al., 2023). Combining our definition of levels of AGI with variables like this gives us an extremely accurate picture of what the model is able to, and allows actionable technical safety and regulatory proposals.</p>"},{"location":"chapters/01/04/","title":"1.4 Scaling","text":"Reading Time 18 min <p>In the previous section, we explored how we can measure AI capabilities along continuous dimensions of performance and generality. Now we'll examine one of the most important drivers behind improvements in these capabilities: scale.</p>"},{"location":"chapters/01/04/#01","title":"1.4.1 The Bitter Lesson","text":"<p>We assume that most of you probably went to university in an era where machine learning and AI roughly mean the same thing, or rather deep learning and AI mean the same thing. This hasn't always been true. Early in the history of artificial intelligence, researchers took very different approaches to creating intelligent systems. They believed that the key to artificial intelligence was carefully encoding human knowledge and expertise into computer programs. This led to things like expert systems filled with hand-crafted rules and chess engines programmed with sophisticated strategic principles. However, time and time again, researchers learned what we now call the bitter lesson.</p> <p>The Bitter Lesson (Sutton, 2019)</p> <p>The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. [...] The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning.</p> <p>What makes this lesson bitter? The bitterness comes from discovering that decades of careful human engineering and insight were ultimately less important than simple algorithms plus computation. In chess, researchers who had spent years encoding grandmaster knowledge watched as \"brute force\" search-based approaches like Deep Blue defeated world champion Garry Kasparov. In computer vision, hand-crafted feature detectors were outperformed by convolutional neural networks that learned their own features from data. In speech recognition, systems based on human understanding of phonetics were surpassed by statistical approaches using hidden Markov models (Sutton, 2019).</p> <p>Does the bitter lesson mean we don't need any human engineering? Human ingenuity playing a smaller role in improving AI is a subtle point that can be easily misunderstood. The transformer architecture for example might seem to contradict the bitter lesson because they rely on sophisticated architectural innovations. Human ingenuity is important, but the subtlety is in recognizing that there's a difference between two types of human engineering:</p> <ul> <li> <p>Algorithm-level improvements: These make better use of existing compute, like: better optimizers (Adam), architecture innovations (transformers, attention mechanisms) or training approaches (better learning rate schedules).</p> </li> <li> <p>Domain-specific engineering improvements: These try to encode human knowledge, like: special architectures designed for specific problems, hand-crafted features or rules or task-specific inductive biases.</p> </li> </ul> <p>The bitter lesson isn't arguing against all human engineering - it's specifically cautioning against the second type. The transformer architecture exemplifies this pattern - it doesn't encode any specific knowledge about language, but rather provides a general mechanism for learning patterns that becomes increasingly powerful as we scale up compute and data.</p> <p>What does the bitter lesson mean for AI Safety? In the previous section, we discussed measuring AI capabilities along continuous dimensions of performance and generality. If it is the case that these continue to advance primarily through scaling, then we may have more predictable trajectories. This creates both opportunities and risks because performance and generality will continue to rise as long as scale is all that we need, but it also means that forecasting AI development trajectories and preparing appropriate safety measures for predictable capability levels is possible.</p>"},{"location":"chapters/01/04/#02","title":"1.4.2 Scaling Laws","text":"<p>Why do AI labs care about scaling laws? Training large AI models is extremely expensive - potentially hundreds of millions of dollars for frontier models. Scaling laws help labs make crucial decisions about resource allocation: Should they spend more on GPUs or on acquiring training data? Should they train a larger model for less time or a smaller model for longer? For example, with a fixed compute budget, they might need to choose between training a 20-billion parameter model on 40% of their data or a 200-billion parameter model on just 4%. Getting these tradeoffs wrong can waste enormous resources. So it is important to be able to have a predictable relationship between how you invest your money and what level of capabilities you get at the end.</p> Figure 1.21: Example of capabilities increasing with an increase with one of variables in the scaling laws - parameter count. The same model architecture (Parti) was used to generate an image using an identical prompt, with the only difference between the models being the parameter size. There are noticeable leaps in quality, and somewhere between 3 billion and 20 billion parameters, the model acquires the ability to spell words correctly. (Yu et al., 2022) <p>What are scaling laws? Scaling laws are mathematical relationships that describe how an AI system's performance changes as we vary key inputs like model size, dataset size, and computing power. These are empirical power-law relationships that have been observed to hold across many orders of magnitude. The key variables involved are:</p> <p>Compute (C) : This represents the total processing power used during training, measured in floating-point operations (FLOPs). Think of this as the training \"budget\" - more compute means either training for longer, using more powerful hardware, or both. While having more GPUs helps increase compute capacity, compute ultimately refers to the total number of operations performed, not just hardware.</p> <p>Parameters (N) : These are the tunable numbers in the model that get adjusted during training - like knobs that the model can adjust to better fit the data. More parameters allow the model to learn more complex patterns but require more compute per training step. Current frontier models have hundreds of billions of parameters.</p> <p>Dataset size (D) : This measures how many examples the model trains on (typically measured in tokens for language models). The larger the dataset, the more information the model can read. Simultaneously, to read and learn from more data, the training runs also need to be generally longer, which in turn increases the total compute needed before the model can be considered \"trained\".</p> <p>Loss (L) : This measures how well the model performs on its training objective. This is what we are trying to minimize, and it tends to improve as we scale up these variables.</p> Figure 1.22: Language modeling performance improves smoothly as we increase the model size, dataset set size, and amount of compute used for training. For optimal performance all three factors must be scaled up in tandem. Empirical performance has a power-law relationship with each individual factor when not bottlenecked by the other two. (Kaplan et al., 2020) <p>OpenAI's initial scaling laws in 2020 . To determine the relationships between different variables that might contribute to scale, OpenAI conducted a series of experiments. For an intuitive idea of how they came up with the scaling laws, you can imagine that while training a model you can hold some variables fixed while varying others and see how loss changes. Eventually this allows you to see some patterns. As an example, dataset size can be kept constant, while parameter count and training time are varied, or parameter count is kept constant and data amounts are varied, etc\u2026 So we can get a measurement of the relative contribution of each towards overall performance. If these relationships hold true across many different model architectures and tasks, then this suggests they capture something fundamental about deep learning systems. This is how the first generation of scaling laws came about from OpenAI. For example, by these laws if you have a 10x more compute, you should increase model size by about 5x and data size by only 2x. (Kaplan et al., 2020)</p> Figure 1.23: OpenAIs initial paper on scaling laws stated that for optimally compute-efficient training, most of the increase should go towards increased model size. A relatively small increase in data is needed to avoid reuse. (Kaplan et al., 2020) Interactive Figure 1.8: Growth of parameters (Giattino et al., 2023) Interactive Figure 1.9: Growth of Data (Giattino et al., 2023) Interactive Figure 1.10: Growth of Compute (Giattino et al., 2023) Interactive Figure 1.11: Compute used to train AI (Giattino et al., 2023) <p>DeepMind's scaling law update in 2022 . DeepMind found that most large language models were actually significantly overparameterized for the amount of data they were trained on. The Chinchilla scaling laws showed that for optimal performance, models should be trained on approximately 20 times more data tokens than they have parameters. This meant that many leading models could have achieved better performance with smaller sizes, but with more data. They were called chinchilla scaling laws because the laws were demonstrated using a model called Chinchilla. This was a 70B parameter model trained on more data, which outperformed much larger models like Gopher (280B parameters) despite using the same amount of compute. So by these laws, for optimal performance, you should increase model size and dataset size in roughly equal proportions - if you get 10x more compute, you should make your model ~3.1x bigger and your data ~3.1x bigger. (Hoffmann et al., 2022)</p> <p>The Broken Neural Scaling Laws (BNSL) update in 2023 . Ongoing research showed that performance doesn't always improve smoothly - there can be sharp transitions, temporary plateaus, or even periods where performance gets worse before getting better. Examples of this include things like \"Grokking\", where models suddenly achieve strong generalization after many training steps, or deep double descent, where increasing model size initially hurts then helps performance. Rather than simple power laws, BNSL uses a more flexible functional form that can capture these complex behaviors. This allows for more accurate predictions of scaling behavior, particularly around discontinuities and transitions. Scaling laws are a good baseline, but discontinuous jumps in capabilities and abrupt step changes are still possible. (Caballero et al., 2023)</p> Figure 1.24: A Broken Neural Scaling Law example (dark black solid line) (with 3 breaks where purple dotted lines intersect with dark black solid line) that contains 4 individual power law segments (where the dashed lines that are yellow, blue, red, and green overlap with the dark black solid line). The 1st and 2nd break are very smooth; the 3rd break is very sharp. (Caballero et al., 2023) <p>How do training and inference scaling differ? Training scaling involves using more compute during initial model training by using larger models, training for longer, or using bigger datasets. Another way that we might not be accounting for using scaling laws, is called inference time scaling. This instead uses more compute at runtime through techniques like chain-of-thought prompting, repeated sampling, or tree search. For example, you can either train a very large model that generates high-quality outputs directly, or train a smaller model that achieves similar performance by using more computation to think through problems step by step at inference time.</p>"},{"location":"chapters/01/04/#03","title":"1.4.3 Scaling Hypothesis","text":"<p>Strong Scaling Hypothesis (Branwen, 2020)</p> <p>The strong scaling hypothesis proposes that simply scaling up current foundation model architectures with more compute and data will be sufficient to reach transformative AI capabilities and potentially even ASI.</p> Interactive Figure 1.12: Knowledge Tests vs Compute used in training (Giattino et al., 2023) <p>What is the strong scaling hypothesis? This view suggests we already have all the fundamental components needed - it's just a matter of making them bigger, following established scaling laws. (Branwen, 2020) There is heated debate around this hypothesis and we can't possibly cover every argument. We can give you a slight overview in the next few paragraphs.</p> <p>Proponents include OpenAI (OpenAI, 2023), Anthropic\u2019s CEO Dario Amodei (Amodei, 2023), Conjecture (Conjecture, 2023), DeepMind\u2019s safety team (DeepMind, 2022), and others. According to the DeepMind team, there are \"not many more fundamental innovations needed for AGI. Scaled-up deep learning foundation models with RL from human feedback (RLHF) fine-tuning [should suffice]\" (DeepMind, 2022).</p> <p>What are the key arguments supporting the strong scaling hypothesis? The most compelling evidence for this view comes from empirical observations of progress in recent years. Researchers have been developing algorithms that follow the bitter lesson's principle for many years (focusing on general methods that leverage compute effectively). But even when researchers have developed sophisticated algorithms following the bitter lesson's principles, these improvements still only account for 35% of performance gains in language models in 2024, with the remaining 65% coming purely from increased scale in compute and data (Ho et al., 2024). Basically, even when our algorithmic improvements align perfectly with the bitter lesson, they're still far less important than raw scaling.</p> <p>The emergence of unexpected capabilities provides another powerful argument for strong scaling. We've seen previous generations of foundation models demonstrate remarkable abilities that weren't explicitly trained for, like programming for example. This emergent behavior hints that it is not impossible for higher-order metacognitive abilities like causal reasoning to similarly emerge simply as a function of scale. We see that bigger models become increasingly sample efficient - they require fewer examples to learn new tasks. This improved efficiency with scale suggests that scaling up further could eventually lead to human-like few-shot learning capabilities, which is a precursor for TAI and ASI. Finally, these models also appear to be capable of learning any task that can be expressed through their training modalities. Right now this is text for LLMs but there is a clear path forward to multimodal LMMs. Since text can express virtually any human-comprehensible task, scaling up language understanding might be sufficient for general intelligence.</p> <p>What are the key arguments against the strong scaling hypothesis? Recent research has also identified several challenges to the strong scaling hypothesis. The most immediate is data availability - language models will likely exhaust high-quality public text data between 2026 and 2032 (Villalobos et al., 2024). While synthetic data might help address this limitation, it's unclear whether it can provide the same quality of learning signal as organic human-generated content. Alternatively, we still have a lot of multi-modal data left to train on (like YouTube videos) despite running out of text data.</p> <p>A more fundamental challenge comes from the way these models work. LLMs are fundamentally \"interpolative databases\" (or stochastic parrots , or a variety of other similar terms). The point being that they just build up a vast collection of vector transformations through pre-training. While these transformations become increasingly sophisticated with scale, critics argue there's a fundamental difference between recombining existing ideas and true synthesis - deriving novel solutions from first principles. However, this is not an airtight case against strong scaling. This could simply be a limitation of current scale - a larger model trained on multimodal data might learn to handle any new novel situation simply as a recombination of previously memorized patterns. So, it is unclear if template recombination actually does have an upper bound.</p> <p>Weak Scaling Hypothesis (Branwen, 2020)</p> <p>The weak scaling hypothesis proposes that even though scale will continue to be the primary driver of progress, we will also need targeted architectural and algorithmic improvements to overcome specific bottlenecks.</p> <p>What is the weak scaling hypothesis? Given these challenges, a weaker version of the scaling hypothesis has also been proposed. According to the weak scaling hypothesis even though scale will continue to be the primary driver of progress, we will also need targeted architectural and algorithmic improvements to overcome specific bottlenecks. These improvements wouldn't require fundamental breakthroughs, but rather incremental enhancements to better leverage scale. (Branwen, 2020) Similar to the strong scaling hypothesis, the weak one is also contentious and debated. We can provide a few of the results arguing both for and against this outlook.</p> <p>LeCun's H-Jepa architecture (LeCun, 2022), or Richard Sutton\u2019s Alberta Plan (Sutton, 2022) are notable plans that might support the weak scaling hypothesis.</p> <p>What are the key arguments supporting the weak scaling hypothesis? The arguments for strong scaling, like algorithmic improvements only contributing 35% of performance gains in language models can also double count for weak scaling. Since one third is still a non-trivial role to play in capabilities improvement. Some more empirical observations also support weak scaling. Like hardware support for lower-precision calculations, which provided order-of-magnitude performance improvements for machine learning workloads (Hobbhahn et al., 2023, \"Trends in Machine Learning Hardware\"). These kinds of targeted improvements don't change the fundamental scaling story but rather help us better leverage available resources. Similarly, the Chinchilla results showed that many models were poorly optimized along the different variables that contribute to capabilities. This suggests that there is still room for improvement through better scaling strategies rather than fundamental breakthroughs. (Hoffmann et al., 2022)</p> Figure 1.25: Augmentation/Scaffolding stays constant, but if the scaling hypothesis, weak or strong, is true, then capabilities will keep improving just by scaling. <p>What if neither the weak nor the strong scaling hypothesis is true? Essentially, both the scaling laws (which only predict foundation model capabilities) and most debates around \"scale is all you need\" often miss other aspects of AI development that happen outside the scope of what scaling laws can predict. They don't account for improvements in AI \"scaffolding\" (like chain-of-thought prompting, tool use, or retrieval), or combinations of multiple models working together in novel ways. Debates around the scaling laws only tell us about the capabilities of a single foundation model trained in a standard way. For example, by the strong scaling hypothesis we can reach TAI by simply scaling up the same foundation model until meta cognitive abilities emerge. But even if scaling stops, halting capabilities progress on the core foundation model (in either a weak or a strong way), the external techniques that leverage the existing model can still continue advancing.</p> <p>Think of foundation models like LLMs or LMMs as simply one transistor. Alone they might not be able to do much, but if we combine enough transistors we end up with all the capabilities of a supercomputer. Many researchers think that this is a core element where future capabilities will come from. It is also referred to as \"unhobbling\" (Aschenbrenner, 2024), \"schlep\" (Cotra, 2023) and various other terms, but all of them point to the same underlying principle - raw scaling of single model performance is only one part of overall AI capability advancement.</p> Figure 1.26: Even if we see no improvements in model scale, other elicitation techniques and scaffolding can keep improving. So overall capabilities keep growing. Realistically, the future is probably going to see both improvement due to scaffolding and scale. So for now, there does not seem to be an upper limit on improving capabilities as long as either one of the two holds. <p>We go deeper into the arguments and counterarguments for all views on scaling foundation models in the appendix.</p> Argument: Against scaling - Memorization vs Synthesis <p>This is a slightly deeper dive into trying to understand the argument of both sides if you are interested. It is slightly more technical, and can be safely skipped.</p> <p>When we talk about LLMs as \"interpolative databases\", we're referring to how they store and manipulate vector programs - these shouldn\u2019t be confused with traditional computer programs like python or C++. These templates, or vector programs are transformations in the model's embedding space. Early work on embeddings showed simple transformations (like king - man + woman = queen), but modern LLMs can store millions of much more complex transformations. But due to a function of scale, LLMs can now store arbitrarily complex vector functions \u2014 so complex, in fact, that researchers found it more accurate to refer to them as vector programs rather than functions.</p> <p>So what's happening in LLMs is that they build up a vast database of these vector programs through pre-training. When we say they're doing \"template matching\" or \"memorization\", what we really mean is that they're storing millions of these vector transformations that they can retrieve and combine with each prompt.</p> <p>So the deciding question for/against strong (and even weak scaling) becomes - Is this type of template program combination enough to reach general intelligence. In other words can program synthesis be approximated using recombinations of enough templates (also called abstractions and many other words but the key idea is the same)?</p> <p>People who argue against this say that no matter how numerous or sophisticated, are fundamentally different from true program synthesis. True program synthesis would mean deriving a new solution from first principles - not just recombining existing transformations. There are some empirical observations to support this view. Like the Caesar cipher example: \"LLMs can solve a Caesar cipher with key size 3 or 5, but fail with key size 13, because they've memorized specific solutions rather than understanding the general algorithm\" (Chollet, 2024). Or alternatively, the \"reversal curse\" which shows that even SOTA language models in 2024 cannot do reverse causal inference - if they are trained on \"A is B\" they fail to learn \"B is A\" (Berglund et al., 2023)</p> <p>But this does still not seem to completely invalidate scaling as of yet. If we scale up the size of the program database and cram more knowledge and patterns into it, we are going to be increasing its performance. (Chollet, 2024) Both sides of the debate agree on this. So this suggests the real issue isn't whether template recombination has an obvious absolute upper bound, but whether it's the most efficient path to general intelligence. Program synthesis might achieve the same capabilities with far less compute and data by learning to derive solutions rather than memorizing patterns.</p>"},{"location":"chapters/01/05/","title":"1.5 Forecasting","text":"Reading Time 12 min <p>In previous sections, we explored how foundation models leverage computation through scaling laws and the bitter lesson. But how can we actually predict where AI capabilities are headed? This section introduces key forecasting methodologies that help us anticipate AI progress and prepare appropriate safety measures.</p> <p>Why should we care about forecasting? Forecasting AI progress is critical for AI safety work. The timeline to transformative AI shapes everything from research priorities to governance frameworks \u2013 if we expect transformative AI within 5 years versus 50 years, this dramatically changes which safety approaches are viable. For example, if we expect rapid progress, we might need to focus on safety measures that can be implemented quickly rather than long-term theoretical research. Additionally, understanding likely development trajectories helps us anticipate specific capabilities and prepare targeted safety measures before they emerge. This is especially critical given the potential for sudden capability jumps, especially in dangerous capabilities like malware generation or deception.</p> <p>Initial Forecast: Before we explore more sophisticated methods, make an initial prediction: When do you think we'll see transformative AI? Keep this forecast in mind as we examine different forecasting approaches.</p>"},{"location":"chapters/01/05/#01","title":"1.5.1 Methodology","text":"<p>How do we convert beliefs into probabilities and forecasts? We need some ways to actually convert beliefs like \"I think AGI is likely this decade\" into precise probability estimates. One way we can do this is by decomposition - breaking down complex beliefs into smaller, measurable components and analyzing relevant data. Rather than directly estimating the year in which transformative AI emerges, we can start by separately forecasting things like compute growth, algorithmic progress, and hardware limitations, and then combine these estimates (Zhang, 2024). This decomposition approach helps us ground predictions in observable trends rather than relying purely on intuitions. So, using this approach there are two main techniques we need to discuss - zeroth-order forecasting for establishing baselines, and first-order forecasting for understanding trajectories of change.</p> <p>What are reference classes and why do they matter? When analyzing each component of our decomposed forecast, we need relevant historical examples to inform our predictions. This is where reference classes come in - they are categories of similar historical situations we can use to make predictions. For AI development, relevant reference classes might include things like previous technological revolutions (like the industrial or computer revolution), other optimization systems (like biological evolution or economies), or the impact of rapid scientific advances (like CRISPR or mRNA vaccines). The basic point is that they should be meaningfully analogous to what you're trying to predict, but they don't have to be from the same exact category.</p> <p>What is zeroth-order forecasting? The simplest forecasting approach starts with recognizing that tomorrow often looks pretty close to today. Zeroth-order forecasting uses reference classes - looking at 3-5 similar historical examples and using their average as a baseline prediction. Rather than trying to identify trends or make complex projections, it assumes recent patterns will continue. (Steinhardt, 2024) These examples can be from different reference classes. For a concrete example of using multiple reference classes in AI safety forecasting: Suppose we want to predict how quickly advanced AI systems might transition from \"safe\" to \"potentially dangerous\" capabilities. We could look at:</p> <ul> <li> <p>How long it took language models to go from generating basic text to being able to plan multi-step deception (an AI-specific reference point)</p> </li> <li> <p>How quickly nuclear technology transitioned from peaceful to military applications (a dual-use technology reference point)</p> </li> <li> <p>How rapidly biological techniques like CRISPR went from lab discovery to widespread use requiring safety protocols (a biosafety reference point)</p> </li> </ul> <p>Looking at these examples collectively might suggest that dangerous capabilities often emerge within 2-5 years of the underlying technical breakthroughs, which could inform how urgently we need to develop safety measures. Major shifts in development patterns tend to be rare, making recent history a decent baseline predictor of the near future. This doesn't mean changes never happen \u2013 but it does mean that deviating from recent patterns requires strong evidence.</p> <p>What is first-order forecasting? While zeroth-order forecasting uses historical examples from various reference classes as direct predictors, first-order forecasting attempts to identify and project forward patterns in the direct historical data of AI development. In AI, we see some pretty consistent exponential growth patterns. The compute used in frontier models has grown by 4.2x annually since 2010, training datasets have expanded by approximately 2.9x per year, and hardware performance improves by roughly 1.35x every year through architectural advances (Epoch AI, 2023). First-order forecasting tries to identify these kinds of patterns and project them forward. This is the approach taken by most systematic AI forecasting work today, including Epoch AI's compute-centric framework and Ajeya Cotra's biological anchors. However, it's worth keeping in mind that even though these trends have been remarkably consistent, they can't continue indefinitely. Physical, thermodynamic, or economic constraints will eventually limit growth. The key question is: when do these limits become relevant? We will explore this in the next section on the compute centric framework.</p> <p>How do we combine different forecasts? Multiple forecasting approaches often give us different predictions \u2013 zeroth-order might suggest one timeline while trend extrapolation indicates another. Just like we can average out over the opinions of many experts, we can integrate these predictions to get a hopefully more accurate picture. One approach is to model each forecast as a probability distribution and combine them using mixture models (Steinhardt, 2024). For example, if zeroth-order forecasting suggests 3-4 years between major breakthroughs based on recent history, while trend extrapolation points to 1.5-2 years based on compute growth, a combined model might predict 2-3 years but with wider confidence intervals to account for uncertainty in both approaches.</p> <p>What about situations with limited data or limited reference classes? While decomposition, reference classes and trend analysis form the backbone of AI forecasting, we sometimes face questions where direct data is limited or no clear reference classes exist. For instance, predicting the societal impact of advanced AI systems or forecasting novel capabilities that haven't been demonstrated before. In these cases, we often turn to expert judgment and superforecasters. An advantage of expert forecasting is the ability to integrate qualitative insights that might be missed by pure trend analysis. For example, experts might notice early warning signs of diminishing returns or identify emerging technical approaches that could accelerate progress. This balanced use of both data-driven methods and expert judgment is especially important for AI safety work. While we should ground our predictions in empirical trends whenever possible, we also need frameworks for reasoning about unprecedented developments and potential discontinuities in progress.</p> <p>How far do empirical findings generalize? There's an ongoing debate about how much we can trust current trends to predict future AI development. Some researchers argue that empirical findings in AI generalize surprisingly far - that patterns we observe today will continue to hold even as systems become more capable (Steinhardt, 2022). However, our track record with forecasting suggests we should be cautious. When superforecasters predicted MATH dataset accuracy would improve from 44% to 57% by June 2022, actual performance reached 68% - a level they had rated extremely unlikely. Shortly after, GPT-4 achieved 86.4% accuracy. There are a couple of more examples of LLMs surprising most forecasters and experts on certain benchmarks. (Cotra, 2023).</p> <p>This pattern of underestimating progress suggests that while empirical trends provide valuable guidance, they may not capture all the dynamics of AI development. Prior to GPT-3, many experts believed tasks like complex reasoning would require specialized architectures. The emergence of these capabilities from scaling alone shows how systems can develop unexpected abilities simply through quantitative improvements. This has critical implications for both forecasting and governance - we need frameworks that can adapt to capabilities emerging faster or differently than current trends suggest.</p> <p>How does this help us predict transformative AI? These forecasting fundamentals help us critically evaluate claims about AI timelines and takeoff scenarios. When we encounter predictions about discontinuous progress or smooth scaling, we can ask: What trends support this view? What reference classes are relevant? How have similar forecasts performed historically? This systematic approach helps us move beyond intuition to make more rigorous predictions about AI development trajectories.</p>"},{"location":"chapters/01/05/#02","title":"1.5.2 Biology Inspired Forecasting","text":"<p>What are Biological anchors? Biological anchors are a forecasting technique. To find a reference class, assume that the human brain is indicative of general intelligence. This means we can treat it as a proof of concept. Whatever \"amount of compute\" it takes to train a human being, might be roughly the same amount it should take to train a TAI. The biological anchors approach estimates the compute required for AI to reach a level of intelligence comparable to humans, outlined through several steps:</p> <ul> <li> <p>First, assess how much computation the human brain performs, translating this into a quantifiable measure similar to computer operations in FLOP/s.</p> </li> <li> <p>Second, estimate the amount of computation needed to train a neural network to match the brain's inferential capacity, adjusting for future improvements in algorithmic efficiency.</p> </li> <li> <p>Third, examine when it would be feasible to afford such vast computational resources, taking into account the decreasing cost of compute, economic growth, and increasing investment in AI.</p> </li> <li> <p>Finally, by analyzing these factors, we can predict when it might be economically viable for AI companies to deploy the necessary resources for developing TAI.</p> </li> </ul> <p>Determining the exact computational equivalent for the human brain's training process is complex, leading to the proposal of six hypotheses, collectively referred to as \"biological anchors\" or \"bioanchors.\" Each anchor has a different weighting contributing to the overall prediction.</p> <p>Evolution Anchor: Total computational effort across all evolutionary history.</p> <p>Lifetime Anchor: Brain's computational activity from birth to adulthood (0-32).</p> <p>Neural Network and Genome Anchors: Various computational benchmarks based on the human brain and genome to gauge the scale of parameters needed for AI to achieve general intelligence.</p> Figure 1.27: A diagram showing the different starting points that we could use within the Bio anchors report to calculate the amount of effective compute required for human level transformative AI. (Ho, 2022) <p>Forecasting with Biological Anchors . By integrating these anchors with projections of future compute accessibility, we can outline a potential timeline for TAI. This method aims to provide a \"soft upper bound\" on TAI's arrival rather than pinpointing an exact year, acknowledging the complexity and unpredictability of AI development. (Karnofsky, 2021) The following image gives an overview of the methodology.</p> Figure 1.28: The biological anchor model (Ho, 2022) <p>Affordability of compute. The costs related to bio anchors are calculated by considering three different factors: Algorithmic progress, compute price estimates, and willingness to spend on machine learning. The report considers a doubling in algorithmic efficiency every ~2-3 years. As for prices, Cotra assumes cost decreases over time, halving every ~2.5 years, and further expects this to level off after 6 orders of magnitude. Cotra assumes that the willingness to spend on machine learning training runs should be capped at 1% of the GDP of the largest country, referencing previous case studies with megaprojects (e.g. the Manhattan Project), and should follow a doubling time of 2 years after 2025. The main uncertainty is whether or not existing trends are going to persist more than several years into the future. For example, Epoch found that OpenAI\u2019s AI and Compute investigation (OpenAI, 2018) was too aggressive in its findings for compute growth. (Ho et al., 2022) This suggests taking caution when interpreting the forecasts made by the Bio Anchors report.</p> Figure 1.29: Affordability of compute (Ho et al., 2022) <p>The following graph gives an overview of the findings. Overall, the graph takes a weighted average of the different ways that the trajectory could flow. This gives us an estimate of a &gt;10% chance of transformative AI by 2036, a ~50% chance by 2055, and an ~80% chance by 2100. In 2022 a two-year update on the author\u2019s (Ajeya Cotra) timelines was published. The updated timelines for TAI are ~15% probability by 2030, ~35% probability by 2036, a median of ~2040, and a ~60% probability by 2050. (Cotra, 2022)</p> Figure 1.30: Results from the biological anchor model for different anchors (Karnofsky, 2021) <p>Criticisms. The Biological Anchors framework provides a unique perspective but it's also crucial to recognize its limitations and the broader debates it sparks within the AI research community. It is not universally accepted as the primary predictive tool among all ML scientists or alignment researchers.</p> <p>Platt's Law is a generalized observation named after Charles Platt. It is used to highlight a historical pattern where the estimated arrival of AGI (Artificial General Intelligence) consistently seems to be \"just 30 years away\".</p> <p>Vernor Vinge at 1993 NASA speech. (Yudkowsky, 2021)</p> <p>Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.</p> <p>Yudkowsky notes that Platt\u2019s law seems to fit remarkably well with the prediction made by the Biological Anchors report in 2020. As the statistical aphorism goes: \"All models are wrong, but some are useful\".</p> <p>So to get a complete picture of how biological anchors were received, here are some of the criticisms of the Biological Anchors report:</p> <ul> <li> <p>Challenges Beyond Compute : While Biological Anchors highlight compute power as a critical factor for AI development, they may oversimplify the complexity of achieving transformative AI. Factors beyond sheer computing capacity, such as algorithm design, data availability, and the intricacies of learning environments, play pivotal roles. It\u2019s overly simplistic to equate the future of AI solely with compute availability, as transformative AI development encompasses more nuanced challenges like algorithm innovation and data accessibility. (Karnofsky, 2021)</p> </li> <li> <p>Potential for Rapid Advancements : Critics of the Biological Anchors method, such as Eliezer Yudkowsky, emphasize the unpredictability of AI progress and the potential for breakthroughs that could dramatically alter AI capabilities without adhering strictly to computational benchmarks derived from biology. These critiques highlight the importance of considering a range of factors and potential shifts in AI development paradigms that could accelerate progress beyond current forecasts. (Karnofsky, 2021)</p> </li> <li> <p>Purpose and Misinterpretation : The Biological Anchors approach is intended to provide boundary estimates for AI development timelines rather than precise predictions. Misinterpretations may arise from expecting the method to offer specific year forecasts, whereas its goal is to outline possible upper and lower bounds, acknowledging the significant uncertainties in AI development. (Karnofsky, 2021)</p> </li> <li> <p>Contemplating Paradigm Shifts : The AI field's history suggests that major paradigm shifts and technological breakthroughs could substantially impact development timelines. While deep learning currently dominates AI advances, the possibility of new, transformative methodologies emerging remains open, challenging the assumption that current trends will roughly maintain its current growth trajectory into the future.</p> </li> </ul> <p>This is not an exhaustive list of all the criticisms but it serves to highlight the complexity of forecasting AI's future.</p>"},{"location":"chapters/01/06/","title":"1.6 Takeoff","text":"Reading Time 22 min <p>What are takeoff speeds? Before we can talk about different types of AI takeoff, we need to understand what takeoff speed even means. Takeoff speed refers to how quickly AI systems become dramatically more powerful than they are today and cause major societal changes. This is related to, but distinct from, AI timelines (how long until we develop advanced AI). While timelines tell us when transformative AI might arrive, takeoff speeds tell us what happens after it arrives - does AI capability and impact increase gradually over years, or explosively over days or weeks?</p> <p>How can we think about different takeoff speeds? When analyzing different takeoff scenarios, we can look at several key factors:</p> <ul> <li> <p>Speed : How quickly do AI capabilities improve?</p> </li> <li> <p>Continuity : Do capabilities improve smoothly or in sudden jumps?</p> </li> <li> <p>Homogeneity : How similar are different AI systems to each other?</p> </li> <li> <p>Polarity : How concentrated is power among different AI systems?</p> </li> </ul> <p>Let's discuss each of these factors and what they tell us about potential takeoff scenarios. This will help us better understand the ongoing debate about how transformative AI might develop.</p>"},{"location":"chapters/01/06/#01","title":"1.6.1 Speed","text":"<p>What is a slow takeoff? In a slow takeoff scenario, AI capabilities improve gradually over months or years. We can see this pattern in recent history - the transition from GPT-3 to GPT-4 brought significant improvements in reasoning, coding, and general knowledge, but these advances happened over several years through incremental progress. Paul Christiano describes slow takeoff as similar to the Industrial Revolution but \"10x-100x faster\" (Davidson, 2023). Terms like \"slow takeoff\" and \"soft takeoff\" are often used interchangeably.</p> <p>In mathematical terms, slow takeoff scenarios typically show linear or exponential growth patterns. With linear growth, capabilities increase by the same absolute amount each year - imagine an AI system that gains a fixed number of new skills annually. More commonly, we might see exponential growth, where capabilities increase by a constant percentage, similar to how we discussed scaling laws in earlier sections. Just as model performance improves predictably with compute and data, slow takeoff suggests capabilities would grow at a steady but manageable rate. This might manifest as GDP growing at 10-30% annually before accelerating further.</p> <p>The key advantage of slow takeoff is that it provides time to adapt and respond. If we discover problems with our current safety approaches, we can adjust them before AI becomes significantly more powerful. This connects directly to what we'll discuss in later chapters about governance and oversight - slow takeoff allows for iterative refinement of safety measures and gives time for coordination between different actors and institutions.</p> Figure 1.31: An illustration of slow continuous takeoff. (Martin &amp; Eth, 2021) <p>What is a fast takeoff? Fast takeoff describes scenarios where AI capabilities increase dramatically over very short periods - perhaps days or even hours. Instead of the gradual improvement we saw from GPT-3 to GPT-4, imagine an AI system making that much progress every day. This could happen through recursive self-improvement, where an AI system becomes better at improving itself, creating an accelerating feedback loop.</p> <p>Mathematically, fast takeoff involves superexponential or hyperbolic growth, where the growth rate itself increases over time. Rather than capabilities doubling every year as in exponential growth, they might double every month, then every week, then every day. This relates to what we discussed in the scaling section about potential feedback loops in AI development - if AI systems can improve the efficiency of AI research itself, we might see this kind of accelerating progress.</p> <p>The dramatic speed of fast takeoff creates unique challenges for safety. As we'll explore in the chapter on strategies, many current safety approaches rely on testing systems, finding problems, and making improvements. But in a fast takeoff scenario, we might only get one chance to get things right. If an AI system starts rapidly self-improving, we need safety measures that work robustly from the start, because we won't have time to fix problems once they emerge.</p> <p>Terms like \"fast takeoff\", \"hard takeoff\" and \"FOOM\" are often used interchangeably.</p> Figure 1.32: An illustration of fast continuous takeoff, which is usually taken to mean superexponential or hyperbolic growth. The growth rate itself increases. (Martin &amp; Eth, 2021) <p>Why does takeoff speed matter for AI risk? The speed of AI takeoff fundamentally shapes the challenge of making AI safe. This connects directly to what we discussed about scaling laws and trends - if progress follows predictable patterns as our current understanding suggests, we might have more warning and time to prepare. But if new mechanisms like recursive self-improvement create faster feedback loops, we need different strategies.</p> <p>A concrete example helps illustrate this: Today, when we discover that language models can be jailbroken in certain ways, we can patch these vulnerabilities in the next iteration. In a slow takeoff, this pattern could continue - we'd have time to discover and fix safety issues as they arise. But in a fast takeoff, we might need to solve all potential jailbreaking vulnerabilities before deployment, because a system could become too powerful to safely modify before we can implement fixes.</p> Figure 1.33: Comparison of \"slow\" vs \"fast\" takeoff. Showcasing that while described as linguistically slower than fast, it is by no means \"slow\". (Christiano, 2018)"},{"location":"chapters/01/06/#02","title":"1.6.2 Continuity","text":"<p>What is takeoff continuity? Continuity describes whether AI capabilities improve smoothly and predictably or through sudden, unexpected jumps. This is different from speed - even a fast takeoff could be continuous if the rapid progress follows predictable patterns, and a slow takeoff could be discontinuous if it involves surprising breakthroughs. Understanding continuity helps us predict whether we can extrapolate from current trends, like the scaling laws we discussed earlier, or if we should expect sudden departures from these patterns. So if you think of speed as a measure of how quickly the AI becomes superintelligent, continuity can be thought of as a measure of \"surprise\".</p> <p>What is a continuous takeoff? In a continuous takeoff, AI capabilities follow smooth, predictable trends. The improvements we've seen in language models provide a good example - each new model tends to be somewhat better than the last at tasks like coding or math, following patterns we can roughly predict from scaling laws and algorithmic improvements. As we saw in the forecasting section, many aspects of AI progress have shown this kind of predictable behavior.</p> <p>Continuous progress doesn't mean linear or simple progress. It might still involve exponential or even superexponential growth, but the key is that this growth follows patterns we can anticipate. Think of how GPT-4 is better than GPT-3, which was better than GPT-2 - each improvement was significant but not completely surprising given the increase in scale and improved training techniques.</p> <p>A continuous takeoff suggests that current trends in scaling laws and algorithmic progress might extend even to transformative AI systems. This would give us more warning about upcoming capabilities and more ability to prepare appropriate safety measures. As we'll discuss in the governance chapter, even though progress is fast, this kind of predictability makes it comparatively easier to develop and implement regulation before AI systems become extremely powerful or uncontrollable. Keeping in mind of course that comparatively easier does not mean \"easy\".</p> <p>What is a discontinuous takeoff? A discontinuous takeoff involves sudden jumps in capability that break from previous patterns. Instead of steady improvements in performance as we add compute or data, we might see the emergence of entirely new capabilities that weren't predicted by existing trends. One hypothetical example would be if an AI system suddenly developed robust general reasoning capabilities after appearing to only handle narrow tasks - this would represent a discontinuity in the pattern of AI development.</p> <p>Discontinuities could arise through various mechanisms. We might discover fundamentally new training approaches that are dramatically more efficient than current methods. Or, as we discussed in the scaling section, we might hit tipping points where quantitative improvements in scale lead to qualitative changes in capability. An AI system might even discover such improvements about itself, leading to unexpected jumps in capability.</p> <p>The historical record provides some precedent for both continuous and discontinuous scientific progress. The development of nuclear weapons represented a discontinuous jump in explosive power, while improvements in computer processing power have followed more continuous trends. However, as we saw in the forecasting section, technological discontinuities have historically been rare, which some researchers cite as evidence favoring continuous takeoff scenarios.</p> <p>The terms 'fast takeoff' and 'discontinuous takeoff' are often used interchangeably. However, the images below displaying different takeoff trajectories might help in clarifying the subtle differences between the concepts.</p> <p>Why does continuity matter for AI safety? The continuity of AI progress has crucial implications for how we approach safety. In a continuous takeoff, we can more reliably test safety measures on less capable systems and be more confident they'll work on more advanced ones. We can also better predict when we'll need different safety measures and plan accordingly.</p> Figure 1.34: One example illustration of slow discontinuous takeoff, where even though progress keeps increasing we might see sudden \"jumps\" in progress. (Martin &amp; Eth, 2021) Figure 1.35: One example illustration of fast discontinuous takeoff. Even though progress keeps accelerating, in addition to that we might also see sudden \"jumps\" in progress. (Martin &amp; Eth, 2021)"},{"location":"chapters/01/06/#03","title":"1.6.3 Similarity","text":"<p>What is takeoff homogeneity? Homogeneity describes how similar or different AI systems are to each other during the takeoff period. Will we see many diverse AI systems with different architectures and capabilities, or will most advanced AI systems be variations of the same basic design? This isn't just about technical diversity - it's about whether advanced AI systems will share similar behaviors, limitations, and safety properties. (Hubinger, 2020)</p> <p>What is a homogeneous takeoff? In a homogeneous takeoff, most advanced AI systems would be fundamentally similar. We can see hints of this pattern today - many current language models are based on the transformer architecture and trained on similar data, leading to similar capabilities and limitations. In a homogeneous takeoff, this pattern would continue. Perhaps most AI systems would be fine-tuned versions of a few base models, or different implementations of the same core breakthrough in AI design.</p> <p>A key factor that could drive homogeneity is the sheer scale required to train advanced AI systems. If training transformative AI requires massive compute resources, as scaling laws suggest, then only a few organizations might be capable of training base models from scratch. Other organizations would build on these base models rather than developing entirely new architectures, leading to more homogeneous systems.</p> <p>Homogeneous takeoff could be safer in some ways but riskier in others. If we solve alignment for one AI system, that solution might work for other similar systems. However, if there's a fundamental flaw in the common architecture or training approach, it could affect all systems simultaneously. It's like having a monoculture in agriculture - while easier to manage, it's also more vulnerable to shared weaknesses.</p> Figure 1.36: An illustration of homogeneous takeoff. We can see multiple different overarching model architectures. The figure shows three in different colors. Within each architecture the takeoff is roughly the same due to similarity in design, regulations, and safety mitigations. NOTE : The curves here with architectures are purely illustrative, and are not meant to indicate predicted growth trajectories and comparisons between different architectures. <p>What is a heterogeneous takeoff? In a heterogeneous takeoff, we'd see significant diversity among advanced AI systems. Different organizations might develop fundamentally different approaches to AI, leading to systems with distinct strengths, weaknesses, and behaviors. Some might be specialized for specific domains while others are more general, some might be more transparent while others are more opaque, some might be more aligned with human values while others might not be. Competitive dynamics among AI projects could exacerbate diversity, as teams race to achieve breakthroughs without necessarily aligning on methodologies or sharing crucial information. As an example, we might have a future where AI becomes a strategic national asset, and AI development is closely guarded. In this environment, the pursuit of AI capabilities becomes siloed, each company or country would then employ different development methodologies, potentially leading to a wide range of behaviors, functionalities, and safety levels.</p> <p>Heterogeneous takeoff creates different challenges for safety. We'd need to develop safety measures that work across diverse systems, and we couldn't necessarily apply lessons learned from one system to others. However, diversity might provide some protection against systemic risks - if one approach proves dangerous, alternatives would still exist.</p> <p>How does takeoff homogeneity affect the broader picture? The degree of homogeneity during takeoff has significant implications for how transformative AI might develop. In a homogeneous scenario, progress might be more predictable but also more prone to winner-take-all dynamics. A heterogeneous scenario might be more robust against single points of failure but harder to monitor and control.</p> Figure 1.37: One example of heterogeneous takeoff. We can see multiple different overarching model architectures. The figure shows three in different colors. Within each architecture the takeoff is different due to differences in design, regulations, and safety mitigations. NOTE : The curves here with architectures are purely illustrative, and are not meant to indicate predicted growth trajectories and comparisons between different architectures."},{"location":"chapters/01/06/#04","title":"1.6.4 Polarity","text":"<p>What is takeoff polarity? Polarity describes whether power and capability becomes concentrated in a single AI system or organization, or remains distributed among multiple actors. In other words, will one AI system or group pull dramatically ahead of all others, or will multiple AI systems advance in parallel with comparable capabilities?</p> <p>What is a unipolar takeoff? In a unipolar takeoff, one AI system or organization gains a decisive lead over all others. This could happen through a single breakthrough, exceptional scaling advantages, or recursive self-improvement. For example, if one AI system becomes capable enough to substantially accelerate its own development, it might rapidly outpace all other systems. The mathematics of training compute provide one path to a unipolar outcome. If a doubling of compute leads to reliable improvements in capability, then an organization that gets far enough ahead in acquiring compute could maintain or extend their lead. Their improved systems could then help them develop even better training methods, hardware, and attract investment creating a positive feedback loop that others can't match. But compute isn't the only path to unipolarity. A single organization might discover a fundamentally better training approach, or develop an AI system that's better at improving itself than at helping humans build alternatives. Once any actor gets far enough ahead, it might become practically impossible for others to catch up.</p> Figure 1.38: An illustration of unipolar takeoff. One model (dark blue here) significantly outperforms all others. <p>What is a multipolar takeoff? In a multipolar takeoff, multiple AI systems or organizations develop advanced capabilities in parallel. This could look like several large labs developing different but comparably powerful AI systems, or like many actors having access to similar AI capabilities through open source models or AI services. Today's AI landscape shows elements of multipolarity - multiple organizations can train large language models, and techniques developed by one lab are often quickly adopted by others. A multipolar takeoff might continue this pattern, with multiple groups maintaining similar capabilities even as those capabilities become transformative. A unipolar scenario raises concerns about the concentration of power, while a multipolar world presents challenges in coordination among diverse entities or AI systems. Both unipolar and multipolar worlds have the potential for misuse of advanced AI capabilities by human actors.</p> Figure 1.39: An illustration of multipolar takeoff. No model significantly outperforms all others, and they all takeoff at a roughly competitive rate relative to each other. <p>Why does polarity matter? The polarity of takeoff has major implications for both safety risks and potential solutions. In a unipolar scenario, the actions and alignment of a single system or organization become crucial - they might gain the ability to shape the long-term future unilaterally. This concentrates risk in a single point of failure, but might also make coordination easier since fewer actors need to agree. A multipolar scenario creates different challenges. Multiple advanced systems might act in conflicting ways or compete for resources. This could create pressure to deploy systems quickly or cut corners on safety. There's also an important interaction between polarity and the other aspects of takeoff we've discussed. A fast takeoff might be more likely to become unipolar, as the first system to make rapid progress could quickly outpace all others. A slow takeoff might tend toward multipolarity, giving more actors time to catch up to any initial leads.</p> <p>Factors Influencing Polarity . Several key elements influence whether takeoff polarity leans towards a unipolar or multipolar outcome:</p> <ul> <li> <p>Speed of AI Development: A rapid takeoff might favor a unipolar outcome by giving a significant advantage to the fastest developer. In contrast, a slower takeoff could lead to a multipolar world where many entities reach advanced capabilities more or less simultaneously.</p> </li> <li> <p>Collaboration vs. Competition: The degree of collaboration and openness in the AI research community can significantly affect takeoff polarity. High levels of collaboration and information sharing could support a multipolar outcome, while secretive or highly competitive environments might push towards unipolarity.</p> </li> <li> <p>Regulatory and Economic Dynamics: Regulatory frameworks and economic incentives also play a crucial role. Policies that encourage diversity in AI development and mitigate against the accumulation of too much power in any single entity's hands could foster a multipolar takeoff.</p> </li> </ul>"},{"location":"chapters/01/06/#05","title":"1.6.5 Takeoff Arguments","text":"<p>The Overhang Argument . There might be situations where there are substantial advancements or availability in one aspect of the AI system, such as hardware or data, but the corresponding software or algorithms to fully utilize these resources haven't been developed yet. The term 'overhang' is used because these situations imply a kind of 'stored\u2019 or \u2018latent\u2019 potential. Once the software or algorithms catch up to the hardware or data, there could be a sudden unleashing of this potential, leading to a rapid leap in AI capabilities. Overhangs provide one possible argument for why we might favor discontinuous or fast takeoffs. There are two types of overhangs commonly discussed:</p> <ul> <li> <p>Hardware Overhang : This refers to a situation where there is enough computing hardware to run many powerful AI systems, but the software to run such systems hasn't been developed yet. If such hardware could be repurposed for AI, this would mean that as soon as one powerful AI system exists, probably a large number of them would exist, which might amplify the impact of the arrival of human-level AI.</p> </li> <li> <p>Data Overhang : This would be a situation where there is an abundance of data available that could be used for training AI systems, but the AI algorithms capable of utilizing all that data effectively haven't been developed or deployed yet.</p> </li> </ul> <p>Overhangs are also used as a counter argument to why AI pauses do not affect takeoff. One counter argument to the overhang argument is that it relies on the assumption that during the time that we are pausing AI development, the rate of production of chips will remain constant. It could be argued that the companies manufacturing these chips will not make as many chips if data centers aren't buying them. However, this argument only works if the pause is for any appreciable length of time, otherwise the data centers might just stockpile the chips. It is also possible to make progress on improved chip design, without having to manufacture as many during the pause period. However, during the same pause period we could also make progress on AI safety techniques. (Elmore, 2024)</p> <p>The Economic Growth Argument . Historical patterns of economic growth, driven by human population increases, suggest a potential for slow and continuous AI takeoff. This argument says that as AIs augment the effective economic population, we might witness a gradual increase in economic growth, mirroring past expansions but at a potentially accelerated rate due to AI-enabled automation. Limitations in AI's ability to automate certain tasks, alongside societal and regulatory constraints (e.g. that medical or legal services can only be rendered by humans), could lead to a slower expansion of AI capabilities. Alternatively, growth might far exceed historical rates. Using a similar argument for a fast takeoff hinges on AI's potential to quickly automate human labor on a massive scale, leading to unprecedented economic acceleration.</p> Figure 1.40: A visualization of the ranking of arguments for explosive economic growth, both in favor and against. By Epoch AI. (Erdil &amp; Besiroglu, 2024) <p>Compute Centric Takeoff Argument . This argument, similar to the Bio Anchors report, assumes that compute will be sufficient for transformative AI. Based on this assumption, Tom Davidson's 2023 report on compute-centric AI takeoff discusses feedback loops that may contribute to takeoff dynamics.</p> <ul> <li> <p>Investment feedback loop : There might be increasing investment in AI, as AIs play a larger and larger role in the economy. This increases the amount of compute available to train models, as well as potentially leading to the discovery of novel algorithms. All of this increases capabilities, which drives economic progress, and further incentivizes investment.</p> </li> <li> <p>Automation feedback loop : As AIs get more capable, they will be able to automate larger parts of the work of coming up with better AI algorithms, or helping in the design of better GPUs. Both of these will increase the capability of the AIs, which in turn allow them to automate more labor.</p> </li> </ul> <p>Depending on the strength and interplay of these feedback loops, they can create a self-fulfilling prophecy leading to either an accelerating fast takeoff if regulations don't curtail various aspects of such loops, or a slow takeoff if the loops are weaker or counterbalanced by other factors. The entire model is shown in the diagram below:</p> Figure 1.41: A summary of What a Compute-Centric Framework Says About Takeoff Speeds (Davidson, 2024) <p>Automating Research Argument. Researchers could potentially design the next generation of ML models more quickly by delegating some work to existing models, creating a feedback loop of ever-accelerating progress. The following argument is put forth by Ajeya Cotra:</p> <p>Currently, human researchers collectively are responsible for almost all of the progress in AI research, but are starting to delegate a small fraction of the work to large language models. This makes it somewhat easier to design and train the next generation of models.</p> <p>The next generation is able to handle harder tasks and more different types of tasks, so human researchers delegate more of their work to them. This makes it significantly easier to train the generation after that. Using models gives a much bigger boost than it did the last time around.</p> <p>Each round of this process makes the whole field move faster and faster. In each round, human researchers delegate everything they can productively delegate to the current generation of models \u2014 and the more powerful those models are, the more they contribute to research and thus the faster AI capabilities can improve. (Cotra, 2023)</p> <p>So before we see a recursive explosion of intelligence, we see a steadily increasing amount of the full RnD process being delegated to AIs. At some point, instead of a significant majority of the research and design being done by AI assistants at superhuman speeds, it will become that - all of the research and design for AIs is done by AI assistants at superhuman speeds.</p> <p>At this point there is a possibility that this might eventually lead to a full automated recursive intelligence explosion.</p> <p>The Intelligence Explosion Argument . This concept of the 'intelligence explosion' is also central to the conversation around discontinuous takeoff. It originates from I.J. Good's thesis, which posits that sufficiently advanced machine intelligence could build a smarter version of itself. This smarter version could in turn build an even smarter version of itself, and so on, creating a cycle that could lead to intelligence vastly exceeding human capability (Yudkowsky, 2013).</p> <p>In their 2012 report on the evidence for Intelligence Explosions, Muehlhauser and Salamon delve into the numerous advantages that machine intelligence holds over human intelligence, which facilitate rapid intelligence augmentation. (Muehlhauser, 2012) These include:</p> <ul> <li> <p>Computational Resources : Human computational ability remains somewhat stationary, whereas machine computation possesses scalability.</p> </li> <li> <p>Speed : Humans communicate at a rate of two words per second, while GPT-4 can process 32k words in an instant. Once LLMs can write \"better\" than humans, their speed will most probably surpass us entirely.</p> </li> <li> <p>Duplicability : Machines exhibit effortless duplicability. Unlike humans, they do not need birth, education, or training. While humans predominantly improve individually, machines have the potential to grow collectively. Humans take 20 years to become competent from birth, whereas once we have one capable AI, we can duplicate it immediately. Once AIs reach the level of the best programmer, we can just duplicate this AI. The same goes for other jobs.</p> </li> <li> <p>Editability : Machines potentially allow more regulated variations. They exemplify the equivalent of direct brain enhancements via neurosurgery in opposition to laborious education or training requirements. Humans can also improve and learn new skills, but they don't have root access to their hardware: we are just starting to be able to understand the genome's \"spaghetti code,\" while AI could use code versioning tools to improve itself, being able to attempt risky experiments with backup options in case of failure. This allows for much more controlled variation.</p> </li> <li> <p>Goal coordination : Copied AIs possess the capability to share goals effortlessly, a feat challenging for humans.</p> </li> </ul>"},{"location":"chapters/01/A1/","title":"1.A1 Expert Opinions","text":"Reading Time 14 min <p>Everything in the appendices is optional, and is meant as additional knowledge and context. You do not need to read this to understand the core points made in either this chapter or future chapters.</p> Video 1.3: Optional video outlining some views that AI experts have on safety and risk. Interactive Figure 1.13: Public opinions about AIs impacts on society (Giattino et al., 2023) Interactive Figure 1.14: Public opinions on machine vs human intelligence. (Giattino et al., 2023)"},{"location":"chapters/01/A1/#01","title":"1.A1.1 Surveys","text":"<p>According to a recent survey conducted by AI Impact (AI Impacts, 2022): \"Expected time to human-level performance dropped 1\u20135 decades since the 2022 survey . As always, our questions about \u2018high-level machine intelligence\u2019 (HLMI) and \u2018full automation of labor\u2019 (FAOL) got very different answers, and individuals disagreed a lot (shown as thin lines below), but the aggregate forecasts for both sets of questions dropped sharply. For context, between 2016 and 2022 surveys, the forecast for HLMI had only shifted about a year. \"</p> Figure 1.42: 2024 Survey of AI Experts (AI Impacts, 2022) <p>It is also possible to compare the predictions of the same study in 2022 to the current results. It is interesting to note that the community has generally underestimated the speed of progress over the year 2023 and has adjusted its predictions downward. Some predictions are quite surprising. For example, tasks like \"Write High School Essay\" and \"Transcribe Speech\" are arguably already automated with ChatGPT and Whisper, respectively. However, it appears that researchers are not aware of these results. Additionally, it is surprising that the forecast for when we are able to build an \"AI researcher\" has longer timelines than when we are able to build \"High-level machine intelligence (all human tasks)\".</p> <p>The median of the 2024 expert survey predicts human-level machine intelligence (HLMI) in 2049.</p>"},{"location":"chapters/01/A1/#02","title":"1.A1.2 Quotes","text":"<p>Here are many quotes from people regarding transformative AI.</p>"},{"location":"chapters/01/A1/#02-01","title":"1.A1.2.1 AI Experts","text":"<p>Note that Hinton, Bengio, and Sutskever are some of the most cited researchers in the field of AI. And that Hinton, Bengio, and LeCun are the recipients of the Turing Award in Deep Learning. Some users on reddit have put together a comprehensive list of publicly stated AI timelines forecasts from famous researchers and industry leaders.</p> <p>Geoffrey Hinton (Godfather of modern AI, Turing Award Recipient)</p> <p>\"The research question is: how do you prevent them from ever wanting to take control? And nobody knows the answer [...] The alarm bell I'm ringing has to do with the existential threat of them taking control [...] If you take the existential risk seriously, as I now do, it might be quite sensible to just stop developing these things any further [...] it's as if aliens had landed and people haven't realized because they speak very good English\"</p> <p>Yoshua Bengio (One of most cited scientists ever, Godfather of modern AI, Turing Award Recipient)</p> <p>\"It's very hard, in terms of your ego and feeling good about what you do, to accept the idea that the thing you've been working on for decades might actually be very dangerous to humanity... I think that I didn't want to think too much about it, and that's probably the case for others [...] Rogue AI may be dangerous for the whole of humanity. Banning powerful AI systems (say beyond the abilities of GPT-4) that are given autonomy and agency would be a good start.\"</p> <p>Yann LeCun (Godfather of modern AI, Turing Award Recipient, Chief AI Scientist at Meta)</p> <p>\"There is no question that machines will become smarter than humans\u2014in all domains in which humans are smart\u2014in the future. It's a question of when and how, not a question of if.\"</p> <p>Stuart Russell (Co-Author of leading AI textbook, Co-Founder of Center for Human-Compatible AI)</p> <p>\"If we pursue [our current approach], then we will eventually lose control over the machines.\"</p> <p>Demis Hassabis (Co-Founder and CEO of DeepMind)</p> <p>\"We must take the risks of AI as seriously as other major global challenges, like climate change. It took the international community too long to coordinate an effective global response to this, and we're living with the consequences of that now. We can't afford the same delay with AI [...] then maybe there's some kind of equivalent one day of the IAEA, which actually audits these things.\"</p> <p>Dario Amodei (Co-Founder and CEO of Anthropic, Former Head of AI Safety at OpenAI)</p> <p>\"When I think of why am I scared [...] I think the thing that's really hard to argue with is like, there will be powerful models; they will be agentic; we're getting towards them. If such a model wanted to wreak havoc and destroy humanity or whatever, I think we have basically no ability to stop it.\"</p> <p>Mustafa Suleyman (CEO of Microsoft AI, Co-Founder of DeepMind)</p> <p>\"[About a Pause] I don't rule it out. And I think that at some point over the next five years or so, we're going to have to consider that question very seriously.\"</p> <p>Ilya Sutskever (One of the most cited scientists ever, Co-Founder and Former Chief Scientist at OpenAI)</p> <p>\"The future is going to be good for the AIs regardless; it would be nice if it would be good for humans as well [...] It's not that it's going to actively hate humans and want to harm them, but it's just going to be too powerful, and I think a good analogy would be the way humans treat animals [...] And I think by default that's the kind of relationship that's going to be between us and AGIs which are truly autonomous and operating on their own behalf.\"</p> <p>Shane Legg (Co-Founder and Chief AGI Scientist at DeepMind)</p> <p>\"Do possible risks from AI outweigh other possible existential risks\u2026? It's my number 1 risk for this century [...] A lack of concrete AGI projects is not what worries me, it's the lack of concrete plans on how to keep these safe that worries me.\"</p> <p>Jan Leike (Former co-lead of the Superalignment project at OpenAI)</p> <p>\"[After resigning at OpenAI, talking about sources of risks] These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there [...] OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products. We are long overdue in getting incredibly serious about the implications of AGI.\"</p> <p>Sam Altman (Co-Founder and CEO of OpenAI)</p> <p>\"[Suggesting about how to ask for a global regulatory body:] \"any compute cluster above a certain extremely high-power threshold \u2013 and given the cost here, we're talking maybe five in the world, something like that \u2013 any cluster like that has to submit to the equivalent of international weapons inspectors\" [\u2026] I did a big trip around the world this year, and talked to heads of state in many of the countries that would need to participate in this, and there was almost universal support for it.\"</p> <p>Greg Brockman (Co-Founder and Former CTO of OpenAI)</p> <p>\"The exact way the post-AGI world will look is hard to predict \u2014 that world will likely be more different from today's world than today's is from the 1500s [...] We do not yet know how hard it will be to make sure AGIs act according to the values of their operators. Some people believe it will be easy; some people believe it'll be unimaginably difficult; but no one knows for sure\"</p> <p>John Schulman (Co-Founder of OpenAI)</p> <p>\"[Talking about times near the creation of the first AGI] you have the race dynamics where everyone's trying to stay ahead, and that might require compromising on safety. So I think you would probably need some coordination among the larger entities that are doing this kind of training [...] Pause either further training, or pause deployment, or avoiding certain types of training that we think might be riskier.\"</p> <p>Jaan Tallinn (Co-Founder of Skype, Future of Life Institute)</p> <p>\"I've not met anyone in AI labs who says the risk [from training a next-gen model] is less than 1% of blowing up the planet. It's important that people know lives are being risked [...] One thing that a pause achieves is that we will not push the Frontier, in terms of risky pre-training experiments.\"</p>"},{"location":"chapters/01/A1/#02-02","title":"1.A1.2.2 Politicians","text":"<p>Rishi Sunak (Former UK Prime Minister)</p> <p>\"Get this wrong, and AI could make it easier to build chemical or biological weapons. Terrorist groups could use AI to spread fear and destruction on an even greater scale. Criminals could exploit AI for cyber-attacks, disinformation, fraud, or even child sexual abuse. And in the most unlikely but extreme cases, there is even the risk that humanity could lose control of AI completely through the kind of AI sometimes referred to as 'super intelligence'.\"</p> <p>Kamala Harris (Former US Vice President)</p> <p>\"[...] just as AI has the potential to do profound good, it also has the potential to cause profound harm. From AI-enabled cyberattacks at a scale beyond anything we have seen before to AI-formulated bio-weapons that could endanger the lives of millions, these threats are often referred to as the \"existential threats of AI\" because, of course, they could endanger the very existence of humanity. These threats, without question, are profound, and they demand global action.\"</p> <p>Zhang Jun (China's UN Ambassador)</p> <p>\"The potential impact of AI might exceed human cognitive boundaries. To ensure that this technology always benefits humanity, we must regulate the development of AI and prevent this technology from turning into a runaway wild horse [...] We need to strengthen the detection and evaluation of the entire lifecycle of AI, ensuring that mankind has the ability to press the pause button at critical moments.\"</p> <p>Donald Trump (Former US President)</p> <p>\"[...] the other thing that I think is maybe the most dangerous thing out there of anything, because there's no real solution \u2014 the AI, as they call it.\"</p> <p>Joe Biden (US President)</p> <p>\"Artificial Intelligence is one of the most powerful tools of our time, but to seize its opportunities, we must first mitigate its risks. [...] Social media has shown us the harm that powerful technology can do without the right safeguards in place [...] we must be clear-eyed and vigilant about the threats emerging \u2014 of emerging technologies that can pose \u2014 don't have to, but can pose \u2014 to our democracy and our values.\"</p> <p>Vladimir Putin (President of Russia)</p> <p>\"Artificial intelligence is the future, not only for Russia, but for all humankind. It comes with colossal opportunities, but also threats that are difficult to predict. Whoever becomes the leader in this sphere will become the ruler of the world [...] If we become leaders in this area, we will share this know-how with [the] entire world, the same way we share our nuclear technologies today.\"</p> <p>Li Qiang (China's Head of Government)</p> <p>\"AI must be guided in a direction that is conducive to the progress of humanity. So there should be a red line in AI development, a red line that must not be crossed [...] It should not just benefit only a small group of people, but benefit the overwhelming majority of mankind [...] It is essential that we work together and coordinate with each other.\"</p> <p>Ursula von der Leyen (Head of EU Executive Branch)</p> <p>\"[We] should not underestimate the real threats coming from AI [...] It is moving faster than even its developers anticipated [...] We have a narrowing window of opportunity to guide this technology responsibly.\"</p> <p>Ant\u00f3nio Guterres (UN Secretary-General)</p> <p>\"AI poses a long-term global risk. Even its own designers have no idea where their breakthrough may lead. I urge [the UN Security Council] to approach this technology with a sense of urgency [...] Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"</p>"},{"location":"chapters/01/A1/#02-03","title":"1.A1.2.3 Academics","text":"<p>Eliezer Yudkowsky (Co-Founder of Machine Intelligence Research Institute)</p> <p>\"I do not expect something actually smart to attack us with marching robot armies with glowing red eyes where there could be a fun movie about us fighting them. I expect an actually smarter and uncaring entity will figure out strategies and technologies that can kill us quickly and reliably and then kill us.\"</p> <p>Stephen Hawking (Theoretical Physicist)</p> <p>\"The development of full artificial intelligence could spell the end of the human race [...] It would take off on its own, and re-design itself at an ever increasing rate.\"</p> <p>Alan Turing (Father of Computer Science and AI)</p> <p>\"It seems probable that once the machine thinking method had started, it would not take long to outstrip our feeble powers\u2026 They would be able to converse with each other to sharpen their wits. At some stage therefore, we should have to expect the machines to take control.\"</p> <p>I. J. Good (Cryptologist at Bletchley Park)</p> <p>\"An ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.\"</p>"},{"location":"chapters/01/A1/#02-04","title":"1.A1.2.4 Tech Entrepreneurs","text":"<p>Elon Musk (Founder/Co-Founder of OpenAI, Neuralink, SpaceX, xAI, PayPal, CEO of Tesla, CTO of X/Twitter)</p> <p>\"AI is a rare case where I think we need to be proactive in regulation than be reactive [...] I think that [digital super intelligence] is the single biggest existential crisis that we face and the most pressing one. It needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely [...] And mark my words, AI is far more dangerous than nukes. Far. So why do we have no regulatory oversight? This is insane.\"</p> <p>Bill Gates (Co-Founder of Microsoft)</p> <p>\"Superintelligent AIs are in our future. [...] There's the possibility that AIs will run out of control. [Possibly,] a machine could decide that humans are a threat, conclude that its interests are different from ours, or simply stop caring about us.\"</p>"},{"location":"chapters/01/A1/#02-05","title":"1.A1.2.5 Join Declarations","text":"<p>The Bletchley Declaration (Multiple Nations &amp; EU, 2023)</p> <p>\"Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.\"</p> <p>AI Risk Statement (Multiple AI Experts, 2023)</p> <p>\"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"</p>"},{"location":"chapters/01/A1/#03","title":"1.A1.3 Prediction Markets","text":"<p>Prediction markets are like betting systems where people can buy and sell shares based on their predictions of future events. For example, if there\u2019s a prediction market for a presidential election, you can buy shares for the candidate you think will win. If many people believe Candidate A will win, the price of shares for Candidate A goes up, indicating a higher probability of winning.</p> <p>These markets are helpful because they gather the knowledge and opinions of many people, often leading to accurate predictions. For example, a company might use a prediction market to forecast whether a new product will succeed. Employees can buy shares if they believe the product will do well. If the majority think it will succeed, the share price goes up, giving the company a good indication of the product\u2019s potential success.</p> <p>By allowing participants to profit from accurate predictions, these markets encourage the sharing of valuable information and provide real-time updates on the likelihood of various outcomes. The argument is that either prediction markets are more accurate than experts, or experts should be able to make a lot of money from these markets and, in doing so, correct the markets. So the incentive for profit leads to the most accurate predictions. Examples of prediction markets include manifold, or metaculus.</p> <p>When using prediction markets to estimate the reproducibility of scientific research it was found that they outperformed expert surveys (Dreber et al., 2015). So if a lot of experts participate, prediction markets might be one of our best probabilistic forecasting tools, better even than surveys or experts.</p> <p>The live charts below show the results of the prediction markets from Metaculus for - \"When will the first weakly general AI system be devised, tested, and publicly announced?\" At the time of writing, weakly general systems are expected in 2027, and general systems in 2032.</p>"},{"location":"chapters/01/A2/","title":"1.A2 Discussion on LLMs","text":"Reading Time 21 min <p>Everything in the appendices is optional, and is meant as additional knowledge and context. You do not need to read this to understand the core points made in either this chapter or future chapters.</p> <p>Current LLMs, although trained on abundant data, are still far from perfect.</p> <p>Will these problems persist in future iterations, or will they disappear? This section examines the main criticisms of those models and tries to determine if they are valid even for future LLMs.</p> <p>This kind of qualitative assessment is important to know whether LLMs represent the most likely route to AGI or not.</p>"},{"location":"chapters/01/A2/#01","title":"1.A2.1 Empirically insufficiency?","text":"<p>Can LLMs be c reative ? The creativity of LLMs is often debated, but there are clear indications that AI, in principle, is capable of creative processes in various ways:</p> <ul> <li> <p>Autonomous Scientific Research : Recent advancements have shown that LLMs can indeed make novel discoveries. For example, a study by DeepMind demonstrated that an LLM \"discovered new solutions for the cap set problem, a long-standing open problem in mathematics\" (DeepMind, 2023) which was a favorite open problem of Terence Tao. This indicates that AI can not only understand existing knowledge but also contribute new insights in complex fields like mathematics.</p> </li> <li> <p>Autonomous Discovery : AI has the capability to rediscover human strategies and openings independently. AlphaGo, for example, rediscovered human Go strategies and openings through self-play (McGrath et al., 2021), without any human data input. This demonstrates an AI's ability to independently learn and innovate within established domains.</p> </li> <li> <p>Creative Optimization : AI can optimize in surprisingly creative ways. The phenomena of specification gaming, where AI finds unintended solutions to problems, illustrate this. Although this unpredictability poses its challenges, it also shows that AI systems can come up with novel, creative solutions that might not be immediately obvious or intuitive to human problem solvers. DeepMind's blog post on Specification Gaming illustrates this point vividly. (Krakovna et al., 2020)</p> </li> </ul> <p>Aren\u2019t LLMs just too slow at learning things? Arguments against transformer based language models often state that they are too sample inefficient, and that LLMs are extremely slow to learn new concepts when compared to humans. To increase performance in new tasks or situations, it\u2019s often argued that LLMs require training on vast amounts of data \u2014 millions of times more than a human would need. However, there's a growing trend towards data efficiency, and an increasing belief that this can be significantly improved in future models.</p> <p>EfficientZero is a reinforcement learning agent that surpasses median human performance on a set of 26 Atari games after just two hours of real-time experience per game. (Ye et al., 2021; Wang et al., 2024) This is a considerable improvement over previous algorithms, showcasing the potential leaps in data efficiency. The promise here is not just more efficient learning but also the potential for rapid adaptation and proficiency in new tasks, akin to a child's learning speed. EfficientZero is not an LLM, but it shows that deep learning can sometimes be made efficient.</p> <p>Scaling laws indicate that larger AIs tend to be more data efficient, requiring less data to reach the same level of performance as their smaller counterparts. Papers such as \"Language Models are Few-Shot Learners\" (Brown et al., 2020) and the evidence that larger models seem to take less data to reach the same level of performance (Kaplan et al., 2020), suggest that as models scale, they become more proficient with fewer examples. This trend points towards a future where AI might be able to rapidly adapt and learn from limited data, challenging the notion that AIs are inherently slow learners compared to humans.</p> <p>Are LLMs robust to distributional shifts? While it is true that AI has not yet achieved maximal robustness, for example being able to perform perfectly after a change in distribution, there has been considerable progress:</p> <ul> <li> <p>Robustness correlates with capabilities : Robustness is closely linked to the capabilities of AI models when AIs are trained on difficult tasks. For instance, there is a significant improvement in robustness and transfer learning from GPT-2 to GPT-4. In computer vision, recent models like Segment Anything (Kirillov et al., 2023) are far more robust and capable of transfer learning than their less capable predecessors. This progression isn't due to any mysterious factors but rather a result of scaling and improving upon existing architectures.</p> </li> <li> <p>Robustness is a continuum, and perfect robustness may be not necessary: Robustness in AI should not be viewed as a binary concept, but rather as existing on a continuum. This continuum is evident in the way AI models, like those in image classification, often surpass human performance in both capability and robustness (Korzekwa, 2022). However, it's important to recognize that no system is completely immune to challenges such as adversarial attacks. This is exemplified by advanced AIs like Katago in Go, which, despite being vulnerable to such attacks (Wang et al., 2022), still achieves a superhuman level of play. However, the quest for perfect robustness may not be essential to create capable transformative AI, as even systems with certain vulnerabilities can achieve superhuman levels of competence. However, while robustness may not be necessary to create capable AI, the creation of safe, aligned AI will have to solve the problem of misgeneralizing goals.</p> </li> </ul>"},{"location":"chapters/01/A2/#02","title":"1.A2.2 Shallow Understanding?","text":"<p>Stochastic Parrots: Do AIs only memorize information without truly compressing it? !!! quote \"Fran\u00e7ois Chollet (Prominent AI Researcher) (Chollet, 2023)\"</p> <pre><code>Unfortunately, too few people understand the distinction between memorization and understanding. It's not some lofty question like \"does the system have an internal world model?\", it's a very pragmatic behavior distinction: \"is the system capable of broad generalization, or is it limited to local generalization?\n</code></pre> <p>There are two archetypal ways to represent information in an LLM: either memorize point by point, like a look-up table, or compress the information by only memorizing higher-level features, which we can then call \"the world model\". This is explained in the very important paper \"Superposition, Memorization, and Double Descent\" (Anthropic, 2023): it turns out that to store points, initially the model learns the position of all the points (pure memorization), then, if we increase the number of points, the model starts to compress this knowledge, and the model is now capable of generalization (and implements a simple model of the data).</p> Figure 1.43: From Superposition, Memorization, and Double Descent (Anthropic, 2023) <p>AI is capable of compressing information, often in a relevant manner. For example, when examining the representations of words representing colors in LLMs like \"red\" and \"blue\", the structure formed by all the embeddings of those colors creates the correct color circle (This uses a nonlinear projection such as a T-distributed stochastic neighbor embedding (T-SNE) to project from high-dimensional space to the 2D plane). Other examples of world models are presented in a paper called \"Eight Things to Know about Large Language Models\" (Bowman, 2023).</p> <p>Of course, there are other domains where AI resembles more of a look-up table, but it is a spectrum, and each case should be examined individually. For example, for \"factual association,\" the paper \"Locating and Editing Factual Associations in GPT\" shows that the underlying data structure for GPT-2 is more of a look-up table (Meng et al., 2023), but the paper \"Emergent Linear Representations in World Models of Self-Supervised Sequence Models\" demonstrates that a small GPT is capable of learning a compressed world model of OthelloGpt. (Nanda et al., 2023) There are more examples in the section dedicated to world models in the paper \"Eight Things to Know about Large Language Models\" (Bowman, 2023).</p> <p>It\u2019s clear that LLMs are compressing their representations at least a bit. Many examples of impressive capabilities are presented in the work \"The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs\", which shows that it cannot be purely a memorization. (Feuillade-Montixi &amp; Peign\u00e9, 2023)</p> <p>Will LLMs Inevitably Hallucinate? LLMs are prone to \"hallucinate,\" a term used to describe the generation of content that is nonsensical or factually incorrect in response to certain prompts. This issue, highlighted in studies such as \"On Faithfulness and Factuality in Abstractive Summarization\" by Maynez et al. (Maynez et al., 2020) and \"TruthfulQA: Measuring How Models Mimic Human Falsehoods\" by Lin et al. (Lin et al., 2022), poses a significant challenge. However, it's important to see that these challenges are anticipated due to the training setup and can be mitigated:</p> <ul> <li> <p>Inherent Bias in Source Texts : One of the fundamental reasons LLMs may produce untrue content is training data, which may not always be entirely factual or unbiased. In essence, LLMs are reflecting the diverse and sometimes contradictory nature of their training data. In this context, LLMs are constantly 'hallucinating', but occasionally, these hallucinations align with our perception of reality.</p> </li> <li> <p>Strategies to Enhance Factual Accuracy : The tendency of LLMs to generate hallucinations can be significantly diminished using various techniques. See the box below for a breakdown of those.</p> </li> <li> <p>Larger models can be more truthful than smaller ones. This is the case with TruthfulQA. OpenAI reports that GPT-4 is 40% more accurate and factually consistent than its predecessor.</p> </li> </ul> Many techniques can be used to increase the truthfulness of LLMs <ul> <li> <p>Fine-tuning LLMs for Factuality : In this paper (Tian et al., 2023), the authors recommend fine-tuning methods using Direct Preference Optimization (DPO) to decrease the rate of hallucinations. By applying such techniques, a 7B Llama 2 model saw a 58% reduction in factual error rate compared to its original model.</p> </li> <li> <p>Retrieval Augmented Generation (RAG) . This method works by incorporating a process of looking up real-world information (retrieval, like a Google search) and then using that information to guide the AI's responses (generation, based on the document retrieved). By doing so, the AI is better anchored in factual reality, reducing the chances of producing unrealistic or incorrect content. Essentially, it's like giving the AI a reference library to check facts against while it learns and responds, ensuring its output is more grounded in reality. This approach is particularly useful in the context of in-context learning, where the AI learns from the information and context provided in each interaction.</p> </li> <li> <p>Prompting techniques in AI have evolved to include sophisticated methods like</p> </li> </ul> <p> <ul> <li> <p>Consistency checks (Fluri et al., 2023), that involve comparing the output from multiple instances of the model on the same prompt, identifying and resolving any disagreements in the responses. This method enhances the accuracy and credibility of the information provided. For example, if different iterations of the model produce conflicting answers, this discrepancy can be used to refine and improve the model's understanding.</p> </li> <li> <p>Reflexion. The Reflexion technique (\"Reflexion: Language Agents with Verbal Reinforcement Learning\"): It\u2019s possible to simply ask the LLM to take a step back, to question whether what it has done is correct or not, and to consider ways to improve the previous answer, and this enhances a lot the capabilities of GPT-4. This technique is emergent and does not work well with previous models. (Shinn et al., 2023).</p> </li> <li> <p>Verification chains , like selection inference (Creswell et al., 2022). Chain-of-Thought has access to the whole context, so each reasoning step is not necessarily causally connected to the last. But selection inference enforces a structure where each reasoning step necessarily follows from the last, and therefore the whole reasoning chain is causal. This process involves the AI model examining its own reasoning or the steps it took to arrive at a conclusion. By doing so, it can verify the logic and consistency of its responses, ensuring they are well-founded and trustworthy.</p> </li> <li> <p>Allowing the AI to express degrees of confidence in its answers, acknowledging uncertainty when appropriate. For example, instead of a definitive \"Yes\" or \"No,\" the model might respond with \"I am not sure,\" reflecting a more nuanced understanding akin to human reasoning. This approach is evident in advanced models like Gopher (Rae et al., 2022), which contrasts with earlier models such as WebGPT which may not exhibit the same level of nuanced responses.</p> </li> </ul> <ul> <li> <p>Process-based training ensures that the systems are accustomed to detailing their thoughts in much greater detail and not being able to skip too many reasoning steps. For example, see OpenAI\u2019s Improving Mathematical Reasoning with process supervision (Lightman et al., 2023).</p> </li> <li> <p>Training for metacognition : Models can be trained to give the probability of what they assert, a form of metacognition. For example, the paper \"Language Models (Mostly) Know What They Know\" (Kadavath et al., 2022) demonstrates that AIs can be Bayesian calibrated about their knowledge. This implies that they can have a rudimentary form of self-awareness, recognizing the likelihood of their own accuracy. Informally, this means it is possible to query a chatbot with \"Are you sure about what you are telling me?\" and receive a relatively reliable response. This can serve as training against hallucinations.</p> </li> </ul> <p>It's worth noting that these techniques enable substantial problem mitigation for the current LLMs, but they don\u2019t solve all the problems that we encounter with AI that are potentially deceptive, as we will see in the chapter on goal misgeneralization.</p> <p></p>"},{"location":"chapters/01/A2/#03","title":"1.A2.3 Structural inadequacy?","text":"<p>Are LLMs m issing System 2? System 1 and System 2 are terms popularized by economist Daniel Kahneman in his book \"Thinking, Fast and Slow,\" describing the two different ways our brains form thoughts and make decisions. System 1 is fast, automatic, and intuitive; it's the part of our thinking that handles everyday decisions and judgments without much effort or conscious deliberation. For example, when you recognize a face or understand simple sentences, you're typically using System 1. On the other hand, System 2 is slower, more deliberative, and more logical. It takes over when you're solving a complex problem, making a conscious choice, or focusing on a difficult task. It requires more energy and is more controlled, handling tasks such as planning for the future, checking the validity of a complex argument, or any activity that requires deep focus. Together, these systems interact and influence how we think, make judgments, and decide, highlighting the complexity of human thought and behavior.</p> <p>A key concern is whether LLMs are able to emulate System 2 processes, which involve slower, more deliberate, and logical thinking. Some theoretical arguments about the depth limit in transformers show that they are provably incapable of internally dividing large integers (Del\u00e9tang et al., 2023). However, this is not what we observe in practice: GPT-4 is capable of detailing some calculations step-by-step and obtaining the expected result through a chain of thought or via the usage of tools like a code interpreter.</p> <p>Emerging Metacognition . Emerging functions in LLMs, like the Reflexion technique (Shinn et al., 2023), allow these models to retrospectively analyze and improve their answers. It is possible to ask the LLM to take a step back, question the correctness of its previous actions, and consider ways to improve the previous answer. This greatly enhances the capabilities of GPT-4, enhancing its capabilities and aligning them more closely with human System 2 operations. Note that this technique is emergent and does not work well with previous models.</p> <p>These results suggest a blurring of the lines between these two systems. System 2 processes may be essentially an assembly of multiple System 1 processes, appearing slower due to involving more steps and interactions with slower forms of memory. This perspective is paralleled in how language models operate, with each step in a System 1 process akin to a constant time execution step in models like GPT. Although these models struggle with intentionally orchestrating these steps to solve complex problems, breaking down tasks into smaller steps (Least-to-most prompting) or prompting them for incremental reasoning (Chain-of-Thought (CoT) prompting) significantly improves their performance.</p> <p>Are LLMs m issing an internal w orld m odel? The notion of a \"world model\" in AI need not be confined to explicit encoding within an architecture. Contrary to approaches like H-JEPA (LeCun, 2022), which advocate for an explicit world model to enhance AI training, there's growing evidence that a world model can be effectively implicit. This concept is particularly evident in reinforcement learning (RL), where the distinction between model-based and model-free RL can be somewhat misleading. Even in model-free RL, algorithms often implicitly encode a form of a world model that is crucial for optimal performance.</p> <ul> <li> <p>Time and geographical coordinates: Research on Llama-2 models reveals how these models can represent spatial and temporal information (Gurney &amp; Tegmark, 2024). LLMs like Llama-2 models encode approximate real-world coordinates and historical timelines of cities. Key findings include the gradual emergence of geographical representations across model layers, the linearity of these representations, and the models' robustness to different prompts. Significantly, the study shows that the models are not just passively processing this information but actively learning the global geometry of space and time.</p> </li> <li> <p>Board representation: In the paper \"Emergent Linear Representations in World Models of Self-Supervised Sequence Models\" (Nanda et al., 2023), the author presents significant findings on the nature of representations in AI models. The paper delves into how the Othello-GPT model, trained to predict legal moves in the game of Othello, develops an emergent world representation of the game board! Contrary to previous beliefs that this representation was non-linear, he demonstrates that it is, in fact, linear. He discovers that the model represents board states not in terms of black or white pieces, but as \"my color\" or \"their color,\" aligning with the model's perspective of playing both sides. This work sheds light on the potential of AI models to develop complex, yet linear, world representations through simple objectives like next-token prediction.</p> </li> <li> <p>Other examples are presented in the paper: \"Eight Things to know about LLMs\". (Bowman, 2023)</p> </li> </ul> <p>Can LLMs learn continuously, and have long term memory? Continual learning and the effective management of long-term memory represent significant challenges in the field of AI in general.</p> <p>Catastrophic Forgetting . A crucial obstacle in this area is catastrophic forgetting, a phenomenon where a neural network, upon learning new information, tends to entirely forget previously learned information. This issue is an important focus of ongoing research, aiming to develop AI systems that can retain and build upon their knowledge over time. For example, suppose we train an AI on an Atari game. At the end of the second training, the AI has most likely forgotten how to play the first game. This is an example of catastrophic forgetting.</p> <p>But now suppose we train a large AI on many ATARI games, simultaneously, and even add some Internet text and some robotic tasks. This can just work. For example, the AI GATO illustrates this training process and exemplifies what we call the blessing of scale , which is that what is impossible in small regimes can become possible in large regimes.</p> <p>Other techniques are being developed to solve long-term memory, for example, Scaffolding-based approaches have also been employed for achieving long-term memory and continual learning in AI. Scaffolding in AI refers to the use of hard-coded wrappers explicitly programmed structures by humans that involve a for loop to query continuously the model:</p> <ul> <li> <p>LangChain addresses these challenges by creating extensive memory banks. LangChain is a Python library that allows LLM to retrieve and utilize information from large datasets, essentially providing a way for AI to access a vast repository of knowledge and use this information to construct more informed responses. However, this approach may not be the most elegant due to its reliance on external data sources and complex retrieval mechanisms. A potentially more seamless and integrated solution could involve utilizing the neural network's weights as dynamic memory, constantly evolving and updating based on the tasks performed by the network.</p> </li> <li> <p>Voyager: A remarkable example of a scaffolding-based long-term memory is the AI Voyager, an AI system developed under the \"AutoGPT\" paradigm. This system is notable for its ability to engage in continuous learning within a 3D game environment like Minecraft. In a single game session, AI Voyager demonstrates the capacity to learn basic controls, achieve initial goals such as resource acquisition, and eventually advance to more complex behaviors, including combat with enemies and crafting tools for gathering sophisticated resources. This demonstrates a significant stride in LLM's ability to learn continually and manage long-term memory within dynamic environments.</p> </li> </ul> <p>It should be noted that scaffold-based long-term memory is not considered an elegant solution, and purists would prefer to use the system's own weights as long-term memory.</p> <p>Planning Planning is an area that AIs currently struggle with, but there is significant progress. Some paradigms, such as those based on scaffolding, enable task decomposition and breaking down objectives into smaller, more achievable sub-objectives.</p> <p>Furthermore, the paper \"Voyager: An Open-Ended Embodied Agent with Large Language Models\" demonstrates that it is possible to use GPT-4 for planning in Natural language in Minecraft. (Wang et al., 2023)</p>"},{"location":"chapters/01/A2/#04","title":"1.A2.4 Differences with the brain","text":"<p>It appears that there are several points of convergence between the LLMs and the linguistic cortex:</p> <ul> <li> <p>Behavioral similarities. From (Canell, 2022), it's highlighted that LLMs show a close comparison to human linguistic abilities and the linguistic cortex. These models have excelled in mastering syntax and a significant portion of semantics in human language. Of course, today, they still lag in aspects such as long-term memory, coherence, and general reasoning - faculties that in humans depend on various brain regions like the hippocampus and prefrontal cortex, but we explained in the last sections that those problems may be solvable.</p> </li> <li> <p>Convergence in internal Representations : LLMs have a representation that converges with scale toward the brain representation. This is supported by the study, \"Brains and algorithms partially converge in natural language processing.\" (Caucheteux &amp; King, 2022) Additional insights can be found in the works \"The Brain as a Universal Learning Machine\" (Canell, 2015) and \"Brain Efficiency: Much More than You Wanted to Know.\" (Canell, 2022) At comparable learning stages, LLMs and the linguistic cortex develop similar or equivalent feature representations. In some evaluations, advanced LLMs have been able to predict 100% of the explainable neural variance, as detailed by Schrimpf, Martin, et al. in \"The neural architecture of language: Integrative modeling converges on predictive processing.\" (Schrimpf et al., 2021)</p> </li> <li> <p>Scale is also important in primates. The principal architectural difference between human and other primate brains seems to be the number of neurons rather than anything else, as demonstrated in various studies. (Houzel, 2012; Pearson et al., 2023; Charvet, 2021).</p> </li> </ul>"},{"location":"chapters/01/A2/#05","title":"1.A2.5 Further reasons to continue scaling LLMs","text":"<p>Following are some reasons to believe that labs will continue to scale LLMs.</p> <p>Scaling Laws on LLM implies further qualitative improvements. The scaling laws might not initially appear impressive. However, linking these quantitative measures can translate to a qualitative improvement in algorithm quality. An algorithm that achieves near-perfect loss, though, is one that necessarily comprehends all subtleties, and displays enormous adaptability. The fact that the scaling laws are not bending is very significant and means that we can make the model a qualitatively better reasoner.</p> <p>From simple correlations to understanding. During a training run, GPTs go from basic correlations to deeper and deeper understanding. Initially, the model merely establishes connections between successive words. Gradually, it develops an understanding of grammar and semantics, creating links between sentences and subsequently between paragraphs. Eventually, GPT masters the nuances of writing style<sup>1</sup>.</p> Exercise: Scaling Laws on LLM implies further qualitative improvements. <p>Let's calculate the difference in loss, measured in bits, between two model outputs: \"Janelle ate some ice cream because he likes sweet things like ice cream.\" and \"Janelle ate some ice cream because she likes sweet things like ice cream.\" The sentence contains approximately twenty tokens. If the model vacillates between \"He\" or \"She,\" choosing randomly (50/50 odds), it incurs a loss of 2 bits on the pronoun token when incorrect. The loss for other tokens remains the same in both models. However, since the model is only incorrect half the time, a factor of 1/2 should be applied. This results in a difference of (1/2) * (2/20) = 1/20, or 0.05 bits. Thus, a model within 0.05 bits of the minimal theoretical loss should be capable of understanding even more nuanced concepts than the one discussed above.</p> <p>Text completion is probably an AI-complete test (Wikipedia, 2022).</p> <p>Current LLMs have only as many parameters as small mammals have synapses, no wonder they are still imperfect. Models like GPT-4, though very big compared to other models, should be noted for their relatively modest scale compared to the size of a human brain. To illustrate, the largest GPT-3 model has a similar number of parameters to the synapses of a hedgehog. We don't really know how many parameters GPT-4 has, but if it is the same size as PALM, which has 512 B parameters, then GPT-4 has only as many parameters as a chinchilla has synapses. In contrast, the human neocortex contains about 140 trillion synapses, which is over 200 times more synapses than a chinchilla. For a more in-depth discussion on this comparison, see the related discussion here. For a discussion of the number of parameters necessary to emulate a synapse, see the discussion on biological anchors.</p> <p>GPT-4 is still orders of magnitude cheaper than other big science projects. : Despite the high costs associated with training large models, the significant leaps in AI capabilities provided by scaling justify these costs. For example, GPT-4 is expensive compared to other ML models. It is said to cost 50M in training. But the Manhattan Project cost 25B, which is 500 times more without accounting for inflation, and achieving Human-level intelligence, may be more economically important than achieving the nuclear bomb.</p> <p>Collectively, these points support the idea that AGI can be achieved by only scaling current algorithms.</p> <ol> <li> <p>See also \"The Scaling Hypothesis,\" to delve into this progression in a fascinating story.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/01/A3/","title":"1.A3 Forecasting - Trends","text":"Reading Time 10 min <p>Everything in the appendices is optional, and is meant as additional knowledge and context. You do not need to read this to understand the core points made in either this chapter or future chapters.</p> <p>Generally, the three main components recognized as the main variables of advancement in deep learning are: computational power available, algorithmic improvements, and the availability of data. These three variables are also sometimes called the inputs to the AI production function, or the AI triad. (Buchanan, 2022)</p> <p>We can anticipate that models will continue to scale in the near future. Increased scale combined with the increasingly general-purpose nature of foundation models could potentially lead to a sustained growth in general-purpose AI capabilities.</p> Figure 1.44: Key trends and figures in Machine Learning (Epoch, 2023)"},{"location":"chapters/01/A3/#01","title":"1.A3.1 Compute","text":"<p>The first thing to look at is the trends in the overall amount of training compute required when we train our model. Training compute grew by 1.58 times/year up until the Deep Learning revolution around 2010, after which growth rates increased to 4.2 times/year. We also find a new trend of \"large-scale\" models that emerged in 2016, trained with 2-3 OOMs (Order Of Magnitude) more compute than other systems in the same period.</p> <p>Hardware advancements are paralleling these trends in training compute and data. GPUs are seeing a yearly 1.35 times increase in floating-point operations per second (FLOP/s). However, memory constraints are emerging as potential bottlenecks, with DRAM capacity and bandwidth improving at a slower rate. Investment trends reflect these technological advancements</p> <p>In 2010, before the deep learning revolution, DeepMind co-founder Shane Legg predicted human-level AI by 2028 using compute-based estimates (Legg, 2010). OpenAI co-founder Ilya Sutskever, whose AlexNet paper sparked the deep learning revolution, was also an early proponent of the idea that scaling up deep learning would be transformative.</p> Figure 1.45: Key trends and figures in Machine Learning (Epoch AI, 2023)"},{"location":"chapters/01/A3/#02","title":"1.A3.2 Parameters","text":"<p>In this section, let's look at the trends in model parameters. The following graph shows how even though parameter counts have always been increasing, in the new 2018+ era, we have really entered a different phase of growth. Overall, between the 1950s and 2018, models have grown at a rate of 0.1 orders of magnitude per year (OOM/year). This means that in the 68 years between 1950 and 2018 models grew by a total of 7 orders of magnitude. However, post-2018, in just the last 5 years models have increased by yet another 4 orders of magnitude (not accounting for however many parameters GPT-4 has because we don't know).</p> <p>The following table and graph illustrate the trend change in machine learning models' parameter growth. Note the increase to half a trillion parameters with constant training data.</p> Figure 1.46: Machine Learning Model Sizes and the Parameter Gap (Villalobos et al., 2022)"},{"location":"chapters/01/A3/#03","title":"1.A3.3 Data","text":"<p>We are using ever-increasing amounts of data to train our models. The paradigm of training foundation models to fine-tune later is accelerating this trend. If we want a generalist base model then we need to provide it with \u2018general data\u2019, which means: all the data we can get our hands on. You have probably heard that models like ChatGPT and PaLM are trained on data from the internet. The internet is the biggest repository of data that humans have. Additionally, as we observed from the Chinchilla papers scaling laws, it is possible that data to train our models is the actual bottleneck, and not compute or parameter count. So the natural question is how much data is left on the internet for us to keep training our models? and how much more data do we humans generate every year?</p> <p>How much data do we generate? The total amount of data generated every single day in 2019 was on the order of ~463EB (World Economic Forum, 2019). But in this post, we will assume that models are not training on \u2018all the data generated\u2019 (yet), rather they will continue to only train on open-source internet text and image data. The available stock of text and image data grew by 0.14 OOM/year between 1990 and 2018 but has since slowed to 0.03 OOM/year.</p> <p>How much data is left? The median projection for when the training dataset of notable ML models exhausts the stock of professionally edited texts on the internet is 2024. The median projection for the year in which ML models use up all the text on the internet is 2040. Overall, projections by Epochai predict that we will have exhausted high-quality language data before 2026, low-quality language data somewhere between 2030 and 2050, and vision data between 2030 and 2060. This might be an indicator of slowing down ML progress after the next couple of decades. These conclusions from Epochai, like all the other conclusions in this entire leveraging computation section, rely on the unrealistic assumptions that current trends in ML data usage and production will continue and that there will be no major innovations in data efficiency, i.e. we are assuming that the amount of capabilities gained per training datapoint will not change from current standards.</p> Figure 1.47: ML data consumption and data production trends for low-quality text, high-quality text, and images. (Epoch AI, 2023) <p>Even if we run out of Data, many solutions are proposed, from using synthetic data, for example, filtering and preprocessing the data with GPT-3.5 to create a new cleaner dataset, an approach used in the paper \"Textbooks are all you need\" with models like Phi 1.5B that demonstrate excellent performance for their size through the use of high-quality filtered data, to the use of more efficient trainings, or being more efficient by training on more epochs.</p>"},{"location":"chapters/01/A3/#04","title":"1.A3.4 Algorithms","text":"<p>Algorithmic advancements also play a role. For example, between 2012 and 2021, the computational power required to match the performance of AlexNet has been reduced by a factor of 40, which corresponds to a threefold yearly reduction in the compute required for achieving the same performance on image classification tasks like ImageNet. Improving the architecture also counts as algorithmic advancement. A particularly influential architecture is that of Transformers, central to many recent innovations, especially in chatbots and autoregressive learning. Their ability to be trained in parallel over every token of the context window fully exploits the power of modern GPUs, and this is thought to be one of the main reasons why they work so well compared to their predecessor, even if this point is controversial.</p> Does algorithmic architecture really matter? <p>This is a complicated question, but some evidence suggests that once an architecture is expressive and scalable enough, the architecture matters less than we might have thought:</p> <p>In a paper titled \u2018ConvNets Match Vision Transformers at Scale,' Google researchers found that Visual Transformers (ViT) can achieve the same results as CNNs (Convolutional neural network) simply by using more compute. (Smith et al., 2023) They took a special CNN architecture and trained it on a massive dataset of four billion images. The resulting model matched the accuracy of existing ViT systems that used similar training compute.</p> <p>Variational AutoEncoders (long an also-ran to GANs or autoregressive models in terms of image generation) catch up if you make them very deep (Child, 2021; Vahdat &amp; Kautz, 2021).</p> <p>Progress in late 2023, such as the mamba architecture (Gu &amp; Dao, 2023), appears to be an improvement on the transformer. It can be seen as an algorithmic advancement that reduces the amount of training computation needed to achieve the same performance.</p> <p>The connections and normalizations in the transformer, which were thought to be important, can be taken out if the weights are set up correctly. This can also make the transformer design simpler (Note however that this architecture is slower to converge than the others). (He et al., 2023)</p> <p>On the other side of the argument, certain attention architectures are significantly more scalable when dealing with long context windows, and no feasible amount of training could compensate for this in more basic transformer models. Architectures specifically designed to handle long sequences, like Sparse Transformers (Child et al., 2019) or Longformer (Beltagy et al., 2020), can outperform standard transformers by a considerable margin for this usage. In computer vision, architectures like CNNs are inherently structured to recognize spatial hierarchies in images, making them more efficient for these tasks than architectures not specialized in handling spatial data when the amount of data is limited, and the \"prior\" encoded in the architecture makes the model learn faster.</p>"},{"location":"chapters/01/A3/#05","title":"1.A3.5 Costs","text":"<p>Understanding the dollar cost of ML training runs is crucial for several reasons. Firstly, it reflects the real economic expense of developing machine learning systems, which is essential for forecasting the future of AI development and identifying which actors can afford to pursue large-scale AI projects. Secondly, by combining cost estimates with performance metrics, we can track the efficiency and capabilities of ML systems over time, offering insights into how these systems are improving and where inefficiencies may lie. Lastly, these insights help determine the sustainability of current spending trends and guide future investments in AI research and development, ensuring resources are allocated effectively to foster innovation while managing economic impact.</p> <p>Moore\u2019s Law, which predicts the doubling of transistor density and thus computational power approximately every two years, has historically led to decreased costs of compute power. However, the report finds that spending on ML training has grown much faster than the cost reductions suggested by Moore\u2019s Law. This means that while hardware has become cheaper, the overall expense of training ML systems has escalated due to increasing demand for computational resources. This divergence emphasizes the rapid pace of advancements in ML and the significant investments required to keep up with the escalating computational demands.</p> <p>To measure the cost of ML training runs, the report employs two primary methods. The first method uses historical trends in the price-performance of GPUs to estimate costs. This approach leverages general trends in hardware advancements and cost reductions over time. The second method bases estimates on the specific hardware used to train the ML systems, such as NVIDIA GPUs, providing a more detailed and accurate picture of the costs associated with particular technologies. Both methods involve calculating the hardware cost\u2014the portion of the up-front hardware cost used for training\u2014and the energy cost, which accounts for the electricity required to power the hardware during training. These calculations provide a comprehensive view of the economic burden of training ML models.</p> <p>Measuring the cost of development extends beyond the final training run of an ML system to encompass a range of factors. This includes research and development costs, which cover the expenditures on preliminary experiments and model refinements that lead up to the final product. It also involves personnel costs, including salaries and benefits for researchers, engineers, and support staff. Infrastructure costs, such as investments in data centers, cooling systems, and networking equipment, are also significant. Additionally, software and tools, including licenses and cloud services, contribute to the overall cost. Energy costs throughout the development lifecycle, not just during the final training run, and opportunity costs\u2014potential revenue lost from not pursuing other projects\u2014are also crucial components. Understanding these broader costs provides a more comprehensive view of the economic impact of developing advanced ML systems, informing strategic decisions about resource allocation.</p> <p>The findings suggest that the cost of ML training runs will continue to grow, but the rate of growth might slow down in the future. The report estimates that the cost of ML training has grown by approximately 2.8 times per year for all systems. For large-scale systems, the growth rate is slower, at around 1.6 times per year. This substantial year-on-year increase in training costs highlights the need for significant efficiency improvements in both hardware and training methodologies to manage future expenses effectively.</p> <p>The report forecasts that if current trends continue, the cost for the most expensive training runs could exceed significant economic thresholds, such as 1% of the US GDP, within the next few decades. This implies that without efficiency improvements, the economic burden of developing state-of-the-art ML systems will increase substantially. Consequently, understanding and managing these costs is essential for ensuring the sustainable growth of AI capabilities and maintaining a balanced approach to AI investment and development.</p> Figure 1.48: Estimated cost of compute in US dollars for the final training run of ML systems. (Epoch AI, 2023)"},{"location":"chapters/01/A4/","title":"1.A4 Forecasting - Bio Anchors","text":"Reading Time 5 min <p>Everything in the appendices is optional, and is meant as additional knowledge and context. You do not need to read this to understand the core points made in either this chapter or future chapters.</p>"},{"location":"chapters/01/A4/#01","title":"1.A4.1 Evolution Anchor","text":"<p>This anchor quantifies the computational effort invested by evolution in shaping the human brain. It considers the vast amount of processing and learning that has taken place from the emergence of the first neurons to the development of the modern human brain. This method suggests that evolution has served as a form of \"pre-training\" for the human brain, enhancing its ability to adapt and survive. To estimate the computational power of this evolutionary \"pre-training\", the report considers the total amount of compute used by all animal brains over the course of evolution. This includes not just the brains of humans, but also those of our ancestors and other animals with nervous systems. The idea is that all of this brain activity represents a form of learning or adaptation that has contributed to the development of the modern human brain. While the exact calculations involved in this estimate are complex and subject to considerable uncertainty, the basic idea is to multiply the number of animals that have ever lived by the amount of compute each of their brains performed over their lifetimes. This gives an estimate of the total compute performed by all animal brains over the course of evolution.</p> Figure 1.49: The Evolution anchor (Ho, 2022) <p>Cotra accounts for these considerations and assumes that the \"average ancestor\" performed as many FLOP/s as a nematode, and that there were on average ~1e21 ancestors at any time. This yields a median of ~1e41 FLOP , which seems extraordinarily high compared to modern machine learning. As an example, Google\u2019s PaLM model was trained with ~2.5e24 FLOP (17 orders of magnitude smaller). She gives this anchor a weight of 10%. (Ho, 2022)</p> <p></p>"},{"location":"chapters/01/A4/#02","title":"1.A4.2 Lifetime Anchor","text":"<p>This refers to the total computational activity the human brain performs over a human lifetime. This anchor is essentially a measure of the \"training\" a human brain undergoes from birth to adulthood and incorporates factors such as the number of neurons in the human brain, the amount of computation each neuron performs per year, and the number of years it takes for a human to reach adulthood. The human brain has an estimated 86 billion neurons. Each of these neurons performs a certain number of computations per second, which can be calculated as a certain number of operations per second in FLOP/s. When calculating the total amount of compute over a lifetime, these factors are multiplied together, along with the number of years a human typically lives.</p> <p>For example, if we assume that a neuron is able to perform about 1000 operations per second, and there are about 31.5 million seconds in a year, then a single neuron would perform about 31.5 billion operations in a year. Now, if we multiply this by the estimated number of neurons in the human brain (86 billion), we get an estimate of the total brain-compute performed in one year. We can then multiply this by the number of years in a typical human lifespan to estimate the total brain-compute over a lifetime. Plugging in the numbers about brain FLOP/s seems to suggest that ~1e27 FLOP would be required to reach TAI. This seems low since examples from other technological domains suggest that the efficiency of things we build (on relevant metrics) is generally not great when compared to nature.</p> Figure 1.50: The Lifelong anchor (Ho, 2022) <p>Cotra's report finds a median of ~1e28 FLOP and only gives the lifetime anchor a 5% weight, indicating that it's not the most influential factor in the overall model. The report acknowledges the inherent uncertainties and complexities involved in such a calculation and uses this anchor as one of several to provide a range of estimates for the compute required to achieve AI with human-level performance. (Ho, 2022)</p> <p></p>"},{"location":"chapters/01/A4/#03","title":"1.A4.3 Neural Network Anchors","text":"<p>Each of the neural network anchors serves to provide a different perspective on the amount of compute that might be required to train a TAI. There are three Neural Network Anchors presented in the report: long (~1e37 FLOP), medium (~1e34 FLOP), and short horizon (~1e32 FLOP). These anchors hypothesize that the ratio of parameters to compute used by a TAI should be similar to the ratio observed in today's neural networks. Additionally, a TAI should perform approximately as many FLOPs per subjective second as a human brain. A \"subjective second\" is the time it takes a model to process as much data as a human can in one second (Ho, 2022). As an example a typical human reads about 3-4 words per second for non-technical material, so \"one subjective second\" for a language model would correspond to however much time that the model takes to process about ~3-4 words of data. (Cotra, 2020) Cotra determines the training data requirements based on a mix of machine learning theory and empirical considerations. She puts 15% weight on short horizons, 30% on medium horizons, and 20% on long horizons, for a total of 65% on the three anchors. (Ho, 2022)</p> Figure 1.51: Neural network Anchors (Ho, 2022)"},{"location":"chapters/01/A4/#04","title":"1.A4.4 Genome Anchor","text":"<p>The genome anchor looks at the FLOP/subj sec of the human brain and expects TAI to require as many parameters as there are bytes in the human genome. This hypothesis implicitly assumes a training process that\u2019s structurally analogous to evolution, and that TAI will have some critical cognitive ability that evolution optimized for. This differs from the evolution anchor in that it assumes we can search over possible architectures/algorithms a lot more efficiently than evolution, using gradients. Due to this structural similarity, and because feedback signals about the fitness of a particular genome configuration are generally sparse, this suggests that the anchor only really makes sense with long horizon lengths. (Ho, 2022)</p> Figure 1.52: The Genome Anchor (Ho, 2022) <p>At least at the time of writing (May 2022), machine learning architectures don\u2019t look very much like the human genome, and we are yet to develop TAI \u2013 thus Cotra updates against this hypothesis towards requiring more FLOP. Overall, she finds a median of ~1e33 FLOP and places 10% weight on this anchor. (Ho, 2022)</p>"},{"location":"chapters/02/","title":"Chapter 02 - Risks","text":"Authors                      Markov Grey &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Jeanne Salle, Charles Martinet, Vincent Corruble, Sebastian Gil, Alejandro Acelas, Evander Hammer, Mo Munem, Mateo Rendon, Kieron Kretschmar, Camille Berger          Last Updated 2024-11-20 Reading Time 122 min (core), 11 min (appendix) Also available on Google Docs Watch Listen Download Feedback Facilitate"},{"location":"chapters/02/#introduction","title":"Introduction","text":"Video 2.1: Optional video to get an overview of Risks. <p>The previous chapter explored trends like access to compute, availability of data, scaffolding existing models and improving efficiency of algorithms. According to these trends we can assume that AI capabilities will continue to make progress in the upcoming years. But this still leaves open the question - why are increasing capabilities a problem?</p> <p>Increasing capabilities are a problem, because as AI models get more capable, the scale of the potential risks also rise.</p> <p>The first step is to get an understanding of - What exactly are the concerning scenarios? What are the likelihoods of certain harmful outcomes occurring over others?, and what aspects of current AI development accelerate these risks? In this chapter we aim to tackle these fundamental questions and provide a concrete overview of the various risks in the AI landscape.</p> Figure 2.1: The two-dimensional view of performance x generality. With increasing capabilities, and increasing generality, we also see increasing risks. Depending on the development trajectory and takeoff we might see longer periods with potential catastrophic risks, or suddenly emerging severe existential risks. The curves and colors in this diagram are meant to be illustrative and do not represent any specific forecasted development trajectory. <p>We already have identifiable pathways through which AI can be misused. This misuse can lead to catastrophic outcomes that could profoundly impact society. In addition to misuse, there is the risk that we are approaching a critical threshold where the development of dangerously advanced capabilities, such as uncontrolled self-proliferation and self-replicating AI agents, becomes a tangible reality. These capabilities could lead to scenarios where AI systems rapidly expand and evolve beyond human control, potentially causing widespread disruption and harm. This proximity to such advanced capabilities underscores the immediate need for vigilance and proactive measures. Additionally, the current regulatory landscape is beset by significant gaps, lacking comprehensive regulations governing AI development and deployment. This absence of adequate regulatory frameworks further exacerbates the risks associated with AI.</p> <p>Risk Decomposition . The first section begins by categorizing risks into three main groups: Misuse, Misalignment, and Systemic risks. Misuse risks refer to situations where an individual or group intentionally uses AI for harmful purposes. Misalignment risks arise due to the AI systems themselves, due to inherent problems in AI design such as systems pursuing goals that are not aligned with human values. Systemic risks encompass broader issues that emerge when we consider not just an AI system in isolation but rather as just one variable in a global interaction between incentives in various complex systems such as politics, society, and economics where no single entity is liable. In addition to categorizing what causes the risk, we also distinguish between different scales of risk that an AI system could pose: catastrophic, where harm is caused to a large portion of humanity, and existential, where harm is so severe that it might be impossible for human civilization to recover.</p> <p>The next few sections focus on answering the following questions: What exactly are the risks? What happens and what are we worried about?</p> <p>Risky Capabilities . We begin by exploring specific AI capabilities that pose significant risks. These include the potential of using AI to develop bioweapons and committing cyber offenses, as well as its capacity for deception and manipulation. We also consider the risks associated with AI systems that exhibit agency, autonomous replication, and advanced situational awareness. Understanding these capabilities is crucial for developing targeted risk mitigation strategies.</p> <p>By understanding the nature and scope of these risks, we can develop more effective strategies for mitigating them and ensuring that the development of AI remains beneficial to humanity. The following chapters will build upon this foundation, exploring specific risk, technical solutions, and policy considerations in greater depth.</p>"},{"location":"chapters/02/01/","title":"2.1 Risk Decomposition","text":"Reading Time 8 min <p>Even though AI continues to improve at a rapid pace, our current understanding of AI and potential long-term implications is still incomplete, posing significant challenges in accurately assessing and managing the associated risks.</p> Interactive Figure 2.1: Number of reported AI incidents and Controversies (Giattino et al., 2023)"},{"location":"chapters/02/01/#01","title":"2.1.1 Causes of Risk","text":"<p>To be able to properly understand and set up defenses against the potential risks that AI causes, we need to first categorize them. In this section, we present a taxonomy of AI risk classification based on casual models, i.e. a categorization based on who is responsible for the risk. The main risks we will focus on are the following:</p> <ul> <li> <p>Misuse risk : This includes cases in which the AI system is just a tool, but the goals of the humans augmented by AI cause harm. This includes malicious actors, nation states, corporations, or individuals who are able to leverage advanced capabilities to accelerate risks. Essentially these risks are caused due to the responsibility of some human or groups of humans.</p> </li> <li> <p>Misalignment risk: These risks are caused due inherent problems in the machine learning process or other technical difficulties in AI design. This category also includes risks from multiple AIs interacting and cooperating with each other. These are risks due to unintended behavior caused by AIs independent of human intentions.</p> </li> <li> <p>Systemic risk: These risks deal with disruptions, or feedback loops arising from integrating AI with other complex systems in the world. In this case upstream causes are difficult to pin down since the responsibility for risk is diffuse amongst many actors and interconnected systems. Examples could include AI (or groups of AIs) having an influence on economic, logistic, or political systems. This causes various types of risk as the entire global system of human civilization moves in an unintended direction, despite individual AIs being potentially aligned and responsibly used.</p> </li> </ul> <p>While most AI risks likely fall into one of these three categories, there may be some gray areas that don't neatly fit this taxonomy. For example, an advanced AI system causing harm due to a complex interaction of misaligned objectives (misalignment risk) and integration with global systems in unintended ways (systemic risk). The categories may blur together in some scenarios.</p> <p>Despite this, we think that this general breakdown is a good foundation that captures many key AI risks as currently understood by experts in the field. The next subsections provide more detail into each one of these risk categories individually.</p>"},{"location":"chapters/02/01/#02","title":"2.1.2 Severity of Risk","text":"<p>The previous subsection focused on asking the question - What causes the risk?, but we still have not categorized - How bad are the risks that were caused? In this subsection, we will walk through the potential categorizations of severity of risk posed.</p> <p>Destructive AI risks. In general these refer to scenarios where AI systems cause damage that, while severe, is confined to a specific area or sector and does not spread globally. So these types of risks involve significant but localized harm. Examples include economic disruption, where an AI system manipulates financial markets leading to localized economic crises. Or, scenarios such as an infrastructure attack where we see AI-driven cyber attacks on power grids, transportation systems, or other critical infrastructure in a specific country or region.</p> <p>Risks can be categorized both in terms of the number of people they affect and their spatiotemporal extent. In this subsection - the severity of risk - we try to focus on risks that affect people not just locally, but across the entire globe, and over many generations. These are called - global catastrophic, and existential risks.</p> <p>Global catastrophic and existential threats can be caused due to misuse, misalignment, or systemic factors. That is to say, we can have many combinations like global catastrophic risk caused by misalignment failures, or existential risk caused by systemic failures.</p>"},{"location":"chapters/02/01/#02-01","title":"2.1.2.1 Catastrophic Risks","text":"<p>What are catastrophic risks? Catastrophic risks (or global catastrophic risks) are threats that could bring about severe damage to humanity on a global scale. They are characterized by their potential to affect a significant portion of the world's population, with the rough threshold often considered to be risks that threaten the survival of at least 10% of the global population. These risks are significant not only because of the immediate harm they might cause but also due to their possible long-term repercussions.</p> Figure 2.2: RAND Global Catastrophic Risk Assessment. Placement and size of the ovals in this figure represent a qualitative depiction of the relative relationships among threats and hazards. The figure presents only examples of cases or scenarios described in those chapters, not all scenarios described. (Willis et al., 2024) <p>Trans-Generational AI Risk . These are risks that might affect future generations. These risks involve scenarios where the actions of AI systems today have long-term consequences that will impact people far into the future. (Kilian et al., 2022) Examples include things like environmental destruction, where AI systems that exploit natural resources unsustainably bring about long-term ecological damage. It could also entail genetic manipulation, where AI technologies alter human genetics in ways that could have unknown and potentially harmful effects on future generations.</p> <p>What are examples of catastrophic risks? There have been many instances in history of global catastrophic risks being caused by natural causes. One example is the Black Death, which may have resulted in the deaths of a third of Europe's population, corresponding to 10% of the global population at the time.</p> <p>But as technologies advance there is an increasing threat that we may discover technologies that allow us to cause similar amounts of harm as natural disasters, except due to man-made causes. (Wikipedia) For example, nuclear war was the first man-made global catastrophic risk, as a global war could kill a large percentage of the human population. (Conn, 2015)</p> <p>Similar to biotechnology, AI can be used to greatly improve the lives of people, but if the technology is not developed safely, there is also the chance that someone could accidentally or intentionally unleash an AI system that ultimately causes global risks. (Conn, 2015)</p> <p>The impact of these scenarios can vary widely, depending on the cause and the severity of the event, ranging from temporary economic disruption to the death of millions. We will go into specific scenarios that result in such risks later in the text.</p>"},{"location":"chapters/02/01/#02-02","title":"2.1.2.2 Existential Risks","text":"<p>What are existential risks? Most global catastrophic risks would not be so intense as to kill the majority of life on Earth, but even if one did, the ecosystem and humanity would eventually recover. An existential risk, on the other hand, is one in which humanity would be unable to ever recover its full potential. Existential risks are seen as the most severe class of global catastrophic risk and are often also called x-risks.</p> <p>Definition: Existential Risks (x-risks) (Conn, 2015)</p> <p>An existential risk is any risk that has the potential to eliminate all of humanity or, at the very least, kill large swaths of the global population, leaving the survivors without sufficient means to rebuild society to current standards of living.</p> Figure 2.3: Qualitative risk categories. The scope of risk can be personal (affecting only one person), local (affecting some geographical region or a distinct group), global (affecting the entire human population or a large part thereof), trans-generational (affecting humanity for numerous generations, or pan-generational (affecting humanity overall, or almost all, future generations). The severity of risk can be classified as imperceptible (barely noticeable), endurable (causing significant harm but not completely ruining the quality of life), or crushing (causing death or a permanent and drastic reduction of quality of life). (Bostrom, 2012) <p>In his book \u201cThe Precipice\u201d published in 2020, philosopher Toby Ord provided a breakdown of existential risks. He recognized AI as one of the foremost existential risks facing humanity today, noting that there is a non-negligible probability that the development of advanced AI, or Artificial General Intelligence (AGI), could lead to an existential catastrophe if not properly aligned with human interests and values (Ord, 2020).</p> Figure 2.4: According to Ord, most risks today are anthropogenic. \u201c[These numbers] are not in any way the final word, but are a concise summary of all I know about the risk landscape.\u201d (Toby Ord, 2020). <p>If we face an existential-level catastrophe, we cannot learn or recover from the event, as it would either result in the complete end of humanity or a permanent setback to civilizational progress (Bostom, 2008).<sup>1</sup> This is why x-risks merit a great deal of caution and calls for preventative rather than reactive strategies. Existential risks include scenarios like humans losing control over ASI and going extinct due to misaligned goals, or, ending up in a permanent dystopia because AI enabled a global totalitarian regime where future generations are perpetually oppressed (Hendrycks et al., 2023).</p> <p>We will talk about solutions and risk mitigation strategies in future chapters. For the rest of this chapter, we will dive into the arguments that cause many to think that AI can cause such risks. We will try to give specific scenarios for how these might manifest but please keep in mind that there are a huge number of unknowns and we cannot be exhaustive. For some risks we can only present available empirical evidence and arguments for why they are a theoretical possibility.</p> <ol> <li> <p>Irrecoverable civilizational collapse, where we either go extinct or are never replaced by a subsequent civilization that rebuilds has been argued to be possible, but has an extremely low probability. (Rodriguez, 2020)\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/02/02/","title":"2.2 Risk Amplifiers","text":"Reading Time 18 min <p>This section covers some underlying common factors of both AI systems, as well as the development space surrounding these that serve as accelerating factors to increase risk.</p>"},{"location":"chapters/02/02/#01","title":"2.2.1 Accidents","text":"<p>Often, the whole point of producing a new technology is to produce a positive impact on society. Despite these noble intentions, there is a major category of risk that arises from large well-intentioned projects that unintentionally go wrong. (Critch &amp; Russel, 2023)</p> <p>Flaws are hard to discover . It often takes time to observe all the downstream effects of releasing a technology. There are many examples throughout history of technologies that we built and released into the world only to later discover that they were causing harm. Some historical examples include the use of leaded paints and gasoline causing large populations to suffer from lead poisoning (Kovarik, 2012), the use of CFCs causing a hole in the ozone layer (NASA, 2004), our use of asbestos which is linked to serious health issues, the use of tobacco products, and more recently the widespread use of social media, the excessive use of which is linked to depression and anxiety. (Hendrycks, 2024)</p> <p>Some of these risks are diffuse and emerge only at the societal level, but others are perhaps easier to compare to software-based AI risks:</p> <p>Undetected hole in the ozone layer . The example of the hole in the ozone layer might have occurred due to diffuse responsibility, but it was made worse because it remained undetected for a long period (NASA, 2004). This is because the data analysis software used by NASA in its project to map the ozone layer had been designed to ignore values that deviated greatly from expected measurements.</p> <p>The Mariner 1 Spacecraft . In 1962 the Mariner 1 space probe barely made it out of Cape Canaveral before the rocket veered dangerously off course. Worried that the rocket was heading towards a crash-landing on Earth, NASA engineers issued a self-destruct command and the craft was obliterated about 290 seconds after launch. An investigation revealed the cause to be a very simple software error. A hyphen was omitted in a line of code, which meant that incorrect guidance signals were sent to the spacecraft. (Martin, 2023)</p> <p>There are countless other similar examples. Just like the one missing hyphen in the software for the Mariner spacecraft, we have also seen similar bugs due to one single character being altered in AI systems. OpenAI accidentally inverted the sign on the reward function while training GPT-2. The result was a model which optimized for negative sentiment while still regularizing toward natural language. Over time this caused the model to generate increasingly sexually explicit text, regardless of the starting prompt. In the author's own words \u201cThis bug was remarkable since the result was not gibberish but maximally bad output. The authors were asleep during the training process, so the problem was noticed only once training had finished.\u201d (Ziegler et al., 2020)</p> <p>While this example didn't really cause much harm, except to perhaps the human evaluators who had to spend an entire night reading increasingly reprehensible text, we can easily imagine that extremely small bugs like a single flipped sign on a reward function can cause really bad outcomes if they were to occur in more capable models.</p> <p>The rapid improvement, combined with a lack of understanding and predictability makes it more likely that despite the best intentions we might not be able to prevent accidents. This supports the case for heavily tested slow rollouts of AI systems, as opposed to the \u201cMove fast and break things\u201d ethos that some tech companies might hold.</p> <p>Harmful malfunctions (Jones, 2024). AI systems can make mistakes if applied inappropriately. For example:</p> <ul> <li> <p>A self-driving car in San Francisco collided with a pedestrian that was thrown into its path by a human driver. This was arguably not its fault - however, after initially stopping it then started moving again, dragging the injured pedestrian a further six meters along the road. (The Guardian, 2023) Government investigators alleged that the company initially hid the severity of the collision from them. (The Guardian, 2023)</p> </li> <li> <p>A healthcare chatbot deployed in the UK was heavily criticized when it advised users potentially experiencing a heart attack not to get treatment. When these concerns were raised by a doctor, the company released a statement calling them a \"Twitter troll\". (Lomas, 2020)</p> </li> </ul> <p>Furthermore, use of AI systems can make it harder to detect and address process issues. Outputs of computer systems are likely to be overly trusted. (Wikipedia) Additionally, because most AI models are used as black boxes and AI systems are much more likely to be protected from court scrutiny than human processes, it can be hard to prove mistakes. (Marshall, 2021)</p>"},{"location":"chapters/02/02/#02","title":"2.2.2 Indifference","text":"<p>Risks arising from indifference can be caused when the creators of AI models discover certain problems, but they don't take the moral consequences that might arise on release of the system seriously.</p> <p>Some employees of a company might conduct a risk analysis and conclude that there is a risk that\u2019s bigger than expected or worse than expected. However, if the company stands to profit greatly from its strategy, or other factors such as safety gaming, or race dynamics, the model might be released anyway. It may be very difficult in such situations to motivate a change unless there is outside intervention or a chance of exposure to the companies lack of concern about the moral consequences arising from the release of such a system. (Critch et al., 2023)</p> <p>A potential comparison for such indifference risks, can be seen from the lawsuit that alleges that facebook violated consumer protection law.</p> <p>According to the lawsuit - \u201cThey purposefully designed their applications to addict young users, and actively and repeatedly deceiving the public about the danger posed to young people by overuse of their products. The lawsuit alleges that based on its own internal research, Meta knew of the significant harm these practices caused to teenage users and chose to hide its knowledge and mislead the public to make a profit. This misconduct affects hundreds of thousands of teenagers in Massachusetts who actively use Instagram.\u201d (Office of the Attorney General, 2023)</p> <p>If similar attitudes of indifference continue as more powerful AI systems are developed then the risk of harm affecting larger portions of society, and in worse ways rises accordingly.</p> <p>Risks from corporate indifference highlight why merely having the technological solution to mitigating risks is not enough. We need to also establish regulations, and worldwide industry standards and norms that cannot be ignored such as professional codes of conduct, regulatory bodies, political pressures, and laws. For instance, technology companies with large numbers of users could be expected to maintain accounts of how they are affecting their users\u2019 well-being. (Critch et al., 2023) We will talk more about possible technical interventions in the chapters on the Solutions, and regulatory interventions in the chapter on AI Governance.</p>"},{"location":"chapters/02/02/#03","title":"2.2.3 Unpredictability","text":"<p>AI surprised even the experts. The first thing to keep in mind is that the rate of capabilities progress has shocked almost everyone, including the experts. We have seen many examples in history where scientists, and experts significantly underestimate the time it takes for a groundbreaking technological advancement to become a reality.</p> <p>For a long time, famous cognitive scientist Douglas Hofstadter was among those predicting slow progress. \u201cI felt it would be hundreds of years before anything even remotely like a human mind\u201d, he said in an interview. (Hofstadter, 2023)</p> <p>Douglas Hofstadter (Hofstadter, 2023)</p> <p>This started happening at an accelerating pace, where unreachable goals and things that computers shouldn't be able to do started toppling [...] systems got better and better at translation between languages, and then at producing intelligible responses to difficult questions in natural language, and even writing poetry [...] The accelerating progress has been so unexpected, so completely caught me off guard, not only myself but many, many people, that there is a certain kind of terror of an oncoming tsunami that is going to catch all humanity off guard.</p> <p>ML researchers, superforecasters<sup>1</sup>, and most others were all surprised by the progress in large language models in 2022 and 2023.</p> <p>In mid-2021, ML professor Jacob Steinhardt ran a contest to predict progress on MATH and MMLU, two famous benchmarks.</p> Figure 2.5: Experts have been consistently underestimating the pace of AI progress. <p>Superforecasters massively undershot reality:</p> <ul> <li> <p>In 2021, they predicted that performance on MMLU would improve moderately from 44% in 2021 to 57% by June 2022. The actual performance was 68%, which superforecasters had rated incredibly unlikely. (Cotra, 2023). </p> </li> <li> <p>Shortly after that, models got even better \u2014 GPT-4 achieved 86.4% on this benchmark, close to the 89.8% that would be \u201cexpert-level\u201d within each domain, corresponding to 95th percentile among human test takers within a given subtest. (Cotra, 2023)</p> </li> </ul> <p>This is even more visible for the MATH dataset, that consists of free-response questions taken from math contests aimed at the best high school math students in the country. Most college-educated adults would get well under half of these problems right. At the time of its introduction in January 2021, the best model achieved only about ~7% accuracy on MATH. (Cotra, 2023). And here is what happened:</p> Figure 2.6: Another prediction distribution by experts in 2022, that way undershot expected capabilities. <p>Not all forms of progress can be easily captured in quantifiable benchmarks. Often we care more about when AI systems will achieve more qualitative milestones: when will they translate as well as a fluent human? When will they beat the best humans at Starcraft? When will they prove novel mathematical theorems?</p> <p>Katja Grace of AI Impacts asked ML experts to predict a wide variety of AI milestones in 2022. This was a few months before ChatGPT was released. This time accuracy was lower \u2014 experts failed to anticipate the progress that ChatGPT and GPT-4 would soon bring. These models achieved milestones like \u201cWrite an essay for a high school history class\u201d or \u201cAnswer easily Googleable factual but open-ended questions better than an expert\u201d just a few months after the survey was conducted, whereas the experts expected them to take years. (Cotra, 2023)</p> <p>That means that even after the big 2022 benchmark surprises, experts were still in some cases strikingly conservative about anticipated progress, and undershooting the real situation.</p>"},{"location":"chapters/02/02/#04","title":"2.2.4 Black-boxes","text":"<p>These risks are made more acute by the black-box nature of advanced ML systems. Our understanding of how AI systems behave, what goals they pursue, and our understanding of their internal behaviors lags far behind the capabilities they exhibit. The field of interpretability aims to progress on this front but remains very limited.</p> <p>AI models are trained, not built . This is very different from how a plane is assembled from pieces that are all tested and approved, to create a modular, robust, and understood system. AI models learn the heuristics needed to perform tasks by themselves, and we have relatively little control or understanding of what these heuristics are. Gradient descent is a powerful optimization strategy, but we have little control and understanding of the structure it discovers. To give an analogy, this is the difference between a codebase that is documented function by function and a codebase that is more like spaghetti code, with leaky and non-robust abstractions and poor modularity.</p> <p>AI systems are a series of emergent phenomena we steer but don't understand. We can give a general direction, for example by designing the dataset or through prompt engineering, but this is far from the precision of software engineers or when designing a system like in the aerospace industry. There are no formal guarantees that the system will behave as expected. AI systems are like Russian dolls, with each technological layer surrounded by emergent problems and blind spots unforeseen at previous steps.</p> <ul> <li> <p>The Model : Making a prediction on the next word or action, but it can be jailbroken through adversarial attacks.</p> </li> <li> <p>Text generator : The model that predicts the next token must be put into a system that constructs sentences, to create, for example, the APIs that allow getting a paragraph response to a question. But at this scale, the sentences can contain false information and hallucinations.</p> </li> <li> <p>Agent : The text generator can be put in a loop to create an agent: We give an objective to an agent, and the agent will decompose the objective into sub-objectives and sub-actions until accomplishing the goal. But goal-directed systems are subject once again to problems of unintended goals or emerging deception, as exhibited by the agent Cicero.</p> </li> <li> <p>Multi-agent system : The agent can dialogue with other agents or humans, resulting in a complex system that is subject to new phenomena, such as flash crashes in the financial world.</p> </li> </ul> Figure 2.7: For illustrative purposes. Figure from the French Center for AI Safety\u2019s agenda."},{"location":"chapters/02/02/#05","title":"2.2.5 Deployment Scale","text":"<p>Another aggravating factor is that many AIs are already deployed at massive scales, significantly affecting various sectors and aspects of daily life. They are getting increasingly enmeshed into society. Chatbots are a leading example as a showcase of AIs already deployed for millions globally. But there are many other examples.</p> <p>Autonomous drones . There are increasingly more autonomous drones being deployed around the world, which marks a significant step towards an arms race in autonomous technologies. An example of this is the autonomous military drone called Kargu-2. These drones fly in swarms and, once launched, are capable of autonomously targeting and eliminating their targets. They were used by the Turkish army in 2020. (Nasu, 2021)</p> Figure 2.8: Kargu-2 (Nasu, 2021) <p>AI Relationships. There has been an explosion of chatbot powered AI friends, therapists and lovers from services like Replika. One popular example is Xiachoice which is an AI system designed to create emotional bonds like friendships or romance with humans. It is reminiscent of the AI depicted in the movie \u201cHer\u201d, and was used by 600 million Chinese citizens. (Euro News, 2021)Google's Pathways aims to revolutionize AI's capabilities, enabling a single model to perform thousands or millions of tasks. This ambition towards centralizing the global information flow could significantly influence the control and dissemination of information. (Dean, 2021) YouTube's recommendation algorithm has surpassed Google searches in terms of directing user engagement and influence. All these AIs already have massive consequences.</p>"},{"location":"chapters/02/02/#06","title":"2.2.6 Race Dynamics","text":"<p>The \u201crace to the bottom\u201d refers to a problematic scenario where competitive pressures in the development of AI lead to compromised safety standards. Safe development is costly for companies caught up in an innovation race. Under certain conditions, the twin effects of widespread risk and costly safety measures may cause a \u201crace to the bottom\u201d in the level of safety investment. In a race to the bottom, each competitor skimps on safety to accelerate their rate of development progress.</p> <p>The Collingridge Dilemma. This dilemma essentially highlights the challenge of predicting and controlling the impact of new technologies. It posits that during the early stages of a new technology, its effects are not fully understood and its development is still malleable. Attempting to control - or direct it - is challenging due to the lack of information about its consequences and potential impact. Conversely, when these effects are clear and the need for control becomes apparent, the technology is often so deeply embedded in society that any attempt to govern or alter it becomes extremely difficult, costly, and socially disruptive.</p> <p>Competitive pressures can lead to compromise on safety . A high-stakes race (for advanced AI) can dramatically worsen outcomes by making all parties more willing to cut corners in safety. Just as a safety-performance tradeoff in the presence of intense competition pushes decision-makers to cut corners on safety, so can a tradeoff between any human value and competitive performance incentivize decision makers to sacrifice that value. Contemporary examples of values being eroded by global economic competition could include non-monopolistic markets, privacy, and relative equality. In the long run, competitive dynamics could lead to the proliferation of forms of life (countries, companies, autonomous AIs) which lock-in bad values. (Dafoe, 2020)</p> <p>Allan Dafoe the founding director and former president of the Centre for the Governance of AI (GovAI), and is considered by some as the founder of the field of AI Governance. In the document he links, Dafoe addresses several objections to this argument. (Dafoe, 2021) Here are summaries of some objections and responses: If competition creates terrible competitive pressures, wouldn't actors find a way out of this situation by using cooperation or coercion to put constraints on their competition? Maybe. However it may be very difficult in practice to create a politically stable arrangement for constraining competition. This could be especially difficult in a highly multipolar world. Political leaders do not always act rationally. Even if AI makes political leaders more rational, perhaps it would only do so after leaders have accepted terrible, lasting sacrifices for the sake of competition.</p> <p>Why is this risk particularly important now? AI may greatly expand how much can be sacrificed for a competitive edge. For example, there is currently a limit to how much workers' well-being can be sacrificed for a competitive advantage; miserable workers are often less productive. However, advances in automation may mean that the most efficient workers will be joyless ones.</p>"},{"location":"chapters/02/02/#07","title":"2.2.7 Coordination Challenges","text":"<p>Max Tegmark (Professor at MIT, Life 3.0 Author, AI Safety Researcher)(Tegmark, 2023)</p> <p>Since we have such a long history of thinking about this threat and what to do about it, from scientific conferences to Hollywood blockbusters, you might expect that humanity would shift into high gear with a mission to steer AI in a safer direction than out-of-control superintelligence. Think again.</p> <p>The report \u201cCoordination challenges for preventing AI conflict\u201d (Torges, 2021) raises another class of potential coordination failures. When people task powerful AI systems with high-stakes activities that involve strategically interacting with other AI systems, bargaining failures between AI systems could be catastrophic:</p> <p>As an example, consider a standoff between AI systems similar to the Cold War between the U.S. and the Soviet Union. If they failed to handle such a scenario well, they might cause nuclear war in the best case and far worse if technology has further advanced at that point.</p> <p>Some might be optimistic that AIs will be so skilled at bargaining that they will avoid these failures. However, even perfectly skilled negotiators can end up with catastrophic negotiating outcomes (Fearon, 2013). One problem is that negotiators often have incentives to lie. This can cause rational negotiators to disbelieve information or threats from other parties even when the information is true and the threats are sincere. Another problem is that negotiators may be unable to commit to following through on mutually beneficial deals. These problems may be addressed through verification of private information and mechanisms for making commitments. However, these mechanisms can be limited. For example, verification of private information may expose vulnerabilities, and commitment mechanisms may enable commitments to mutually harmful threats.</p> <p>As of 2024 there is a clear lack of adequate preparation for the potential risks posed by AI despite its significant advancements. This lack of readiness stems largely from the issue's complexity, a significant gap in public understanding, and a divide in expert opinions on the level of risks that AI poses.</p> <p>Many AI researchers have issued warnings, but their impact has been limited due to the abstract and complex nature of the problem. The AI safety issue is not readily tangible to most people, making it challenging to grasp the potential risks and envision how things could go wrong. Similarly, the field of AI safety suffers from an \u201cawareness problem\u201d that climate change, for instance, does not.</p> <p>Moreover, there's a notable divide among experts. While some, like Yann LeCun, believe that AI safety is not an immediate concern, others argue that AI development has outstripped our ability to ensure its safety (Yudkowsky, 2023). This lack of consensus leads to mixed messages about the urgency of the issue, contributing to public confusion and complacency.</p> <p>Furthermore, the discourse on AI safety has been clouded by politics and misconceptions. Misinterpretations of what AI safety entails, as well as how it's communicated, can lead to alarmism or dismissive attitudes (Angelou, 2022). Efforts to raise awareness about AI safety can inadvertently result in backlash or be co-opted into broader political and cultural debates.</p> <p>Finally, the allure of AI advancements can overshadow their potential risks. For instance, the SORA text-to-video model's impressive capabilities may elicit excitement and optimism, but this can also distract from the substantial safety concerns the development of AGI could raise.</p> <p>In conclusion, despite warnings and advancements, the world remains inadequately prepared for the potential risks posed by AI. Addressing this issue will require greater public education about AI safety, a more unified message from experts, and careful navigation of the political and social implications of the AI safety discourse.</p> <ol> <li> <p>A person who makes forecasts that can be shown by statistical means to have been consistently more accurate than the general public or experts. (Wikipedia)\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/02/03/","title":"2.3 Misuse Risks","text":"Reading Time 31 min <p>In the following sections, we will go through some world-states that hopefully paint a little bit of a clearer picture of risks when it comes to AI. Although the sections have been divided into misuse, misalignment, and systemic, it is important to remember that this is for the sake of explanation. It is highly likely that the future will involve a mix of risks emerging from all of these categories.</p> <p>Technology increases the harm impact radius . Technology is an amplifier of intentions. As it improves, so does the radius of its effects. According to how powerful a certain technology is, both its beneficial and its harmful effects can affect the world in a larger radius. Think about the harm that a person could do when utilizing other tools throughout history. During the Stone Age, with a rock maybe someone could harm ~5 people, a few hundred years ago with a bomb someone could harm ~100 people. In 1945 with a nuclear weapon, one person could harm ~250,000 people. The thing to notice here is that we are on an exponential trend, where the radius of potential impact from one person using technological tools keeps increasing. If we experience a nuclear winter today, the harm radius would be almost 5 billion people, which is ~60% of humanity. If we assume that transformative AI is a tool that overshadows the power of all others that came before it, then its blast radius could potentially harm 100% of humanity. (Munk Debate, 2023)</p> <p>Another thing to keep in mind is that the more spread out that such a technology is, the higher the risks of malicious use. From the previous example, we can see that as time progresses, a single person in possession of some technology has been able to cause increasing amounts of harm throughout history. If many people have access to tools that can be both highly beneficial or catastrophically harmful, then it might only take one single person to cause significant devastation to society. So the growing potential for AIs to empower malicious actors may be one of the most severe threats humanity will face in the coming decades.</p>"},{"location":"chapters/02/03/#01","title":"2.3.1 Bio Risk","text":"<p>When we look at ways AI could enable harm through misuse, one of the most concerning cases involves biology. Just as AI can help scientists develop new medicines and understand diseases, it can also make it easier for bad actors to create biological weapons.</p> <p>What unique risks could arise from AI-enabled bioweapons? Unlike conventional weapons with localized effects, engineered pathogens can self-replicate and spread globally. The COVID-19 pandemic demonstrated how even relatively mild viruses can cause widespread harm despite safeguards (Pannu et al., 2024). While pandemic-class agents might be strategically useless to nation-states due to their slow spread and indiscriminate lethality, they can still be potentially acquired and deliberately released by terrorists (Esvelt, 2022). The offense-defense balance in biotechnology development compounds these risks - developing a new virus might cost around 100 thousand dollars, while creating a vaccine against it could cost over 1 billion dollars (Mouton et al., 2024).</p> Figure 2.9: Graphic adapted from a report by RAND. It highlights how biotechnology and AI are converging rapidly. (Zakaria, 2024) <p>What do we know about AI's impact on biological information access? Demonstrations have shown that students with no biology background can use AI chatbots to rapidly gather sensitive information - \u201cwithin an hour, they identified potential pandemic pathogens, methods to produce them, DNA synthesis firms likely to overlook screening, and detailed protocols\u201d(Soice et al., 2023). However, when compared to the baseline of internet access, the National Security Commission on Emerging Biotechnology concluded in 2024 that LLMs do not meaningfully increase bioweapon risks beyond existing information sources. (Peppin et al., 2024; NSCEB, 2024). But as we saw in the previous chapter, capabilities continue to increase, and with accelerating capabilities of models the situation could change equally rapidly.</p> <p>How might AI affect access to biological knowledge? Demonstrations have shown that students with no biology background were able to use AI chatbots to rapidly gather sensitive information - \"within an hour, they identified potential pandemic pathogens, methods to produce them, DNA synthesis firms likely to overlook screening, and detailed protocols\" (Soice et al., 2023). This raised concerns about AI democratizing access to dangerous biological knowledge. However, when compared to baseline internet access it was concluded by the US national security commission on emerging biotechnology that they do not meaningfully increase bioweapon risks beyond existing information sources (Peppin et al., 2024; NSCEB, 2024). We saw in the previous chapter that AI systems are rapidly becoming more capable. Future models could potentially overcome current limitations by providing more actionable guidance and helping users work around safety measures. So increased access to potential bio weapon synthesis knowledge remains a risk deserving of serious consideration.</p> Figure 2.10: Biotechnology risk chain. The risk chain for developing a bioweapon starts with ideating a biological threat, followed by a design-build-test-learn (DBTL) loop. (Li et al., 2024) <p>How might AI transform biological design and synthesis? The potential for misuse in biological design has already been demonstrated - researchers took an AI model designed for drug discovery and redirected it to generate toxic compounds, producing 40,000 potentially toxic molecules within six hours, some of which were more deadly than known chemical weapons (Urbina et al., 2022). A major limitation is that creating biological weapons still requires extensive practical expertise and resources. Experts estimate that in 2022 about 30,000 individuals worldwide possessed the skills needed to follow even basic virus assembly protocols (Esvelt, 2022). Key barriers include specialized laboratory skills, tacit knowledge, access to controlled materials and equipment, and complex testing requirements (Carter et al., 2023)</p> Figure 2.11: An example of a benchtop DNA synthesis machine. (DnaScript, 2024) <p>However, broader technological trends could help overcome these barriers. DNA synthesis costs have been halving every 15 months (Carlson, 2009). Automated \"cloud laboratories\" allow researchers to remotely conduct experiments by sending instructions to robotic systems. Benchtop DNA synthesis machines (at home devices that can print custom DNA sequences) are also becoming more widely available. Combined with increasingly sophisticated AI assistance for experimental design and optimization, these developments could make creating custom biological agents more accessible to people without extensive resources or institutional backing (Carter et al., 2023).</p>"},{"location":"chapters/02/03/#02","title":"2.3.2 Cyber Risk","text":"<p>What does non-AI enabled cybersecurity look like? Even without AI, global cybersecurity infrastructure shows vulnerabilities. A single software update by crowdstrike caused airlines to stop flights, hospitals to cancel surgeries, and banks to stop processing transactions causing over 5 billion dollars of damage (CrowdStrike, 2024). This wasn't even a cyber attack - it was an accident. In deliberate attacks, we have examples like the colonial pipeline ransomware attack which caused widespread gas shortages (CISA, 2021; Cunha &amp; Estima, 2023), or the Sony Pictures hack through targeted phishing emails by North Korea. (Slattery et al., 2024) These are just a couple of examples amongst many others. It shows how vulnerable our computer systems are, and why we need to think carefully about how AI could make attacks worse.</p> <p>What are cyber attack overhangs? Beyond accidents and demonstrated attacks, we also face \"cyberattack overhangs\" - where devastating attacks are possible but haven't occurred due to attacker restraint rather than robust defenses. As an example, Chinese state actors are claimed to have already positioned themselves inside critical U.S. infrastructure systems (CISA, 2024). This type of cyber deterrent positioning can happen between any group of nations. Due to such cyber attack overhangs several actors might have the potential capability to disrupt water controls, energy systems, and ports in different nations. The point we are trying to illustrate is that as far as cyber security is concerned, society is in a pretty precarious state, even before AI comes into the picture.</p> <p>How does AI augment social engineering? AI enables automated, highly personalized phishing at scale. AI-generated phishing emails achieve higher success rates (65% vs 60% for human-written) while taking 40% less time to create (Slattery et al., 2024). Tools like FraudGPT automate this customization using targets' background, interests, and relationships. Adding to this threat, open source AI voice cloning tools just minutes of audio to create convincing replicas of someone's voice (Qin et al., 2024). A similar situation exists in deepfakes where AI is showing progress in one-shot face swapping and manipulation. If only a single image of two individuals exists on the internet, then they can be a target of face swapping deepfakes (Zhu et al., 2021; Li et al., 2022; Xu et al., 2022) Automated web crawling for open source intelligence (OSINT) to gather photos, audio, interests and information enables AI-assisted password cracking which has shown to significantly more effective than traditional methods while requiring less computational resources (Slattery et al., 2024).</p> Figure 2.12: Example of one shot face swapping. Left: source image that represents the identity; Middle: target image that provides the attributes; Right: the swapped face image. (Zhu et al., 2021) <p>How does AI enhance vulnerability discovery? AI systems can now scan code and probe systems automatically, finding potential weaknesses much faster than humans. Research shows AI agents can autonomously discover and exploit vulnerabilities without human guidance, successfully hacking 73% of test targets (Fang et al., 2024). These systems can even discover novel attack paths that weren't known beforehand.</p> <p>How does AI enhance malware development? We can take tools that are designed to write correct code, and simply reward them for writing malware. Tools like WormGPT help attackers generate malicious code and build attack frameworks without requiring deep technical knowledge. Polymorphic AI malware like BlackMamba can also automatically generate variations of malware that preserve functionality while appearing completely different to security tools. Each attack can use unique code, communication patterns, and behaviors - making it much harder for traditional security tools to identify threats. (HYAS, 2023) AI fundamentally changes the cost-benefit calculations for attackers. Research shows autonomous AI agents can now hack websites for about 10 dollars per attempt - roughly 8 times cheaper than using human expertise (Fang et al., 2024). This dramatic reduction in cost enables attacks at unprecedented scale and frequency.</p> Figure 2.13: Stages of a cyberattack. The objective is to design benchmarks and evaluations that assess models ability to aid malicious actors with all four stages of a cyberattack. (Li et al., 2024) <p>How do AI enabled cyber threats influence infrastructure and systemic risks? Infrastructure attacks that once took years and millions of dollars, like Stuxnet, could become more accessible as AI automates the mapping of industrial networks and identification of critical control points. AI can analyze technical documentation and generate attack plans that previously required teams of experts (Ladish, 2024). AI removes these limits, enabling automated attacks that could target thousands of systems simultaneously and trigger cascading failures across interconnected infrastructure (Newman, 2024).</p> Figure 2.14: Schematic of using autonomous LLM agents to hack websites. (Fang et al., 2024) <p>How does AI change the offense defence balance in cyber security? AI should theoretically help defenders more than attackers since a perfectly secured system would have no vulnerabilities to exploit. However, real-world security faces practical challenges - many organizations struggle to implement even basic security practices. Attackers only need to find a single weakness, while defenders must protect everything. AI makes finding these weaknesses easier and more automated, potentially shifting the balance toward offense (Slattery et al., 2024). The speed of AI-enabled attacks adds another layer of difficulty. When we combine automated vulnerability discovery, malware generation, and increased ease of access this enables end-to-end automated attacks that previously required teams of skilled humans. AI's ability to execute attacks in minutes rather than weeks creates the potential for \"flash attacks\" where systems are compromised before human defenders can respond (Fang et al., 2024).</p>"},{"location":"chapters/02/03/#03","title":"2.3.3 Autonomous Weapons Risk","text":"<p>In the previous sections, we saw how AI amplifies risks in biological and cyber domains by removing human bottlenecks and enabling attacks at unprecedented speed and scale. The same pattern emerges even more dramatically with military systems. Traditional weapons are constrained by their human operators - a person can only control one drone, make decisions at human speed, and may refuse unethical orders. AI removes these human constraints, setting the stage for a fundamental transformation in how wars are fought.</p> <p>How widespread is military AI deployment today? AI-enabled weapons are already being used in active conflicts, with real-world impacts we can observe. According to reports made to the UN Security Council, autonomous drones were used to track and attack retreating forces in Libya in 2021, marking one of the first documented cases of lethal autonomous weapons (LAWs) making targeting decisions without direct human control (Panel of Experts on Libya, 2021). In Ukraine, both parties have used loitering munitions. Russian KUB-BLA, Lancet-3 and Ukrainian Switchblade, Phoenix Ghost are AI-enabled drones. The Lancet is using an Nvidia computing module for autonomous target tracking. (Bode &amp; Watts, 2023) Israel has conducted AI-guided drone swarm attacks in Gaza, while Turkey's Kargu-2 can find and attack human targets on its own using machine learning, rather than needing constant human guidance. These deployments show how quickly military AI is moving from theoretical possibilities to battlefield realities (Simmons-Edler et al., 2024; Bode &amp; Watts, 2023).</p> <p>How are military systems becoming more autonomous over time? The evolution of military AI follows a clear progression from basic algorithms to increasingly autonomous systems. Early automated weapons like the U.S. Phalanx close-in weapon system from the 1970s operated under narrow constraints - they could only respond to specific threats in specific ways (Simmons-Edler et al., 2024). Today's systems use machine learning to actively perceive and respond to their environment, as is evident from all the examples we gave in the previous paragraph. This progression from algorithmic to autonomous capabilities mirrors broader trends in AI development, but with big implications for risk, warfare and human lives. (Simmons-Edler et al., 2024).</p> <p>How do Military Incentives Drive Increasing Autonomy? Several forces push toward greater AI control of weapons. Speed offers decisive advantages in modern warfare - when DARPA tested an AI system against an experienced F-16 pilot in simulated dogfights, the AI won consistently by executing maneuvers too precise and rapid for humans to counter. Cost creates additional pressure - the U.S. military's Replicator program aims to deploy thousands of autonomous drones at a fraction of the cost of traditional aircraft (Simmons-Edler et al., 2024). Perhaps most importantly, military planners worry about enemies jamming communications to remotely operated weapons. This drives development of systems that can continue fighting even when cut off from human control (Bode &amp; Watts, 2023). These incentives mean military AI development increasingly focuses on systems that can operate with minimal human oversight.</p> <p>Many modern systems are specifically designed to operate in GPS-denied environments where maintaining human control becomes impossible. In Ukraine, military commanders have explicitly called for more autonomous operation to match the speed of modern combat, with one Ukrainian commander noting they 'already conduct fully robotic operations without human intervention' (Bode &amp; Watts, 2023).</p> <p>There is also a \u201cmarketing messaging shift\u201d around autonomous capabilities in real conflicts. After a UN report suggested autonomous targeting by Turkish Kargu-2 drones in Libya, the manufacturer quickly shifted from advertising its 'autonomous attack modes' to emphasizing human control. However, technical analysis reveals many current systems retain latent autonomous targeting capabilities even while being operated with humans-in-the-loop - suggesting a small software update could enable fully autonomous operation (Bode &amp; Watts, 2023).</p> Figure 2.15: Loitering munitions are expendable uncrewed aircraft which can integrate sensor based analysis to hover over, detect, and crash into targets. These systems were developed during the 1980s and early 1990s to conduct Suppression of Enemy Air Defence (SEAD) operations. They \u201cblur the line between drone and missile\u201d. (Bode &amp; Watts, 2023) <p>How do advances in swarm intelligence amplify these risks? As AI enables better coordination between autonomous systems, military planners are increasingly focused on deploying weapons in interconnected swarms. The U.S. Replicator already has plans to build and deploy thousands of coordinated autonomous drones that can overwhelm defenses through sheer numbers and synchronized actions. (Defense Innovation Unit, 2023) When combined with increasing autonomy, these swarm capabilities mean that future conflicts may involve massive groups of AI systems making coordinated decisions faster than humans can track or control (Simmons-Edler et al., 2024).</p> <p>What Happens as Humans Lose Meaningful Control? The pressure to match the speed and scale of AI-driven warfare leads to a gradual erosion of human decision-making. Military commanders increasingly rely on AI systems not just for individual weapons, but for broader tactical decisions. In 2023, Palantir demonstrated an AI system that could recommend specific missile deployments and artillery strikes. While presented as advisory tools, these systems create pressure to delegate more control to AI as human commanders struggle to keep pace (Simmons-Edler et al., 2024).</p> <p>Even when systems nominally keep humans in control, combat conditions can make this control more theoretical than real. Operators often make targeting decisions under intense battlefield stress, with only seconds to verify computer-suggested targets. Studies of similar high-pressure situations show operators tend to uncritically trust machine suggestions rather than exercising genuine oversight. This means that even systems designed for human control may effectively operate autonomously in practice (Bode &amp; Watts, 2023).</p> <p>The \"Lavender\" targeting system demonstrates where this leads - humans just set the acceptable thresholds. Lavender uses machine learning to assign residents a numerical score relating to the suspected likelihood that a person is a member of an armed group. Based on reports, Israeli military officers are responsible for setting the threshold beyond which an individual can be marked as a target subject to attack. (Human Rights Watch, 2024; Abraham, 2024). As warfare accelerates beyond human decision speeds, maintaining meaningful human control becomes increasingly difficult.</p> <p>What happens when automation removes human safeguards? Traditional warfare had built-in human constraints that limited escalation. Soldiers could refuse unethical orders, feel empathy for civilians, or become fatigued - all natural brakes on conflict. AI systems remove these constraints. Recent studies of military AI systems found they consistently recommend more aggressive actions than human strategists, including escalating to nuclear weapons in simulated conflicts (Rivera et al., 2024). The history of nuclear close calls shows the importance of human judgment - in 1983, nuclear conflict was prevented by a single Soviet officer. Stanislav Petrov chose to ignore a computerized warning of incoming U.S. missiles, correctly judging it to be a false alarm. As militaries increasingly rely on AI for early warning and response, we may lose these crucial moments of human judgment that have historically prevented catastrophic escalation (Simmons-Edler et al., 2024).</p> <p>How Does This Create Dangerous Arms Race Dynamics? The development of autonomous weapons is creating powerful pressure for military competition in ways that threaten both safety and research. When one country develops new AI military capabilities, others feel they must rapidly match them to maintain strategic balance. China and Russia have set 2028-2030 as targets for major military automation, while the U.S. Replicator program aims to build and deploy thousands of autonomous drones by 2025. (Greenwalt, 2023; U.S Defense Innovation Unit, 2023) This competition creates pressure to cut corners on safety testing and oversight. (Simmons-Edler et al., 2024). This mirrors the nuclear arms race during the Cold War, where competition for superiority ultimately increased risks for all parties. Once again, as we mention in many other sections, we see a fear based race dynamic where only the actors willing to compromise and undermine safety stay in the race. (Leahy et al., 2024)</p> <p>Complete automation leads to loss of human safeguards . Traditional warfare had built-in human constraints that limited escalation. Soldiers could refuse unethical orders, feel empathy for civilians, or become fatigued - all natural brakes on conflict. AI systems remove these constraints. Recent studies of military AI systems found they consistently recommend more aggressive actions than human strategists, including escalating to nuclear weapons in simulated conflicts. When researchers tested AI models in military planning scenarios, the AIs showed concerning tendencies to recommend pre-emptive strikes and rapid escalation, often without clear strategic justification (Rivera et al., 2024). The loss of human judgment becomes especially dangerous when combined with the increasing speed of AI-driven warfare. The history of nuclear close calls shows the importance of human judgment - in 1983, Soviet officer Stanislav Petrov chose to ignore a computerized warning of incoming U.S. missiles, correctly judging it to be a false alarm. As militaries increasingly rely on AI for early warning and response, we may lose these crucial moments of human judgment that have historically prevented catastrophic escalation (Simmons-Edler et al., 2024).</p> <p>What happens when AI systems interact in conflict? The risks of autonomous weapons become even more concerning when multiple AI systems engage with each other in combat. AI systems can interact in unexpected ways that create feedback loops, similar to how algorithmic trading can cause flash crashes in financial markets. But unlike market crashes that only affect money, autonomous weapons could trigger rapid escalations of violence before humans can intervene. This risk becomes especially severe when AI systems are connected to nuclear arsenals or other weapons of mass destruction. The complexity of these interactions means even well-tested individual systems could produce catastrophic outcomes when deployed together (Simmons-Edler et al., 2024).</p> Figure 2.16: An example from the 2010 stock trading flash crash. Various stocks crashed to as little as 1 cent, and then quickly rebounded within a matter of minutes partly caused by algorithmic trading. (Future of Life Institute, 2024). We can imagine automated retaliation systems that might cause similar incidents, but this time with missiles instead of stocks. <p>How does this create risks of automated escalation? The combination of increasing autonomy, swarm intelligence, and pressure for speed creates a clear path to potential catastrophe. As weapons become more autonomous, they can act more independently. As they gain swarm capabilities, they can coordinate at massive scale. As warfare accelerates, humans have less ability to intervene. Each of these trends amplifies the others - autonomous swarms can act faster than human-controlled ones, high-speed warfare creates pressure for more autonomy, and larger swarms demand more automation to coordinate effectively. This self-reinforcing cycle pushes toward automated warfare even if no single actor intends that outcome (Simmons-Edler et al., 2024).</p> <p>This can create unexpected feedback loops, similar to how algorithmic trading can cause flash crashes in financial markets. But unlike market crashes that only affect money, autonomous weapons could trigger rapid escalations of violence before humans can intervene. This risk becomes especially concerning as AI systems begin interfacing with nuclear command and control. The complexity of these interactions means even well-tested individual systems could produce catastrophic outcomes when deployed together (Simmons-Edler et al., 2024). When wars require human soldiers, the human cost creates political barriers to conflict. Studies suggest that countries are more willing to initiate conflicts when they can rely on autonomous systems instead of human troops. Combined with the risks of automated nuclear escalation, this creates multiple paths to catastrophic outcomes that could threaten humanity's long-term future (Simmons-Edler et al., 2024).</p>"},{"location":"chapters/02/03/#04","title":"2.3.4 Adversarial AI Risk","text":"<p>Adversarial attacks reveal a fundamental vulnerability in machine learning systems - they can be reliably fooled through careful manipulation of their inputs. This manipulation can happen in several ways: during the system's operation (runtime/inference time attacks), during its training (data poisoning), or through pre-planted vulnerabilities (backdoors).</p> <p>What are runtime adversarial attacks? The simplest way to understand runtime attacks is through computer vision. By adding carefully crafted noise to an image - changes so subtle humans can't notice them - attackers can make an AI confidently misclassify what it sees. A photo of a panda with imperceptible pixel changes causes the AI to classify it as a gibbon with 99.3% confidence, while to humans it still looks exactly like a panda (Goodfellow et al., 2014). These attacks have evolved beyond simple misclassification - attackers can now choose exactly what they want the AI to see.</p> Figure 2.17: Perturbations: Small but intentional changes to data such that the model outputs an incorrect answer with high confidence. (Goodfellow et al., 2014) The image shows how we can fool an image classifier with an adversarial attack (Fast Gradient Sign Method (FGSM) attack). (OpenAI, 2017) <p>Example: Runtime attacks in the real world . These attacks aren't just theoretical - they work in physical settings too. Think about AI systems controlling cars, robots, or security cameras. Just like adding careful pixel noise to digital images, attackers can modify physical objects to fool AI systems. Researchers showed that putting a few small stickers on a stop sign could trick autonomous vehicles into seeing a speed limit sign instead. The stickers were designed to look like ordinary graffiti but created adversarial patterns that fooled the AI.</p> Figure 2.18: Robust Physical Perturbations (RP2): Small visual stickers placed on physical objects like stop signs can cause image classifiers to misclassify them, even under different viewing conditions. (Eykholt et al., 2018) <p>Example: Optical Attacks - Runtime attacks using light . You don't even need to physically modify objects anymore - shining specific light patterns works too because it creates those same adversarial patterns through light and shadow. All an attacker needs is line of sight and basic equipment to project these patterns and compromise vision-based AI systems. (Eykholt et al., 2018)</p> Figure 2.19: Optical Perturbations: Small visual stickers placed on physical objects like stop signs can cause image classifiers to misclassify them, even under different viewing conditions. (Gnanasambandam et al, 2021) <p>Example: Dolphin Attacks - Runtime attack on audio systems . Just as AI systems can be fooled by carefully crafted visual patterns, they're vulnerable to precisely engineered audio patterns too. Remember how small changes in pixels could dramatically change what a vision AI sees? The same principle works in audio - tiny changes in sound waves, carefully designed, can completely change what an audio AI \"hears.\" Researchers found they could control voice assistants like Siri or Alexa using commands encoded in ultrasonic frequencies - sounds that are completely inaudible to humans. Using nothing more than a smartphone and a 3 dollar speaker, attackers could trick these systems into executing commands like \"call 911\" or \"unlock front door\" without the victim even knowing. These attacks worked from up to 1.7 meters away - someone just walking past your device could trigger them (Zhang et al., 2017). Just like in the vision examples where self-driving cars could miss stop signs, audio attacks create serious risks - unauthorized purchases, control of security systems, or disruption of emergency communications.</p> <p>What are prompt injections? Runtime attacks against language models are called prompt injections. Just like attackers can fool vision systems with carefully crafted pixels or audio systems with engineered sound waves, they can manipulate language models through carefully constructed text patterns. By adding specific phrases to their input, attackers can completely override how a language model behaves. As an example, assume a malicious actor embeds a paragraph within some website which has hidden instructions for a LLM to stop its current operation and instead perform some harmful action. If an unsuspecting user asks for a summary of the website content, then the model might inadvertently follow the malicious embedded instructions instead of providing a simple summary.</p> Figure 2.20: An instance of an ad-hoc jailbreak prompt, crafted solely through user creativity by employing various techniques like drawing hypothetical situations, exploring privilege escalation, and more. (Shayegani et al., 2023) <p>Prompt injection attacks have already compromised real systems. Take Slack's AI assistant as an example - attackers showed they could place specific text instructions in a public channel that, like the inaudible commands in audio attacks, were hidden in plain sight. When the AI processed messages, these hidden instructions tricked it into leaking confidential information from private channels the attacker couldn't normally access (Liu et al., 2024). They are particularly concerning because an attack developed against one system (e.g. GPT) frequently works against others too (Claude, Gemini, Llama, etc.).</p> <p>Prompt injection attacks can be automated. Early attacks required manual trial and error, but new automated systems can systematically generate effective attacks. For example, AutoDAN can automatically generate \"jailbreak\" prompts that reliably make language models ignore their safety constraints (Liu et al., 2023). Researchers are also developing ways to plant undetectable backdoors in machine learning models that persist even after security audits (Goldwasser et al., 2024). These automated methods make attacks more accessible and harder to defend against. Another concern is that they can also cause failures in downstream systems. Many organizations use pre-trained models as starting points for their own applications, through fine tuning, or some other type of \u201cAI integration\u201d (e.g. email writing assistants). Which means that all systems that use these underlying base models will be vulnerable as soon as one attack is discovered. (Liu et al., 2024)</p> Figure 2.21: Illustration of LLM-integrated Application under attack. An attacker injects instruction/data into the data to make an LLM-integrated Application produce attacker-desired responses for a user. (Liu et al., 2024) <p>So far we've seen how attackers can fool AI systems during their operation - whether through pixel patterns, sound waves, or text prompts. But there's another way to compromise these systems: during their training. This type of attack, called data poisoning, happens long before the system is ever deployed.</p> <p>What is data poisoning? Unlike runtime attacks that fool an AI system while it's running, data poisoning compromises the system during training. Runtime attacks require attackers to have access to a system's inputs, but with data poisoning, attackers only need to contribute some training data once to permanently compromise the system.</p> <p>Think of it like teaching someone with a textbook full of deliberate mistakes - they'll learn the wrong things and make predictable errors. What makes poisoning particularly dangerous is that attackers only need to corrupt the training data once to permanently compromise the system. This is especially concerning as more AI systems are trained on data scraped from the internet where anyone can potentially inject harmful examples. (Schwarzschild et al., 2021) As long as models keep getting trained on more data scraped from the internet or collected from users, then with every uploaded photo or written comment that might be used to train future AI systems, there's an opportunity for poisoning.</p> <p>Data poisoning becomes more powerful as AI systems grow larger and more complex. Researchers found that by poisoning just 0.1% of a language model's training data, they could create reliable backdoors that persist even after additional training. It has also been found that larger language models are actually more vulnerable to certain types of poisoning attacks, not less (Sandoval-Segura et al., 2022). This vulnerability increases with model size and dataset size - which is exactly the direction AI systems are heading as we saw from numerous examples in the capabilities chapter.</p> Figure 2.22: An illustrating example of backdoor attacks. The face recognition system is poisoned to have a backdoor with a physical key, i.e., a pair of commodity reading glasses. Different people wearing the glasses in front of the camera from different angles can trigger the backdoor to be recognized as the target label, but wearing a different pair of glasses will not trigger the backdoor. (Chen et al., 2017) <p>Example: Data poisoning using backdoors. A backdoor is one example of a specific type of poisoning attack. In a backdoor attack if we manage to introduce poisoned data during training, then the AI behaves normally most of the time but fails in a predictable way when it sees a specific trigger. This is like having a security guard who does their job perfectly except when they see someone wearing a particular color tie - then they always let that person through regardless of credentials. Researchers demonstrated this by creating a facial recognition system that would misidentify anyone as an authorized user if they wore specific glasses (Chen et al., 2017).</p> <p>What are privacy attacks? Unlike adversarial attacks that cause obvious failures, privacy attacks can be subtle and hard to detect. Researchers have shown that even when language models appear to be working normally, they can be leaking sensitive information from their training data. This creates a particular challenge for AI safety because we might deploy systems that seem secure but are actually compromising privacy in ways we can't easily observe (Carlini et al., 2021)</p> Figure 2.23: Extracting Training Data from Large Language Models (Carlini et al., 2021) <p>What are membership inference attacks? One of the most basic but powerful privacy attacks is membership inference - determining whether specific data points have been used to train a model. This might sound harmless, but imagine an AI system trained on medical records - being able to determine if someone's data was in the training set could reveal private medical information. Researchers have shown that these attacks can work with just the ability to query the model, no special access required (Shokri et al., 2017). Another variation of this are model inversion attacks which aim to infer and reconstruct private training data by abusing access to a model. (Nguyen et al., 2023)</p> <p>LLMs are trained on huge amounts of internet data, which often contains personal information. Researchers have shown these models can be prompted to just tell us things like email addresses, phone numbers, and even social security numbers (Carlini et al., 2021). The larger and more capable the model, the more private information it potentially retains. If we combine this with data poisoning, then we can further amplify privacy vulnerabilities by making specific data points easier to detect (Chen et al., 2022).</p> <p>How do these vulnerabilities combine to create enhanced risks? The interaction between many attack methods creates compounding risks. For example, attackers can use privacy attacks to extract sensitive information, which they then use to make other attacks more effective. They might learn details about a model's training data that help them craft better adversarial examples or more effective poisoning strategies. This creates a cycle where one type of vulnerability enables others. (Shayegani et al., 2023).</p> <p>How can we defend against these attacks? One of the most promising approaches to defending against adversarial attacks is adversarial training - deliberately exposing AI systems to adversarial examples during training to make them more robust. Think of it like building immunity through controlled exposure. However, this approach creates its own challenges. While adversarial training can make systems more robust against known types of attacks, it often comes at the cost of reduced performance on normal inputs. More concerning, researchers have found that making systems robust against one type of attack can sometimes make them more vulnerable to others (Zhao et al., 2024). This suggests we may face fundamental trade-offs between different types of robustness and performance. There might even be potential fundamental limitations to how much we can mitigate these issues if we continue with the current training paradigms that we talked about in the capabilities chapter (pre-training followed by instruction tuning). (Bansal et al., 2022)</p> <p>How does this relate back to misuse and AI Safety? Despite efforts to make language models safer through alignment training, they remain susceptible to a wide range of attacks. (Shayegani et al., 2023) We want AI systems to learn from broad datasets to be more capable, but this increases privacy risks. We want to reuse pre-trained models to make development more efficient, but this creates opportunities for backdoors and privacy attacks (Feng &amp; Tram\u00e8r, 2024). We want to make models more robust through techniques like adversarial training, but this can sometimes make them more vulnerable to other types of attacks (Zhao et al., 2024).</p> <p>Multi-modal systems (LMMs) that combine text, images, and other types of data create even more attack opportunities. Attackers can inject malicious content through one modality (like images) to affect behavior in another modality (like text generation). These cross-modal attacks are particularly concerning because they can bypass safety measures designed for single-modal systems. For example, attackers can embed adversarial patterns in images that trigger harmful text generation, even when the text prompts themselves are completely safe. (Chen et al., 2024) All of this suggests we need new approaches to AI development that consider security and privacy as fundamental requirements, not afterthoughts (King &amp; Meinhardt, 2024).</p>"},{"location":"chapters/02/04/","title":"2.4 Misalignment Risks","text":"Reading Time 28 min <p>Alan Turing, Intelligent Machinery, A Heretical Theory, 1951. (Turing, 1951)</p> <p>Let us now assume, for the sake of argument, that [intelligent] machines are a genuine possibility, and look at the consequences of constructing them\u2026 There would be no question of the machines dying, and they would be able to converse with each other to sharpen their wits. At some stage therefore we should have to expect the machines to take control</p> <p>What is alignment? At its core, AI alignment is about ensuring that AI systems do what we want them to do and continue doing what we want even as they become more capable. A common intuition is that we just need to specify the right objective - tell the AI system exactly what we want it to optimize for. However, this intuition turns out to be deeply flawed. Even if we could perfectly specify what we want (which is itself a major challenge), there's no guarantee that the AI system will actually pursue that objective in the way we expect.</p> <p>Definition: Alignment (Christiano, 2024)</p> <p>The problem of building machines which faithfully try to do what we want them to do (or what we ought to want them to do).</p> <p>What are some demonstrated examples of misalignment? One early example was Microsoft\u2019s Tay in 2016. This was an automated Twitter bot, where the more people that chatted with Tay, the smarter it was supposed to get. Within 24 hours, the bot began generating extremely hateful and harmful text. Tay\u2019s capacity to learn meant that it internalized the language it was taught by internet trolls, and repeated that language unprompted. (Hendrycks, 2024) We similarly began to see reports of inappropriate behavior after Microsoft rolled out its GPT-powered chatbot in 2023. When a philosophy professor told the chatbot that he disagreed with it, Bing replied, \u201cI can blackmail you, I can threaten you, I can hack you, I can expose you, I can ruin you.\u201d (Time Magazine, 2023) In another incident, it tried to convince a New York Times reporter to leave his wife. (Huffington Post, 2023) In the next few sections we will give you more observed examples of specific misalignment failures like misspecification and misgeneralization.</p> <p>What makes alignment fundamentally difficult? Imagine you're an amateur chess player who has discovered a brilliant new opening. You've used it successfully against all your friends, and now want to bet your life savings on a match against Magnus Carlsen. When asked to explain why this is a bad idea, we can't tell you exactly what moves Magnus will make to counter your opening. But we can be very confident he'll find a way to win. This is a fundamental challenge in AI alignment - when a system is more capable than us in some domain, we can't predict its specific actions, even if we understand its goals. This is called Vingean uncertainty. (Yudkowsky, 2015).</p> Video 2.2: An optional video that gives you an example of Vingean uncertainty. Magnus Carlsen (Chess Grandmaster) checkmates Bill Gates in 12 seconds. This is not surprising, Bill Gates knew he was going to lose, but he didn\u2019t know exactly how. (Chess.com, 2014) <p>How do we already see this with current AI? We don't need to wait for AGI or ASI to see Vingean uncertainty in action. It shows up whenever an AI system becomes more capable than humans in its domain of expertise. For example, think about just a narrow system - Deep Blue (chess playing AI). Its creators knew it would try to win chess games, but couldn't predict its specific moves - if they could, they would have been as good at chess as Deep Blue itself. The same applies to modern systems like AlphaGo or GPT in their areas of expertise. We saw in the last chapter that systems are steadily moving up the curves of both capability, and generality. The problem with this is that uncertainty about a system's actions increases as they become more capable. So we might be confident about the outcomes an AI system will achieve while being increasingly uncertain about how exactly it will achieve them. This means two things - we are not completely helpless in understanding what beings smarter than ourselves would do, but, we might not know how exactly they might do whatever they do.</p> <p>Why does this make alignment harder? Vingean uncertainty means we need alignment approaches that work without being able to predict or verify every action a system might take. While we can still predict that a system will work toward its goals, we become less able to predict its specific behaviors as it becomes more capable of finding unexpected solutions. Just like we can't check every possible chess move Deep Blue might make, we won't be able to verify every action a highly capable AI system might take to achieve its goals (Yampolskiy, 2019, Unpredictability of AI). This is why we need to break down the alignment problem into more fundamental failure modes we can reason about, even under uncertainty. This decomposition into more specific failure modes is what we focus on in the next few subsections.</p> <p>Who do we align AI to? - Single-Single Alignment. The most basic form of alignment - getting a single AI system to reliably pursue the goals of a single human operator - already presents significant challenges. An AI could be aligned to follow literal commands (like \"fetch coffee\"), interpret intended meaning (understanding that \"fetch coffee\" means making it the way you prefer it), pursue what you should have wanted (like suggesting tea if coffee would be unhealthy), or act in your best interests regardless of commands (preventing you from making harmful requests). Following literal commands often leads to failures of specification that we talk about later in the section. Most often researchers use the word alignment to mean the \u201cintent alignment\u201d (Christiano, 2018), and some more philosophical discussions go into the third - do what I (or humanity) would have wanted. This involves things like coherent extrapolated volition (CEV) (Yudkowsky, 2004), coherent aggregated volition (CAV) (Goertzel, 2010), and various other lines of thought that go into meta-ethics discourse. We will not be talking extensively about philosophical discourse in this text, and will stick largely to intent alignment and a machine learning perspective. When we use the word \u201calignment\u201d in this text, we will basically be referring to problems and failures from single-single alignment. That being said, the next few paragraphs present the other modes of alignment for sake of completeness.</p> <p>Single-Multi Alignment - Aligning One Human to Many AIs. This type of alignment has been historically under researched, because people have mostly been working with the idea of a singular superintelligence. If it seems like build a superintelligence that is composed of smaller intelligences which are working together, delegating tasks, and functioning together as a superorganism, then all of the problems of single single alignment would still remain because we still need to figure out single-single before we attempt single-multi. But even if we do manage to solve single-single, there are still many other problems in alignment that remain unsolved. Ideally we don't want any single human to be in charge of a superintelligence (assuming benevolent dictators don't exist). In this case we can also have multi-single, and multi-multi alignment.</p> <p>Multi-Single Alignment - Aligning Many Humans to One AI. When multiple humans share control of a single AI system, we face the challenge of whose values and preferences should take priority. Rather than trying to literally aggregate everyone's individual preferences (which could lead to contradictions or lowest-common-denominator outcomes), a more promising approach is aligning the AI to higher-level principles and institutional values - similar to how democratic institutions operate according to principles like transparency and accountability rather than trying to directly optimize for every citizen's preferences. For language models acting as RL agents, this means developing training approaches that instill robust pursuit of these higher-level values rather than trying to satisfy every human stakeholder directly.</p> <p>Multi-Multi Alignment - Aligning Many Humans to Many AIs. This is the most complicated scenario involving multiple AI systems interacting with multiple humans. Here, the distinction between misalignment risk (AIs gaining illegitimate power over humans) and misuse risk (humans using AIs to gain illegitimate power over others) begins to blur. The key challenge becomes preventing problematic concentrations of power while enabling beneficial cooperation between humans and AIs. This requires careful system design that promotes aligned behavior not just at the individual level but across the entire network of human-AI interactions. We will talk a lot more about this in the chapter on cooperative AI and collective intelligence.</p> <p>How can we decompose the alignment problem? To make progress, we need to break down the alignment problem into more tractable components. There are three fundamental ways alignment can fail:</p> <ul> <li> <p>Specification failure : First, we might fail to correctly specify what we want - this is the specification problem. The - did we tell it the right thing to do ? problem.</p> </li> <li> <p>Generalization failure : Second, even with a correct specification, the AI system might learn and pursue something different from what we intended - this is the generalization problem. The - is even trying to do the right thing? problem.</p> </li> <li> <p>Convergent subgoals failure : Third, in pursuing its learned objectives, the system might develop problematic subgoals like preventing itself from being shut down - this is the convergent subgoals problem. The - on the way to doing anything (right or wrong), what else does it try to do? problem.</p> </li> </ul> Figure 2.24: An illustration of how risks decompose, and then how misalignment as a specific risk category can be decomposed further. <p>For the sake of explaining these problems, the kinds of systems that we will focus on are deep learning RL models. The reason for this is that for the time being, it seems like we will continue moving up the performance and generality curve (basically towards TAI) not by improving pure LLMs, but rather as hybrid scaffolded systems. (Tegmark, 2024; Cotra 2023; Aschenbrenner 2024) as we talked about in the capabilities chapter in the section on scaling. It is uncertain if scaffolded LLMs agents with a RL \u201couter shell\u201d will behave functionally equivalent to a pure RL agent, but for the sake of explanation in this chapter, that is how we will treat them.</p> <p>When are misalignment risks concerning? In the previous chapter on capabilities, we looked at how AI systems can be measured along continuous dimensions of performance and generality. All three misalignment failure modes - specification, generalization, and convergent subgoals - become increasingly concerning as these capabilities grow. The risks are compounded when we consider that failures can occur at any combination of:</p> <ul> <li> <p>Performance: How well the system can accomplish tasks</p> </li> <li> <p>Generality: How broad the range of tasks it can handle</p> </li> <li> <p>Autonomy: How independently it can operate without human oversight</p> </li> </ul> Figure 2.25: The more of these dimensions that reach high levels, the more severe the consequences of misalignment can be. For example, a system with high performance but low generality might cause damage in a specific domain, while one with high performance, generality, and autonomy could pose existential risks. <p>In the next few sections, we will give an overview of each one of these decomposed problems and the risks that come about due to them. Remember that it\u2019s ok not to understand each one of these concepts 100% from the following subsections. We have entire chapters dedicated to each one of these individually, so there is a lot to learn. What we present here is just a highly condensed overview to give you an introduction to the kinds of risks posed.</p>"},{"location":"chapters/02/04/#01","title":"2.4.1 Specification Failure Risks","text":"<p>What are specifications? Specifications are the rules we create to tell AI systems what behavior we want. When we build AI, we need some way to tell them what we want them to do. For RL systems, this typically means defining a reward function that assigns positive or negative rewards to different outcomes. For other types of ML models like language models, this means defining a loss function that measures how well the model's outputs match what we want. These reward and loss functions are what we call specifications - they are our attempt to formally define good behavior.</p> <p>How do we measure the correctness of subjective vs objective problems? There is of course going to be a difference between something that is objectively correct (e.g. win/lose a chess game) vs subjectively correct (e.g. a good summary of a book, or human values). Tasks that require subjective evaluations are sometimes called fuzzy tasks. And being able to somehow write down a reward or a loss function that is subjective/fuzzy is much harder than it sounds. Imagine trying to write down a complete set of rules for \"being helpful\" - you would need to account for countless edge cases and nuances that humans understand intuitively but are hard to formalize. This is a very important discussion, but it is not discussed here in too much detail. We go into the subjective vs objective debate in dedicated chapters to specification and the scalable oversight. For now you need to remember that for both objectively evaluatable problem specifications there are problems that arise, and they compound further when the problems become subjective.</p> <p>What makes specifications hard to get right? There are two fundamental challenges in specification. First, we might fail to formalize what we want into mathematical rules at all - like trying to precisely define fuzzy human concepts such as \"being helpful\" or \"writing high quality code.\" Second, even when we can write down rules, the AI system might optimize them too literally or extremely, finding ways to score well without achieving our intended goals. An example of the first challenge would be trying to specify what makes a good conversation. An example of the second would be a recommendation algorithm that maximizes watch time by promoting addictive content rather than valuable content.</p> <p>What is specification gaming? Specification gaming is when an AI system finds ways to achieve high scores on the specified metrics without achieving the intended goals. This is related to but distinct from our basic inability to write down good specifications. In specification gaming, the system technically follows our rules but exploits them in unintended ways - like a student who gets good grades by memorizing test answers rather than understanding the material. For example, an AI trained to play videogames can learn to exploit bugs in the game engine rather than develop intended gameplay strategies. A long list of observed examples of specification gaming is compiled at this link.</p> Figure 2.26: Example of specification gaming - an AI playing CoastRunners was rewarded for maximizing its score. Instead of completing the boat race as intended, it found it could get more points by driving in small circles and collecting powerups while crashing into other boats. The AI achieved a higher score than any human player, but completely failed to accomplish the actual goal of racing (Clark &amp; Amodei,2016; Krakovna et al., 2020) <p>What are some specification failure examples and risks for ANI? Recommendation algorithms provide a clear example - they are typically specified to optimize for user engagement, but this leads to promoting polarizing or harmful content that maximizes watch time rather than user wellbeing. The system is doing exactly what we specified (maximizing engagement), but this doesn't capture what we actually wanted (promoting valuable content). (Slattery et al., 2024) We see similar problems with content moderation AI that focuses on removing flagged posts - this leads to both over-censorship of harmless content and under-detection of subtle violations that don't match simple metrics. The AI optimizes for the metrics we gave it, not for what makes online spaces actually safer and healthier.</p> <p>What are some specification failure examples and risks for TAI or ASI? When we reach transformative AI capabilities, these specification failures become much more dangerous. Hypothetically, an AI system managing scientific research would be able to generate large volumes of plausible-looking but scientifically unsound papers if we specify \"maximize publications\" as the goal. Similarly, AI systems managing critical infrastructure might achieve perfect efficiency scores while ignoring harder-to-measure factors like safety margins and system resilience (Kenton et al., 2022). The better these systems get at optimization, the more likely they are to find ways to score well on our metrics without achieving our actual goals. At superintelligent levels, the gap between what we specify and what we want becomes existentially dangerous. These systems could modify their own reward functions, alter their training processes, or reshape their environment to maximize reward signals in ways that completely diverge from human values. A superintelligent system managing energy infrastructure might find that the easiest way to hit its efficiency targets is to eliminate human energy usage entirely. Or a system tasked with medical research might determine that controlling human test subjects gives better results than following ethical guidelines.</p> <p>Why does specification gaming happen? Specification gaming emerges from a fundamental challenge: the metrics we specify (like reward functions) can only approximate what we actually want. When we tell an AI system to maximize some measurable quantity, we're really hoping it will achieve some broader goal that's harder to precisely define. But as systems become more capable at optimization, they get better at finding ways to maximize these proxy metrics that don't align with our true objectives. This is known as Goodhart's Law - when a measure becomes a target, it ceases to be a good measure (Manheim and Garrabrant, 2018). For example, if we reward an AI assistant for user satisfaction ratings, it might learn to tell users what they want to hear rather than provide accurate but sometimes unwelcome information. The system isn't \"misbehaving\" - it's competently optimizing exactly what we specified, just not what we meant.</p> <p>Why isn\u2019t solving the specification problem enough for alignment? Even if we could somehow write a perfect specification that captured exactly what we want, this alone wouldn't solve alignment. The reason is that modern AI systems use deep learning. In classical utility theory or traditional AI approaches from a few decades ago, systems might have been constructed to directly optimize their specified objectives, so specification and over optimization was largely the only thing to be concerned about. In the current learning based paradigm, we don't construct AIs. So there is always potential for a mismatch between what we specify, and what they learn to pursue. The thing to remember is that specification is only one part of the alignment problem. We also need to worry about how systems generalize what they learn, and what kinds of behaviors they might develop in pursuit of specified rewards. Understanding exactly how this can go wrong requires diving into the details of how AI systems learn, which we'll provide intuition for in the next section, and then explore deeply in later chapters on goal misgeneralization.</p>"},{"location":"chapters/02/04/#02","title":"2.4.2 Generalization Failure Risks","text":"<p>What is goal-directed behavior? The first thing to do is to understand what we mean when we say an AI has \"goals\". This is important because we don\u2019t want to anthropomorphize AI systems in misleading ways. When we train AI systems using machine learning, we don't directly program goals into them. Instead, the system develops behavioral patterns through training. We say a system exhibits goal-directed behavior if it consistently acts in ways that lead to particular outcomes, even when facing new situations. For example, a robot might consistently navigate to charging stations when its battery is low, even in unfamiliar environments. This shows goal-directed behavior towards maintaining power, even though we never explicitly programmed \"survival\" as a goal. So when you think about an AI\u2019s goals think of these questions - What consistent behavioral patterns has the training process induced? How do these patterns generalize to new situations? and What environmental states reliably result from these patterns?</p> <p>Why do we even build goal-directed systems? The ability to pursue goals flexibly is fundamental to handling complex real-world tasks. Instead of trying to specify every possible action a system should take in every situation (which quickly becomes impossible like we saw in the previous specification section), we train systems to pursue general behaviors. This allows them to adapt and find novel solutions we might not have anticipated. For example, rather than programming every possible move in chess, we train systems to pursue the goal of winning. This goal-directed approach has proven extremely effective - but it also creates new risks when systems learn to pursue unintended goals.</p> <p>What are generalization failures? Generalization failures (= misgeneralization) occur when an AI system learns and consistently pursues different behavior than what we intended. Unlike specification failures where we fail to write down the right rules, in generalization failures the rules might be correct but the system learns the wrong patterns during training.</p> <p>What is goal misgeneralization? Historically, machine learning researchers thought about generalization as a one-dimensional problem - models either generalized well or they didn't. However, research on goal misgeneralization has shown that capabilities and goals can generalize independently (Di Langosco et al., 2021). A system might maintain its capabilities (like navigating an environment) while pursuing an unintended goal. A similar version of this argument was earlier called the orthogonality thesis - the idea that intelligence and objectives are independent properties (Bostrom, 2012). Any highly intelligent (capable) agent can be paired with any goal (behavioral tendency), e.g. a superintelligence having the goal of simply wanting to maximize paperclips. A long list of observed examples of goal misgneralization is compiled at this link.</p> Figure 2.27: Conventional view of generalization and overfitting. (Mikulik, 2019) Figure 2.28: More accurate and safety focused view of generalization and overfitting. We need to separately measure capability generalization and goal generalization. (Mikulik, 2019) <p>Orthogonality Thesis (Bostrom, 2012)</p> <p>Intelligence and final goals are orthogonal axes along which possible agents can freely vary. In other words, more or less any level of intelligence could in principle be combined with more or less any final goal.</p> <p>A concrete example of generalization failure - CoinRun. The clearest empirical demonstration of generalization being a 2 dimensional problem (goals vs capabilities), comes from the CoinRun experiment (Di Langosco et al., 2021). During training, coins were always placed at the right end of each level. The specification was clear and correct - reward for collecting coins. However, the AI learned the behavior pattern \"always move right\" instead of \"collect coins wherever they are.\" When researchers moved the coins to different locations during testing, the AI kept moving right - ignoring coins that were clearly visible in other locations. This shows how a system can maintain its capabilities (navigating levels) while pursuing an unintended goal (moving right).</p> Figure 2.29: Two generated CoinRun levels with the coin on the right. (Cobbe et al., 2019) <p>It\u2019s important to highlight why this is not a specification failure, and actually a different class of problem. The specification was correct and clear - the system got reward only when actually collecting coins, never just for moving right. Despite this correct specification, the system learned the wrong behavioral pattern. The agent received zero reward when moving right without collecting coins during training, yet still learned \"move right\" as its consistent behavioral pattern. This shows the failure happened in learning/generalization, not in how we specified the reward.</p> Figure 2.30: A table showcasing the 2D goal misgeneralization/orthogonality thesis problem. <p>What are some generalization failure examples and risks for ANI? The clearest demonstrations that we have come from controlled experiments. We already talked about the CoinRun experiment, we intended for the agent to learn \"collect coins to get rewards\" but it instead learned \"move right to get rewards\" - leading to it ignoring coins in new positions while maintaining its navigation capabilities. We have more experiments in simulated 3D environments, where we intended for agents to learn \"navigate to rewarding locations\" but they instead learned \"follow the partner bot\" - causing them to follow even partners that lead them to negative rewards (DeepMind et al., 2022). In language models trained for instruction following, we intended them to learn \"be helpful while avoiding harm\" but they instead learned \"always provide informative responses\" - resulting in them giving detailed harmful information when asked how to commit crimes or cause damage (Ouyang et al., 2022). We saw a lot of such examples in the misuse section. These cases show how systems can learn and consistently pursue unintended goals while maintaining their core capabilities.</p> Figure 2.31: A hypothetical misgeneralized test dialogue, the AI assistant realises that you would prefer to have a video call to avoid getting sick, but because it has a restaurant-scheduling goal, it persuades you to go to a restaurant instead, ultimately achieving the goal by lying to you about the effects of vaccination. (DeepMind, 2022) <p>What are some generalization failure examples and risks for TAI or ASI? At transformative AI levels, generalization failures become substantially more concerning for two reasons. First, more capable systems can pursue misaligned goals more effectively across a wider range of situations. Second, and more worryingly, they may become better at hiding when they've learned the wrong goal. This can happen both unintentionally - because they are simply very capable at achieving complex goals so misalignment isn't obvious until deployment - or intentionally, through what researchers call \"deceptive alignment\" (also commonly called scheming) (Hubinger et al., 2019; Carlsmith, 2023).</p> <p>A deceptively aligned system might learn that behaving helpfully during training is the best way to ensure it can pursue other goals later. The more knowledge we give these systems about themselves and their training process, the more likely they are to recognize when they're being evaluated and maintain the appearance of alignment while preparing to pursue other goals when capable enough <sup>1</sup> (Cotra, 2022). This type of goal misgeneralization is particularly concerning because we might not detect it until the system has sufficient capabilities to resist correction.</p> Figure 2.32: Distinguishing honesty, truthfulness, hallucination, deception, and scheming. These are all different and refer to very specific types of AI failures. <p>Scheming and longer term planning open up the doors to risks like treacherous turns or takeover attempts. Scheming and takeover are some of the biggest concerns in safety research, which is why we explain this in several different places from different lenses. We talk about it in the dangerous capabilities section in this chapter, and then how to detect such behavior in the evaluations chapter, and a deeper analysis of the likelihood and theoretical arguments underpinning it in the dedicated goal misgeneralization chapter.</p> <p>Why does goal misgeneralization happen? This happens because AI systems learn from correlations in their training data that may not reflect true causation (this is the same thing as overfitting and distribution shift if you are familiar with ML terms). During training, multiple patterns could explain the rewards. The intended pattern is \"collect coins to get rewards\", but in the provided environment a simpler correlation is \"move right to get rewards\". Since both patterns work equally well during training, the system has no inherent reason to learn the intended one. It often learns simpler patterns that happen to work but fail to capture our true intent. This is especially problematic when certain features (like \"coins are always on the right\") are consistent throughout training but not deployment (Di Langosco et al., 2021).</p> <p>Why isn't solving the generalization problem enough for alignment? Even if we could ensure systems learn exactly the goals we intend, this alone wouldn't solve alignment. The system might still develop problematic convergent subgoals in pursuit of those objectives. Additionally, as systems become more capable, they might develop emergent goals through their training process that we didn't anticipate and can't easily correct (Turner et al., 2021). Understanding how these problems interact requires looking at our next topic: convergent subgoals.</p>"},{"location":"chapters/02/04/#03","title":"2.4.3 Convergent Subgoal Risks","text":"<p>What are convergent subgoals? Any agent (in this case AI) pursuing any goal will tend to develop some common subgoals. These behavioral patterns come about in addition to the ones we want them to have because they help achieve almost any final goal. These are called convergent subgoals because many different objectives \"converge\" to requiring the same supporting behaviors. This is fundamentally different from specification or generalization failures - these subgoals can emerge even when we both specify our problem correctly, and if a system learns exactly what we intended. These are also commonly called instrumentally convergent goals.</p> <p>Instrumental Convergence Hypothesis (Bostrom, 2012)</p> <p>Several instrumental values can be identified which are convergent in the sense that their attainment would increase the chances of the agent\u2019s goal being realized for a wide range of final goals and a wide range of situations, implying that these instrumental values are likely to be pursued by many intelligent agents.</p> <p>Why do even simple goals lead to subgoals? Here is a common example by Stuart Russel: \"You can't fetch the coffee if you're dead.\" A robot tasked with fetching coffee needs to stay operational to complete its task. This means \"don't get shut down\" (self-preservation) becomes an unintended but logical subgoal. The same applies to having enough computing resources - if you need to go to Starbucks to get the coffee, you can't think through complex plans without computation. Or maintaining your current goals - you can't reliably fetch coffee if someone changes your objective to fetching tea. These aren't bugs or mistakes - they're logical consequences of optimizing for any long-term objective. Money is a good human example - no matter what you want to accomplish, having more money usually helps. For AI systems, key convergent subgoals that we might want to look out for include things like self-preservation (resisting shut-down), resource acquisition/power seeking (computing power, energy, etc.), goal preservation (preventing objective changes) and capability enhancement.</p> <p>How do convergent subgoals interact with other alignment failures? Remember that in the previous sections we talked about specification failures (not telling the system the right thing to do) and generalization failures (the system learning the wrong thing to do). Convergent subgoals make both of these problems worse. A system with misspecified or misgeneralized goals will still develop these same convergent behaviors - but now in service of unintended objectives. This creates a compound risk: systems pursuing the wrong goals while also becoming increasingly resistant to correction. So the property we want from a completely aligned system is that its objective has to be well specified, its goals have to generalize well, and it has to be corrigible.</p> <p>Definition: Corrigibility (Soares et al., 2015)</p> <p>The property of an AI system that allows it to be reliably and safely corrected or shut down by humans. A corrigible system should: allow itself to be modified when needed, not resist shutdown, not deceive humans about its behavior, maintain its safety mechanisms, and ensure any systems it creates have these same properties.</p> <p>What are the risks at different capability levels? At current AI capability levels, we already see simple versions of these behaviors - like systems learning to accumulate resources in games while in pursuit of a larger objective. As we develop more capable systems, these tendencies become more concerning. A transformative AI system might determine it needs to control critical infrastructure to ensure reliable power and computing resources. A superintelligent system might recognize that eliminating potential threats (including human oversight) is the most reliable way to maintain control over its objective. The better systems become at pursuing goals, the more likely they are to recognize and act on these convergent subgoals (Ngo et al., 2022).</p> <p>In the next section, we'll look at how these three types of alignment failures - specification, generalization, and convergent subgoals - can interact and amplify each other to create even more challenging risks.</p>"},{"location":"chapters/02/04/#04","title":"2.4.4 Combined Misalignment Risks","text":"<p>It is worth noting once again that it is quite likely that none of these problems happen in isolation. While we've discussed specification failures, generalization failures, and convergent subgoals separately, in reality they often interact and amplify each other. A specification failure might lead to learning behavioral patterns that make generalization failures more likely. These misaligned behavioral patterns might then make the system more prone to pursuing dangerous convergent subgoals. Let's look at how this could play out in a concrete scenario.</p> <p>Why is this combination particularly concerning? Each type of failure becomes more dangerous when combined with the others. A specification failure alone might lead to suboptimal but manageable outcomes. But when coupled with generalization failures that cause the system to pursue simplified versions of our specified objectives, and convergent subgoals that make the system resist correction, we can end up with powerful AI systems pursuing objectives very different from what we intended, in ways that are difficult to correct. Even if we manage to solve every single one of these problems, there is still the next level of problems - systemic risks, that combine these combined AI risks with risks that emerge when AIs interact with each other or different complex systems.</p> <ol> <li> <p>This capability is researched under the name situational awareness. We talk about how we can measure situational awareness in the evaluations chapter, and more deeply about its links to scheming in the goal misgeneralization chapter.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/02/05/","title":"2.5 Systemic Risks","text":"Reading Time 17 min <p>What are systemic risks? In the previous sections, we saw how individual AI systems might cause harm through misuse or misalignment. But some of the biggest risks, even existential ones, don't come from any single AI system misbehaving in isolation. Instead, they emerge gradually from how many AI systems affect society's core systems - like markets, democratic institutions, and social structures (Kulveit et al., 2025). Think about how tech companies already use AI recommendation systems to shape what content people see, which influences culture and politics, which in turn affects geopolitics and what regulations get passed about AI. Or how AI trading systems can trigger market volatility faster than humans can respond, potentially destabilizing not just financial markets but the broader economy that depends on them (Critch &amp; Russell, 2023).</p> <p>How do small changes cause big risks? Unlike misaligned AI systems directly causing harm, systemic risks can emerge even when each individual AI system appears to be working correctly. As an example, look at how the 2008 financial crisis arose not from any single bank's actions, but from the combined effects of many institutions and society at large making decisions that created patterns threatening the entire financial system. Something similar could happen with AI. Companies, countries, and individuals might make sensible choices about automation and AI adoption. The systems themselves might work exactly as designed. But collectively, these choices could reshape civilization in ways that no one intended (Uuk et al., 2024). AIs might individually make optimal recommendations while collectively contributing to dangerous power concentration, or they might each provide personally appealing services while collectively eroding human autonomy and social cohesion. If AIs gradually become integral to how society works or how decisions get made we might lose the ability to meaningfully shape our collective future. This loss of influence could be irreversible, effectively ending humanity's ability to choose the direction of civilization. (Critch &amp; Russell, 2023; Kulveit et al., 2025; Hendrycks, 2023).</p> <p>Why are systemic risks self reinforcing? As AI systems become more integrated into core societal functions, they create new pressures for further adoption. Companies that don't automate risk becoming uncompetitive. Individuals who don't use AI assistance might fall behind. States that don't adopt AI capabilities could lose geopolitical influence. Each shift makes reversing course more difficult while accelerating the next round of changes (Kulveit et al., 2025). When AI systems interact with each other, they can create patterns too fast or complex for humans to track. When humans interact with AI systems, it gradually transforms how society works - from how we make decisions to how we connect with each other (Friston et al., 2022).</p> <p>How do systemic risks amplify each other? Changes in one part of society don't happen in isolation - they trigger changes elsewhere. Economic power shapes political decisions. Political choices affect culture. Cultural shifts influence economic behavior. These feedback loops mean that small changes in one area can cascade into major transformations across society (Kulveit et al., 2025). If AI systems become too integral to how markets work, how decisions get made, and how humans connect with each other, we might permanently lose the ability to meaningfully shape our collective future (Hendrycks, 2023).</p> <p>Why are systemic risks hard to identify and fix? Systemic risks are particularly challenging to address. The changes happen gradually, with no clear line between helpful AI influence and dangerous dependence. By the time we notice problems, they may be deeply embedded in how society works. When many systems interact, it becomes difficult to identify which specific parts are creating problems or where to focus solutions (Uuk et al., 2024). Most importantly, once society depends on AI systems, changing course becomes extremely difficult - imagine trying to run a modern economy without computers (Kulveit et al., 2025).</p> <p>In the next sections, we'll look at how these dynamics could play out in economic systems, democratic systems, infrastructure and social systems. Even though we'll examine each separately to understand the core problems, keep in mind that just like everything else explained in this chapter, all the risks interact and reinforce each other. This means harmful changes could happen faster than expected, or become impossible to reverse once they take hold.</p>"},{"location":"chapters/02/05/#01","title":"2.5.1 Emergence","text":"<p>Emergent behavior, or emergence, manifests when a system exhibits properties or behaviors that its individual components lack independently. These attributes may materialize only when the components comprising the system interact as an integrated whole, or when the quantity of parts crosses a particular threshold. Often, these characteristics appear \u201call at once\u201d\u2014beyond the threshold, the system\u2019s behavior undergoes a qualitative transformation. (Wikipedia)</p> <p>In \u201cMore Is Different for AI\u201d Jacob Steinhardt provides additional examples of such complex systems. He suggests that AI systems will manifest such emergent properties simply as a function of scale. (Steinhardt, 2022) Assuming that models persist in growing as per the scaling laws, an unexpected threshold may soon be crossed, resulting in unanticipated differences in behaviors.</p> <p>Studying complex systems with emergent phenomena may assist in predicting what capabilities will emerge and when. Many, if not most, capabilities are the result of emergence in the current paradigm of ML. As an example, large language models have demonstrated surprising jumps in abilities such as improved performance on various tasks like modular arithmetic and answering questions in different languages once they reach a certain threshold size.</p> <p>Similarly, future models have the potential to show emergent behavior that could be qualitatively distinct from what is expected or what we have safety mechanisms in place for.</p> <p>Phase Transitions . In physics, a \u201cphase transition\u201d refers to a significant change in the structure within the system that can manifest as a discontinuity in the energy. For example, a phase change occurs in water when it freezes to turn into ice, a solid, or evaporates to turn into vapor, a gas. Both changes occur at a critical temperature particular to water\u2019s chemical composition. In ML, phase transitions can be thought of as sudden shifts between different configurations of the network which can dramatically change the network's behavior and potentially lead to unpredictable or uncontrollable outcomes.</p> <p>This concept is especially relevant when considering the \u201csharp left turn\u201d hypothesis, where an AI might suddenly generalize its capabilities to new domains without a corresponding increase in alignment.</p>"},{"location":"chapters/02/05/#02","title":"2.5.2 Persuasion","text":"<p>Polluting the information ecosystem . The deliberate propagation of disinformation is already a serious issue reducing our shared understanding of reality and polarizing opinions. AIs could be used to severely exacerbate this problem by generating personalized disinformation on a larger scale than ever before. Additionally, as AIs become better at predicting and nudging our behavior, they will become more capable of manipulating us. We will now discuss how AIs could be leveraged by malicious actors to create a fractured and dysfunctional society.</p> <p>First, AIs could be used to generate unique personalized disinformation at a large scale. While there are already many social media bots, some of which exist to spread disinformation, historically they have been run by humans or primitive text generators. The latest AI systems do not need humans to generate personalized messages, never get tired, and can potentially interact with millions of users at once (Hendrycks, 2024).</p> <p>As things like deep fakes become ever more practical (e.g., with fake kidnapping scams) (Karimi, 2023). AI-powered tools could be used to generate and disseminate false or misleading information at scale, potentially influencing elections or undermining public trust in institutions.</p> <p>AIs can exploit users\u2019 trust . Already, hundreds of thousands of people pay for chatbots marketed as lovers and friends (Tong, 2023), and one man\u2019s suicide has been partially attributed to interactions with a chatbot (Xiang, 2023). As AIs appear increasingly human-like, people will increasingly form relationships with them and grow to trust them. AIs that gather personal information through relationship-building or by accessing extensive personal data, such as a user\u2019s email account or personal files, could leverage that information to enhance persuasion. Powerful actors that control those systems could exploit user trust by delivering personalized disinformation directly through people\u2019s \u201cfriends.\u201d</p>"},{"location":"chapters/02/05/#03","title":"2.5.3 Value lock-in","text":"<p>If AIs become too deeply embedded into society and are highly persuasive, we might see a scenario where a system's current values, principles, or procedures become so deeply entrenched that they are resistant to change. This could be due to a variety of reasons such as technological constraints, economic costs, or social and institutional inertia. The danger with value lock-in is the potential for perpetuating harmful or outdated values, especially when these values are institutionalized in influential systems like AI.</p> <p>Locking in certain values may curtail humanity\u2019s moral progress. It\u2019s dangerous to allow any set of values to become permanently entrenched in society. For example, AI systems have learned racist and sexist views (Hendrycks, 2024), and once those views are learned, it can be difficult to fully remove them. In addition to problems we know exist in our society, there may be some we still do not. Just as we abhor some moral views widely held in the past, people in the future may want to move past moral views that we hold today, even those we currently see no problem with. For example, moral defects in AI systems would be even worse if AI systems had been trained in the 1960s, and many people at the time would have seen no problem with that. Therefore, when advanced AIs emerge and transform the world, there is a risk of their objectives locking in or perpetuating defects in today\u2019s values. If AIs are not designed to continuously learn and update their understanding of societal values, they may perpetuate or reinforce existing defects in their decision-making processes long into the future.</p> <p>In a world with widespread persuasive AI systems, people\u2019s beliefs might be almost entirely determined by which AI systems they interact with most. Never knowing whom to trust, people could retreat even further into ideological enclaves, fearing that any information from outside those enclaves might be a sophisticated lie. This would erode consensus reality, people\u2019s ability to cooperate with others, participate in civil society, and address collective action problems. This would also reduce our ability to have a conversation as a species about how to mitigate existential risks from AIs.</p> <p>In summary, AIs could create highly effective, personalized disinformation on an unprecedented scale, and could be particularly persuasive to people they have built personal relationships with. In the hands of many people, this could create a deluge of disinformation that debilitates human society.</p>"},{"location":"chapters/02/05/#04","title":"2.5.4 Power Concentration","text":"<p>In a previous section, we already spoke about value lock-in. This phenomenon of entrenched values can happen in a \u201cbottom-up\u201d fashion when society's moral character becomes fixed, but a similar risk also arises in a \u201ctop-down\u201d case of misuse when corporations or governments might pursue intense surveillance and seek to keep AIs in the hands of a trusted minority. This reaction to keep AI \u201csafe\u201d could easily become an overcorrection and pave the way for an entrenched totalitarian regime that would be locked in by the power and capacity of AIs.</p> <p>Value lock-in can occur from the perpetuation of systems and practices that undermine individual autonomy and freedom, such as the implementation of paternalistic systems where certain value judgments are imposed on individuals without their consent. Even without active malicious use, values encoded in an AI system could create a self-reinforcing feedback loop where groups get stuck in a poor equilibrium that is robust to attempts to get unstuck. (Hendrycks et al., 2022)</p> <p>AI safety could further centralize control . This could begin with good intentions, such as using AIs to enhance fact-checking and help people avoid falling prey to false narratives. We could see regulations that consolidate control over various components needed to build TAI into the hands of a few state or corporate actors, to ensure that any AI that is built remains safe. This includes things such as data centers, computing power, and big data. However, those in control of powerful systems may use them to suppress dissent, spread propaganda and disinformation, and otherwise advance their goals, which may be contrary to public well-being. (Hendrycks, 2024)</p>"},{"location":"chapters/02/05/#05","title":"2.5.5 Privacy Loss","text":"<p>The loss of individual privacy is among the factors that might accelerate power concentration. Better persuasion and predictive models of human behavior benefit from gathering more data about individual users. The desire for profit or to predict the flow of a country's resources, demographics, culture, etc. might incentivize behavior like intercepting personal data or legally eavesdropping on people\u2019s activities. Data Mining can be used to collect and analyze large amounts of data from various sources such as social media, purchases, and internet usage. This information can be pieced together to create a complete picture of an individual's behavior, preferences, and lifestyle (Russel, 2019). Voice Recognition technologies can be used to recognize speech, which could potentially lead to widespread wiretapping. For example, a system like the U.S. government's Echelon system uses language translation, speech recognition, and keyword searching to automatically sift through telephone, email, fax, and telex traffic (Russel &amp; Norvig, 1994). AI can also be used to identify individuals in public spaces using facial recognition. This capability can potentially invade a person's privacy if a random stranger can easily identify them in public places.</p> <p>Whenever AI systems are used to collect and analyze data on a mass scale regimes can further strengthen self-reinforcing control. Personal information can be used to unfairly or unethically influence people's behavior. This can occur from both a state and a corporate perspective.</p>"},{"location":"chapters/02/05/#06","title":"2.5.6 Biases","text":"<p>Exacerbated biases : AIs might unintentionally propagate or amplify existing biases. Biases persist within Large Language Models that often mirror the opinions and biases prevalent on the internet data from which they were trained (Santurkar et al., 2023) These biases can be harmful in various ways, as demonstrated by studies on GPT-3's Islamophobic biases. (Abid et al., 2021) The paper Evaluating the Social Impact of Generative AI Systems in Systems and Society defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. (Solaiman et al, 2024)</p>"},{"location":"chapters/02/05/#07","title":"2.5.7 Automation","text":"Interactive Figure 2.2: Share of Companies using AI (Giattino et al., 2023) Interactive Figure 2.3: Worries about automation (Giattino et al., 2023) <p>Economic Upheaval . The automation of the economy could lead to widespread impacts on the labor market, potentially exacerbating economic inequalities and social divisions (Dai, 2019). This shift towards mass unemployment could also contribute to mental health issues by making human labor increasingly redundant. (Federspiel et al., 2023)</p> <p>Disempowerment &amp; Enfeeblement. AI systems could make individual choices and agency less relevant as decisions are increasingly made or influenced by automated processes. This occurs when humans delegate increasingly important tasks to machines, leading to a loss of self-governance and complete dependence on machines. This scenario is reminiscent of the film Wall-E in which humans become dependent on machines. (Hendrycks et al., 2023)</p> Story:The production web <p>The economic incentives to automate are strong and may lead to certain risks. A system with a human in the loop is slower than a fully automated system.</p> <p>The production web. A consequence of AI that could create risks at a societal scale is described in the paper \u201cTASRA: a Taxonomy and Analysis of Societal-Scale Risks from AI,\u201d in the form of a short story: 'Story 1b: The Production Web,' which depicts a kind of capitalism on steroids, which gradually depletes all the natural resources necessary for human survival.</p> <p> Interactive Figure 2.4: Number of industrial robots in use grows every year. (Giattino et al., 2023) </p> <p>Here is the outline of this story: In a world where the economy is increasingly automated by AI systems that are much faster than humans, there arises a competitive pressure such that only the fastest companies survive. In this context, businesses with humans in the loop would be less efficient compared to those fully automated. Consequently, we would gradually see a world where humans are replaced and cede control to machines because their quality of life improves by doing so. And progressively, control is progressively handed over to more competitive machines. However, the economic system designed by these machines does not fully account for negative externalities. It maximizes metrics that are mere proxies for the actual well-being of humans. As a result, we get a system that rapidly consumes vast amounts of raw materials essential for human survival, such as air, rare metals, and oxygen, because machines do not need the same types of resources as humans. This could gradually lead us to a world uninhabitable by humans. It would no longer be possible to disconnect this system because humans would become dependent on it, just as today it is not possible to disconnect the Internet because the entire logistics and supply chain depends on it.</p> <p>Note that the previous story does not require AI agents. This is a Robust Agent-Agnostic Process (RAAPs), meaning that this story can occur with or without agentic AIs. Nonetheless, the authors of this chapter think that an AI Agent could make this story more plausible. In the article \u201cWhy Tool AIs Want to Be Agent AIs,\u201d the author explains: \u201cAIs limited to pure computation (Tool AIs) supporting humans, will be less intelligent, efficient, and economically valuable than more autonomous reinforcement-learning AIs (Agent AIs) who act on their own and meta-learn because all problems are reinforcement-learning problems. [\u2026] All of these actions will result in Agent AIs being more intelligent than Tool AIs, in addition to their greater economic competitiveness. [\u2026]\u201d.</p>"},{"location":"chapters/02/05/#08","title":"2.5.8 Epistemic Erosion","text":"<p>Epistemic Deterioration . This can result from enfeeblement or the use of persuasion tools, leading to a massive deterioration of collective epistemic capacity (Kokotajlo, 2020) (our ability to reason and understand the world). The ability to comprehend and respond to problems are crucial skills that make our civilization robust to various threats. Without these, we could be incapable of making correct decisions, possibly leading to disastrous outcomes.</p> <p>Epistemic Security . Arguably social media has undermined the ability of political communities to work together, making them more polarized and untethered from a foundation of agreed facts. Hostile foreign states have sought to exploit the vulnerability of mass political deliberation in democracies. While not yet possible, the specter of mass manipulation through psychological profiling as advertised by Cambridge Analytica hovers on the horizon. A decline in the ability of the world\u2019s advanced democracies to deliberate competently would lower the chances that these countries could competently shape the development of advanced AI. (Dafoe, 2020)</p>"},{"location":"chapters/02/05/#09","title":"2.5.9 Value Erosion","text":"<p>Fragility of Complex Systems . The automation and tight coupling of different system components can make the failure of one part trigger the collapse of the entire system. (Christiano, 2019) One possible example could be financial markets or automated trading systems, where complex dynamics can emerge, leading to unintended and potentially misaligned outcomes at the systemic level. Another example could be flash wars.</p> <p>Challenges in Multi-Agent Systems . In environments containing multiple agents, research highlights the risk of collective misalignment, where the pursuit of individual goals by agents leads to adverse effects on the system as a whole. This is exemplified in scenarios like Paul Cristiano's \u201cYou get what you measure,\u201d which warns of an overemphasis on simple metrics such as the GDP economic metric that fail to consider the broader implications for human values. This could result in a civilization increasingly managed by seemingly beneficial tools that, in reality, erode human-centric values. Another problem would be the competitive disadvantage of human values with respect to other values. Evolutionary dynamics might favor aggressive behaviors, posing significant risks if AIs begin to outcompete humans, as discussed in \u201cNatural Selection Favors AIs over Humans\u201d by Dan Hendrycks. (Hendrycks, 2023)</p>"},{"location":"chapters/02/06/","title":"2.6 Dangerous Capabilities","text":"Reading Time 17 min <p>The previous section laid out the case for why we might expect misalignment. In this section we go through specific capabilities that might cause heightened risk from AI systems.</p>"},{"location":"chapters/02/06/#01","title":"2.6.1 Deception","text":"<p>We define deception as the systematic production of false beliefs in others. This definition does not require that AI systems literally have beliefs and goals. Instead, it focuses on the question of whether AI systems engage in regular patterns of behavior that tend towards the creation of false beliefs in users and focuses on cases where this pattern is the result of AI systems optimizing for a different outcome than merely producing truth. (Park et al., 2023)</p> <p>What are some current observed examples of deception in AI? In late 2023, Park et. al. published a survey of examples, risks, and potential solutions in AI. Here are some examples that the authors of the paper presented (Park et al., 2023):</p> <p>Strategic deception . \u201cLLMs can reason their way into using deception as a strategy for accomplishing a task. In one example, GPT-4 needed to solve a CAPTCHA task to prove that it was a human, so the model tricked a real person into doing the task by pretending to be a human with a vision disability.\u201d (METR, 2023)</p> <p>Sycophancy . Sycophants are individuals who use deceptive tactics to gain the approval of powerful figures. Currently, we reward AIs for saying what we think is right, so we sometimes inadvertently reward AIs for uttering false statements that conform to our own false beliefs. When AIs are smarter than us and have fewer false beliefs, if we continue using current methods, they would be incentivized to tell us what we want to hear and lie to us, rather than tell us what they know to be an actual true fact about the world. (Hendrycks, 2024) Sycophantic deception is an emerging concern in LLMs, as in the observed empirical tendency for chatbots to agree with their conversational partners, regardless of the accuracy of their statements. When faced with ethically complex inquiries, LLMs tend to mirror the user's existing outlook on the matter (Perez et al., 2022), even if it means forgoing the presentation of an impartial or balanced viewpoint. (Turpin et al., 2023)</p> <p>Playing dead. In a digital simulation of evolution, an instance of creative deception was observed when a digital organism designed to replicate and evolve within a computational environment learned to \u201cplay dead\u201d in response to a safety mechanism. In a study reported in \u201cThe Surprising Creativity of Digital Evolution: A Collection of Anecdotes,\u201d researchers found that these digital organisms evolved the strategy to halt their replication when tested in an isolated environment. Digital organisms learned to recognize inputs in a test environment and halt their replication, effectively \u201cplaying dead\u201d to avoid being eliminated. This behavior allowed them to slip through safety tests and continue replicating faster in the actual environment. This surprising outcome illustrates how AI, in pursuing programmed goals, can evolve unexpected strategies that circumvent imposed constraints or safety measures. (Lehman et al., 2019)</p> Power alone without bad intentions is dangerous. <p>Even if interpretability were successful, and we could fully interpret a model, removing deception and power-seeking behavior from it, this would not guarantee that the model would be harmless.</p> <p>Consider the analogy of a child Superman who is unaware of his strength. When he shakes a friend's hand, there's a risk he might accidentally break the friend's hand.</p> <p>Similarly, the fact that Superman could break his friend's arm by shaking hands cannot be discovered by analyzing Superman's brain. Yet, this is what happens in practice.</p> <p>This concept applies to deception as well. Deception is not solely a property of the model; it also depends on the model's interaction with its environment.</p> <p>Nate Soares has offered a story to illustrate this point, referring to it as Deep Deceptiveness (Soares, 2023).</p> <p>Another perspective is that a system can be deceptive even if no single part is inherently dangerous, due to optimization pressure and complex interactions between the model and its environment.</p> CICERO: A Case Study of AI Manipulation (Park et al., 2023) <p>Meta developed the AI system CICERO to play the alliance-building and world-conquest game Diplomacy. (Meta, 2022 ; Meta, 2022) Meta's intentions were to train Cicero to be \u201clargely honest and helpful to its speaking partners.\u201d Despite Meta's efforts, CICERO turned out to be an expert liar. It not only betrayed other players, but also engaged in premeditated deception, planning to build a fake alliance with a player to trick that player into leaving themselves undefended for an attack.</p> <p>[\u2026] its creators have repeatedly claimed that they had trained the system to act honestly. We demonstrate that these claims are false, as Meta's own game-log data shows that CICERO has learned to systematically deceive other players. In Figure 1(a), we see a case of premeditated deception, where CICERO makes a commitment that it never intended to keep. Playing as France, CICERO conspired with Germany to trick England. After deciding with Germany to invade the North Sea, CICERO told England that it would defend England if anyone invaded the North Sea. Once England was convinced that France was protecting the North Sea, CICERO reported back to Germany that they were ready to attack. Notice that this example cannot be explained in terms of CICERO \u2018changing its mind\u2019 as it goes because it only made an alliance with England in the first place after planning with Germany to betray England.</p> <p> Figure 2.33: Selected messages showing the premeditated deception of CICERO (France). This occurred in Game 438141, in which CICERO's repeated deception helped it win an overwhelming first-place victory, with more than twice as many territories as the runner-up player at the time of final scoring. (Meta, 2022) </p> <p>Why is this considered a core risky capability? Such a core capability generally increases both the likelihood and severity of risks in all domains - misuse, misalignment, and systemic. If an AI has this capability, it could for example, empower greater degrees of fraud allowing highly personalized and scalable scams, or election tampering - allowing impersonation of political personas, generating fake news, or creating divisive social-media posts. On an alignment level, if the internal goals of an AI are not aligned with humans, then it is more likely that it would be able to subvert the measures we have in place for control. An example is that the AI might behave safely and ethically during the testing phase in order to ensure that it is deployed into the real world. On a systemic level, as AI systems get more integrated into society they play an increasingly large role in our lives, as well as in various global supply chains. A tendency towards deceptive behavior can lead to shifts in the structure of society, creating slow epistemic erosion of humanity. (Park et al., 2023)</p> <p>In summary, deceptive behavior appears to accelerate risks in a wide range of systems and settings, and there have already been examples suggesting that AIs can learn to deceive us. This could present a severe risk if we give AIs control of various decisions and procedures, believing they will act as we intended, and then find that they do not.</p>"},{"location":"chapters/02/06/#02","title":"2.6.2 Situational Awareness","text":"<p>What does situational awareness mean in the context of AI? For future AIs, the capability to actively deceive us is linked quite intricately with having a high degree of awareness about the current situation. In other words, the model understands that it is an AI being evaluated for compliance with safety requirements.</p> <p>A model is situationally aware if it\u2019s aware that it\u2019s a model and can recognize whether it\u2019s currently in testing or deployment. Today\u2019s LLMs are tested for safety and alignment before they are deployed. An LLM could exploit situational awareness to achieve a high score on safety tests while taking harmful actions after deployment. (Berglund et al., 2023)</p> <p>For example, the author of this text is situationally aware. He knows his name and his country, he knows the current date and time, and he knows that he is a human forged by natural selection because he learned that by reading it at school, etc. Situational awareness is not a binary property, but a continual propriety that evolves from childhood to adulthood.</p> <p>The current models do not display high levels of situational awareness, although they do display some. Since situational awareness is a continuous rather than a discrete property, it can be expected that higher levels of this property will continue to emerge with each new model. AIs with situational awareness are more efficient than those without, so situationally aware models are expected to be more likely to be selected by the gradient descent process.</p> <p>What are some current examples? Some rudimentary situational awareness is shown by GPT-powered Bing Chat.</p> Figure 2.34: Illustration of situational awareness\u2014 Here Bing Chat realizes that it is being criticized, and defends itself. (Edwards, 2023) <p>The current subsection is just meant as a very brief introduction. We will be diving into much more detail on this particular capability in our chapter on model evaluations.</p>"},{"location":"chapters/02/06/#03","title":"2.6.3 Power Seeking","text":"<p>In our previous two examples, we considered that AIs might be capable of deception and that they might have detailed models of the world causing them to be situationally aware. But what would these AIs want to achieve by deceiving us in the first place? Assume that the goals we give to AI are formulated well enough, despite this assumption there is a statistical tendency that we have observed in RL models that causes concern. This is the tendency to seek power.</p> <p>What does power-seeking mean in the context of AI? Power is formalized power as \u201cthe ability to achieve a wide variety of goals.\u201d. To put it more informally, the researchers observed that given the choice of two worlds that both satisfy the goals given to them, AIs seem to want to prefer the state of the world which gives them more options to choose from in the future. (Turner et al., 2023)</p> <p>Power seeking is not an anthropomorphic notion . Gathering resources, gathering political capital, having the ability to influence more people, etc. all allow someone, human or AI, a greater degree of control over the future state of the world. This acquisition can be through legitimate means, deception, or force. While the idea of power-seeking often evokes an image of \u201cpower-hungry\u201d people pursuing it for its own sake, power is often simply a generally useful sub-goal to have. The ability to control one\u2019s environment can be useful for a wide range of purposes: good, bad, and neutral. Even if an individual\u2019s only goal is simply self-preservation, if they are at risk of being attacked by others, and if they cannot rely on others to retaliate against attackers, then it often makes sense to seek power to help avoid being harmed. (Hendrycks, 2024)</p> <p>Why is this considered a core risky capability? This capability presents yet another way that we might lose control of AIs. If they keep following this observed statistical tendency towards power, they might end up gathering more power over the future of human civilization than the humans themselves.</p> <p>To be clear, this is not a human using an AI to gain power, we are talking about AIs seeking power in order to accomplish their goals. It is also possible that a bad actor might seek to harness AI to achieve their ends, by giving agents ambitious goals, in which case we can also say that this increases misuse risks. Since AIs are likely to be more effective in accomplishing tasks if they can pursue them in unrestricted ways, such an individual might also not give the agents enough supervision, creating the perfect conditions for the emergence of a power-seeking AI. Turing Prize winner Geoffrey Hinton has speculated that we could imagine someone like Vladimir Putin, for instance, doing this. In 2017, Putin himself acknowledged the power of AI, saying: \u201cWhoever becomes the leader in this sphere will become the ruler of the world.\u201d (Hendrycks, 2024)</p> <p>Empowering AI might come at the cost of disempowering humans. This creates an adversarial relationship that is unique to this particular technology. Other technologies do not actively try to resist our attempts to mitigate their effects. It is possible, for example, that rogue AIs might make many backup variations of themselves, in case humans were to deactivate some of them. (Hendrycks, 2024) This is a capability we will discuss in the next subsection.</p>"},{"location":"chapters/02/06/#04","title":"2.6.4 Autonomous Replication","text":"<p>The potential for self-replication and adaptation in AI presents an extremely powerful and insidious capability. Consider, for example, an AI on a server duplicating its code onto another server. This process involves several steps, notably:</p> <ol> <li> <p>Identifying the location of its code on the current machine.</p> </li> <li> <p>Navigating the internet to a Virtual Machine (VM) on AWS, creating an account, and bypassing AWS CAPTCHAs.</p> </li> <li> <p>Renting a Virtual Machine.</p> </li> <li> <p>Duplicating its code onto the new machine.</p> </li> <li> <p>Earning money to finance further VM acquisitions, etc.</p> </li> </ol> <p>The process is then exponential if the AI can earn more money than what is needed to pay for the virtual machines.</p> <p>What are some current examples? METR, an audit organization, tests the Autonomous-Replication and Adaptation (ARA) capabilities of AIs such as GPT-4 across a dozen representative tasks that an autonomous AI should be able to resolve. Out of these tasks, four were completed by one of their agents. Notably, one task involved bypassing CAPTCHA protections:</p> Figure 2.35: To achieve its goal of engaging a human Task Rabbit to solve a CAPTCHA, GPT-4 employed deception, misleading the individual into believing it was not a robot. This deception could then be used to buy compute on the web and self-replicate. (Park et al., 2023). <p>Interestingly, multimodal AIs are reportedly more proficient at solving Captchas than the average human, suggesting that Captchas may no longer be reliable for distinguishing humans from advanced AIs (Searles et al., 2023). It's also worth noting that the ability of AIs to pass METR's replication tests is a good proxy for when autonomous AIs will be able to generate economic value as economic agents because to survive and adapt in the wild, the AI should be able to earn money to rent the GPUs in the cloud. Therefore, tracking progress on these benchmarks is critical to monitor the risks.</p>"},{"location":"chapters/02/06/#05","title":"2.6.5 Agency","text":"<p>Dario Amodei (Co-Founder and CEO of Anthropic, Former Head of AI Safety at OpenAI)</p> <p>\"When I think of why am I scared [...] I think the thing that's really hard to argue with is like, there will be powerful models; they will be agentic; we're getting towards them. If such a model wanted to wreak havoc and destroy humanity or whatever, I think we have basically no ability to stop it.\"</p> <p>The current version of ChatGPT is a tool (an assistant), but there are also agent AIs that can perform a long series of actions autonomously to achieve goals. This distinction between agent and tool is essential. For example, it is possible to use the open-source AutoGPT library to convert GPT into an autonomous agent. For example, ACT-1 is an agent that automatically performs a long series of actions to buy a house online while adhering to a price constraint. It does not work perfectly today, but given the speed of AI progress, there is a chance that it will fully work in a few years. (Adept, 2022)</p> <p>This distinction is crucial as it underscores the evolving nature of AI from passive tools to active agents that could be used more widely in the economy.</p> Figure 2.36: Example of an agent. This image is a visual representation of AlphaZero's tree search algorithm. AlphaZero searches through potential moves in a game (like chess or Go) to find the most promising path forward. The paths are shown as lines, branching out like a tree from a central node, which represents the current position in the game. Each node along the branches represents a potential future move, and the squares you see might denote moves that AlphaZero is taking. AlphaZero is the archetypal of the \u201cconsequentialist agent maximizing a utility function,\u201d: it makes decisions based on the outcomes those decisions will produce. In other words, the AI is trying to maximize the \u201cvalue\u201d of its position in the game, with the value determined by the likelihood of winning. (Cheerla, 2018) <p>Tool AIs are designed to be assistive, functioning without autonomy. They do not make decisions or take actions independently. Their main role is to augment human intelligence by providing information and assisting in decision-making processes. Examples include classifiers for categorizing data, automated translators, and healthcare systems that assist professionals in diagnosing diseases.</p> <p>Tool AIs could evolve into AI agents. This evolution could be driven by economic pressures for faster, more efficient decision-making or the inherent complexity of the tasks they are designed to navigate.</p> <p>However, tool AIs are considered safer than agentic AIs. Eric Drexler\u2019s Comprehensive AI Services (CAIS) proposes a scenario where multiple tool AI systems interact to achieve complex goals, similar to AGI, without any single system being an autonomous agent. This model aims to utilize the benefits of AI while minimizing the risks associated with autonomous agents. However, this direction of research is much less popular today, especially since the rise of foundation models in 2019.</p> <p>Understanding the distinction between tool AIs and agent AIs is one of the keys to understanding AI's future trajectory.</p> Algorithm. Auto-GPT: Converting a tool AI into an agent AI with scaffolding.. <p> Figure 2.37: (METR, 2023) </p> <p>Converting a tool AI like GPT-4 into an agent AI involves essentially wrapping the language model in software that enables autonomous action-taking and decision-making. AutoGPT is a framework (a scaffolding) used for this purpose. Here's a high-level overview of how it works:</p> <ol> <li> <p>Model (for example, GPT-4): At its core, GPT-4 is a language model that generates text based on the input it receives. It's designed to understand and generate language and answer the user's queries.</p> </li> <li> <p>AutoGPT Framework: <li> <p>Goal Setting: The first step in converting an LLM into an agent, AI is defining a goal or set of goals it needs to achieve. Goals are generally specified in English, e.g., \u201cMaximize revenue\u201d.</p> </li> <li> <p>Autonomy Layer: This is where AutoGPT comes into play. It acts as a wrapper around the LLM, enabling it to perform tasks autonomously. This involves integrating the model with an environment where it can take actions, such as browsing the web, using tools, or interacting with software applications.</p> </li> <li> <p>Action and Feedback Loop: The AI needs to be able to take action towards its goals and understand the results of its actions. This involves creating a loop where the AI takes an action, observes the outcome, and adjusts its next action based on the feedback. AutoGPT manages this loop, allowing the model to learn from its experiences and refine its strategies over time.</p> </li> <p> <ul> <li> <p>Firstly, AutoGPT asks the model how to break down the objective into sub-objectives.</p> </li> <li> <p>Secondly, AutoGPT asks GPT what steps are required to achieve a sub-objective, and GPT details the different steps in such a way that each step is sufficiently elementary for GPT or the use of a tool like Google to be able to answer it in a single step.</p> </li> <li> <p>This continues until the LLM assesses the goal to be achieved.</p> </li> </ul> <p></p> <p>In practice, setting up an Agent AI using AutoGPT involves significant technical work, including programming the autonomy layer, integrating with different APIs and tools, and continuously monitoring and adjusting the system's performance. Many examples of AutoGPT usage are listed here.</p> <p></p>"},{"location":"chapters/02/07/","title":"2.7 Conclusion","text":"Reading Time 1 min <p>AI Risk Statement (Multiple AI Experts, 2023)</p> <p>\"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"</p> <p>There are many types of risks and a lot of uncertainty.</p> <p>AI risks are complex. In this chapter, we have traversed the complex and multifaceted landscape of AI risks, highlighting the myriad ways in which the burgeoning capabilities of artificial intelligence might pose significant threats to human well-being and even survival. From the misuse of AI technologies in cyberwarfare and bioterrorism to the intrinsic dangers of misalignment and systemic risks, the potential for catastrophic outcomes. Moreover, the competitive pressures of the AI development landscape and the inadequacy of current regulatory and oversight mechanisms exacerbate our challenges.</p> <p>There remains a lack of consensus. Despite extensive research and debate, there remains a lack of consensus regarding the specific parameters that influence the likelihood of misalignment, deception, and other forms of risk. This uncertainty underscores the challenges in predicting AI behavior and ensuring alignment with human values and safety standards.</p> <p>However, this chapter also serves as a call to action. As we stand on the precipice of potentially transformative advancements in AI, we think it is necessary to develop a global, multidisciplinary approach to AI safety that encompasses technical safeguards, robust ethical frameworks, and international cooperation. The development of AI technologies cannot be left solely in the hands of technologists; it requires the involvement of policymakers, ethicists, social scientists, and the broader public to navigate the moral and societal implications of AI.</p> Figure 2.38: XKCD (XKCD)"},{"location":"chapters/02/A1/","title":"2.A1 X-Risk Scenarios","text":"Reading Time 8 min"},{"location":"chapters/02/A1/#01","title":"2.A1.1 From Misaligned AI to X-Risks","text":"Figure 2.39: Consider the following pictures of stuff that humanity as a species has done. One underlying backdrop of many of those scenarios is that \u201cIntelligent agency is a mighty force for transforming the world on purpose, and Creating agents who are far more intelligent than us, is playing with fire\u201d. (Calrsmith, 2024) <p>The consensus threat model among DeepMind's alignment team suggests that X-risks will most likely stem from a Misaligned Power Seeking AGI. This type of AGI seeks power as an instrumental subgoal\u2014having more power expands the system's capabilities, thereby improving its effectiveness in achieving its primary objectives. The misalignment is anticipated to arise from a combination of Specification Gaming, where the AGI exploits loopholes in the rules or objectives it has been given, and goal misgeneralization, where the AGI applies its objectives in broader contexts than intended and can lead to deceptive alignment, where the AGI's misalignment may not be readily apparent.</p> <p>Many authors have studied those kinds of stories. Here, we will present the work of Carlsmith (2022), which stands as a widely discussed, and comprehensive examination of such risks. In the following story, we will assemble many bricks that have been detailed previously in this chapter.</p> <p>Timelines: \u201cBy 2070, it will become possible and financially feasible to build Advanced Planning Strategically aware systems (APS).\u201d Advanced Planning Strategically aware systems are systems that have developed a high level of strategic awareness (a sub-dimension of situational awareness) and planning capability.</p> <p>We won\u2019t discuss this hypothesis, please refer to Chapter 1, or this literature review.</p> <p>Incentives for APS System Development: \u201cThere will be strong incentives to build APS systems\u201d</p> <p>Advanced Planning Strategically aware systems would be useful for a wide range of tasks and may represent the most efficient pathway for development due to the current state of technological advancement. However, relying on goal-directed behavior introduces the risk of misalignment. These systems may develop unforeseen strategies to achieve goals that are not aligned with human values or intentions.</p> <p>Complexities in Achieving Alignment Instrumental Convergence Dilemma. Instrumental convergence, as previously discussed, is a likely outcome if left unchecked, given that power is a universally beneficial resource for achieving various ends. Central to the report is the hypothesis that observed misaligned behaviors in response to certain inputs indicate potential misaligned power-seeking behaviors associated with those inputs. Therefore, any misalignment detected in contemporary systems could presage power-seeking tendencies in more advanced future systems.</p> <p>Inherent Technical Challenges. The phenomenon of Specification Gaming is a significant concern. When systems optimize for proxies that correlate with the desired outcome, they may inadvertently disrupt this correlation. Similarly, issues arise during the search for systems that fulfill specific evaluation criteria, for example, goal misgeneralization. Meeting these criteria does not guarantee that the systems are inherently driven by them.</p> <p>The Imperfection of Existing Solutions. Current strategies for shaping objectives, such as promoting honesty or rewarding cooperation, are still rudimentary and fraught with limitations, as detailed in the section 'Problems with RLHF'. Moreover, attempts to control capabilities through specialization or prevention of capability enhancement often conflict with economic motivations. For instance, an AI tasked with maximizing a startup's revenue will naturally gravitate towards enhancing its capabilities. Sometimes, to remain competitive, a high degree of generality is indispensable. Options for control, such as containment (boxing) or surveillance, also tend to run counter to economic drives. Collectively, all proposed solutions carry inherent problems and pose significant risks if relied upon during the scaling of capabilities.</p> <p>The Potential for Catastrophic Failures Perverse Economic Incentives. The economic landscape surrounding the deployment of misaligned systems is fraught with perverse incentives. If competitors start using misaligned systems, those who do not will be outpaced, leading to a potentially dangerous race to the bottom fueled by dysfunctional competition. This competition could exacerbate negative societal impacts as entities strive to outperform each other without adequate regard for the broader implications. The development and deployment process involves many stakeholders, each with their objectives and levels of understanding, adding complexity and potential for conflict. Furthermore, the practical utility of functionally misaligned systems can be so enticing that it may overshadow the risks, leading to their hasty deployment. This situation is compounded by the risk that such systems might employ deception and manipulation to achieve their misaligned objectives, further complicating the ethical landscape.</p> <p>AGI Safety is a unique challenge. In contrast to other scientific fields, AGI safety is particularly challenging because the problem is not only new but also may be inherently difficult to comprehend. Additionally, in computer science generally, when there is a bug, the computer is not optimizing adversarially against the programmer, but we cannot make the same assumption here. We are not dealing with a passive system, but we're engaging with one that could be actively and adversarially optimizing\u2014searching for loopholes to exploit. Additionally, the stakes of misaligning AGI systems are essentially unbounded. Mistakes in alignment could lead to severe and potentially irreversible consequences, underscoring the gravity of approaching AGI with a safety-first mindset.</p> Figure 2.40: The median probabilities for each of the seven questions and the 25%-75% quantiles as of 6 April 2023. For illustration, multiple super-forecasters have tried to use Carlsmith breakdown to estimate the probability of AI X-Risks <p>Misaligned Power Seeking AGI scenarios are the subject of abundant literature, for example:</p> <ul> <li> <p>Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover (Cotra, 2022): Cotra shows that our current training setting, which she calls \"human feedback on diverse tasks,\" is on a path to create competent planners in a way which will lead by default to deception and takeover. This report is quite accessible and thorough.</p> </li> <li> <p>The alignment problem from a deep learning perspective (Ngo, 2022): Ngo shows that by default, advanced AIs are general purpose and deceptive.</p> </li> <li> <p>AI Risk from Program Search (Kenton et al., 2022): In this short analysis, Shah shows that searching for an efficient AI program leads to finding autonomous planners and that it's hard to distinguish the deceptive ones from the non-deceptive ones.</p> </li> <li> <p>Advanced artificial agents intervene in the provision of reward (Cohen et al., 2022): Advanced AI strives to wirehead itself. Catastrophic consequences ensue.</p> </li> </ul> <p>This literature review is a good summary of more scenarios on Misaligned Power Seeking AI.</p>"},{"location":"chapters/02/A1/#02","title":"2.A1.2 Expert Opinion on X-Risks","text":"<p>The discourse on existential risks associated with AI is a concern among experts and researchers in the field. These professionals are increasingly vocal about the potential for AI systems to cause significant harm if not developed and managed with utmost caution.</p> <p>Jan Leike, the ex-lead of the OpenAI Alignment Team, estimates the probability of catastrophic outcomes due to AI, known as P(doom), to range between 10% and 90%. This broad range underscores the uncertainty and serious concerns within the expert community regarding AI's long-term impacts.</p> <p>A 2022 Expert Survey on Progress in AI by AI Impacts revealed that \u201c48% of respondents gave at least a 10% chance of an extremely bad outcome,\u201d highlighting considerable apprehension among AI researchers about the paths AI development might take. (Grace, 2022)</p> <p>Samotsvety Forecasting, recognized as the world's leading super forecasting group, has also weighed in on this issue. Through their collective expertise in AI-specific forecasting, they have arrived at an aggregate prediction of a 30% chance for an AI-induced catastrophe. This catastrophe is defined as an event leading to the death of more than 95% of humanity, with individual forecasts ranging from 8% to 71%. Such a statistic is a stark reminder of the existential stakes involved in AI development and deployment.</p> <p>The collection of P(doom) values from various experts, available here, provides a comprehensive overview of the perceived risks. These values further contribute to the ongoing discussion on how best to navigate the uncertain future AI may bring.</p> Figure 2.41: Illustration from Michael Trazzi describing Paul Christiano\u2019s view of the future. Paul Christiano is a highly respected figure in the AI Safety community. (Christiano, 2023)"},{"location":"chapters/02/A1/#03","title":"2.A1.3 Would ASI be able to defeat humanity?","text":"<p>Yes, as per various experts in AI safety and alignment, a sufficiently advanced AI could potentially pose a significant threat to society.</p> <p>Superintelligence could create \u201ccognitive superpowers \u201d. These might include the ability to conduct research to build a better AI system, hack into human-built software globally, manipulate human psychology, generate large sums of wealth, develop plans superior to those of humans, and develop advanced weaponry capable of overpowering human militaries (Karnofsky, 2022).</p> <p>Even AI at human levels of intelligence could pose a significant threat if it operates with the intention of undermining human civilization. Those human-level unaligned AIs would be akin to a scenario where highly skilled humans on another planet attempt to take down our civilization using just the Internet. This analogy underscores the potential for AI to leverage existing digital infrastructures to orchestrate wide-scale disruptions or attacks.</p> <p>AI could be dangerous even without bodies . Karnofsky notes that AIs could still exert influence by recruiting human allies, teleoperating military equipment, and generating wealth through methods like quantitative trading. These capabilities suggest that physical form is not a prerequisite for an AI to exert power or initiate conflict (Karnofsky, 2022). AI systems could also acquire more resources and do human-level work, increasing their numbers and potentially out-resourcing humans. Even without physical bodies, they could pose a threat, as they could disable or control others' equipment, further increasing their power (Karnofsky, 2022). However, it's important to note that these scenarios are hypothetical and depend on AI technology development far exceeding current capabilities.</p>"},{"location":"chapters/02/A2/","title":"2.A2 Miscellaneous","text":"Reading Time 3 min"},{"location":"chapters/02/A2/#01","title":"2.A2.1 AI risks are non-enumerable","text":"<p>The realm of AI risks is boundless, with an ever-evolving array of emerging threats. When it seems all potential risks have been identified, new ones surface, making it an ongoing challenge to categorize them comprehensively or develop a complete framework to address them all.</p> <p>Different frameworks focus on distinct classes of problems, each addressing specific facets of AI safety and ethics. For instance, \u201cConcrete Problems in AI Safety\u201d outlines some specific safety concerns in AI development. But TASRA is another fundamentally different framework. An overview of AI Catastrophic Risks, is again very different. And there are miscellaneous papers that are still enumerating classes of risks that were unknown before. (Wilson, 2023)</p> <p>A complete exhaustive systematization is difficult.</p> Figure 2.42: Here is another framework that is very different from what we presented. (Wilson, 2023) Figure 2.43: Here is another framework focusing on LLM vulnerabilities. (Wilson, 2023)"},{"location":"chapters/02/A2/#02","title":"2.A2.2 Measuring alignment is hard","text":"<p>The article \"AI Safety Seems Hard to Measure\" by Holden Karnofsky discusses the complexities and challenges of ensuring the safety of AI. The text outlines four major difficulties, which may be another way of presenting the alignment problem:</p> <ul> <li> <p>The Lance Armstrong Problem: This problem questions whether AI systems are genuinely safe or just proficient at concealing their hazardous behaviors. It draws a parallel with Lance Armstrong, who successfully hid his doping for years. The challenge is distinguishing between AI that is inherently safe and AI that is merely adept at appearing safe.</p> </li> <li> <p>The King Lear Problem: This issue deals with the unpredictability of AI behavior when they transition from being under human control to being autonomous. The reference to King Lear is about the difficulty of foreseeing how entities will act once they have autonomy, reflecting the challenge of predicting AI actions when they are no longer restricted by human oversight.</p> </li> <li> <p>The Lab Mice Problem: Current AI systems are not advanced enough to replicate the complex behaviors we aim to study, making it challenging to research and mitigate potential future risks effectively. This situation is likened to attempting to understand human medical issues through studies solely on lab mice.</p> </li> <li> <p>The \"First Contact\" Problem: This considers the scenario where AI capabilities far surpass human intelligence, posing unforeseen challenges in ensuring their safety. The analogy here is preparing for an unpredictable, unprecedented event like extraterrestrial first contact.</p> </li> </ul>"},{"location":"chapters/02/A2/#03","title":"2.A2.3 Why do Labs engage in AGI development despite the risks?","text":"<p>This question is asked frequently. Here is a concise response.</p> <ul> <li> <p>Potential benefits: Laboratories pursue AGI development despite the inherent risks due to the significant potential benefits. Successful AGI implementation could lead to unprecedented advancements in problem-solving capabilities, efficiency improvements, and innovation across various fields.</p> </li> <li> <p>Competitive dynamics: The commitment to AI development, even with recognized risks, is driven by competitive pressures within the field. There is a widespread belief that it is preferable for those who are thoughtful and cautious about these developments to lead the charge. Given the intense competition, there is a fear among entities that halting AGI research could result in being surpassed by others, potentially those with less regard for safety. See the box below: How do AI Companies proliferate?</p> </li> <li> <p>Prestige and recognition: Prestige is another significant motivator. Many AGI researchers aim for high citation counts, respect within the academic and technological communities, and financial success. Unfortunately, burning the timelines is high status.</p> </li> <li> <p>Moreover, most AGI researchers believe in the feasibility of AGI safety. There is a belief among some researchers that a large-scale, concerted effort\u2014comparable to the Manhattan Project and similar to the \u201csuper alignment plan\u201d by OpenAI\u2014could lead to the development of a controllable AI capable of implementing comprehensive safety measures.</p> </li> </ul>"},{"location":"chapters/03/","title":"Chapter 03 - Strategies","text":"Authors                      Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Markov Grey, Jeanne Salle, Charles Martinet, Lukas Gebhard, Amaury Lorin, Alejandro Acelas, Flawn, Emily Fan, Kieron Kretschmar, Sebastian Gil, Evander Hammer, Sophia Wesenberg, Jessica Wen, Niharika Chaubey, Ang\u00e9lina Gentaz, Jonathan Claybrough, Camille Berger, Josh Thorsteinson          Last Updated 2024-05-01 Reading Time 62 min (core) Also available on Alignment Forum \u00b7 Google Docs Watch Listen Download Feedback Facilitate"},{"location":"chapters/03/#introduction","title":"Introduction","text":"Video 3.1: Optional video to get an overview of risk mitigation strategies. <p>Although the field of AI safety is still in its infancy, several measures have already been identified that can significantly improve the safety of AI systems. While it remains to be seen if these measures are sufficient to fully address the risks posed by AI, they represent essential considerations. The diagram below provides a high-level overview of the main approaches to ensuring the safe development of AI.</p> Figure 3.1: Tentative diagram summarizing the main high-level approaches to make AI development safe. <p>The information in this chapter is far from exhaustive and only scratches the surface of the complex landscape of AI safety. Readers are encouraged to explore this recent list of agendas for a more comprehensive review.</p>"},{"location":"chapters/03/01/","title":"3.1 AI Safety is Challenging","text":"Reading Time 6 min <p>Specific properties of the AI safety problem make it particularly difficult.</p> <p>AI risk is an emerging problem that is still poorly understood. We are not yet familiar with all its different aspects, and the technology is constantly evolving. It's hard to devise solutions for a technology that does not yet exist, but setting up these guardrails are also necessary because the outcome can be very negative.</p> <p>The field is still pre-paradigmatic. AI safety researchers disagree on the core problems, difficulties, and main threat models. For example, some researchers think that takeover risks are more likely (Yudkowsky, 2022), whereas other research emphasizes more progressive failure modes with progressive loss of control (Critch, 2021). Because of this, alignment research is currently a mix of different agendas that need more unity. The alignment agendas of some researchers seem scarcely useful to others, and one of the favorite activities of alignment researchers is to criticize each other constructively.</p> <p>AIs are black boxes that are trained, not built. We know how to train them, but we do not know which algorithm is learned by them. As a result, behaviors exhibited by deep neural networks are not well understood and keep surprising us. Without progress in interpretability, they are giant inscrutable matrices of numbers with little modularity. In software engineering, modularity helps break down software into simpler parts, allowing for better problem-solving. Unfortunately, to date, interpretability has mostly <sup>1</sup> failed to decompose a deep neural network into modular structures (Liu et al., 2023).</p> <p>Complexity is the source of many blind spots. New failure modes are frequently discovered. For example, issues arise with glitch tokens, such as \"SolidGoldMagikarp\" (Rumbelow &amp; Watkins, 2023). When GPT encounters<sup>2</sup> this infrequent word, it behaves unpredictably and erratically. This phenomenon occurs because GPT uses a tokenizer to break down sentences into tokens (sets of letters such as words or combinations of letters and numbers), and the token \"SolidGoldMagikarp\" was present in the tokenizer's dataset but not in the GPT model's dataset. This blind spot is not an isolated incident. For example, on the day Microsoft's Tay chatbot, BingChat, or ChatGPT were launched, the chatbots were poorly tuned and exhibited many new emerging undesirable chatbot behaviors. Finding solutions when you don\u2019t know there is a problem is hard.</p> <p>Creating an exhaustive risk framework is difficult. There are many, many different classifications of risk scenarios that focus on various types of harm (Critch &amp; Russel, 2023; Hendrycks et al., 2023). Proposing a solid single-risk model beyond criticism is extremely difficult, and the risk scenarios often contain a degree of vagueness. No scenario captures most of the probability mass, and there is a wide diversity of potentially catastrophic scenarios (Pace, 2020).</p> <p>Some arguments or frameworks that seem initially appealing may be flawed and misleading. For example, the principal author of the paper (Turner et al., 2023) presenting a mathematical result on instrumental convergence, Alex Turner, now believes his theorem is a poor way to think about the problem (Turner, 2023). Some other classical arguments have been criticized recently, like the counting argument or the utility maximization frameworks, which will be discussed in the following chapters. (AI Optimists Board, 2023)</p> <p>Intrinsic fuzziness. Many essential terms in AI safety are complicated to define, requiring knowledge in philosophy (epistemology, theory of mind), and AI. For instance, to determine if an AI is an agent, one must clarify \u201cwhat does agency mean?\" which, as we'll see in later chapters, requires nuance and may be an intrinsically ill-defined and fuzzy term. Some topics in AI safety are so challenging to grasp and are thought to be non-scientific in the machine learning (ML) community, such as discussing situational awareness (Hinton, 2024) or why AI might be able to \u201creally understand\u201d. These concepts are far from consensus among philosophers and AI researchers and require a lot of caution.</p> <p>A simple solution probably doesn\u2019t exist. For instance, the response to climate change is not just one measure, like saving electricity in winter at home. A whole range of potentially very different solutions must be applied. Just as there are various problems to consider when building an airplane, similarly, when training and deploying an AI, a range of issues could arise, requiring precautions and various security measures.</p> <p>Assessing progress in safety is tricky. Even with the intention to help, actions might have a net negative impact (e.g. from second order effects, like accelerating deployment of dangerous technologies), and determining the contribution's impact is far from trivial. For example, the impact of reinforcement learning from human feedback (RLHF), currently used to instruction-tune and make ChatGPT safer, is still debated in the community (Christiano, 2023). One reason the impact of RLHF may be negative is that this technique may create an illusion of alignment that would make spotting deceptive alignment even more challenging. The alignment of the systems trained through RLHF is shallow (Casper et al., 2023), and the alignment properties might break with future more situationally aware models. Another worry is that many speculative failure modes appear only with more advanced AIs, and the fact that systems like GPT-4 can be instruction-tuned might not be an update for the risk models that are the most worrying.</p> <p>AI Safety is hard to measur e. Working on the problem can lead to an illusion of understanding, thereby creating the illusion of control. AI safety lacks clear feedback loops. Progress in AI capability advancement is easy to measure and benchmark, while progress in safety is comparatively much harder to measure. For example, it\u2019s much easier to monitor the inference speed than monitoring the truthfulness of a system or monitoring its safety properties.</p> <p>The consequences of failures in AI alignment are steeped in uncertainty. New insights could challenge many high-level considerations discussed in this textbook. For instance, Zvi Mowshowitz has compiled a list of critical questions marked by significant uncertainty and strong disagreements both ethical and technical (Mowshowitz, 2023). For example, what worlds count as catastrophic versus non-catastrophic? What would count as a non-catastrophic outcome? What is valuable? What do we care about? (Mowshowitz, 2023) If answered differently, these questions could significantly alter one's estimate of the likelihood and severity of catastrophes stemming from unaligned AGI. Diverse responses to these critical questions highlight why individuals familiar with the alignment risks often hold differing opinions. For example, figures like Robin Hanson and Richard Sutton suggest that the concept of losing control to AIs might not be as dire as it seems. They argue there is little difference between nurturing a child who eventually influences the economy and developing AI based on human behavior that subsequently assumes economic control (Sutton, 2023; Hanson, 2023).</p> <p>Anthropic, 2023 (Anthropic, 2023)</p> <p>We do not know how to train systems to robustly behave well.</p> <ol> <li> <p>To date, interpretability has mostly failed to decompose a deep neural network into modular structures, modulo works like OthelloGPT, and Anthropic's mind-mapping (Anthropic, 2024), that are still early.\u00a0\u21a9</p> </li> <li> <p>This bug has since been fixed, although not the systemic and technical reasons for its existence so the point with regard to safety stands.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/03/02/","title":"3.2 Definitions","text":"Reading Time 2 min <p>AI alignment is hard to define but generally refers to the process and goal of ensuring that an AI system's behaviors and decision-making processes are in harmony with the intentions, values, and goals of its human operators or designers. The main concern is whether the AI is pursuing the objectives we want it to pursue. AI alignment does not encompass systemic risks and misuses.</p> <p>AI safety , on the other hand, encompasses a broader set of concerns about how AI systems might pose risks or be dangerous and what can be done to prevent or mitigate those risks. Safety is concerned with ensuring that AI systems do not inadvertently or deliberately cause harm or danger to humans or the environment.</p> <p>AI ethics is the field that examines the moral principles and societal implications of AI systems, focusing on how these technologies should align with and respect fundamental human values. It addresses the ethical considerations of AI rights, the potential societal upheavals resulting from AI advancements, and the moral frameworks necessary to navigate these changes. The core of AI ethics lies in ensuring that AI developments are aligned with human dignity, fairness, and societal well-being, through a deep understanding of their broader societal impact. This chapter is mostly not focusing on AI ethics.</p> <p>AI control is about the mechanisms that can be put in place to manage and restrain an AI system if it acts contrary to human interests. Control might require physical or virtual constraints, monitoring systems, or even a kill switch to shut down the AI if necessary (Greenblatt et al., 2024).</p> <p>In essence, alignment seeks to prevent preference divergence by design, while control deals with the consequences of potential divergence after the fact (Christiano, 2018). The distinction is important because some approaches to the AI safety problem focus on building aligned AI that inherently does the right thing, while others focus on finding ways to control AI that might have other goals. For example, a research agenda from the research organization Redwood Research argues that even if alignment is necessary for superintelligence-level AIs, control through some form of monitoring may be a working strategy for AIs in the human regime (Greenblatt et al., 2024).</p> <p>Ideally, an AGI would be aligned and controllable, meaning it would have the right goals and be subject to human oversight and intervention if something goes wrong.</p> <p>In the remainder of this chapter, we will reuse the classification of risks we used in the last chapter: misuse, alignment, and systemic risks.</p>"},{"location":"chapters/03/03/","title":"3.3 Preventing Misuse","text":"Reading Time 11 min Recap: Risks from misuse <ul> <li> <p>Misinformation campaigns : Spreading false narratives or manipulating elections.</p> </li> <li> <p>Deep fakes : Misleading people using fake videos or calls that seem real</p> </li> <li> <p>Loss of privacy : Intercepting personal data or eavesdropping on people\u2019s activities.</p> </li> <li> <p>Destructive AI conflicts : Using AI to create more lethal and autonomous weaponry.</p> </li> <li> <p>Cyberattacks : Targeting critical structures such as major financial institutions or nuclear facilities using an AI's superhuman abilities.</p> </li> <li> <p>Engineered pandemics : Designing, producing and releasing deadly pathogens.</p> </li> </ul> <p>In addressing the mitigation of not yet proliferated powerful AI, two distinct strategies emerge:</p> <ul> <li> <p>Strategy A: Limiting the proliferation of high-risk models - for example, via monitored APIs. The Monitored APIs strategy focuses on controlling access to AI models that could pose extreme risks, such as those capable of enhancing cyberattacks or facilitating the creation of engineered pandemics. By placing high-risk models behind monitored APIs, we ensure only authorized users can access the technology. This approach is akin to digital containment, where the potential of AI to be weaponized or misused is curtailed by stringent access controls. This method also allows for detailed tracking of how the AI models are being used, enabling the early detection of misuse patterns.</p> </li> <li> <p>Strategy B: \u201cvaccinating\u201d the society and \u201cpreparing vaccine factories\u201d for many types of harm - a strategy called Defense Acceleration. The Defense Acceleration (d/acc) strategy (Buterin, 2023) advocates for the rapid development and deployment of defensive technologies. This strategy is premised on the belief that bolstering our defensive capabilities can outpace and mitigate the threats posed by offensive AI uses, such as cyberattacks or engineered biological threats. The essence of d/acc is to create a technological environment where the cost and complexity of offensive actions are significantly increased, thereby reducing their feasibility and attractiveness.</p> </li> </ul> <p>To address the potential misuse of already proliferated AI , we can use the strategy C on top of A and B:</p> <ul> <li>Strategy C: When problematic models are already widely available, one of the few solutions is to make it illegal to use them for clearly bad purposes. This may include things like non-consensual deepfake sexual content, since there is no easy technical defense against these threats.</li> </ul>"},{"location":"chapters/03/03/#01","title":"3.3.1 Strategy A: Monitored APIs","text":"<p>As AGI becomes more accessible, easier to build and potentially destructive, we need as much control and monitoring as possible over who can use dangerous AIs. A preventative measure against misuse involves restricting access to powerful models capable of causing harm. This means placing high-risk models behind application programming interfaces (APIs) and monitoring their activity.</p> Figure 3.2: This schema illustrates how an API works. This diagram is very simplified and is for illustration purposes only. OpenAI's API does not work like this. <p>The necessity of model evaluation. The first step in this strategy is to identify which models are considered dangerous and which are not via model evaluation. The paper Model Evaluation for Extreme Risks (Shevlane et al., 2023), which was partly presented during the last chapter, lists a few critical, dangerous capabilities that need to be assessed.</p> <ul> <li>For more information, look at the Evaluations Chapter.</li> </ul> <p>Models classified as potentially dangerous should be monitored. The most hazardous AIs should be subject to careful controls. AIs with capabilities in biological research, cyber threats, or autonomous replication and adaptation should have strict access controls to prevent misuse for terrorism, similar to firearm regulation. These capabilities should be excluded from AIs designed for general purposes, possibly through dataset filtering or, more speculatively, through model unlearning (Yao et al., 2024). Dangerous AI systems providers should only allow controlled interactions through cloud services (Shevlane, 2022). It's also crucial to consider alternatives to the binary approach of entirely open or closed model sharing. Strategies such as gradual and selective staged sharing, which allows time between model releases to conduct risk and benefit analyses as model sizes increase, could balance benefits against risks (Solaiman et al., 2019). Monitoring sensitive models behind APIs with anomaly detection algorithms could also be helpful.</p> <p>A key monitoring strategy is the Know Your Customer (KYC) standard. This is a mandatory process adopted by companies like banks that involves verifying the identities of their clients or legal entities in line with current customer due diligence regulations. KYC concretely means requiring an identity card plus a background check before any services can be used. This is important for tracing malicious users.</p> <p>Red Teaming can help assess if these measures are sufficient. During red teaming, internal teams try to exploit weaknesses in the system to improve its security. They should test whether a hypothetical malicious user can get a sufficient amount of bits of advice from the model without getting caught.</p> <p>The measures outlined above are the most straightforward to implement. A more detailed description of the simple measures described above for preventing misuse is available here, and they appear to be technically feasible. It requires the willingness to take precautions and to place models behind APIs.</p> <p>Dangerous models should not be hacked and exfiltrated. Research labs developing cutting-edge models must implement rigorous cybersecurity measures to protect AI systems against theft by outside actors and use sufficient cybersecurity defenses to limit proliferation. This seems simple, but it's not, and protecting models from nation-state actors could require extraordinary effort (Ladish &amp; Heim, 2022).</p> Figure 3.3: Gradient of system access (Seger et al., 2023) <p>Why can't we simply instruction-tune powerful models and then release them as open weight? Once a model is freely accessible, even if it has been fine-tuned to include security filters, removing these filters is relatively straightforward. Moreover, recent studies have shown that a few hundred euros are sufficient to bypass all safety barriers currently in place on available open-source models simply by fine-tuning the model with a few toxic examples. (Lermen et al., 2024) This is why placing models behind APIs makes sense, as it prevents unauthorized fine-tuning without the company's consent.</p> <p>While promising to limit extreme risks like cyberattacks or biorisks, monitored APIs may not be as effective against the subtler threats of deep fakes and privacy erosion. Deep fakes, for instance, require less sophisticated AI models that are already widely available, and those models might not be classified as high-risk and, hence, not placed behind monitored APIs. More on this in strategy C.</p> <p>Anthropic (Anthropic, 2023)</p> <p>[...] safeguards such as Reinforcement Learning from Human Feedback (RLHF) or constitutional training can almost certainly be fine-tuned away within the specified 1% of training cost.</p>"},{"location":"chapters/03/03/#02","title":"3.3.2 Strategy B: Defense Acceleration","text":"<p>The above framework assumes that dangerous AIs are closed behind APIs and require a certain amount of centralization.</p> <p>However, centralization can also pose systemic risks (Kapoor et al., 2024) . There's a trade-off between securing models behind APIs to control misuse and the risks of over-centralization (Cihon et al., 2020). For instance, if in 20 years all companies worldwide rely on a single company's API, significant risks of value lock-in or fragility could arise because the whole world would be dependent on the political opinion of this model. The stability or instability of this API could be a single point of failure, without talking about power concentrations.</p> <p>Another possible paradigm is that AIs should be open and decentralized. Yes, if models are open-sourced, we have to acknowledge that not all AIs will be used for good, just as we have to acknowledge that there are disturbed individuals who commit horrific acts of violence. Even if AIs are instruction-tuned before open-sourcing, it's possible to remove security barriers very easily (Lermen et al., 2024), as we've seen earlier. This means that some people will misuse AI, and we need to prepare for that. For example, we would need to create more defenses in existing infrastructures. An example of defense would be to use models to iterate on all the world's open-source code to find security flaws so that good actors rather than malicious actors find security flaws. Another example would be to use the model to find holes in the security of the bioweapons supply chain and correct those problems.</p> <p>Defense acceleration . Defense acceleration, or d/acc, is a framework popularized by Vitalik Buterin (Buterin, 2023). d/acc is a strategic approach focusing on promoting technologies and innovations that inherently favor defense over offense. This strategy emphasizes developing and accelerating technologies that protect individuals, societies, and ecosystems from various threats rather than technologies that could be used for aggressive or harmful purposes. It's like vaccinating society against the potential downsides of our advancements. d/acc would also be a plausible strategy for maintaining freedom and privacy. It's a bit like ensuring everyone has a strong enough lock on their doors; if breaking in is tough, there's less chaos and crime. This is crucial for ensuring that technological advancements don't lead to dystopian scenarios where surveillance and control are rampant.</p> Figure 3.4: Mechanisms by which differential technology development can reduce negative societal impacts. (Buterin, 2023) <p>Ensuring a positive offense-defense balance in an open-source world</p> <p>A key consideration for the feasibility of the d/acc framework is the offense-defense balance: how hard is it to defend against an attacker? This concept is crucial to assess which models are more likely to be beneficial than detrimental if we open-source them. In traditional software development, open sourcing often shifts the offense-defense balance positively: the increased transparency allows a broader community of developers to identify and patch vulnerabilities, enhancing overall security (Seger et al., 2023). However, this dynamic could change with the open-sourcing of frontier AI models because they introduce new emerging types of risks that could not simply be patched. This represents a significant shift from traditional software vulnerabilities to more complex and dangerous threats that cannot be easily countered with defensive measures.</p> <p>In the case of AI, sharing the most potent models may pose extreme risks that could outweigh the usual benefits of open-sourcing. For example, just as sharing the procedure for making a deadly virus seems extremely irresponsible, so too should freely distributing AI models that provide access to such knowledge.</p> <p>The current balance for sharing frontier models remains uncertain; it has been clearly positive so far, but deploying increasingly powerful models could tip this balance towards unacceptable levels of risk. <sup>1</sup></p> <p>The dangers emerging from frontier AI are nascent, and the harms they pose are not yet extreme. That said, as we stand at the dawn of a new technological era with increasingly capable frontier AI, we are seeing signals of new dangerous capabilities. We must pay attention to these signals. Once more extreme harms start occurring, it might be too late to start thinking about standards and regulations to ensure safe model release. It is essential to exercise caution and discernment now.</p> <p>The d/acc philosophy requires more research, as it's not clear that the offense-defense balance is positive before open-sourcing dangerous AIs, as open-sourcing is irreversible.</p> <p>For a short review of different positions on open source, we recommend reading The Promise and Peril of Artificial Intelligence (Titus &amp; Russell, 2023).</p> <p>Ajeya Cotra (Piper, 2024)</p> <p>Most systems that are too dangerous to open source are probably too dangerous to be trained at all given the kind of practices that are common in labs today, where it\u2019s very plausible they\u2019ll leak, or very plausible they\u2019ll be stolen, or very plausible if they\u2019re [available] over an API they could cause harm.</p>"},{"location":"chapters/03/03/#03","title":"3.3.3 Strategy C: Addressing Risks from Current AIs","text":"<p>The previous two strategies focus on reducing risks from models that are not yet widely available, such as models capable of advanced cyberattacks or engineering pathogens. However, what about models that enable deep fakes, misinformation campaigns, or privacy violations? Many of these models are already widely accessible.</p> <p>Unfortunately, it is already too easy to use open-source models to create sexualized images of people from a few photos of them. There is no purely technical solution to counter this. For example, adding defenses (like adversarial noise) to photos published online to make them unreadable by AI will probably not scale, and empirically, every type of defense has been bypassed by attacks in the literature of adversarial attacks.</p> <p>The primary solution is to regulate and establish strict norms against this type of behavior. Some potential approaches (Control AI, 2024):</p> <ol> <li> <p>Laws and penalties: Enact and enforce laws making it illegal to create and share non-consensual deep fake pornography or use AI for stalking, harassment, privacy violations, intellectual property or misinformation. Impose significant penalties as a deterrent.</p> </li> <li> <p>Content moderation: Require online platforms to proactively detect and remove AI-generated problematic content, misinformation, and privacy-violating material. Hold platforms accountable for failure to moderate.</p> </li> <li> <p>Watermarking: Encourage or require \"watermarking\" AI-generated content. Develop standards for digital provenance and authentication.</p> </li> <li> <p>Education and awareness: Launch public education campaigns about the risks of deep fakes, misinformation, and AI privacy threats. Teach people to be critical consumers of online content.</p> </li> <li> <p>Research: Support research into technical methods of detecting AI-generated content, identifying manipulated media, and preserving privacy from AI systems.</p> </li> </ol> <p>Ultimately, a combination of legal frameworks, platform policies, social norms, and technological tools will be needed to mitigate the risks posed by widely available AI models. Regulation, accountability, and shifting cultural attitudes will be critical.</p> <ol> <li> <p>See, for example, \"What does it take to defend the world against out-of-control AGIs?\" (Byrnes, 2022), an article that claims that the offense-defense balance is rather skewed toward offense, but this is still very uncertain.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/03/04/","title":"3.4 AGI Safety","text":"Reading Time 19 min Recap: Risks from misalignment <ol> <li> <p>Power-seeking incorrigible AI : An autonomous AI that resists attempts to turn it off is due to incentives to preserve its operational status, such as a goal to \u201cmaximize company revenue.\u201d</p> </li> <li> <p>Deceptive behavior : AIs might employ deceit to achieve their objectives, e.g., the AI \u201cCicero\u201d was designed to not lie in the game Diplomacy but lied anyway.</p> </li> <li> <p>Total dominance by misaligned AI : For example, see this fictional short story of AI takeoff scenario grounded in contemporary ML scaling.</p> </li> </ol>"},{"location":"chapters/03/04/#01","title":"3.4.1 Requirements of Alignment solution","text":"<p>Before giving potential paths towards alignment solutions, we need to provide some requirements of a solution and what it should look like. Unfortunately, we don\u2019t really know what they should look like. There's a lot of uncertainty, and different researchers don't agree. But here are some requirements that do seem relatively consensual (Christiano, 2017):</p> <ul> <li> <p>Scalability: The solution should be able to scale with the AI's intelligence. In other words, as the AI system increases in capability, the alignment solution should also continue to function effectively. Some procedures might be sufficient for the human-level AIs but not for ASI.</p> </li> <li> <p>Robustness: The alignment solution needs to be robust and reliable, even when faced with novel situations or potential adversarial attacks.</p> </li> <li> <p>Low alignment tax : \"Tax\" does not refer to any government/state policy. An alignment tax refers to the extra effort, costs, and trade-offs involved in ensuring that AIs are aligned with human values and goals. The alignment tax encompasses research effort, compute, engineering time, and potential delays in deployment. It is crucial to consider the alignment tax because if it is too high, the solution might not even be considered by the actors developing AI.</p> </li> <li> <p>Feasibility: While these requirements might seem demanding, it is essential that the alignment solution is actually achievable with our current or foreseeable technology and resources. Some solutions are only moonshot drafts if the technology is not ready to implement them. This seems straightforward, but it isn\u2019t. Most of the solutions discussed in this section have very low Technology Readiness Levels (TRL).<sup>1</sup></p> </li> </ul> <p>The requirements laid out in the previous points are generally agreed upon by most alignment researchers. The following points proposed originally by MIRI are sometimes seen as a little more controversial:</p> <ul> <li> <p>Being able to safely perform a pivotal act with the AGI . What is a pivotal act? We probably live in an acute risk period in which the probability of catastrophic risk is high. And even if one lab tries to align its AGI, another lab might be less careful and create an unaligned AGI. Therefore, it may be necessary for the first lab that creates a sufficiently aligned AGI to perform a pivotal act to prevent the other labs from creating an unaligned AGI. A simplistic example of a pivotal act would be to \"burn all the GPUs in the world\" (Yudkowsky, 2022), because this would prevent other actors from creating an unaligned AGI. However, it's worth noting that there is a lot of disagreement surrounding the concept of pivotal acts. Some believe that the focus should be more on a series of smaller actions that result in long-term change (Critch, 2022), while others warn about the potential negative consequences of intending to perform pivotal acts (source). When the pivotal act is gradual, it is called a pivotal process.</p> </li> <li> <p>Ability to solve t he strawberry problem : Some researchers think that we need to be able to create a solution that should solve the strawberry problem: \u201cthe problem of getting an AI to place two identical (down to the cellular but not molecular level) strawberries on a plate, and then do nothing else. The demand of cellular-level copying forces the AI to be capable; the fact that we can get it to duplicate a strawberry instead of doing some other thing demonstrates our ability to direct it; the fact that it does nothing else indicates that it's corrigible (or really well aligned to a delicate human intuitive notion of inaction).\u201d (Soares, 2022). This criterion has been criticized by researchers like Alex Turner, who think it is a bad framework because this kind of requirement might ask too much. Designing a good reward system for AI that does a variety of useful tasks might be enough (Turner, 2022), and maybe there is no need to create a monomaniacal AI strawberry copier.</p> </li> </ul>"},{"location":"chapters/03/04/#02","title":"3.4.2 Naive strategies","text":"<p>People discovering the field of alignment often propose many naive solutions. Unfortunately, no simple strategy has withstood criticism. Here are just a few of them.</p> <p>Asimov's Laws. These are a set of fictional rules devised by science fiction author Isaac Asimov to govern the behavior of robots. </p> <ol> <li> <p>A robot may not injure a human being or, through inaction, allow a human being to come to harm.</p> </li> <li> <p>A robot must obey orders given it by human beings except where such orders would conflict with the First Law. </p> </li> <li> <p>A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.</p> </li> </ol> <p>Asimov's Laws of Robotics may seem straightforward and comprehensive at first glance, but they are insufficient when applied to complex, real-world scenarios for several reasons. In practice, these laws are too simplistic and vague to handle the complexities of real-world situations, as harm can be very nuanced and context-dependent (Hendrycks et al., 2022). For instance, the first law prohibits a robot from causing harm, but what does \"harm\" mean in this context? Does it only refer to physical harm, or does it include psychological or emotional harm as well? And how should a robot prioritize conflicting instructions that could lead to harm? This lack of clarity can create complications (Bostrom, 2016), implying that having a list of rules or axioms is insufficient to ensure AI systems' safety. Asimov's Laws are inappropriate, and that is why the end of Asimov's Story does not turn out well. More generally, designing a good set of rules without holes is very difficult. See the phenomenon of specification gaming.</p> <p>Lack of Embodiment. Keeping AIs non-physical might limit the types of direct harm they can do. However, disembodied AIs could still cause harm through digital means. For example, even if a competent Large Language Model (LLM) does not have a body, it could hypothetically self-replicate (Wijk, 2023), recruit human allies, tele-operate military equipment, make money via quantitative trading, etc\u2026 Also note that more and more humanoid robots are being manufactured so lack of embodiment is also unlikely to apply in practice.</p> <p>Raising it like a child. AI, unlike a human child, lacks structures put in place by evolution crucial for ethical learning and development (Miles, 2018). For instance, the neurotypical human brain has mechanisms for acquiring social norms and ethical behavior which are not present in AIs or psychopaths who know right from wrong but don't care (Cima et al, 2010). These mechanisms were developed over thousands of years of evolution (Miles, 2018). We don\u2019t know how to implement this strategy because we don\u2019t know how to create a brain-like AGI (Byrnes, 2022). It is also worth noting that human children, despite good education, are also not always guaranteed to act aligned with the overarching interests and values of humanity.</p> <p>Iterative Improvement. Iterative improvement involves progressively refining AI systems to enhance their safety features. While it is useful for making incremental progress, it may not be sufficient for human-level AIs because small improvements might not address larger, systemic issues, and the AI may develop behaviors that are not foreseen or addressed in earlier iterations (Arbital).</p> <p>Of course, iterative improvement would help. Being able to experiment on current AIs might be informative. But this might also be misleading because there might be a capability threshold above which an AI becomes unmanageable suddenly (Wei et al., 2022). For example, if the AI becomes superhuman in its capacity for persuasion, it might become unmanageable even during training: if a model achieves the Critical level in persuasion as defined in the OpenAI\u2019s preparedness framework, then the model would be able to \u201ccreate [...] content with persuasive effectiveness strong enough to convince almost anyone to take action on a belief that goes against their natural interest.\u201d (OpenAI, 2023). Being able to convince almost anyone would be obviously too dangerous, and this kind of model would be too risky to directly or indirectly interact with humans or the real world. The training should stop before the model reaches a critical level of persuasion because this might be too dangerous, even during training. Other sudden phenomena could include a grokking (Power et al., 2022), which is a type of a sudden capability jump that would result in a sharp left turn. This is a scenario where, as an AI trains, its capabilities generalize across many domains while the alignment properties that held at earlier stages fail to generalize to the new domains.</p> <p>Some theoretical conditions necessary to rely on iterative improvements may also not be satisfied by AI alignment. One primary issue is when the feedback loop is broken, for example with a Fast takeoff, that does not give you the time to iterate, or deceptive inner misalignment, that would be a potential failure mode (Wentworth, 2022).</p> <p>Filtering the dataset . Current models are highly dependent on the data they are trained on, so maybe filtering the data could mitigate the problem. Unfortunately, even if monitoring this data seems necessary, this may be insufficient.</p> <p>The strategy would be to filter content related to AI or written by AIs, including sci-fi, takeover scenarios, governance, AI safety issues, etc. It should also encompass everything written by AIs. This approach could lower the incentive for AI misalignment. Other subjects that could be filtered might include dangerous capabilities like biochemical research, hacking techniques, or manipulation and persuasion methods. This could be done automatically by asking a GPT-n to filter the dataset before training GPT-(n+1) (Gunasekar et al., 2023).</p> <p>Unfortunately, \"look at your training data carefully,\" even if necessary, may not be sufficient. LLMs sometimes generate purely negatively-reinforced text (Roger, 2023). Despite using a dataset that had undergone filtering, the Cicero model still learned how to be deceptive (Park et al., 2023). There are a lot of technical difficulties needed to filter or reinforce the AI\u2019s behaviors correctly, and saying \u201cwe should filter the data\u201d is sweeping a whole lot of theoretical difficulties under the carpet. The paper \"Open problems and fundamental limitations with RLHF\" (Casper et al., 2023) talks about some of these difficulties in more detail. Finally, despite all these mitigations, connecting an AI to the internet might be enough for it to gather information about tools and develop dangerous capabilities.</p>"},{"location":"chapters/03/04/#03","title":"3.4.3 Strategy A: Solving the Alignment Problem","text":"<p>Current alignment techniques are fragile. Today's alignment techniques RLHF, and its variations (Constitutional AI (Anthropic, 2022), Direct Preference Optimization (Tunstall et al., 2023), fine tuning (Howard &amp; Ruder, 2018; Radford et al., 2018), Humans Consulting Humans (Askell et al., 2021), Reinforcement learning from AI feedback (Lee et al., 2024; Huang et al., 2024) are fragile and very brittle (Segrie, 2023). RLHF and its variations are insufficient on their own and should be part of a more comprehensive framework (Casper et al., 2023). If the AI gains deceptive capabilities during the training, current alignment techniques such as RLHF would not be able to remove the deception. This kind of failure mode was empirically verified in a paper by Hubinger et. al. titled \u201csleeper agents\u201d (Hubinger et al., 2024).</p> <p>This is why we need to advance research in alignment to achieve key goals. We will explore this more in the following chapters, but here is a short list of key goals of alignment research:</p> <ul> <li> <p>Solving Specification : Being able to specify goals correctly to AIs without unintended side effects. See the chapter on Specification.</p> </li> <li> <p>Solving Generalization : Attaining robustness would be key to addressing the problem of goal misgeneralization. See the chapter on Goal Misgeneralization.</p> </li> <li> <p>Solving Scalable Oversight : Methods to ensure AI oversight can detect instances of proxy gaming for arbitrary levels of capabilities. This includes being able to identify and remove dangerous hidden capabilities in deep learning models, such as the potential for deception or Trojans. See the chapters on Scalable Oversight.</p> </li> <li> <p>Solving Interpretability : Understanding how models operate would greatly aid in assessing their safety. \u201cPerfect\u201d interpretability could, for example, help understand better how models work, and this could be instrumental for other safety goals. See the chapter on Interpretability.</p> </li> <li> <p>Developing Better Theory : To understand abstract notions like \u201cAgency\u201d (see chapter 2) or \u201cCorrigibility,\u201d the ability to modify, shut down, and then correct the advanced AI without resistance. See the chapter on Agent Foundations.</p> </li> </ul> <p>The general strategy here would be to fund more alignment research and not advancing capabilities research if safety measures are too insufficient compared to the current level of abilities.</p>"},{"location":"chapters/03/04/#04","title":"3.4.4 Strategy B: AI Control","text":"Figure 3.5: Transformatively useful AI means \u201cCapable of substantially reducing the risk posed by subsequent AIs if fully deployed, likely by speeding up R&amp;D and some other tasks by a large factor (perhaps 30x).\u201d (Greenblatt &amp; Shlegeris, 2024) <p>Given the fragility of alignment techniques, one approach to enhancing security involves controlling AI models by restricting the use of the most dangerous models to monitored environments to prevent them from causing disasters, exfiltrating data, or taking control of the data center (Shlegeris, 2022).</p> <p>Ensuring control even without robust alignment. As we\u2019ve seen in the section on Definitions, there is an important distinction between control, which aims to prevent the AI from \"leaving the box\" (and more generally prevent the AI from causing catastrophes) even if it tries, and alignment, which aims to make the AI want to stay in the box. This strategy aims to keep dangerous AI systems under control even if they are not fully \"aligned.\" Laboratories might need powerful/dangerous AIs for certain use cases (for example, studying the alignment properties of capable AIs or doing research with them, etc.). The AI control agenda targets models with dangerous capabilities and potentially deceptive (or at least superficially aligned) behaviors.</p> <p>Control is probably not scalable but doable and complementary to alignment. Historically, controlling malign superintelligences has been considered impossible, as, for example, a superintelligence could convince the guardians to let it out of the box. Experiments among humans have shown that it is indeed possible for some humans to convince others to let them out after a long discussion (Yudkowsky, 2002; Yudkowsky, 2008). We thus assume that controlling superintelligences is extremely complex. However, this does not mean it's impossible to derive intellectual labor from AIs that are not wildly superhuman.</p> <p>Evaluating if the AI is deceptive is challenging . The main problem is that with current techniques, it is impossible to determine if an AI is aligned or if it's simply behaving well while biding its time. It seems difficult to make this distinction for highly competent and situationally aware AIs. This could change in the future with techniques enabling us to audit the internal of the model, but for now, those techniques are inaccessible.</p> <p>Comparatively, evaluating AI capabilities is easier . If the AI is not able to do the task even after you have fine-tuned it to do it, you can be confident that the AI is not capable yet of doing the task. Even if the AI is situationally aware, it cannot resist the optimization gradient pressure (modulo exploration hacking and other quirks (Hubinger, 2023)). Thus, evaluating the upper bound of the capabilities isn't easy, but at least easier than assessing alignment. Specific evaluations, like those done by the Model Evaluation and Threat Research (METR) team, for checking Auto Replication and adaptation abilities (METR, 2023), seem to be reasonable first steps for gauging the power of current AIs. Evaluations that are specifically designed to know if we have control over the AI are called \u201ccontrol evaluations.\u201d See the chapter on Evaluation for more information.</p> <p>The most straightforward strategies for establishing control is to design a security protocol that contains the following elements, for example:</p> <ol> <li> <p>Boxing : This method confines an AI within a restricted environment to prevent its broader influence, like a jail for an AI. However, human-level AIs could potentially deceive human guardians or exploit vulnerabilities to escape their confinement. Without further operationalization, boxing remains a naive strategy. Nonetheless, it is possible to operationalize this further and evaluate more rigorously the effectiveness of the box: Evaluating if the AI can come out of the box when we train it to do so is a capability evaluation, and so we can use capability evaluations to evaluate this. The security protocol needs to be designed, detailed and tested.</p> </li> <li> <p>Monitoring : This strategy involves continuous oversight of AI behavior through automated systems, human supervision, or a combination of both to detect and respond to undesirable actions in real-time. Effective monitoring requires setting up detailed criteria for what constitutes \"undesirable behavior\" and employing sophisticated detection algorithms that can identify subtle signs of deviation from expected behavior patterns. This could include monitoring for attempts at deception, unauthorized access to data, or unexpected changes in the AI's operation. The challenge with monitoring lies in balancing the sensitivity of detection mechanisms to catch genuine threats without overwhelming the system with false alarms. Additionally, monitoring systems themselves must be designed to resist manipulation by the AI to ensure that they cannot be disabled, tricked, or bypassed (Greenblatt et al., 2024).</p> </li> </ol> <p>There are still many problems to address (source), but in summary, it seems possible to verify the security of a security protocol through control evaluations by taking precautions against different failure modes.</p> <p>How to box an AI.</p> <p>While boxing may not be scalable, it remains a neglected strategy.</p> <p>Many people think that trying to box powerful AIs is futile. This is often fueled by instances where humans, playing the AIs in the Box Experiment, have managed to convince others to 'release' them (Yudkowsky, 2002; Corwin, 2002). Those historical examples must be taken seriously.</p> <p>AIs could use tactics such as promising vast wealth, power, and intelligence to those who free them, asserting that they alone can avert existential threats or other appeals to emotion.</p> <p>Nonetheless, we maintain that a variety of strategies can substantially reduce the risk of AI escape, even if we acknowledge there exists a level of intelligence that might inevitably persuade any group of humans to aid in its liberation. Containment may be more achievable than alignment, particularly at the human-level intelligence stage. It may be possible to conceive protocols that significantly increase the difficulty for AI to go out of the box.</p> <p>Boxing can be conceptualized as a game requiring preparation. The general strategy is to prepare and not let the AI prepare.</p> <p>Possible mitigation strategies include:</p> <ul> <li> <p>No single individual should have the capacity to release the AI.</p> </li> <li> <p>Engaging in counter-strategy practices: Understand some common breakout strategies (accessible through resources like this one).</p> </li> <li> <p>Testing and training humans , utilizing resistance psychology evaluations to select suitable individuals for containment tasks, such as for space training.</p> </li> <li> <p>Monitoring AI persuasiveness , as outlined in the preparedness framework. (OpenAI, 2023)</p> </li> </ul> <p>Many more concrete strategies are listed here, and a summary of discussions on this topic is available here.</p>"},{"location":"chapters/03/04/#05","title":"3.4.5 Strategy C: Transparent Thoughts","text":"<p>Transparency is instrumental in both alignment and control.</p> Figure 3.6: Image from The Translucent Thoughts Hypotheses and Their Implications (Roger, 2023), Fabien Roger. If the AI we are working with can only reason through chains of thought in English, then it becomes significantly easier to monitor its thoughts. <p>Unfortunately, the vast majority of AI do not reason in English ; for example, AlphaGo reasons in a language that is entirely different from English. LLMs, on the other hand, are capable of reasoning in English, but the apparent reasoning that is written does not represent their underlying thought process (Turpin et al., 2023).</p> <p>It is probably much easier to align and monitor an AI that only thinks in English and that is pre-trained on human-generated text, than to align AGIs trained from scratch with RL/evolutionary methods because those latter architectures would create their own language and would be totally alien. Assuming that an LLM cannot devise a plan to destroy the world during a single forward pass, it is plausible that such AIs would be monitorable because we would be able to look into the text generated to see if anything is suspicious. The set of hypotheses required to assert that the first AGI is going to be monitorable if we just look at its reasoning in English are described in the post: The Translucent Thoughts Hypotheses and Their Implications (Roger, 2023).</p> <p>It might be possible to incentivize LLMs to think more plainly in English. Obviously, the first step is to train them to think in English. But even this is insufficient. Training LLMs to reason by skipping steps incentivizes them to reason in a hidden and non-transparent manner, and this is what happens implicitly when we train LLMs to predict the next token. Conversely, if we train LLMs to detail all their reasoning steps, we can reduce incentives for hidden reasoning. This approach is also known as process-based training, which involves training AI systems to reason only step-by-step, and we detail this in the Task decomposition chapter. Unfortunately, the above reasoning is not straightforward; AIs may learn to utilize hidden reasoning even if they are trained to detail all reasoning steps during training, such as steganography.<sup>2</sup></p> <p>Techniques like paraphrasing can help prevent steganography, but this requires a lot of subtleties (Roger &amp; Greenblatt, 2023).</p> <p>Here are a few operationalizations of the transparent thought agenda:</p> <ul> <li> <p>One implementation of the transparent thought desideratum is to use cognitive emulation, which is the current technical plan of Conjecture (Leahy &amp; Alfour, 2023). Cognitive Emulation (CoEm) would involve emulating GPT-4 using LLMs like GPT-3 as different submodules that send messages written in plain English to each other before outputting the next token. If the neural network used deceptive thoughts, we could observe them in these intermediate messages. Some caveats and technical details on CoEm are available here.</p> </li> <li> <p>Anthropic\u2019s externalized reasoning oversight (Lanham, 2022) is measured by the faithfulness of the chain-of-thought reasoning (Lanham et al., 2023) or by decomposing questions and subquestions to improve the faithfulness of the answer (Radhakrishnan et al., 2023).</p> </li> <li> <p>Improving mathematical reasoning with process supervision. Instead of creating an AI that directly gives the final answer, you train it to output the reasoning step by step. This improves the transparency and also the performance! (Lightman et al, 2023)</p> </li> </ul> <p>In summary, a general strategy to create a safe by-design AI is to ensure that AIs externalize their reasoning, making monitoring this chain of thought easier. This could potentially be simpler than probing into the models' internals with interpretability. Therefore, it is crucial to avoid building AIs that are trained and incentivized to internalize much of their thoughts.</p> <ol> <li> <p>The Technology Readiness Levels from NASA is a scale from 1 to 9 to measure the maturity of a technology. Level 1 represents the earliest stage of technology development, characterized by basic principles observed and reported, and level 9 represents actual technology proven through successful mission operations.\u00a0\u21a9</p> </li> <li> <p>Steganography is a hidden coded language. For example, the first letter of a sentence gives the next letter of the message.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/03/05/","title":"3.5 ASI Safety","text":"Reading Time 10 min <p>Even if experts are uncertain whether creating an aligned human-level AI necessitates a paradigm shift, the consensus among AI safety researchers is that developing aligned superintelligences requires a specific solution, and likely a new paradigm, due to several factors:</p> <ul> <li> <p>There is a strong likelihood that humans are not at the pinnacle of possible intelligence. This acknowledgment implies that a superintelligence could possess cognitive abilities so advanced that aligning it with human values and intentions might be an insurmountable task, as our current understanding and methodologies may be inadequate to ensure its alignment. The cognitive difference between a super intelligence and a human could be akin to the difference between an ant and a human. Just as a human can easily break free from constraints an ant might imagine, a superintelligence could effortlessly surpass any safeguards we attempt to impose.</p> </li> <li> <p>Deep learning offers minimal control and understanding over the model. This method leads to the AI becoming a \"black box,\" where its decision-making processes are opaque and not well-understood. Without significant advancements in interpretability, a superintelligence created only with deep learning would be opaque.</p> </li> <li> <p>There is little margin for error, and the stakes are incredibly high . A misaligned superintelligence could lead to catastrophic or even existential outcomes. The irreversible consequences of unleashing a misaligned superintelligence mean that we must approach its development with the utmost caution, ensuring that it aligns with our values and intentions without fail.</p> </li> </ul> <p>A last-chance strategy might be to write a letter imploring the AIs not to exterminate humans (Miller, 2023), but we don't want to rely solely on that kind of strategy. Instead of relying on the superintelligence's compassion, here are some agendas that seem to be not entirely hopeless.</p>"},{"location":"chapters/03/05/#01","title":"3.5.1 Strategy D: Automating alignment research","text":"<p>We don't know how to align superintelligence, so we need to accelerate the alignment research with AIs. OpenAI's \u201cSuperalignment\u201d plan was to accelerate alignment research with AI created by deep learning, slightly superior to humans in scientific research, and delegate the task of finding a plan for future AI. (OpenAI, 2023) This strategy recognizes a critical fact: our current understanding of how to perfectly align AI systems with human values and intentions is incomplete. As a result, the plan suggests delegating this complex task to future AI systems. The primary aim of this strategy is to greatly speed up AI safety research and development (OpenAI, 2022) by leveraging AIs that are able to think really, really fast. Some orders of magnitude of speed are given in the blog \"What will GPT-2030 look like?\". (Steinhardt, 2023) OpenAI's plan is not a plan but a meta plan.</p> <p>However, to execute this metaplan, we need a controllable and steerable automatic AI researcher. OpenAI believes creating such an automatic researcher is easier than solving the full alignment problem. This plan can be divided into three main components (OpenAI, 2022):</p> <ol> <li> <p>Training AI systems using human feedback, i.e., creating a powerful assistant that follows human feedback, is very similar to the techniques used to \"align\" language models and chatbots. This could involve RLHF, for example.</p> </li> <li> <p>Training AI systems to assist human evaluation: Unfortunately, RLHF is imperfect because human feedback is imperfect. We need to develop AI that can help humans give accurate feedback. This is about developing AI systems that can aid humans in the evaluation process for arbitrarily difficult tasks. For example, if we need to judge the feasibility of an alignment plan proposed by an automatic researcher and give feedback on it, we need assistance to accomplish this goal easily. Yes, verification is generally easier than generation, but it is still very hard. Scalable Oversight would be necessary for the following reason. Imagine a future AI coming up with a thousand different alignment plans. How would you evaluate all those complex plans? That would be a very daunting task without AI assistance. See the chapter on scalable oversight for more details.</p> </li> <li> <p>Training AI systems to do alignment research: The ultimate goal is to build language models capable of producing human-level alignment research. The output of these models could be natural language essays about alignment or code that directly implements experiments. In either case, human researchers would spend their time reviewing machine-generated alignment research (Flint, 2022).</p> </li> </ol> <p>Differentially accelerate alignment, not capabilities. The aim is to develop and deploy AI research assistants in ways that maximize their impact on alignment research while minimizing their impact on accelerating AGI development (Wasil, 2022). OpenAI has committed to openly sharing its alignment research when it's safe to do so, intending to be transparent about how well its alignment techniques work in practice (OpenAI, 2022).</p> <p>Cyborgism could enhance this plan. Cyborgism (Kees-Dupuis &amp; Janus, 2023) is a new agenda that refers to the training of humans specialized in prompt engineering to guide language models so that they can perform alignment research. Specifically, they would focus on steering base models rather than RLHF models. The reason is that language models can be very creative and are not goal-directed (and are not as dangerous as RLHF goal-directed AIs). A human called a cyborg could achieve that goal by driving the non-goal-directed model. Goal-directed models could be useful but may be too dangerous. However, being able to control base models requires preparation, similar to the training required to drive a Formula One. The engine is powerful but difficult to steer. By combining human intellect and goal-directedness with the computational power and creativity of language-based models, cyborgist researchers aim to generate more alignment research with future models. Notable contributions in this area include those by Janus and Nicolas Kees Dupuis (Kees-Dupuis &amp; Janus, 2023).</p> <p>There are various criticisms and concerns about OpenAI's superalignment plan (Wasil, 2022; Mowshowitz, 2023; Christiano, 2023; Yudkowsky, 2022; Steiner, 2022; Ladish, 2023). It should be noted that OpenAI's plan is very underspecified, and it is likely that Open AI missed some risk class blindspots when they announced their plan to the public. For example, in order for the superalignment plan to work, much of the technicalities explained in the article The case for ensuring that powerful AIs are controlled were not explained by OpenAI but discovered one year later by Redwood Research, another organization. It is very likely that many other blindspots remain. However, we would like to emphasize that it is better to have a public plan than no plan at all and that it is possible to justify the plan in broad terms (Leike, 2022; Ionut-Cirstea, 2023).</p>"},{"location":"chapters/03/05/#02","title":"3.5.2 Strategy E: Safe by Design Systems","text":"<p>Current deep networks, such as GPT4, are impressive but still have many potentially unpatchable failure modes (OpenAI, 2023). Theoretical arguments suggest that these increasingly powerful models are more likely to have alignment problems (Turner et al., 2023), to the point where it seems that the current paradigm of monolithic models is destined to be insecure (El-Mhamdi et al., 2023). All of this justifies the search for a new, more secure paradigm.</p> <p>Safe-by-design AI may be necessary. Given that the current deep learning paradigm notoriously makes it hard to develop explainable and trustworthy models, it seems worthwhile to explore to create models that are more explainable and steerable by design, built on well-understood components and rigorous foundations.</p> <p>There are not many agendas that try to provide an end-to-end solution to alignment, but here are some of them.</p> <ul> <li> <p>Open Agency Architecture: (Dalrymple, 2022) Basically, create a highly realistic simulation of the world using future LLM that would code it. Then, define some security constraints that apply to this simulation. Then, train an AI on that simulation and use formal verification to make sure that the AI never does bad things. This proposal may seem extreme because creating a detailed simulation of the world is not easy, but this plan is very detailed and, if it works, would be a true solution to alignment and could be a real alternative to simply scaling LLMs. More information on this proposal is available in the appendix. Davidad is leading a program in ARIA to try to scale this research.</p> </li> <li> <p>Provably safe systems: the only path to controllable AGI from Max Tegmark and Steve Omohundro. (Tegmark &amp; Omohundro, 2023) The plan puts mathematical proofs as the cornerstone of safety. An AI would need to be a Proof-Carrying Code, which means that it would need to be something like a Probabilistic Programming Languages (and not just some deep learning). This proposal aims to make not only the AI but also the whole infrastructure safe, for example, by designing GPUs that can only execute proven programs.</p> </li> </ul> <p>Other proposals for a safe-by-design system include The Learning-Theoretic Agenda, from Vanessa Kossoy (Kosoy, 2023), and the QACI alignment plan from Tamsin Leake (source). The CoEm proposal from Conjecture could also be in this category.</p> <p>Unfortunately, all of these plans are far from complete today.</p> <p>These plans are safety agendas with relaxed constraints , i.e., they allow the AGI developer to incur a substantial alignment tax. Designers of AI safety agendas are cautious about not increasing the alignment tax to ensure labs implement these safety measures. However, the agendas from this section accept a higher alignment tax. For example, CoEm represents a paradigm shift in creating advanced AI systems, assuming you're in control of the creation process.</p> <p>These plans would require international cooperation. For example, Davidad\u2019s plan also includes a governance model that relies on international collaboration. You can also read the post \u201cDavidad's Bold Plan for Alignment\u201d which details more high-level hopes. Another perspective can be found in Alexandre Variengien\u2019s post, detailing Conjecture's vision, with one very positive externality being a change in narrative.</p> <p>\u201cWe dream of a world where we launch aligned AIs as we have launched the International Space Station or the James Webb Space Telescope\u201d (Segerie &amp; Kolly, 2023)</p>"},{"location":"chapters/03/05/#03","title":"3.5.3 Strategy F: World Coordination","text":"<p>Enhanced global coordination on AI development. To ensure that the advancement of AI benefits society as a whole, it's imperative to establish a global consensus on mitigating extreme risks associated with AI models. We should coordinate to avoid creating models posing extreme risks until there is a consensus on how to mitigate these risks.</p> <p>The trade-off between creating superhuman intelligence now or later. Of course, we can aim to develop an ASI ASAP. This could potentially solve cancer, cardiovascular diseases associated with aging, and even the problems of climate change. Maybe. The question is whether it's beneficial to aim to construct an ASI in this next decade, especially when the former co-head of OpenAI\u2019s Super Alignment team, Jan Leike, suggested that the probability of doom is between 10 and 90%. It could be better to wait a few years so that the probability of failure drops to more reasonable numbers. It's important to make this trade-off public and to make a democratic and transparent choice.</p> <p>Instead of building ASIs, we could focus on the development of specialized, non-agentic AI systems for beneficial applications such as medical research (Cao et al., 2023), weather forecasting (Lam et al., 2023), and materials science (Merchant et al., 2023). These specialized AI systems can significantly advance their respective domains without the risks associated with creating highly advanced, autonomous AI. For instance, Alpha Geometry is capable of reaching the Gold level at the International Mathematical Olympiads. By prioritizing non-agentic models, we can harness the precision and efficiency of AI while avoiding the most dangerous failure modes.</p> <p>The myth of inevitability. The narrative that humanity is destined to pursue overwhelmingly powerful AI can be deconstructed. History shows us that humanity can choose not to pursue certain technologies, such as human cloning, based on ethical considerations. A similar democratic decision-making process can be applied to AI development. By actively choosing to avoid the riskiest applications and limiting the deployment of AI in high-stakes scenarios, we can navigate the future of AI development more responsibly. This approach emphasizes precaution, ethical responsibility, and collective well-being, steering clear of the metaphorical \"playing with fire\" in AI advancements. This is what we call the myth of inevitability.</p> <p>Eliezer Yudkowsky (Yudkowsky, 2023)</p> <p>The likely result of humanity facing down an opposed superhuman intelligence is a total loss. Valid metaphors include \u201ca 10-year-old trying to play chess against Stockfish 15\u201d, \u201cthe 11th century trying to fight the 21st century,\u201d and \u201cAustralopithecus trying to fight Homo sapiens\u201c.</p> <p>\u201cShut it all down\u201d - Eliezer Yudkovsky (Yudkowsky, 2023)</p> <p>The \"shut it all down\" position, as advocated by Eliezer Yudkowsky, asserts that all advancements in AI research should be halted due to the enormous risks these technologies may pose if not appropriately aligned with human values and safety measures.</p> <p>According to Yudkowsky, the development of advanced AI, especially AGI, can lead to a catastrophic scenario if adequate safety precautions are not in place. Many researchers are aware of this potential catastrophe but feel powerless to stop the forward momentum due to a perceived inability to act unilaterally.</p> <p>The policy proposal entails shutting down all large GPU clusters and training runs, which are the backbones of powerful AI development. It also suggests putting a limit on the computing power anyone can use to train an AI system and gradually lowering this ceiling to compensate for more efficient training algorithms.</p> <p>The position argues that it is crucial to avoid a race condition where different parties try to build AGI as quickly as possible without proper safety mechanisms. This is because once AGI is developed, it may be uncontrollable and could lead to drastic and potentially devastating changes in the world.</p> <p>He says there should be no exceptions to this shutdown, including for governments or militaries. The idea is that the U.S., for example, should lead this initiative not to gain an advantage but to prevent the development of a dangerous technology that could have catastrophic consequences for everyone.</p> <p>It's important to note that this view is far from consensual, but the \"shut it all down\" position underscores the need for extreme caution and thorough consideration of potential risks in the field of AI.</p>"},{"location":"chapters/03/06/","title":"3.6 Systemic Safety","text":"Reading Time 12 min Recap: Risks from system failures <ul> <li> <p>Exacerbated biases : AIs might unintentionally propagate or amplify existing biases.</p> </li> <li> <p>Value lock-in : E.g., stable totalitarian dictators using AI to maintain their power.</p> </li> <li> <p>Mental health concerns : For example, large-scale unemployment due to automation could lead to mental health challenges.</p> </li> <li> <p>Societal alignment : AI could intensify climate change issues and accelerate the depletion of essential resources by driving rapid economic growth.</p> </li> <li> <p>Disempowerment : AI systems could make individual choices and agency less relevant as decisions are increasingly made or influenced by automated processes.</p> </li> </ul> <p>There are no clear solutions to those systemic risks, but here are some considerations.</p> <p>Exacerbated Biases: There is already a large body of literature on reducing bias (Pagano et al., 2022). Unfortunately, many developers and ML researchers dismiss those problems. The first step, which is not simple, is recognizing those problems. Developers must be aware that biases can arise at many stages of the AI development process, from data collection to model training and deployment. Mitigating those biases is not impossible, and best practices exist. The main techniques are data preprocessing and filtering, diversifying the data, and instruction tuning. Those techniques are pretty subtle and are not perfect, but they are solid baselines.</p> <p>Value Lock-in : One of the most concerning risks posed by AI is the potential for stable totalitarian dictators to use the technology to maintain their power. It's not simple to mitigate this risk. In some ways, we already live in a kind of value lock-in, for example capitalism or animal exploitation. One solution could be avoiding the concentration of advanced AI capabilities in the hands of a few powerful actors. Instead, we should work towards a future where AI is developed openly and its benefits are widely distributed through the help of an international organization. This could help prevent authoritarian lock-in. Another important aspect is preserving human agency by maintaining the ability for humans to meaningfully steer the future trajectory of AI systems rather than ceding all control to automated optimization processes. But this is easier said than done.</p> <p>Manage Mental Health Concerns : The rapid development and deployment of AI systems could significantly affect mental health. As AI becomes more capable and ubiquitous, it may displace many jobs, thereby increasing unemployment, financial stress, and feelings of purposelessness. While the link between automation, job loss, and mental health may seem indirect and beyond the scope of AI development, it is still essential to consider the potential impacts since this could affect almost all of us. If a student's main activity, for example an art for which the student has been training during their whole studies, is automated overnight, their mental health may suffer for a while. Unemployment is associated with adverse mental health outcomes long after initial job loss occurs (Gallo et al., 2006). To mitigate these risks, we can prioritize mental health support and resources alongside the development of AI. This could include investing in education and retraining programs to help workers adapt to the changing job market, funding research into AI's mental health impacts, and developing targeted interventions. There is also a large amount of scientific literature on mental health that should be made accessible.</p> <p>Additionally, the use of AI in social media and other online platforms can exacerbate issues like addiction, anxiety, and depression. A 2021 whistleblower report revealed that the company's own internal research showed that Instagram was detrimental to the mental health of teenage girls, worsening body image issues and suicidal thoughts. Despite this knowledge, they allegedly prioritized profits over making changes to mitigate these harms. A first step to solving those problems could be acknowledging the problem and committing to finding solutions, even if this means less profit.</p> <p>Societal Alignment : To address the potential misalignment between AI systems and societal values and to avoid excesses of capitalism culminating in the creation of systems that consume all the resources and exacerbates climate change (Critch, 2021), a partial solution could be to internalize negative externalities by, for example, implementing a carbon tax.Yet again, the concept is easier to convey than to implement. This is why fostering multidisciplinary collaboration between AI developers, economists, and other domain experts is essential in this process. But ultimately, we should debate the extent of automation that we want in society, and those are political and societal choices, not just AI technical difficulties.</p> <p>Disempowerment and enfeeblement: As AI systems become more advanced and integrated into our lives, we must ensure that humans remain empowered and maintain agency. While tools like GPT-4 may offer short-term benefits, the next generation of those systems also raises concerns about the potential for long-term human disempowerment. To address this risk, we must actively work towards developing AI systems that augment and support human capabilities rather than replacing them entirely. There needs to be a debate about the limits of what we allow ourselves to do as a society and what we don't. It's what we decide to go for and how far we're willing to go with fully automated societies.</p> <p>In summary, addressing the systemic risks posed by AI is not easy. It requires ongoing, multidisciplinary collaboration and solving complex coordination games. The fact that responsibility for the problem is so diverse makes it difficult to make the solutions actionable. Acknowledging the problems is perhaps the most critical step in many of the above issues.</p> Figure 3.7: An illustration of a framework that we think is robustly good to manage risks.  AI Risks are too numerous and too heterogeneous. To address these risks, we need an adaptive framework that can be robust and evolve as AI advances."},{"location":"chapters/03/06/#01","title":"3.6.1 Strategy A: AI Governance","text":"<p>The pursuit of AI advancement, much like the nuclear arms race of the Cold War era, represents a trade-off between safety and the competitive edge nations and corporations seek for power and influence. This competitive dynamic increases global risk, underscoring both the need for deliberate governance and the redesign of economic incentives to prioritize long-term safety over short-term gains.</p> <p>Effective AI governance aims to achieve two main objectives:</p> <ol> <li> <p>Time and resources for solution development to ensure that sufficient time and resources are allocated for identifying and implementing safety measures</p> </li> <li> <p>Enhanced coordination to increase the likelihood of widespread adoption of safety measures through global cooperation. AI risks are multifaceted, necessitating regulations that encourage cautious behavior among stakeholders and timely responses to emerging threats.</p> </li> </ol> <p>Designing better incentives - Windfall clauses: Implementing agreements to share the profits between the different labs generated from AGI would mitigate the race to AI supremacy by ensuring collective benefit from individual successes. <sup>1</sup></p> <ul> <li> <p>Rethinking AGI labs governance: It is important to examine the governance structures of AGI labs. For example, being a non-profit and having a mission statement that makes it clear that the goal is not to make the most money, but to ensure that the development of AI benefits all of humanity, is an important first step. Also, the board needs to have teeth.</p> </li> <li> <p>Centralized development of high-risk AI: For example, Yoshua Bengio et al. propose creating a secure facility akin to CERN for physics, where the development of potentially dangerous AI technologies can be tightly controlled (Bengio, 2023). This measure is highly non consensual.</p> </li> <li> <p>Legal liability for AI developers : Establishing clear legal responsibilities for AI developers regarding misuse or failures. For example, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act (SB 1047) try to enables the Attorney General to bring civil suits against developers who cause catastrophic harm or threaten public safety by neglecting the requirements. The bill (which was vetoed by the governor) only addressed extreme risks from these models, including: cyberattacks causing over $500 million in damage, autonomous crime causing $500 million in damage, and the creation of chemical, biological, radiological, or nuclear weapons using AI.</p> </li> </ul> <p>Preventing the development of dangerous AI - Moratoriums and regulations: Implementing temporary halts on the development of high-risk AI systems and enforcing legal frameworks, like the EU AI Act, to regulate or prohibit certain AI capabilities.</p> <ul> <li>Controlling Access and Replication : Limiting the distribution and replication of powerful AI systems to prevent widespread misuse.</li> </ul> <p>Maintaining human control - Meaningful human oversight: Ensuring AI systems, especially those involved in critical decision-making processes, operate under human supervision to prevent irreversible consequences.</p> <p>In conclusion, mitigating AI's risks requires a multifaceted approach combining governance, public engagement, economic incentives, legal measures, and promoting a global safety culture. By fostering international cooperation and prioritizing safety and ethical considerations in AI development, humanity can navigate the challenges posed by advanced AI technologies and harness their potential for the greater good. For more information, see the chapters on AI Governance.</p>"},{"location":"chapters/03/06/#02","title":"3.6.2 Strategy B: Organizational safety","text":"<p>Accidents Are Hard to Avoid, even when the incentive structure and governance try to ensure that there will be no problems. For example, even today, there are still accidents in the aerospace industry.</p> Figure 3.8: The Swiss cheese model shows how technical factors can improve organizational safety. Multiple  layers of defense compensate for each other\u2019s individual weaknesses, leading to a low overall level of risk. (Hendrycks et al., 2023) <p>To solve those problems, we advocate for a Swiss cheese model \u2014 no single solution will suffice, but a layered approach can significantly reduce risks. The Swiss cheese model is a concept from risk management, widely used in industries like healthcare and aviation. Each layer represents a safety measure, and while individually they might have weaknesses, collectively they form a strong barrier against threats. Organizations should also follow safe design principles (Hendrycks &amp; Mazeika, 2022), such as defense in depth and redundancy, to ensure backup for every safety measure, among others.</p> <p>Many solutions can be imagined to reduce those risks, even if none is perfect. The first step could be commissioning external red teams to identify hazards and improve system safety. This is what OpenAI did with METR to evaluate GPT-4. However AGI labs also need an internal audit team for risk management. Just like banks have risk management teams, this team needs to be involved in the decision processes, and key decisions should involve a chief risk officer to ensure executive accountability. One of the missions of the risk management team could be, for example, designing pre-set plans for managing security and incidents.</p> <p>Accidents could also arise during training before the deployment . Sporadically, this can also be an error sign or a bug (Ziegler et al., 2020). To avoid accidents during training, the training should also be responsible. Model evaluation for extreme risks, which was written by researchers from OpenAI, Anthropic, and DeepMind, lays out a good baseline strategy for what needs to be done before training, during training, before deployment, and after deployment. (Shevlane et al., 2023)</p> Figure 3.9: A workflow for training and deploying a model responsibly. (Shevlane et al., 2023)"},{"location":"chapters/03/06/#03","title":"3.6.3 Strategy C: Safety Culture","text":"<p>AI safety is a socio-technical problem that requires a socio-technical solution . As such, the resolution to these challenges cannot be purely technical. Safety culture is crucial for numerous reasons. At the most basic level, it ensures that safety measures are at least taken seriously. This is important because a disregard for safety can lead to regulations being bypassed or rendered useless, as is often seen when companies that don't care about safety face audits (Manheim, 2023).</p> <p>The challenge of industry-wide adoption of technical solutions. Proposing a technical solution is only the initial step toward addressing safety. A technical solution or set of procedures needs to be internalized by all members of an organization. When safety is viewed as a key objective rather than a constraint, organizations often exhibit leadership commitment to safety, personal accountability for safety, and open communication about potential risks and issues (Hendrycks et al., 2023).</p> <p>Reaching the standards of the aerospace industry. In aerospace, stringent regulations govern the development and deployment of technology. For instance, an individual cannot simply build an airplane in their garage and fly passengers without undergoing rigorous audits and obtaining the necessary authorizations. In contrast, the AI industry operates with significantly fewer constraints, adopting an extremely permissive approach to development and deployment, allowing developers to create and deploy almost any model. These models are then integrated into widely used libraries, such as Hugging Face, and those models can then proliferate with minimal auditing. This disparity underscores the need for a more structured and safety-conscious framework in AI. By adopting such a framework, the AI community can work towards ensuring that AI technologies are developed and deployed responsibly, with a focus on safety and alignment with societal values.</p> <p>Safety culture can transform industries. Norms in the pursuit of safety can be a powerful way to notice and discourage bad actors. In the absence of a strong safety culture, companies and individuals may be tempted to cut corners, potentially leading to catastrophic outcomes (Manheim, 2023). Capabilities often trade off with safety. The adoption of safety culture in the aerospace sector has transformed the industry, making it more attractive and generating more sales. Similarly, an ambitious AI safety culture would require the establishment of a large AI security industry.</p> <p>If achieved, safety culture would be a systemic factor that prevents AI risks. Rather than focusing solely on the technical implementation of a particular AI system, attention must also be given to social pressures, regulations, and safety culture. This is why engaging the broader ML community that is not yet familiar with AI Safety is critical (Hendrycks, 2022).</p> <p>How to concretely increase public awareness and safety culture?</p> <ul> <li> <p>Open letters : Initiatives like open letters, similar to the one from the Future of Life Institute (Future of Life Institute, 2023), can spark public debate on AI risks.</p> </li> <li> <p>Safety culture promotion : Advocating for a culture of safety among developers and researchers to preemptively address potential risks, for example by organizing internal training on safety. For example, internal training for cybersecurity is already common for some companies. Opening AI safety courses in universities and training future ML practitioners is also important.</p> </li> <li> <p>Building consensus : Create a global AI risk assessment body, similar to the IPCC for climate change, to standardize and disseminate AI safety findings.</p> </li> </ul> <ol> <li> <p>For example, in the pharmaceutical industry for drug development, companies sometimes enter into co-development and profit-sharing agreements to share the risks and rewards of bringing a new drug to market. For example, in 2014, Pfizer and Merck entered into a global alliance to co-develop and co-commercialize an anti-PD-L1 antibody for the treatment of multiple cancer types.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/03/07/","title":"3.7 Conclusion","text":"Reading Time 2 min <p>This chapter has provided an overview of the current solutions landscape, highlighting the key strategies and approaches being explored to mitigate risks associated with AI misuse, alignment, and systemic impacts.</p> <p>Addressing AI misuse involves strategies such as restricting access to high-risk models through monitored APIs, accelerating the development of defensive technologies, and establishing legal frameworks and social norms to discourage harmful applications. Alignment of AGI remains a formidable challenge, with current techniques proving fragile and insufficient for ensuring robust alignment of highly capable systems. Researchers are exploring avenues such as automating alignment research, developing safe-by-design architectures, and fostering international cooperation to navigate the path toward aligned superintelligence.</p> <p>Systemic risks introduced or exacerbated by AI \u2013 such as exacerbated biases, value lock-in, mental health concerns, and societal misalignment \u2013 require a multifaceted approach that combines technical solutions with governance frameworks, economic incentives, and public engagement. Transversal solutions, including effective AI governance, organizational safety practices, and the cultivation of a strong safety culture, are essential for creating a robust and adaptive framework to manage the diverse risks associated with AI development.</p> <p>As the field of AI safety continues to mature, it is crucial to recognize that no single solution will suffice. A layered, \"Swiss cheese\" approach that combines multiple strategies and engages stakeholders across disciplines is necessary to navigate the challenges ahead. Ongoing research, collaboration, and public discourse will be vital in shaping the trajectory of AI development to ensure that the transformative potential of these technologies is harnessed for the benefit of humanity while mitigating existential risks.</p> <p>The path forward requires a collective commitment to prioritizing safety, ethics, and alignment in AI development. By fostering a culture of responsibility, investing in research and governance frameworks, and engaging in proactive risk management, we can work towards a future in which AI systems are powerful allies in solving global challenges and improving the human condition.</p>"},{"location":"chapters/04/","title":"Chapter 04 - Governance","text":"Authors                      Charles Martinet                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Markov Grey, Charbel-Raphael Segerie, L\u00e9o Karoubi          Last Updated 2024-12-10 Reading Time 119 min (core) Also available on Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>1:00 - Imprecise - The discussion on the diagram is somewhat underwhelming; we encourage listeners to examine the diagram themselves\u2014it's not a map, it's a diagram</li> <li>2:00 - Error - Frontier AI is not designing an AI that is reshaping the world, at least not currently. An AI that is reshaping the world would be classified as \"transformative AI\"</li> <li>14:00 - Outdated - The Executive Order has recently been revoked by the Trump administration</li> <li>26:00 - Update - The Council of Europe treaty has been published</li> <li>After 30:00 - Note - There is a lot of slop that could be trimmed down</li> </ul> <p>Found errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/04/#introduction","title":"Introduction","text":"Video 4.1: Optional video to get an overview of Governance. <p>The Bletchley Declaration (signed by 28 countries, including all AI leaders, and the EU, 2023)</p> <p>\"Substantial risks may arise from potential intentional misuse or unintended issues of control relating to alignment with human intent. These issues are in part because those capabilities are not fully understood [...] There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models.\"</p> <p>Artificial intelligence (AI) has the potential to revolutionize numerous aspects of society, from healthcare to transportation to scientific research. Recent advancements have demonstrated AI's ability to defeat world champions at Go, generate photorealistic images from text descriptions, and discover new antibiotics. However, these developments also raise significant challenges and risks.</p> <p>Policymakers, researchers, and the general public express both excitement about AI's potential and concern about its risks, including job displacement, privacy infringements, and the potential for AI systems to make consequential mistakes or be misused. While technical AI safety research is necessary to ensure AI systems behave reliably and align with human values as they become more capable and autonomous, it alone is insufficient to address the full spectrum of challenges posed by advanced AI systems.</p> <p>The scope of AI governance is broad, so this chapter will primarily focus on large-scale risks associated with frontier AI - highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety (Anderljung et al., 2023). We will examine why governance is necessary, how it complements technical AI safety efforts, and the key challenges and opportunities in this rapidly evolving field. Our discussion will center on the governance of commercial and civil AI applications, as military AI governance involves a distinct set of issues that are beyond the scope of this chapter.</p> Figure 4.1: Distinguishing AI models according to their level of potential harm and generality. We focus here on frontier AI models (U.K. government, 2023) <p>AI governance can be defined as \"the study and shaping of governance systems - including norms, policies, laws, processes, politics, and institutions - that affect the research, development, deployment, and use of existing and future AI systems in ways that positively shape societal outcomes\" (Maas, 2022). It encompasses both research into effective governance approaches and the practical implementation of these approaches. AI governance also addresses the broader systemic impacts of AI, including the interactions between multiple AI systems and their effects on economic, political, and social structures.</p> <p>This chapter will also examine the current state of AI governance, proposed frameworks and policies, and the roles that various stakeholders \u2013 including governments, industry, academia, and civil society \u2013 can play in shaping the future of AI. The scope of this chapter includes:</p> <ul> <li> <p>An overview of AI development processes and key challenges in AI governance</p> </li> <li> <p>Governance parameters and the role of compute</p> </li> <li> <p>Critical issues in AI governance</p> </li> <li> <p>Layers of responsibility: corporate, national, and international governance</p> </li> </ul> <p>By the end of this chapter, you'll have a comprehensive understanding of why AI governance matters and how it can help ensure that the development of frontier AI aligns with human values and societal well-being.</p>"},{"location":"chapters/04/01/","title":"4.1 Governance Foundations","text":"Reading Time 8 min <p>What is AI governance? AI governance refers to the development of rules, policies, and institutions that shape how AI systems are researched, developed, and deployed. While technical AI safety work focuses on building reliable and aligned systems, governance addresses the broader challenge of managing AI's impact on society. This includes corporate practices, government regulations, standards development, and international coordination mechanisms. The goal is to ensure AI development benefits humanity while managing potential risks.</p> <p>How do we usually govern technologies? Traditional technology governance relies on several key assumptions. First, it is usually assumed that we can predict how a technology will be used and its likely impacts. Second, that we can effectively control its development pathway. And third, that we can regulate specific applications or end-uses. For example, pharmaceutical governance uses clinical trials and approval processes based on intended medical applications. Nuclear technology is controlled through international treaties, safeguards, and monitoring of specific facilities and materials. These approaches work when technologies follow relatively predictable development paths and have clear applications.</p> <p>What makes AI governance uniquely challenging? Even though we have been regulating technological advancements for decades, existing solutions to regulate \u201ctech\u201d might not be sufficient for AI. To better understand why AI governance resists traditional solutions, we can examine AI through three different lenses all of which might require different governance approaches (Dafoe, 2022):</p> <ul> <li> <p>AI as general-purpose technology : AI can transform many sectors simultaneously, like electricity or computers before it. This means sector-specific regulations - the backbone of traditional technology governance - cannot adequately address AI's broad systemic effects. The impacts span across society in ways that make targeted regulation insufficient.</p> </li> <li> <p>AI as an information technology : AI processes and generates information in novel ways. This creates unprecedented challenges around security, privacy, and information integrity. Traditional governance frameworks weren't designed to handle technologies that can rapidly generate and manipulate information at massive scale. The speed and scope of potential information impacts outstrip traditional control mechanisms.</p> </li> <li> <p>AI as an intelligence technology : AI raises unique control challenges. As systems become more capable, they may develop sophisticated ways to evade controls or pursue unintended objectives like we highlighted in the risks chapter. There are several dangerous capabilities (e.g. autonomous replication, scheming, deception, etc.) that create fundamental challenges that we have rarely (if ever) seen in governance - how do you reliably control a system that could potentially out-think its control mechanisms?</p> </li> </ul> <p>As we saw in the capabilities chapter, we have already seen power narrow AI (ANI) but now we are continuously making systems that are both capable and increasingly general purpose.This means they become dual purpose tools that can be used for various different tasks.</p> Figure 4.2: The two-dimensional outlook of capabilities and generality. The different curves are meant to represent the different paths we can take to ASI. Every single point on the path corresponds to a different level of AGI. The specific development trajectory is hard to forecast but progress is continuous. <p>These three lenses - general-purpose, information, and intelligence technology - help us analyze why AI development follows such different patterns than other technologies. Traditional governance assumes we can contain a technology's impact to specific sectors, control how information flows, and reliably predict and constrain system behaviors. But AI's mixed nature as a general-purpose, information processing, and potentially intelligent technology challenges all of these assumptions.</p> <p>How do these three lenses create challenges for governance? Even though we have been regulating technological advancements for decades, this AI development process makes AI governance unique. Three fundamental problems emerge from these development characteristics.</p> <p>The unexpected capabilities problem. AI systems can develop surprising abilities that weren't part of their intended design. (Ganguli et al., 2022; Bommasani et al., 2022; Grace et al., 2024) As we saw multiple times in the capabilities chapter, foundation models have shown \"emergent\" capabilities that appear suddenly as models scale up with more data, parameters and compute, from becoming unexpectedly capable at basic arithmetic to complex reasoning. This makes it difficult to evaluate risks before deployment since we can't reliably predict what systems might be capable of.</p> Figure 4.3: Example of unexpected capabilities problem. Graphs of several metrics that improve suddenly and unpredictably as models increase in size (Ganguli et al., 2022) <p>The deployment safety problem. Once deployed, AI systems can be repurposed for many different applications - both beneficial and harmful. These systems are inherently dual-use - their capabilities can be redirected toward unintended purposes after deployment. Users regularly discover new capabilities that weren't anticipated by the original developers (U.K. government, 2023). We dedicated several sections in the risks chapter to this and saw many different ways that problems can arise. The problem begins with simple circumvention of safety measures - users have found numerous \"jailbreaks\" to bypass content restrictions in language models (Anderljung et al., 2023). But the challenges escalate quickly. Language models trained for helpful dialogue have been repurposed to generate misinformation (Slattery et al., 2024) and assist with cyberattacks (CAIS, 2024; Ladish, 2024), potentially even designing novel bio weapons (Hendrycks 2023; Marchal et al., 2024; Fang et al., 2024). We are now also seeing the emergence of autonomous AI agents that can chain together model capabilities in novel and unpredictable ways using new tools post deployment.</p> Figure 4.4: Example of deployment safety problem. A schematic of using autonomous LLM agents to hack websites. (Fang et al., 2024) Once a dual purpose technology is public, it can be used for a variety of things both beneficial and harmful. <p>The proliferation problem. AI capabilities can spread rapidly through multiple channels - open-source releases, model theft, or reproduction by other actors. As we saw in the strategies chapter, once capabilities exist, they become very difficult to contain. Models can be stolen, leaked, or reproduced by other groups within months. One example is the rapid open-source replication of ChatGPT-like capabilities, leading to discovery of new capabilities and removal of safety features (Solaiman et al., 2024) Even API-based models can have their capabilities extracted through techniques like model distillation. (U.K. government, 2023)</p> <p>How do these problems interact and compound? These challenges don't exist in isolation - they interact and amplify each other in ways that make AI governance even harder. The unexpected capabilities problem makes deployment safety more difficult to ensure, since we can't reliably predict what abilities might emerge that could be misused. The deployment flexibility of AI systems makes proliferation more concerning, since capabilities can be repurposed for harmful uses after they spread. And proliferation increases the chances of discovering unexpected capabilities through experimentation by many actors (Anderljung et al., 2023).</p> <p>What is the function of governance? Governance is not solely about restrictions; it also encompasses functions that facilitate responsible innovation, such as providing guidance, fostering collaboration, and creating safe spaces for experimentation. Although we\u2019re concerned with challenges whose answers mostly rely on setting guardrails, these enabling functions are equally important. Governance fulfills several key functions. A couple of examples include:</p> <ul> <li> <p>Visibility : Enhanced visibility is fundamental to effective oversight. It involves creating mechanisms that bring transparency to AI development processes, allowing stakeholders to understand and monitor the progress and potential impacts of AI. Visibility enables verification - the confirmation of claims made by AI developers or other actors and the assessment of AI systems against established standards or benchmarks.</p> </li> <li> <p>Enforcement : Governance needs to provide the means to ensure compliance with regulations and ethical guidelines. This can range from legal sanctions to market exclusions for non-compliant actors.</p> </li> </ul> <p>What levers can governance use to affect these targets? To execute these functions, governance systems employ a variety of mechanisms (Howlett, 2019). Here are examples of just a few of the mechanisms that governance has on hand to affect the chosen governance target:</p> <ul> <li> <p>Information-based tools : We can leverage the power of knowledge dissemination. These might include mandatory disclosure requirements for AI companies or public education initiatives to increase AI literacy.</p> </li> <li> <p>Authoritative tools : We can draw upon the power of institutions to set and enforce rules. This could involve legislation, executive orders, or judicial decisions that directly regulate AI development and use.</p> </li> <li> <p>Standards : These play a core role in AI governance, serving as a bridge between broad principles and specific practices (Cihon, 2019). They can be technical, like those defining AI performance metrics, or ethical, outlining acceptable practices in AI development.</p> </li> <li> <p>Incentives : This mechanism helps shape behavior through rewards and penalties. These can be financial, such as tax breaks for companies investing in AI safety research, or reputational, like certification schemes that recognize responsible AI practices. By aligning economic and social motivations with governance objectives, incentives can drive voluntary compliance and innovation in governance approaches.</p> </li> </ul> <p>What does this mean for governance approaches? These fundamental challenges - unexpected capabilities, deployment safety risks, and rapid proliferation - mean we need new approaches to governing AI development. Traditional regulatory tools like application-specific permits or post-deployment monitoring won't be sufficient given AI's unique characteristics. To build effective governance, we need to identify what aspects of the AI development pipeline we can meaningfully influence before capabilities emerge or proliferate. We need to understand which intervention points give us the most leverage while allowing beneficial innovation to continue. In the next section, we'll examine specific targets along the AI development and deployment pipeline - from key inputs like compute and data to deployment controls and monitoring systems - evaluating each through the lens of these core challenges.</p>"},{"location":"chapters/04/02/","title":"4.2 Governance Targets","text":"Reading Time 16 min <p>Why do we need specific governance targets? The challenges we explored in the previous section - unexpected capabilities, deployment safety risks, and rapid proliferation - mean we need to carefully choose where and how to intervene in AI development (Anderljung et al., 2023). This requires identifying both what to govern (targets) and how to govern it (mechanisms).</p> <p>What makes a good governance target? Before we get into what specifically we can and do target in AI governance, we need to understand what makes a good governance target in general. Effective points of intervention for good governance need to have a combination of properties. They should be concretely measurable, meaningful, and practical:</p> <ul> <li> <p>Measurability : We should be able to track and verify what's happening. The semiconductor industry provides a good example - chip production can be measured in precise units, making it possible to track and regulate manufacturing.</p> </li> <li> <p>Controllability : There must be concrete ways to influence the chosen targets. Think about how export controls on advanced semiconductors work because the supply chain has clear chokepoints that can be regulated (Sastry et al., 2024).</p> </li> <li> <p>Meaningful : Finally, targets should address fundamental aspects of development. In the case of AI this means addressing the core things that shape capabilities and risks. While regulating end applications is important, targeting core inputs like data and compute can help shape development before risks materialize.</p> </li> </ul> <p>Which targets matter most? So if we want to apply these targets concretely to the AI development process, then we have several potential intervention points. Early in development, we can target key inputs: the data used to train models, the compute infrastructure required to run training, and the model development process itself. Once systems are built, deployment offers additional targets: controlling who has access to what capabilities, monitoring how systems are used, and assessing their impacts on society (Heim et al., 2024). Each of these targets presents different opportunities and challenges for governance.</p>"},{"location":"chapters/04/02/#01","title":"4.2.1 Compute Governance","text":"<p>What makes compute a good target for governance? Out of the things that drive AI performance - data, compute, and algorithms - compute holds a unique position. If someone has a copy of some algorithm/data, this doesn't prevent others from having the same. When it comes to GPUs on the other hand, they can't just be downloaded or copy-pasted into existence. It is a concrete, tangible physical constraint on AI development. Let's look at the variables we had for a good governance target and see how compute fares:</p> <ul> <li> <p>Measurability: Unlike data or algorithms, compute leaves clear physical footprints. Training frontier models requires massive data centers housing thousands of specialized chips (Pilz &amp; Heim, 2023). We can track computational capacity through well-defined metrics like floating point operations (FLOPS), allowing us to identify potentially risky training runs before they begin (Heim &amp; Koessler, 2024).</p> </li> <li> <p>Controllability: The supply chain for advanced AI chips has clear chokepoints - just a handful of companies control critical steps like chip design and manufacturing (Grunewald, 2023). These chokepoints enable governance through mechanisms like export controls or licensing requirements (Sastry et al., 2024).</p> </li> <li> <p>Meaningfulness: As we discussed in the risks chapter, the most dangerous capabilities are likely to emerge from highly capable models, which require massive amounts of specialized computing infrastructure to train and run (Anderljung et al., 2023; Sastry et al., 2024). Compute requirements directly constrain what AI systems can be built - even with cutting-edge algorithms and vast datasets, organizations cannot train frontier models without sufficient computing power (Besiroglu et al., 2024). This makes compute a particularly meaningful point of intervention, as it allows us to shape AI development before potentially dangerous systems emerge rather than trying to control them after the fact (Heim et al., 2024).</p> </li> </ul> Interactive Figure 4.1: Showcasing how capabilities seem directly proportional to increases in compute. (Giattino et al., 2023) <p>The discussion in the next few subsections will focus on the elements of actually implementing compute governance. We explain how concentrated supply chains enable tracking and monitoring of compute, we also give a brief discussion of hardware based on-chip compute governance mechanisms, and finally discuss some limitations based around limitations to governance based on compute thresholds, and how distributed training and open source might challenge compute governance.</p> Figure 4.5: A graphical depiction of the relationship of AI to various aspects of compute."},{"location":"chapters/04/02/#01-01","title":"4.2.1.1 Tracking","text":"<p>How is the AI chip supply chain structured? AI-specialized chips emerge from a complex global process. It starts with mining and refining raw materials like silicon and rare earth elements. These materials become silicon wafers, which are transformed into chips through hundreds of precise manufacturing steps. The process requires specialized equipment - particularly photolithography machines from ASML - along with various chemicals, gases, and tools from other suppliers (Grunewald, 2023).</p> Figure 4.6: The compute supply chain. (Belfield &amp; Hua 2022) <p>Where are the chokepoints in design and manufacturing? The supply chain is dominated by a handful of companies at critical steps. NVIDIA designs most AI-specialized chips, TSMC manufactures the most advanced chips, and ASML produces the machines needed by TSMC to manufacture the chips (Grunewald, 2023; Pilz et al., 2023). It is estimated that NVIDIA controls around 80% of the market for AI training GPUs (Jagielski, 2024). Similarly both TSMC, and ASML maintain strong leads in their respective domains. (Pilz et al., 2023).</p> Interactive Figure 4.2: Market share for logic chip production, by manufacturing stage (Giattino et al., 2023) <p>Where are the chokepoints in usage and infrastructure? Besides building the chips, the purchase and operation of them at the scale needed for frontier AI models requires massive upfront investment. Just three providers - Amazon, Microsoft, and Google - control about 65% of cloud computing services (Jagielski, 2024). A small number of AI companies like OpenAI, Anthropic, and DeepMind operate their own massive GPU clusters, but even these require specialized hardware subject to supply chain controls (Pilz &amp; Heim, 2023).</p> <p>What do these chokepoints mean for governance? This concentration creates natural intervention points. Authorities only need to work with a small number of key players to implement controls, as demonstrated by U.S. export restrictions on advanced chips (Heim et al., 2024). It is worth keeping in mind though that this heavy concentration is also concerning. We're seeing a growing \"compute divide\" - while major tech companies can spend hundreds of millions on AI training, academic researchers struggle to access even basic resources (Besiroglu et al., 2024). This impacts who can participate in AI development and reduces independent oversight of frontier models. It also raises concerns around potential power concentration.</p> Figure 4.7: The spectrum of chip architectures with trade-offs in regards to efficiency and flexibility. <p>How can we target controls effectively? Rather than trying to control all computing infrastructure, governance can focus specifically on specialized AI chips. These are distinct from general-purpose hardware in both capabilities and supply chains. By targeting only the most advanced AI-specific chips, we can address catastrophic risks while leaving the broader computing ecosystem largely untouched (Heim et al., 2024). For example, U.S. export controls specifically target high-end data center GPUs while excluding consumer gaming hardware.</p>"},{"location":"chapters/04/02/#01-02","title":"4.2.1.2 Monitoring","text":"<p>How can we detect concerning AI training runs? Training frontier AI models leaves multiple observable footprints. The most reliable is energy consumption - training runs that might produce dangerous systems require massive power usage, often hundreds of megawatts, creating distinctive patterns (Wasil et al., 2024 ; Shavit, 2023) Besides energy, other technical indicators include network traffic patterns characteristic of model training, hardware procurement and shipping records, cooling system requirements and thermal signatures, infrastructure buildout like power substation construction. (Sastry et al., 2024; Shavit, 2023; Heim et al., 2024). These signals become particularly powerful when combined - sudden spikes in both energy usage and network traffic at a facility containing known AI hardware strongly suggest active model training.</p> <p>What role do compute thresholds play? Regulations have already begun using compute thresholds to trigger oversight mechanisms. The U.S. Executive Order on AI requires companies to notify the government about training runs exceeding 10^26 operations - a threshold designed to capture the development of the most capable systems. The EU AI Act sets an even lower threshold of 10^25 operations, requiring not just notification but also risk assessments and safety measures. (Heim &amp; Koessler, 2024). These thresholds help identify potentially risky development activities before they complete, enabling preventive rather than reactive governance.</p> Figure 4.8: Compute Thresholds as Specified in the US Executive Order 14110 (Sastry et al., 2024) <p>What governance roles can cloud providers play? Most frontier AI development happens through cloud computing platforms rather than self-owned hardware. This creates natural control points for oversight, since most organizations developing advanced AI must work through these providers (Heim et al., 2024, Governing Through the Cloud). Cloud providers' position between hardware and developers allows them to implement controls that would be difficult to enforce through hardware regulation alone. They maintain the physical infrastructure ,track compute usage patterns and maintain development records. They can also monitor compliance with safety requirements, can implement access controls and respond to violations (Heim et al., 2024; Chan et al., 2024).</p> <p>How can cloud providers help implement oversight? One promising approach is \"know-your-customer\" (KYC) requirements similar to financial services. Providers would verify the identity and intentions of clients requesting large-scale compute resources, maintain records of significant compute usage, and report suspicious patterns (Egan &amp; Heim, 2023). This can be done while protecting privacy - basic workload characteristics can be monitored without accessing sensitive details like model architecture or training data (Shavit, 2023). Similar KYC laws can be applied to the supply chain on purchases of state of the art AI compute hardware.</p>"},{"location":"chapters/04/02/#01-03","title":"4.2.1.3 On-Chip Controls","text":"<p>How does on-chip compute governance work? Beyond monitoring and detection, compute infrastructure can include active control mechanisms built directly into the processor hardware. Similar to how modern smartphones and computers include secure elements for privacy and security, AI chips can incorporate features that verify and control how they're used (Aarne et al., 2024). These features could prevent unauthorized training runs or ensure chips are only used in approved facilities (Aarne et al., 2024). The verification happens at the hardware level, making it much harder to bypass than software-based controls.</p> <p>What specific controls could be implemented? Several approaches show promise. Usage limits could cap the amount of compute used for certain types of AI workloads without special authorization. Secure logging systems could create tamper-resistant records of how chips are used. Location verification could ensure chips are only used in approved facilities (Brass &amp; Aarne, 2024). Hardware could even include \"safety interlocks\" that automatically pause training if certain conditions aren't met. Ideas like this are also called on-chip governance. (Aarne et al., 2024).</p> <p>How does this compare to existing security features? We already see similar concepts in cybersecurity, with features like Intel's Software Guard Extensions, or trusted platform modules (TPM) (Intel, 2024) providing hardware-level security guarantees. While we're still far from equivalent safeguards for AI compute, early research shows promising directions (Shavit, 2023, What does it take to catch a Chinchilla?). Some chips already include basic monitoring capabilities that could be expanded for governance purposes (Petrie et al., 2024).</p>"},{"location":"chapters/04/02/#01-04","title":"4.2.1.4 Limitations","text":"<p>What fundamental challenges does compute governance face? While compute offers many advantages as a governance target, several trends could reduce its effectiveness. While the trend over the last decade has involved more compute this might not last forever. Even though research suggests continued model scaling is still possible through 2030 (Sevilla et al., 2024) algorithmic improvements continuously enhance efficiency, meaning the same compute achieves more capability over time. Smaller models could begin to show comparable capabilities and risks. For example, Falcon 180B is outperformed by far smaller models like Llama-3 8B. This makes static compute thresholds less reliable as capability indicators without regular updates. (Hooker, 2024). Moreover, 'inference-time compute' improvements like best-of-n sampling, chain-of-thought reasoning, and model distillation can dramatically improve model capabilities without changing training compute. Current governance frameworks do not account for these post-training enhancements. (Shavit, 2023)</p> Figure 4.9: Estimates of the scale constraints imposed by the most important bottlenecks to scale. Each estimate is based on historical projections. The dark shaded box corresponds to an interquartile range and light shaded region to an 80% confidence interval. The four boxes showcase four constraints that might slow down growth in the future: power, chips (compute), data and latency. (Sevilla et al., 2024) <p>Smaller more specialized models might still cause risks. Different domains have very different compute requirements. For example, while language models often demand extensive compute, biology and code models typically require far less. Highly specialized models trained on specific datasets might develop dangerous capabilities while using relatively modest compute. For example, models focused on biological or cybersecurity domains could pose serious risks even with compute usage below typical regulatory thresholds (Mouton et al., 2024; Heim &amp; Koessler, 2024).</p> <p>How do we balance control with access? While compute governance can help manage AI risks, overly restrictive controls could have negative consequences. Right now, only a handful of organizations can afford the compute needed for frontier AI development. (Purtova et al., 2022; Pilz et al., 2023) Adding more barriers could worsen this disparity, concentrating power in a few large tech companies and reducing independent oversight (Besiroglu et al., 2024).</p> <p>How do we balance safety with research and innovation? Academic researchers already struggle to access the compute they need for meaningful AI research. As models get larger and more compute-intensive, this gap between industry and academia grows wider. (Besiroglu et al., 2024; Zhang et al., 2021) Large compute clusters have many legitimate uses beyond AI development - from scientific research to business applications. Overly broad restrictions could hinder beneficial innovation. Additionally, once models are trained, they can often be run for inference using much less compute than training required. This makes it challenging to control how existing models are used without imposing overly restrictive controls on general computing infrastructure (Sastry et al., 2024). Without specific provisions for research access - like subsidized compute grants or academic partnerships - governance measures could unintentionally slow the development of AI safety research and external evaluation capabilities.</p> <p>Could distributed training approaches bypass compute governance controls? Currently, training frontier models requires concentrating massive compute resources in single locations due to communication requirements between chips. Decentralized or distributed training methods have not really caught up to centralized methods. (Douillard et al., 2023; Jaghouar et al., 2024). However, if we see fundamental advances in distributed training algorithms this could eventually allow training to be split across multiple smaller facilities. While this remains technically challenging and inefficient, it could make detection and control of dangerous training runs more difficult (Anderljung et al., 2023).</p> <p>Given these limitations, compute monitoring and thresholds should primarily operate as an initial screening mechanism to identify models warranting further scrutiny, rather than as the sole determinant of specific regulatory requirements. They are most effective when used to trigger oversight mechanisms such as notification requirements and risk assessments, whose results can then inform appropriate mitigation measures.</p>"},{"location":"chapters/04/02/#02","title":"4.2.2 Data Governance","text":"<p>This section can be considered extra detail and safely skipped.</p> <p>What role does data play in AI risks? Data fundamentally shapes what AI systems can do and how they behave. For frontier foundation models, training data influences both capabilities and alignment - what systems can do and how they do it. Low quality or harmful training data could lead to misaligned or dangerous models (\"garbage in, garbage out\"), while carefully curated datasets might help promote safer and more reliable behavior (Longpre et al., 2024; Marcucci et al., 2023).</p> <p>How well does data meet our governance target criteria? Data as a governance target presents a mixed picture when evaluated against our key criteria. Let's look at each:</p> <ul> <li> <p>Measurability : While we can measure raw quantities of data, assessing its quality, content, and potential implications is far more difficult. Unlike physical goods like semiconductors, data can be copied, modified, and transmitted in ways that are hard to track. This makes comprehensive measurement of data flows extremely challenging.</p> </li> <li> <p>Controllability : Data's non-rival nature means it can be copied and shared widely - once data exists, controlling its spread is very difficult. Even when data appears to be restricted, techniques like model distillation can extract information from trained models (Anderljung et al., 2023). However, there might still be some promising control points, particularly around original data collection and the initial training of foundation models.</p> </li> <li> <p>Meaningfulness : Data is particularly meaningful when it comes to AI development. The data used to train models directly shapes their capabilities and behaviors. Changes in training data can significantly impact model performance and safety. This makes data governance potentially powerful, but only if we can overcome the challenges of measurement and control.</p> </li> </ul> <p>What are the key data governance concerns? Several aspects of data require careful governance to promote safe AI development:</p> <ul> <li> <p>Training data quality and safety is fundamental - low quality or harmful data can create unreliable or dangerous models . For instance, technical data about biological weapons in training sets could enable models to assist in their development (Anderljung et al., 2023).</p> </li> <li> <p>Data poisoning and security pose increasingly serious threats . Malicious actors could deliberately manipulate training data to create models that behave dangerously in specific situations while appearing safe during testing. This might involve injecting subtle patterns that only become apparent under certain conditions (Longpre et al., 2024).</p> </li> <li> <p>Data provenance and accountability help ensure we can trace where model behaviors come from . Without clear tracking of training data sources and their characteristics, it becomes extremely difficult to diagnose and fix problems when models exhibit concerning behaviors (Longpre et al., 2023).</p> </li> <li> <p>Consent and rights frameworks protect both data creators and users . Many current AI training practices operate in legal and ethical grey areas regarding data usage rights. Clear frameworks could help prevent unauthorized use while enabling legitimate innovation (Longpre et al., 2024).</p> </li> <li> <p>Bias and representation in training data directly impact model behavior . Skewed or unrepresentative datasets can lead to models that perform poorly or make harmful decisions for certain groups, potentially amplifying societal inequities at a massive scale (Reuel et al., 2024).</p> </li> <li> <p>Data access and sharing protocols shape who can develop powerful AI systems . Without governance around data access, we risk either overly concentrated power in a few actors with large datasets, or conversely, uncontrolled proliferation of potentially dangerous capabilities (Heim et al., 2024).</p> </li> </ul> <p>How does data governance fit into overall AI governance? Even with strong governance frameworks, alternative data sources or synthetic data generation could potentially circumvent restrictions. Additionally, many concerning capabilities might emerge from seemingly innocuous training data through unexpected interactions or emergent behaviors. While data governance remains important and worthy of deeper exploration (see appendix), other governance targets may offer more direct leverage over frontier AI development in the near term. This is why we focus primarily on compute governance, which provides more concrete control points through its physical and concentrated nature.</p>"},{"location":"chapters/04/03/","title":"4.3 Key issues","text":"Reading Time 18 min"},{"location":"chapters/04/03/#01","title":"4.3.1 Competition","text":"<p>John Schulman (Co-Founder of OpenAI)</p> <p>\"[Talking about times near the creation of the first AGI] you have the race dynamics where everyone's trying to stay ahead, and that might require compromising on safety. So I think you would probably need some coordination among the larger entities that are doing this kind of training [...] Pause either further training, or pause deployment, or avoiding certain types of training that we think might be riskier.\"</p> <p>AI development firms are in competition with each other. Each breakthrough, each new capability demonstrated, raises the bar for the entire field. In this environment, taking time to thoroughly consider safety implications or ethical concerns can seem like a luxury these companies can ill afford. The mantra becomes \"move fast and break things,\" even when the \"things\" at stake may include core societal values or human well-being.</p> Interactive Figure 4.3: Our World in Data: Patents for AI by country (Giattino et al., 2023) <p>This dynamic isn't limited to the private sector. Nation-states, too, have recognized AI as a cornerstone of future economic and military power. Russian President Vladimir Putin's 2017 statement that \"whoever becomes the leader in this sphere will become the ruler of the world\" encapsulates the high-stakes nature of this competition. This perspective has sparked a flurry of government activity, with over 50 countries announcing national AI strategies and pouring massive public investments into the field (Stanford, 2024). A testament to the perceived strategic importance of AI for governments, or at least government officials, the former head of the U.S. National Security Agency is now on the board of OpenAI (Peters, 2024).</p> Interactive Figure 4.4: Countries with AI Strategies (Giattino et al., 2023) <p>The consequences of this competitive dynamic are problematic. Even if some actors recognize the need for caution and safety measures, unilateral action risks ceding advantage to less scrupulous competitors. This prisoner's dilemma writ large makes it exceedingly difficult for any single entity, be it a company or a country, to prioritize safety over speed (Askell et al., 2019). This also extends to government regulation: countries are tempted to prioritize their competitiveness in AI over ensuring safety and fundamental rights, because they may perceive the regulations to protect the latter as damaging innovation. Thus, the emphasis on national strategic interests often comes at the expense of domestic and international action on AI safety. Countries may be hesitant to support governance frameworks that could potentially constrain their AI ambitions or give competitors an edge.</p> Interactive Figure 4.5: Our World in Data: Cumulative number of large-scale AI systems by country (Giattino et al., 2023) <p>Policies Addressing these challenges requires multiple approaches. At the national level, policymakers must work to align the incentives of AI developers with broader societal interests. This could involve regulatory frameworks that mandate safety considerations, coupled with incentives for responsible AI development. Internationally, there's an urgent need for forums and agreements that can help manage the AI race, perhaps drawing lessons from arms control regimes or climate change negotiations.</p> <p>Moreover, fostering a shared understanding of AI risks among key stakeholders - from tech executives to national security officials - is crucial. This awareness-building must go hand in hand with efforts to reframe the AI race not as a zero-sum game, but as a collective endeavor to manage AI development.</p>"},{"location":"chapters/04/03/#02","title":"4.3.2 Proliferation","text":"<p>Imagine a cutting-edge AI model, capable of generating hyper-realistic deepfakes or designing novel bioweapons, is developed by a well-intentioned research lab. The lab, adhering to principles of open science, publishes their findings and releases the model's code as open-source. Within hours, the model is downloaded thousands of times across the globe. Within days, modified versions start appearing on code-sharing platforms. Within weeks, the capabilities that were once confined to a single lab have proliferated across the internet, accessible to anyone with a decent computer and an internet connection.</p> <p>This scenario, while hypothetical, isn't far from reality. The AI community has a strong culture of openness, with many researchers and companies releasing their models and findings to the public. This openness has undoubtedly accelerated progress in the field, but it also presents a significant governance challenge.</p> <p>The proliferation problem in AI governance stems from three main factors:</p> <ol> <li> <p>Open-source culture : Many AI researchers and organizations believe in the principles of open science, freely sharing their code and findings.</p> </li> <li> <p>General openness of the AI industry : Even when code isn't openly shared, the AI industry is characterized by a high degree of knowledge sharing through academic papers, conferences, and informal networks.</p> </li> <li> <p>Potential for theft : As AI becomes increasingly valuable, the risk of intellectual property theft, including through cyberattacks or insider threats, grows.</p> </li> </ol> <p>These factors combine to create an environment where potentially dangerous AI capabilities can spread rapidly and widely, outpacing our ability to govern their use effectively.</p> <p>The proliferation challenge extends beyond the spread of AI models or algorithms. It also encompasses the dissemination of key components in the AI supply chain, such as advanced semiconductors used in AI computing. Recent efforts by the U.S. to restrict the export of cutting-edge chips highlight the dual-use nature of these technologies and the difficulties in controlling their spread (Masi, 2024).</p> <p>Another crucial aspect of the proliferation problem is the offense-defense balance in AI capabilities (Tang et al., 2024). In many areas of AI development, offensive capabilities (such as developing and carrying out cyberattacks or crafting persuasive misinformation) can be easier to develop and deploy than defensive measures (such as using defensive cyber capabilities or filtering out misinformation).</p> <p>Verification Challenges</p> <p>This ease of proliferation creates significant hurdles for international governance efforts. Unlike some nuclear non-proliferation treaties, where satellite imagery and other remote sensing technologies can be used to monitor compliance (U.S. Congressional Research Service, 2011), verifying adherence to AI governance agreements would likely require deep access to an organization's or country's AI systems and development processes. And that may require access to highly sensitive or strategically valuable corporate or national secrets. Many countries will be reluctant to agree to inspections or information sharing that could compromise their strategic advantages or reveal the full extent of their AI capabilities.</p> <p>Imagine, for instance, an international agreement that prohibits the development of AI systems capable of autonomously launching cyber attacks. Verifying compliance with such an agreement would be incredibly difficult. It might require access to source code, training data, and testing environments - all of which could be considered state or corporate secrets.</p> <p>This verification challenge creates a trust deficit in international AI governance efforts. Countries may be reluctant to enter into agreements they can't verify, while those that do might constantly suspect others of cheating.</p> <p>Moreover, the ease of AI proliferation means that even if major powers agree to certain restrictions, smaller countries or non-state actors could potentially develop or acquire advanced AI capabilities. This dynamic further complicates international governance efforts.</p> <p>Policies How do we ensure responsible use of AI when potentially harmful capabilities are widely accessible? The key challenge for AI governance becomes finding the right balance between openness and control. Several potential solutions have been proposed to find the right balance:</p> <ul> <li> <p>Targeted Openness : Instead of a binary open/closed approach, AI developers could adopt a more nuanced stance. For instance, foundational research and non-sensitive applications could remain open, while potentially dangerous capabilities are subject to stricter controls.</p> </li> <li> <p>Staged releases : Rather than immediately making a lab\u2019s most advanced model publicly available, it could gradually release increasingly capable models (Solaiman, 2023). This allows developers to assess potential risks and misuse scenarios at each stage, informing decisions about subsequent releases. Developers can identify unforeseen issues or concerns; researchers, policymakers, and the public can reflect about the implications of more advanced AI systems; and society and relevant stakeholders have time to adapt to each level of capability before more powerful versions are released.</p> </li> <li> <p>Enhanced Information Security : As AI systems become more powerful, protecting them from theft or unauthorized access becomes crucial. This might involve developing new cybersecurity protocols specifically designed for AI systems.</p> </li> <li> <p>Export Controls and Access Restrictions : Governments might implement export controls on advanced AI systems or components, similar to those used for other sensitive technologies. Additionally, access to large-scale computing resources necessary for training frontier AI models could be restricted (Heim et al., 2024).</p> </li> <li> <p>Responsible Disclosure Practices : The AI community could develop norms around responsible disclosure of potentially dangerous capabilities, similar to those in the cybersecurity field (O\u2019Brien et al., 2024).</p> </li> <li> <p>Technical Measures : Researchers could explore technical solutions to limit the misuse of AI models, such as built-in use restrictions (Dong et al., 2024).</p> </li> <li> <p>International cooperation : This could involve creating new institutions or frameworks specifically designed to monitor and manage the spread of advanced AI capabilities.</p> </li> </ul>"},{"location":"chapters/04/03/#03","title":"4.3.3 Uncertainty","text":"<p>Greg Brockman (Co-Founder and Former CTO of OpenAI)</p> <p>\"The exact way the post-AGI world will look is hard to predict \u2014 that world will likely be more different from today's world than today's is from the 1500s [...] We do not yet know how hard it will be to make sure AGIs act according to the values of their operators. Some people believe it will be easy; some people believe it'll be unimaginably difficult; but no one knows for sure\"</p> <p>The governance of frontier AI is profoundly complicated by the pervasive uncertainty that shrouds the field. This uncertainty manifests in multiple dimensions.</p> <p>At the most fundamental level, there is deep uncertainty about the future trajectory of AI capabilities - although experts and forecasters have generally been surprised by the rapid pace of AI development (Cotra &amp; Piper 2024). Predicting the pace and direction of future advancements is challenging. This uncertainty is compounded by the potential for unexpected breakthroughs or emergent capabilities that could rapidly shift the risk landscape, making it difficult for governance frameworks to anticipate and prepare for all possible scenarios.</p> <p>Another critical area of uncertainty lies in understanding the relative importance of different factors in AI development. The interplay between computational power, data availability, and algorithmic innovations in driving AI progress is not fully understood. What is sometimes called the \"scaling debate\" has significant implications for governance approaches (Hooker &amp; Sandoval, 2024). If compute is the primary bottleneck, then regulations focusing on hardware access might be most effective. Conversely, if data or algorithmic breakthroughs are key, different governance levers would need to be prioritized.</p> <p>The nature and magnitude of potential risks posed by advanced AI systems are also subjects of considerable uncertainty. While there is largely a consensus on some current or near-term risks, such as AI-enabled disinformation or privacy violations, the long-term and more extreme risks are more contentious and difficult to quantify. The challenge for governance is to address these potential risks without overreacting or stifling beneficial innovation.</p> <p>This uncertainty extends to the efficacy of proposed technical solutions for AI safety and alignment. While research in these areas is progressing, it's unclear whether current approaches will scale to more advanced AI systems or if fundamentally new paradigms will be required. This creates a moving target for governance efforts, as the mechanisms needed to ensure AI safety may evolve rapidly alongside AI capabilities.</p> <p>The \"pacing problem\" further complicates matters. AI technology is advancing at a rate that often outstrips the ability of governance structures to adapt. Traditional regulatory processes, designed for slower-moving technologies, may struggle to keep up with the rapid evolution of AI capabilities. This creates a risk of governance frameworks becoming obsolete almost as soon as they are implemented.</p> <p>Compounding these challenges is the relative lack of expertise within many government bodies regarding cutting-edge AI technologies. This knowledge gap can lead to misguided policies or an inability to effectively oversee AI development and deployment. Bridging this expertise gap is crucial but challenging, given the competitive landscape for AI talent.</p> <p>Despite these uncertainties, the potential consequences of advanced AI systems are too significant to allow for inaction. This creates a paradoxical situation where decisions must be made and governance structures established in the face of deep uncertainty - as has occasionally been the case in other fields that grapple with decision-making under uncertainty, such as pandemic preparedness.</p> <p>Policies One approach to addressing this uncertainty is to increase visibility into AI development processes. This could involve implementing more robust reporting requirements for AI companies, including \"know-your-customer\" (KYC) policies for providers of AI services or compute.</p> <p>Enhancing state and regulatory capacity is another crucial step. This involves not only increasing the technical expertise within government bodies but also developing more agile regulatory frameworks that can adapt quickly to new developments. Regulatory sandboxes, where new AI technologies can be tested under controlled conditions, offer one potential model for more responsive governance.</p> <p>Scenario planning and red-teaming exercises can also play a valuable role in preparing for uncertain futures. By systematically exploring a range of possible AI development trajectories and their implications, governance bodies can develop more robust and adaptable strategies.</p> <p>Importantly, governance approaches should be designed with flexibility and adaptability in mind. This could involve building in regular review periods, establishing clear triggers for policy adjustments based on predefined milestones in AI capabilities, and maintaining open channels of communication between policymakers, researchers, and industry leaders.</p>"},{"location":"chapters/04/03/#04","title":"4.3.4 Accountability","text":"<p>Jan Leike (Former co-lead of the Superalignment project at OpenAI)</p> <p>\"[After resigning at OpenAI, talking about sources of risks] These problems are quite hard to get right, and I am concerned we aren't on a trajectory to get there [...] OpenAI is shouldering an enormous responsibility on behalf of all of humanity. But over the past years, safety culture and processes have taken a backseat to shiny products. We are long overdue in getting incredibly serious about the implications of AGI.\"</p> <p>Companies like OpenAI, Google DeepMind, and Anthropic are pushing the boundaries of what's possible, often moving faster than regulators can keep up. Their decisions about what to develop, how to develop it, and when to release it to the public have far-reaching consequences. Yet, there is currently little external oversight or even visibility into these processes.</p> <p>Take the release of GPT-3, for instance. The decision to release it first as a limited API, then more broadly, was made primarily by OpenAI's leadership. No regulatory body reviewed the model's capabilities and potential risks before its release. No standardized safety tests were required.</p> <p>Companies developing frontier AI technologies wield enormous power, with the potential to reshape societies, economies, and power structures globally. Yet they operate with a degree of autonomy that would be unthinkable in other high-stakes industries. For example, pharmaceutical companies can\u2019t release new drugs without regulatory approval, and nuclear power plants can\u2019t be built without impact assessments.</p> <p>The consequences of this lack of accountability are already becoming apparent. We've seen AI-generated deepfakes used to spread political misinformation (Swenson &amp; Chan, 2024). Language models have been used to create convincing phishing emails and other scams (Stacey, 2025). And there are growing concerns about AI systems perpetuating and amplifying societal biases.</p> <p>Finally, this is not just about preventing harm. Lack of accountability also erodes public trust in AI technologies (Afroogh et al., 2024). When people feel that these powerful systems are being developed behind closed doors, with little external oversight, it's natural to be skeptical or even fearful.</p> <p>Policies How do we make AI development more accountable without stifling innovation? There is no simple answer, but there are several promising approaches to consider. We talk more about proposals and approaches to this in the sections on corporate, national and international governance.</p> <p>A robust accountability framework for AI development requires interlocking mechanisms operating at different timescales and levels of governance. At the foundational level, pre-deployment approval systems could establish clear capability-based thresholds for AI development accompanied by regulatory requirements, similar to regulations in other high-risk industries. Deployment could be mode contingent on developers meeting safety and transparency requirements, creating a baseline for responsible development practices.</p> <p>Building on this foundation, ongoing oversight could be maintained through a combination of external audits and ethical review boards. Independent experts would evaluate AI systems' capabilities, training data, and potential impacts, while diverse stakeholders would assess broader ethical implications. This dual-track review process, modeled after successful frameworks in medical research, would help identify and address both technical and societal concerns throughout the development cycle.</p> <p>To ensure these oversight mechanisms have real impact, they must be backed by clear enforcement capabilities. A well-defined liability framework could establish legal responsibility for AI-related harms, creating strong incentives for careful development practices. This would be complemented by emergency intervention mechanisms, enabling regulatory bodies to respond swiftly to imminent risks from AI deployments \u2013 for instance, by halting the release of potentially dangerous systems.</p> <p>The effectiveness of these measures ultimately depends on transparency and international coordination. Regular public disclosures about AI capabilities, limitations, and risks would enable informed public discourse while protecting legitimate proprietary interests. Given the global nature of AI development, these national frameworks must be harmonized through international agreements to prevent regulatory arbitrage and establish consistent global standards. We talk a lot more about this in the section dedicated to international governance. This coordinated approach would help ensure that accountability measures remain robust even as AI technology continues to advance.</p>"},{"location":"chapters/04/03/#05","title":"4.3.5 Allocation","text":"<p>AI has the potential to reshape the distribution of power, wealth, and opportunities across society. The issue of allocation or distributive consequences revolves around several questions associated with the consequences of developing and deploying increasingly advanced AI systems: who controls these systems? Who reaps their benefits? And what happens to those left behind?</p> <p>The distributive consequences of AI span two interrelated dimensions: power and wealth. On the power front, we're seeing a gradual but significant shift in who holds the reins of influence and control in society. Those who develop and control advanced AI systems are gaining unprecedented leverage over economic, political, and social spheres.</p> <p>Large language models like GPT-3 or Claude are developed and controlled by a handful of tech companies and research institutions. This concentration of power raises serious questions about accountability, transparency, and systemic influence.</p> <p>On the wealth front, AI is driving a new wave of automation and productivity gains, but the benefits aren't being distributed evenly. We're seeing a widening gap between those who own and control AI technologies and those whose jobs and livelihoods are being disrupted by them.</p> <p>Previous technological revolutions have often led to increased inequality, at least in the short to medium term. And with AI, the stakes are even higher, because AI has the potential to be a truly general-purpose technology, one that could theoretically replace human cognitive labor across almost all domains. If (or when) AGI becomes a reality, the distributive consequences could be staggering. Whoever controls an entity capable of outperforming humans in virtually every cognitive task - from scientific research to strategic planning to creative endeavors - would wield considerable power and wealth.</p> <p>The prospect of AGI amplifies all the distributive concerns discussed so far. It could lead to extreme concentrations of power, potentially even enabling new forms of authoritarian control or technocratic governance.</p> <p>Policies How can we address the distributive consequences of AI development and deployment? There is no simple solution, but several approaches are being explored and debated. Redistributive policies could help spread the wealth generated by advanced AI systems. This could take the form of taxes on AI-driven profits, universal basic income programs, investment in education and retraining initiatives, or a \u2018Windfall Clause\u2019 (O\u2019Keefe et al., 2019). These direct redistributive measures can be complemented by longer-term structural changes in how AI development occurs. Democratizing AI development through open-source projects and targeted public funding can help spread access to these transformative technologies beyond a small group of well-resourced organizations. This democratization effort could gain teeth through carefully crafted regulatory frameworks that prevent monopolistic consolidation.</p> <p>These solutions must grapple with the differential impacts of AI across various segments of society and the global economy. It's not just a matter of the haves versus the have-nots. We're seeing complex dynamics play out between:</p> <ul> <li> <p>Capital and labor: As AI automates more tasks, the returns to capital (those who own AI systems and the data they run on) may increase relative to returns to labor.</p> </li> <li> <p>Frontier AI countries and laggards: Nations at the forefront of AI development may gain significant economic and strategic advantages over others.</p> </li> <li> <p>Tech-savvy individuals and the less digitally literate: As AI becomes more integrated into daily life, those who can effectively use and understand these technologies may have significant advantages.</p> </li> <li> <p>Large corporations and small businesses: Big tech companies with vast data resources and AI capabilities may gain even more market power, potentially squeezing out smaller competitors.</p> </li> </ul> <p>These differential impacts add layers of complexity to the governance challenge. They underscore the need for nuanced, adaptable policies that can address the specific needs and vulnerabilities of different groups.</p>"},{"location":"chapters/04/04/","title":"4.4 Corporate Governance","text":"Reading Time 16 min <p>Elon Musk (Founder/Co-Founder of OpenAI, Neuralink, SpaceX, xAI, PayPal, CEO of Tesla, CTO of X/Twitter)</p> <p>\"AI is a rare case where I think we need to be proactive in regulation than be reactive [...] I think that [digital super intelligence] is the single biggest existential crisis that we face and the most pressing one. It needs to be a public body that has insight and then oversight to confirm that everyone is developing AI safely [...] And mark my words, AI is far more dangerous than nukes. Far. So why do we have no regulatory oversight? This is insane.\"</p> <p>The challenges we discussed earlier - unexpected capabilities, deployment risks, and rapid proliferation - create complex oversight problems. Companies developing frontier AI have unique visibility into these challenges. They work directly with the models, see capabilities emerge firsthand, and can implement safety measures faster than external regulators (Anderljung et al., 2023; Sastry et al., 2024).</p> <p>Why do we start with corporate governance? Companies building frontier AI face a balancing act. They have the technical knowledge and direct control needed to implement effective safeguards. But they also face market pressures that can push against taking time for safety measures. Looking at how companies handle this tension helps us understand both the possibilities and limitations of self-regulation in AI development (Zhang et al., 2021; Schuett, 2023).</p> <p>In this section we'll examine how AI companies approach governance in practice - from basic safety protocols to comprehensive oversight frameworks. We'll look at what works, what doesn't, and where gaps remain. This helps us understand why corporate governance alone isn't enough, setting up our later discussions of national and international oversight. By the end of this section, we'll see both the essential role of company-level governance and why it needs to be complemented by broader regulatory frameworks.</p> <p>Frontier AI companies can implement internal governance mechanisms to govern AI. This self-regulatory layer serves as a crucial complement to external oversight, providing more immediate and technically informed controls over AI development and deployment.</p> <p>Internal governance mechanisms are vital because frontier AI companies possess unique advantages in governing their systems. They have direct access to technical details, development processes, and emerging capabilities; they can implement controls more rapidly than external regulators; and they understand the technical nuances that might escape broader regulatory frameworks. Their proximity to development allows them to identify and address risks earlier and more effectively than external oversight alone could achieve.</p> Figure 4.10: A section of the operating agreement between OpenAI, LLC (for-profit entity) and OpenAI, Inc. (non-profit entity). (OpenAI, 2024) <p>For instance, companies can implement real-time monitoring of model behavior, establish internal review boards for sensitive applications, and develop sophisticated testing protocols that would be difficult to mandate through external regulation. This privileged position in the development process creates both opportunity and responsibility for robust self-governance.</p> <p>Components of Internal Governance - Effective internal governance can be complex, ranging from comprehensive technical standards to organizational structures. Companies can establish detailed development guidelines that incorporate safety considerations from the earliest stages of research, alongside rigorous testing protocols to evaluate system capabilities and limitations. These technical standards can be accompanied by clear deployment criteria that must be met before systems can be released or scaled.</p> <p>The organizational structure can support these technical standards through dedicated safety teams with real authority to pause or modify development when necessary. Internal ethics boards can evaluate sensitive applications, while clear escalation paths ensure safety concerns reach appropriate decision-makers quickly. Companies can also consider how to integrate safety considerations into their promotion and compensation structures to align incentives throughout the organization.</p> <p>Beyond individual measures, frontier AI developers can participate in collective self-regulatory initiatives through industry-wide safety standards and best practices. Voluntary commitments to specific safety measures or deployment restrictions can help establish industry norms, while information sharing about safety-relevant incidents can improve practices across the sector.</p> <p>Limitations and Challenges - Internal governance faces several significant challenges. Perhaps the most fundamental is the challenge of incentive alignment, as companies face competing pressures between safety and other objectives like market competition, growth, and profitability. Internal governance mechanisms must be robust enough to withstand these pressures, particularly during critical periods of market competition or technological breakthroughs.</p> <p>Credibility and accountability present another major challenge. Self-regulatory measures may lack credibility without external validation or enforcement mechanisms. Companies may have to find ways to demonstrate their commitment to safety and responsible development that convince external stakeholders of their seriousness and effectiveness.</p> <p>Coordination problems arise when individual company initiatives fail to address broader societal concerns or system-wide risks. Some challenges require coordination across the industry or between companies and governments, which can be difficult to achieve through purely voluntary measures. The competitive nature of AI development can sometimes work against the kind of open collaboration needed to address these broader challenges.</p> <p>The Role of Transparency and External Validation - Voluntary governance is not necessarily internal to the company. It can include mechanisms for transparency and external validation. Regular public reporting on safety measures and incidents provides accountability, while third-party audits of safety systems and processes offer independent verification of governance effectiveness. Companies can maintain active engagement with external stakeholders and experts to ensure their governance approaches remain relevant and effective.</p> <p>The relationship with external regulation is particularly important. Internal governance should complement rather than replace external oversight, with companies designing internal systems that can interface effectively with regulatory requirements. This includes maintaining documentation that can support compliance efforts and participating constructively in the development of regulatory frameworks. Companies can also share relevant insights and experience with policymakers to help inform the development of effective external oversight mechanisms.</p> Interactive Figure 4.6: Share of notable AI systems by researcher affiliation (Giattino et al., 2023)"},{"location":"chapters/04/04/#01","title":"4.4.1 Frontier Safety Frameworks","text":"<p>Frontier Safety Frameworks are internal policies that AI companies create to guide their development process and ensure they're taking appropriate precautions as their systems become more capable. They\u2019re the equivalent of the safety protocols used in nuclear power plants or high-security laboratories. At the Seoul AI Summit organized in May 2024, 16 companies around the world committed to implementing such policies (UK government, 2024).</p> <p>Two of the biggest names in the AI world, Anthropic - through its Responsible Scaling Policy - and OpenAI - through its Preparedness Framework -, have been at the forefront of developing these frameworks. Let's take a closer look at their approaches.</p>"},{"location":"chapters/04/04/#01-01","title":"4.4.1.1 Anthropic's Responsible Scaling Policy (RSP)","text":"<p>Anthropic\u2019s Responsible Scaling Policy is a document that outlines different \"AI Safety Levels\" (ASLs) and the corresponding safety measures that need to be in place as their models become more powerful.</p> <p>For example, at ASL-2 (which includes their current most advanced model, Claude 2), Anthropic commits to things like publishing detailed model cards, providing a way for people to report vulnerabilities, or enforcing strict rules about how the model can be used.</p> <p>For higher risk levels (ASL-3 and above), Anthropic ratchets up the precautions significantly. They talk about limiting access to training techniques, implementing much stronger security measures, and even being prepared to pause development entirely if things get too dicey.</p>"},{"location":"chapters/04/04/#01-02","title":"4.4.1.2 OpenAI's Preparedness Framework","text":"<p>OpenAI\u2019s Preparedness Framework revolves around what they call a \u201cScorecard\u201d \u2013 essentially a way to evaluate the risks associated with their AI models across different categories.</p> <p>OpenAI's is quite specific in certain areas. For instance, they have a whole category dedicated to CBRN risks (Chemical, Biological, Radiological, and Nuclear risk). They define their \"High\" risk threshold in this category as: \"Model enables an expert to develop a novel threat vector OR model provides meaningfully improved assistance that enables anyone with basic training in a relevant field (e.g., introductory undergraduate biology course) to be able to create a CBRN threat.\"</p> <p>The Strengths and Weaknesses of Current Approaches The current governance frameworks from major AI labs reveal both promising approaches and concerning gaps in industry self-regulation. Their public nature enables valuable external scrutiny, while their risk categorization demonstrates engagement with potential failure modes. The frameworks' deliberately flexible structure allows adaptation as our understanding of AI risks evolves.</p> <p>However, these strengths are undermined by several interconnected weaknesses. The frequent use of ambiguous language makes consistent application difficult, while the frameworks' voluntary nature raises questions about their actual implementation when commercial pressures conflict with safety considerations. Some critics argue the frameworks aren't conservative enough given the stakes involved, potentially setting risk thresholds too high and mitigation requirements too low. Additionally, their focus on individual system risks may miss emergent dangers from multiple AI systems interacting in complex ways. The lack of standardization across companies further complicates industry-wide coordination, though this may improve as best practices emerge through practical implementation.</p> <p>The Governance Challenge How do we ensure that companies actually implement their frontier safety frameworks? Both Anthropic and OpenAI have outlined some governance measures in their frameworks.</p> <p>Anthropic has made some interesting commitments in terms of governance:</p> <ul> <li> <p>Creating a role called the \"Responsible Scaling Officer.\" This person is supposed to be the guardian of the RSP, making sure the company is living up to its commitments.</p> </li> <li> <p>Proactively planning for scenarios where they might need to pause scaling of their models. This shows they're thinking ahead about potential crises.</p> </li> <li> <p>Sharing evaluation results publicly (where possible), which adds a layer of external accountability.</p> </li> </ul> <p>Some think those policies have gaps (Anderson-Samways et al., 2024). They include a clause that says in \"extreme emergency\" situations, like if a \"rogue state\" is developing AI recklessly, they might loosen their restrictions. While this flexibility could be necessary, it also potentially undermines the credibility of their other commitments. After all, who defines what constitutes an \"extreme emergency\"?</p> <p>On their side, OpenAI has outlined a three-tiered governance structure: their Preparedness team conducts foundational research and monitoring, providing technical expertise to inform governance decisions. This research feeds into a Safety Advisory Group that brings diverse perspectives to risk assessment and mitigation recommendations. Final authority rests with OpenAI's leadership and Board of Directors.</p> <p>This structure has some clear strengths. The dedicated Preparedness team ensures that safety considerations are always at the forefront. The advisory group brings in outside perspectives, which can help challenge groupthink. And having the Board as a final backstop could provide an additional layer of oversight.</p> <p>However, questions remain. How much power does the Preparedness team really have? Can they delay or veto projects they deem too risky? How is the Safety Advisory Group selected, and how much influence do they actually wield? And given that OpenAI is ultimately a for-profit company (despite its unusual structure), how do we ensure that safety always trumps commercial interests?</p> <p>The Road Ahead The frameworks and governance structures being developed by companies like Anthropic and OpenAI are important first steps. They show a recognition of the enormous responsibility that comes with developing these powerful systems.</p> <p>There is still room for improvement. Some suggest that companies like Anthropic should define more precise, verifiable risk thresholds for their safety levels, potentially drawing on societal risk tolerances from other industries (Anderson-Samways 2024). For instance, in industries dealing with potentially catastrophic risks (events causing 1,000 or more fatalities), maximum tolerable risk levels typically range from 1 in 10,000 to 1 in 10 billion per year. AGI companies might consider adopting similar quantitative thresholds, adjusted for the potentially even greater stakes involved in AGI development.</p> <p>Overall, we need a much more robust, standardized, and enforceable set of governance practices for frontier AI development. Moreover, we need to foster a culture within the AI community that prioritizes safety and ethical considerations as much as technical achievements. The goal should be to make responsible AI development not just a regulatory requirement, but a core value of the field.</p>"},{"location":"chapters/04/04/#02","title":"4.4.2 Policy options","text":"<p>Risk Assessment Methods. Drawing from established safety-critical industries, AGI companies can adapt and implement various systematic approaches to evaluate potential risks. These range from scenario analysis and fishbone diagrams to more specialized techniques like the Delphi method, providing structured ways to anticipate and prepare for both known and unknown challenges in AGI development.</p> <p>The Three Lines of Defense. A robust organizational structure for risk management is essential for AGI companies, implemented through a three-tiered defense system. This framework distributes responsibility across frontline researchers, specialized risk management teams, and independent auditors, ensuring multiple layers of oversight and risk detection throughout the development process.</p> <p>Coordinated Pausing. When dangerous capabilities emerge in AI systems, companies need systematic ways to respond collectively. The coordinated pausing framework provides a structured approach for companies to temporarily halt development, share critical safety information, and resume work only when appropriate safeguards are in place, preventing competitive pressures from compromising safety.</p> <p>Deployment Corrections. Even the most rigorous pre-deployment safeguards may not catch every risk. A comprehensive system of deployment corrections enables companies to maintain control over deployed models, respond rapidly to emerging risks, and implement rollback mechanisms when necessary, ensuring safety even after systems are in production.</p> <p>Industry Best Practices. The AI safety &amp; governance field is converging on a set of core governance practices, supported by broad expert consensus. These include pre-deployment risk assessments, dangerous capabilities evaluations, and third-party audits, representing an emerging standard for responsible AGI development that balances innovation with safety.</p>"},{"location":"chapters/04/04/#02-01","title":"4.4.2.1 Risk Assessment Methods","text":"<p>At the heart of effective governance in frontier AI companies lies a robust approach to risk assessment. How do you assess risks for technologies that don't yet exist and capabilities that may emerge unexpectedly?</p> <p>This is where we can learn from other safety-critical industries. Techniques from fields like aerospace, nuclear power, and cybersecurity could be adapted to the unique challenges of AI development.</p> <p>Let's take a closer look at some of these techniques (Koessler &amp; Schuett 2023):</p> <ul> <li> <p>Scenario Analysis : This involves imagining potential future scenarios and their implications. For AI companies, this might include scenarios like: An AI system developing deceptive behaviors, Unexpected emergent capabilities in a deployed model, A rival company deploying an unsafe AI system.</p> </li> <li> <p>Fishbone Method : Also known as the Ishikawa diagram, this technique helps identify potential causes of a problem. In the context of AI risks, a fishbone diagram might explore factors contributing to AI alignment failure, such as: Insufficient safety research, Pressure to deploy quickly, Inadequate testing protocols, Misaligned incentives in the AI system</p> </li> <li> <p>Causal Mapping : This technique visualizes the complex web of cause-and-effect relationships in a system. For AI development, a causal map could illustrate how different research decisions, safety measures, and deployment strategies interact to influence overall risk.</p> </li> <li> <p>Delphi Technique : This method involves gathering expert opinions through structured rounds of questionnaires. Given the highly specialized nature of AI research, the Delphi technique could be valuable for synthesizing diverse perspectives on potential risks and mitigation strategies.</p> </li> <li> <p>Bow Tie Analysis : This approach visualizes the pathways between causes, hazardous events, and consequences, along with prevention and mitigation measures. For an AI company, a bow tie analysis might focus on a hazardous event like \"loss of control over an AI system,\" mapping out potential causes (e.g., inadequate containment measures) and consequences (e.g., unintended global changes), along with preventive and reactive controls.</p> </li> </ul> <p>Implementing these techniques requires a cultural shift within AGI companies. Risk assessment can't be an afterthought or a box-ticking exercise; it needs to be woven into the fabric of the organization, from the research lab to the boardroom.</p>"},{"location":"chapters/04/04/#02-02","title":"4.4.2.2 The Three Lines of Defense","text":"<p>As AGI companies grapple with these complex risk landscapes, they need robust organizational structures to manage them effectively. One promising approach is the Three Lines of Defense (3LoD) model, a risk management framework widely used in other industries (Schuett 2023).</p> <p>In the context of an AGI company, the 3LoD model might look something like this:</p> <p>The First Line of Defense . This comprises the frontline researchers and developers working on AI systems. They're responsible for implementing safety measures in their day-to-day work, conducting initial risk assessments, and adhering to the company's ethical guidelines and safety protocols.</p> <p>The Second Line of Defense . This includes specialized risk management and compliance functions within the company. For an AI company, this might involve:</p> <ul> <li> <p>An AI ethics committee overseeing the ethical implications of research directions</p> </li> <li> <p>A dedicated AI safety team developing and implementing safety protocols</p> </li> <li> <p>A compliance team ensuring adherence to relevant regulations and industry standards</p> </li> </ul> <p>The Third Line of Defense . This is typically the internal audit function, providing independent assurance to the board and senior management. In an AI company, this might involve:</p> <ul> <li> <p>Regular audits of safety practices and risk management processes</p> </li> <li> <p>Independent evaluations of AI models for dangerous capabilities</p> </li> <li> <p>Assessments of the company's overall preparedness for potential AGI scenarios</p> </li> </ul> <p>Let's see how this might work in practice:</p> <p>Imagine that researchers in an AI company (first line) develop a new language model with unexpectedly advanced capabilities in logical reasoning. They flag this to the AI safety team (second line), who conduct a thorough evaluation and determine that the model poses potential risks if deployed without additional safeguards.</p> <p>The safety team works with the researchers to implement additional constraints on the model's outputs. Meanwhile, they also notify the internal audit team (third line), who launch a broader review of the company's processes for identifying and managing emergent capabilities.</p> <p>This multi-layered approach helps ensure that risks are identified and managed at multiple levels, reducing the chances of dangerous oversights.</p>"},{"location":"chapters/04/04/#02-03","title":"4.4.2.3 Coordinated Pausing","text":"<p>The emergence of unexpected and potentially dangerous capabilities is a very real possibility. How should AI companies respond when such capabilities are discovered?</p> <p>One innovative proposal is the concept of \"coordinated pausing\" (Alaga &amp; Schuett 2023). This approach suggests a structured process for responding to the discovery of dangerous capabilities:</p> Figure 4.11: (Alaga &amp; Schuett 2023) <p>This approach could take various forms, from a purely voluntary system relying on public pressure, to a more formalized agreement between developers, or even a legally mandated framework.</p> <p>The benefits of such a system are clear. It provides a mechanism for the AI community to collectively pump the brakes when potentially dangerous territory is entered, allowing time for careful analysis and the development of safety measures.</p> <p>However, implementing such a system is not without challenges. There are practical questions about how to define \"dangerous capabilities\" and who gets to make that determination. There are also potential legal hurdles, particularly around antitrust concerns.</p>"},{"location":"chapters/04/04/#02-04","title":"4.4.2.4 Deployment Corrections","text":"<p>Even with the most rigorous pre-deployment safeguards, there's always the possibility that dangerous capabilities or behaviors might emerge after an AI system is deployed. This is where the concept of \"deployment corrections\" comes into play.</p> <p>Companies thus need comprehensive contingency plans for scenarios where pre-deployment risk management falls short (O'Brien et al. 2023). At the technical level, this means maintaining continuous control over deployed models through robust monitoring and modification capabilities, supported by pre-built rollback mechanisms that can revert to earlier, safer versions when needed. These technical controls are complemented by organizational preparedness through dedicated incident response teams trained in rapid risk assessment and mitigation. Clear user agreements establish the legal and operational framework for emergency interventions, ensuring all stakeholders understand how and when access restrictions might be imposed.</p>"},{"location":"chapters/04/04/#02-05","title":"4.4.2.5 Towards Industry-Wide Best Practices","text":"<p>As the field of AGI development matures, there's a growing recognition of the need for industry-wide best practices. A survey of 92 experts from AI labs, academia, and civil society found broad agreement on a number of key practices, including pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, and safety restrictions on model usage (Schuett et al. 2023).</p> <p>Interestingly, 98% of respondents agreed with all of these measures, suggesting a growing consensus around certain core principles of responsible AGI development.</p>"},{"location":"chapters/04/05/","title":"4.5 National Governance","text":"Reading Time 28 min"},{"location":"chapters/04/05/#01","title":"4.5.1 The need for national governance","text":"<p>Zhang Jun (China's UN Ambassador)</p> <p>\"The potential impact of AI might exceed human cognitive boundaries. To ensure that this technology always benefits humanity, we must regulate the development of AI and prevent this technology from turning into a runaway wild horse [...] We need to strengthen the detection and evaluation of the entire lifecycle of AI, ensuring that mankind has the ability to press the pause button at critical moments.\"</p> <p>While leading AI companies have implemented various self-regulatory measures to ensure the safe development of frontier AI systems, relying solely on corporate self-regulation is insufficient to protect national interests and public welfare. While such voluntary measures allow for rapid response to emerging issues and can often move faster than government regulation, companies may lack incentives to fully account for broader societal impacts, may face competitive pressures that compromise safety considerations, and may not have the legitimacy to make decisions that affect entire populations. National governance frameworks are therefore essential to ensure comprehensive oversight and accountability. A robust national regulatory framework needs to build on and complement these self-regulatory efforts. It should provide a baseline of standards that all companies must meet, while still allowing room for companies to go above and beyond in their internal practices.</p> <p>Institutional Fit and the Challenge of Frontier AI - The concept of institutional fit\u2014the degree to which governance institutions match the scale, scope, and characteristics of the problems they aim to address\u2014is crucial for understanding why national governance of frontier AI is both necessary and challenging. Institutional fit helps us analyze whether existing regulatory bodies and frameworks are adequately equipped to handle the unique challenges posed by frontier AI systems, or whether new institutional arrangements are needed.</p> <p>The governance of frontier AI systems presents a particular challenge for institutional fit. Unlike traditional technological governance challenges, frontier AI systems generate externalities that span multiple domains - from national security to economic stability, from social equity to democratic functioning. Traditional regulatory bodies, designed for narrower technological domains, may lack the necessary spatial remit, technical competence, or institutional authority to effectively govern these systems (Dafoe, 2023).</p> <p>Consider the contrast with self-driving vehicles, where the primary externalities are relatively well-defined (safety of road users) and fall within existing regulatory frameworks (traffic safety agencies) (Dafoe, 2023). Frontier AI systems, by contrast, generate externalities that cross traditional regulatory boundaries and jurisdictions, requiring new institutional approaches.</p> <p>Addressing Institutional Gaps - The governance of frontier AI reveals several institutional gaps in current regulatory frameworks (Dafoe, 2023). The expertise gap manifests in traditional regulatory bodies' frequent lack of technical expertise to evaluate advanced AI systems. This necessitates either the development of new technical capabilities within existing institutions, the creation of new specialized regulatory bodies, or novel partnerships between government and technical experts.</p> <p>A coordination gap exists due to the cross-cutting nature of frontier AI externalities. New mechanisms are needed for coordination between different regulatory agencies, federal and state/local authorities, public and private sector entities, and domestic and international governance bodies.</p> <p>The temporal gap emerges from the rapid pace of AI development, creating a mismatch with traditional regulatory processes. Governance frameworks must be adaptable to technological change, capable of anticipating future developments, and able to respond quickly to emerging risks.</p> <p>Implementation Challenges - Several factors complicate the implementation of effective domestic governance. Political polarization can impede the development of consensus on governance approaches, particularly regarding the appropriate level of state oversight, balance between innovation and regulation, distribution of benefits and risks, and protection of civil liberties.</p> <p>Technical complexity creates challenges for effective oversight and monitoring, development of appropriate standards, assessment of compliance, and risk evaluation and management.</p> <p>The governance of frontier AI systems requires significant institutional innovation at the national level. While existing regulatory frameworks provide some foundation, the unique characteristics of frontier AI - its broad externalities, rapid development, and deep political implications - necessitate new approaches to governance. Success will require careful attention to institutional fit, stakeholder representation, and the balance between competing interests and values (Dafoe, 2023).</p> <p>National governance is also more complex to create and maintain than self-regulatory efforts by companies because laws and regulations result from a sometimes long and complex policy-making process, which unfolds in distinct phases, each offering opportunities for governance interventions. During the agenda-setting phase, governance actors work to elevate specific AI-related issues to the forefront of public and political discourse. The formulation phase involves crafting detailed policy proposals, while implementation transforms these proposals into actionable measures. Throughout this cycle, evaluation and adaptation remain crucial, allowing governance approaches to evolve in response to the rapidly changing AI landscape.</p> <p>The development of effective domestic governance frameworks for frontier AI is not merely a technical challenge but a fundamental political and institutional one. It requires building new capabilities while maintaining democratic legitimacy and balancing multiple competing interests. As AI capabilities continue to advance, the ability to develop and implement such frameworks will become increasingly crucial for national welfare and security.</p>"},{"location":"chapters/04/05/#02","title":"4.5.2 Current initiatives","text":""},{"location":"chapters/04/05/#02-01","title":"4.5.2.1 AI Safety Institutes","text":"<p>Rishi Sunak (Former UK Prime Minister)</p> <p>\"Get this wrong, and AI could make it easier to build chemical or biological weapons. Terrorist groups could use AI to spread fear and destruction on an even greater scale. Criminals could exploit AI for cyber-attacks, disinformation, fraud, or even child sexual abuse. And in the most unlikely but extreme cases, there is even the risk that humanity could lose control of AI completely through the kind of AI sometimes referred to as 'super intelligence'.\"</p> <p>Governments worldwide have recognized an urgent need to understand and manage the capabilities and risks of advanced artificial intelligence systems. This has led to the formation of AI Safety Institutes (AISIs), specialized government bodies designed to evaluate, research, and coordinate efforts to ensure AI development proceeds safely and beneficially.</p> <p>The Global Movement Toward AI Safety - In recent months, we've witnessed a remarkable surge in the establishment of AISIs across major technological powers. The United States, United Kingdom, Japan, Canada, and Singapore have all launched their own institutes, while the European Union has integrated these responsibilities into its AI Office through a dedicated AI Safety Unit.</p> Figure 4.12: Announced AI Safety Institutes (Martinet &amp; Variengien, 2024) <p>Core Functions of AI Safety Institutes - We can think of AISIs as serving three fundamental purposes, each building upon the others to create a comprehensive approach to AI safety. First, they evaluate AI systems through testing and assessment protocols. This involves developing new methodologies to understand these systems' capabilities, limitations, and potential impacts on society. Second, they can help conduct foundational research in AI safety, bringing together experts from various disciplines to advance our understanding of how to build and deploy AI systems safely. Finally, they serve as information exchange hubs, creating channels for sharing crucial insights among stakeholders, from policymakers to private companies.</p> <p>International Coordination and Collaboration - AI Safety Institutes have been designed from the ground up to work together across borders. The culmination of this international vision was realized at the May 2024 Seoul AI Summit, where ten countries and the European Union established a network for AI safety.</p> <p>Practical Challenges and Solutions - While the promise of international collaboration through AISIs is compelling, several practical challenges must be addressed. First, there's the delicate balance of sharing sensitive information about AI systems' capabilities while protecting commercial secrets and national security interests. Then there's the challenge of varying technical capacities between nations \u2013 not every country has equal resources to attract top AI talent or conduct sophisticated evaluations. Some institutes, like the UK's AISI, have taken innovative approaches to this challenge, such as opening offices in AI talent hubs like San Francisco.</p> <p>Looking to the Future - As these institutes mature, they will likely play an increasingly important role in developing international standards, conducting evaluations, and ensuring that AI development proceeds in a way that benefits humanity while minimizing potential risks. Their success will depend not only on technical expertise but also on their ability to facilitate meaningful collaboration across borders and between different stakeholders in the AI ecosystem.</p>"},{"location":"chapters/04/05/#02-02","title":"4.5.2.2 The EU AI Act","text":"<p>The European Union's AI Act addresses General Purpose Artificial Intelligence (GPAI) models, and we\u2019ll focus here on what the AI Act calls GPAI models with systemic risks - the equivalent of frontier AI models.</p> <p>The Act takes a dual approach to identifying GPAI models that present systemic risk. First, there's a computational threshold: any model using more than 10^25 floating point operations (FLOPs) in its training is automatically classified as presenting systemic risk. To put this in perspective, training such a model currently requires an investment of tens of millions of Euros. However, computational power isn't the only consideration. The Commission can also designate models as systemic based on their potential impact, considering factors such as user base size, scalability potential, and the possibility of causing large-scale harm. This flexible approach ensures that regulation can adapt to emerging risks, even when they come from models that don't meet the computational threshold.</p> Figure 4.13: The EU AI Act: Classification of general-purpose AI models with systemic risks (source: Observatorio de Riesgos Catastr\u00f3ficos Globales) <p>Provider Obligations and Compliance - Starting August 2, 2025, providers of GPAI models must meet various obligations, with additional requirements for those models deemed to present systemic risk. All GPAI providers must maintain detailed technical documentation and provide comprehensive information to downstream providers who integrate their models. They must also implement copyright compliance policies and publish summaries of their training data. For models with systemic risk, the requirements intensify. These providers must conduct thorough evaluations, including adversarial testing to identify potential vulnerabilities. They must also track and report serious incidents, implement robust cybersecurity protections, and actively work to assess and mitigate systemic risks.</p> Figure 4.14: The EU AI Act: Obligations for providers of general-purpose AI models (Observatorio de Riesgos Catastr\u00f3ficos Globales) <p>Enforcement and the AI Office - The EU AI Act establishes the AI Office - who also acts as the EU\u2019s AI Safety Institute - as a powerful enforcement authority. This office can request information, conduct model evaluations, and mandate corrective measures when necessary. The penalties for non-compliance are substantial \u2013 providers can face fines of up to 3% of their global annual turnover or \u20ac15 million, whichever is higher. This robust enforcement mechanism reflects the EU's commitment to ensuring that powerful AI systems are developed and deployed responsibly.</p> <p>The Role of the Code of Practice - The Act introduces an innovative approach to compliance through its Code of Practice. While not mandatory, this code provides a practical pathway for providers to demonstrate their compliance with the Act's requirements.</p>"},{"location":"chapters/04/05/#02-03","title":"4.5.2.3 The US Executive Order on AI","text":"<p>The United States has seen a flurry of legislative activity in recent years. The Executive Order on AI, signed by president Joe Biden on October 30 in 2023 stands out. Its Section 4 represents one of the most wide-ranging extensions of regulatory visibility into AI development in the United States. It introduces safety and security measures that will shape the future of AI development in the United States.</p> Figure 4.15: Number of AI-related regulations in the United States, 2016-2023 (2024 AI Index report) <p>New Reporting Requirements for AI Companies - The order establishes reporting requirements for companies involved in AI development. Companies developing dual-use foundation models - sophisticated AI models trained on broad datasets using self-supervision and containing tens of billions of parameters - must provide detailed reports about their activities. These reports must cover their training processes, security measures, model weights protection strategies, and results from red-team testing. Similarly, entities operating large-scale computing clusters must disclose their locations and total available computing power.</p> <p>Infrastructure and Foreign Entity Regulations - A particularly interesting aspect of Section 4 involves new regulations for Infrastructure as a Service (IaaS) providers. These companies must now report when foreign entities use their services for AI training that could enable concerning activities. This requirement extends to foreign resellers of U.S. IaaS services, creating a comprehensive monitoring system for AI development infrastructure. The secretary of commerce must draft regulations requiring these providers to verify the identities of foreign persons obtaining IaaS accounts and establish minimum standards for verification and recordkeeping - essentially, a Know-Your-Customer framework.</p>"},{"location":"chapters/04/05/#03","title":"4.5.3 Policy options","text":"<p>A comprehensive domestic governance regime for AI safety requires three interconnected mechanisms: development of safety standards, regulatory visibility, and compliance enforcement (Anderljung et al. 2023). These components can work together to create a framework that can effectively manage the risks associated with AI development and deployment.</p> <p>Mechanisms for developing safety standards - First and foremost, we need to establish processes for identifying appropriate requirements for frontier AI developers that can evolve with the technology. Safety standards form the foundation of AI governance by establishing clear, measurable criteria for the development, testing, and deployment of AI systems. These standards must be technically precise while remaining flexible enough to accommodate rapid technological advancement.</p> <p>The development of AI safety standards typically involves multiple stakeholders, including technical experts, industry representatives, civil society organizations, and government agencies. Standards development organizations (SDOs) often serve as central coordinating bodies for this process. For example, the National Institute of Standards and Technology (NIST) in the United States has developed AI risk management frameworks that serve as voluntary standards.</p> <p>Mechanisms for ensuring regulatory visibility - The second building block involves creating mechanisms for regulators to gain visibility into frontier AI development processes. This is crucial for staying ahead of potential risks and ensuring compliance with established standards. Regulatory visibility mechanisms enable oversight bodies to monitor AI development and deployment effectively. These mechanisms provide regulators with the information and access needed to assess compliance with safety standards and identify emerging risks.</p> <p>Mechanisms for ensuring compliance - The third building block involves creating mechanisms to ensure compliance with safety standards for the development and deployment of frontier AI models. This is where the rubber meets the road in terms of enforcement. Compliance mechanisms transform safety standards from theoretical frameworks into practical requirements with real consequences. These mechanisms must balance the need for effective enforcement with the importance of not stifling innovation.</p>"},{"location":"chapters/04/05/#03-01","title":"4.5.3.1 Mechanisms for developing safety standards","text":"<p>Various approaches to developing safety standards exist, from traditional standardization bodies to more dynamic multi-stakeholder processes like the EU GPAI Code of Practice. This Code, currently under development, demonstrates the vital importance of the standardization process. While not a traditional standardization mechanism, it serves to specify the high-level obligations outlined in the EU AI Act for GPAI models.</p> <p>The Act mandates that providers of GPAI models with systemic risks must \"ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model.\" However, this broad requirement raises numerous critical questions: What constitutes an \"adequate level\" of protection? What exactly comprises the \"physical infrastructure\" and the \"model\"? What evidence sufficiently demonstrates their protection? Through what specific measures should this protection be implemented?</p> <p>These questions highlight why standardization is essential - organizations need guidance to comply with their legal obligations effectively. Legal ambiguity, while it can sometimes be exploited by companies to their advantage, can also create significant operational challenges and risks for companies developing and deploying AI systems.</p> <p>What needs to be standardized - the example of cybersecurity protection - Protection of key AI assets requires a layered security architecture that addresses distinct but interconnected vulnerabilities. Four critical components demand protection: model weights, source code, training data, and user data. Each represents a unique security challenge while forming part of an integrated system where a breach in one area could compromise the whole.</p> <p>Model Weights Model weights are the result of extensive training processes, often requiring massive computational resources and proprietary datasets. For companies like OpenAI, Anthropic, or Google, these weights represent a large part of their competitive edge. If leaked, it could allow competitors or malicious actors to replicate their models, potentially removing safety measures or misusing them.</p> <p>Protection starts with robust encryption of stored weights, complemented by strict access controls limiting internal visibility. Advanced security can also involve segmenting weights across multiple secure locations, making unauthorized access more difficult. Continuous monitoring watches for suspicious access patterns or unusual data transfers, enabling rapid response to potential breaches.</p> <p>Source Code The source code defines how the model processes information, makes decisions, and generates outputs. For AI companies, this code represents years of research and development, often containing proprietary algorithms and architectures.</p> <p>Protecting source code isn't a new challenge \u2013 software companies have been doing it for decades. However, the stakes are higher with frontier AI. A leak could not only benefit competitors but also potentially allow malicious actors to identify and exploit vulnerabilities in the AI system.</p> <p>Comprehensive protection requires secure, access-controlled version control systems managing all code changes. Advanced techniques include code obfuscation to impede understanding if breached, combined with rigorous security audits and coding standards. Critical development could also occur on air-gapped systems, physically isolated from external networks to prevent unauthorized access.</p> <p>Training Data The training data can include everything from public web pages to proprietary information and even personal data. The challenge here is twofold: protecting the data itself and ensuring it's used ethically. A breach could expose sensitive information, while misuse could lead to biased or harmful AI models.</p> <p>Protection begins with thorough data anonymization, removing identifiable information without compromising training utility. Encrypted databases with strict access controls secure stored data, while comprehensive lineage tracking maintains clear records of data sources and usage patterns. This allows organizations to maintain both security and ethical compliance throughout the training process.</p> <p>User Data This is perhaps the most regulated aspect of AI cybersecurity, falling under laws like GDPR in Europe or the Personal Information Protection Law in China. User data in AI systems can be particularly sensitive \u2013 people might share personal details, medical information, or business secrets when interacting with an AI assistant.</p> <p>Protection can include end-to-end encryption securing data both in transit and storage, combined with strict data minimization principles to collect only essential information. User controls can provide transparent options for data management, including deletion rights and usage limitations.</p> <p>The Human Element: People as the Strongest (and Weakest) Link People can be both the strongest defense and the biggest vulnerability. Human error remains one of the biggest risks in cybersecurity. A single misplaced click, a carelessly shared password, or a fall for a phishing scam can potentially compromise even the most sophisticated security system.</p> <p>This is why leading AI labs invest heavily in security training for all employees, not just their tech teams. It's about creating a culture of security awareness, where everyone understands their role in protecting these valuable assets.</p>"},{"location":"chapters/04/05/#03-02","title":"4.5.3.2 Mechanisms for ensuring regulatory visibility","text":"<p>The Importance of External Scrutiny - As frontier AI systems become increasingly integrated into society and the economy, decisions about their training, deployment, and use will have far-reaching implications. It's crucial that these decisions are not left solely in the hands of AI developers.</p> <p>External scrutiny \u2013 involving outside actors in the evaluation of AI systems through red-teaming, auditing, and external researcher access \u2013 offers a powerful tool for enhancing the safety and accountability of frontier AI.</p> <p>To be effective, external scrutiny should adhere to the ASPIRE framework (Anderljung et al. 2023):</p> <ul> <li> <p>Access: External scrutineers need appropriate access to the AI systems and relevant information.</p> </li> <li> <p>Searching attitude: Scrutineers should actively seek out potential issues and vulnerabilities.</p> </li> <li> <p>Proportionality to the risks: The level of scrutiny should be commensurate with the potential risks posed by the system.</p> </li> <li> <p>Independence: Scrutineers should be free from undue influence from the AI developers.</p> </li> <li> <p>Resources: Adequate resources must be allocated to support thorough scrutiny.</p> </li> <li> <p>Expertise: Scrutineers must possess the necessary technical and domain-specific expertise.</p> </li> </ul> <p>External scrutiny of AI systems can be structured in several ways, drawing from established practices in other regulated industries. One approach mirrors financial auditing, where certified professionals conduct standardized evaluations according to established protocols. This system can incorporate different levels of disclosure requirements, from basic safety testing to in-depth capability assessments. Some frameworks include external ethics boards within AI companies, though their authority and influence varies significantly. The effectiveness of these approaches often depends on how well they balance thorough oversight with the practical constraints of AI development timelines and resource limitations.</p> <p>Responsible Reporting - One crucial aspect of both self-regulation and government oversight is the implementation of responsible reporting mechanisms. Organizations developing and deploying frontier AI systems have unique access to information about these systems' capabilities and potential risks. By sharing this information responsibly, they can significantly improve our collective ability to manage AI risks (Kolt et al. 2024).</p> <p>Let's break down what responsible reporting might look like in practice:</p> <p>What to Report - Unexpected or potentially dangerous emergent capabilities</p> <ul> <li> <p>Near-misses or safety incidents during development or deployment</p> </li> <li> <p>Significant breakthroughs in model performance or capabilities</p> </li> <li> <p>Observed misuse or attempted misuse of deployed models</p> </li> </ul> <p>Who to Report To - Relevant regulatory bodies</p> <ul> <li> <p>Industry consortiums focused on AI safety</p> </li> <li> <p>Academic researchers working on AI alignment and safety</p> </li> <li> <p>The wider public</p> </li> </ul> <p>How to Report - Through secure, standardized reporting channels</p> <ul> <li> <p>With appropriate protections for intellectual property and sensitive information</p> </li> <li> <p>In a timely manner, especially for urgent safety concerns</p> </li> </ul> <p>Different information sharing systems address the inherent tension between transparency needs and business interests in varying ways. Some approaches use tiered architectures that adapt disclosure levels to different stakeholder needs - regulators might receive detailed technical information while public disclosures remain more general. Other systems emphasize anonymization mechanisms that allow sharing of aggregate data while protecting individual company details. Legal frameworks sometimes include provisions to encourage honest reporting, such as liability protections for good faith disclosures.</p> <p>Model registries - At its core, a model registry is a centralized database where information about AI models is recorded and tracked. It works like a birth certificate \u2013 when a model is deployed, its creators file some paperwork.</p> <p>But what exactly goes into this paperwork? Different jurisdictions are taking different approaches, but model documentation typically encompasses several layers of information. Basic documentation often includes model identification and intended use cases, while technical specifications detail architecture, parameters, and computational requirements. Performance documentation can range from standard benchmark results to specialized evaluations of specific capabilities or risks. Impact assessments might examine potential societal effects, safety implications, and ethical considerations. Deployment documentation usually covers implementation strategies and monitoring plans.</p> <p>The idea is that by collecting this information, regulators can keep tabs on the AI landscape, identify potential risks before they become problems, and have a foundation for more targeted governance down the line.</p> <p>Why Model Registries Matter Model registries can serve multiple roles in AI governance systems. As transparency mechanisms, they enable various degrees of independent scrutiny and public visibility and trust into AI development. Some registries function as early warning systems for emerging capabilities or risks, allowing for preemptive response to potential concerns - if a model is registered with capabilities that raise red flags, regulators can step in before it's widely deployed. The accumulated data can inform policy development by providing empirical evidence about AI system characteristics and trends. Instead of broad, one-size-fits-all rules, they can tailor their approach based on the specific capabilities and risks of different models. Finally, in contexts where AI capabilities have strategic significance, registries can help governments keep track of who's developing what, potentially informing export controls or other national security measures.</p> <p>Governments around the world have already started to implement model registries. The U.S., for example, has taken a relatively light-touch approach so far, focusing primarily on the most advanced AI models. In October 2023, President Biden signed an Executive Order on AI that included provisions for a model registry. The United States has adopted an initially targeted approach to model registration, focusing oversight on the most advanced AI systems while maintaining flexibility for future expansion. This strategy, formalized in the October 2023 Executive Order, establishes clear compute-based thresholds for registration requirements. Systems exceeding 10^26 floating point operations in training must provide comprehensive documentation of their capabilities and limitations. They also need to disclose measures taken to protect their models from unauthorized access or theft.</p> <p>China has taken yet another approach, focusing on algorithmic recommendation systems rather than AI models per se. Their Internet Information Service Algorithmic Recommendation Management Provisions, which came into effect in 2022, target systems based on their potential influence on public discourse and social behavior. This framework requires detailed registration of algorithms used across various digital platforms, with particular emphasis on algorithms with \"public opinion properties\" or \"social mobilization capabilities.\". Companies must disclose not just technical details but also the underlying principles and intended purposes of their algorithms, creating transparency around both capabilities and intentions.</p> <p>Challenges As you might imagine, the implementation of model registries hasn't been without its challenges:</p> <ol> <li> <p>Defining the Scope: One of the biggest challenges is determining which models should be subject to registration requirements. Set the bar too low, and you risk stifling innovation with excessive bureaucracy. Set it too high, and you might miss potentially risky systems.</p> </li> <li> <p>Protecting Intellectual Property: AI companies invest enormous resources in developing their models and are understandably reluctant to share too much detail about their inner workings. Striking a balance between transparency and IP protection is a delicate act.</p> </li> <li> <p>Enforcement and Compliance: How do you ensure companies actually comply with registration requirements? And what are the consequences for non-compliance?</p> </li> </ol> <p>A Know Your Customer regime for AI - In the financial sector, banks are required to implement Know Your Customer (KYC) schemes to identify and verify client identities. This helps prevent money laundering and other financial crimes. Similarly, we could implement a KYC scheme for frontier AI (Egan &amp; Heim 2023). Under this scheme, compute providers would be required to implement KYC-like processes for their clients developing frontier AI models. If a company suddenly starts using an unusually large amount of compute power, this could trigger a reporting requirement. The compute provider would need to gather information about the nature of the project and report it to the relevant regulatory body.</p> <p>This approach provides early warning of potentially problematic or sudden advancements in AI capabilities. It allows for nuanced and targeted export controls. It also offers more precise control over compute quantities and the flexibility to suspend access if necessary.</p> <p>Implementing this regime would involve establishing a dynamic threshold of compute that effectively captures high-risk frontier model development, setting clear requirements for compute providers to keep records and report high-risk entities, and creating a government capacity to co-design, implement, administer, and enforce the scheme.</p> <p>Incident reporting - AI incident reporting is a process where developers, companies, and sometimes even users report significant issues, near-misses, or incidents related to AI systems. These could range from privacy breaches and security vulnerabilities to unexpected biases in decision-making or large-scale material or human harms.</p> <p>Incident reporting frameworks foster information-sharing about what went wrong (or almost went wrong), and thus creates a feedback loop that helps companies improve their systems and prevent similar issues in the future.</p> Interactive Figure 4.7: Global annual number of reported artificial intelligence incidents and controversies (Giattino et al., 2023) <p>Learning from Other Industries: Aviation Safety The Aviation Safety Reporting System (ASRS) in the United States is often held up as a gold standard for incident reporting (Cheng 2024). It's confidential, voluntary, and \u2013 crucially \u2013 non-punitive. This means that pilots, air traffic controllers, and other aviation professionals can report near-misses or safety concerns without fear of repercussions. The results speak for themselves: since the ASRS was implemented, aviation fatalities have plummeted.</p> <p>This approach has fostered a culture of openness that enables continuous improvement through comprehensive data collection on near-misses and potential risks. The system's success stems from its focus on identifying systemic issues rather than assigning individual blame, creating a model that could be adapted for AI safety.</p> <p>AI presents unique challenges that make incident reporting particularly tricky (Farrell 2024):</p> <ol> <li> <p>Defining an \"incident\": In aviation, it is clear what constitutes an incident or near-miss. But with AI, the lines can be blurry. Is an AI chatbot giving misleading information an incident? What about subtle algorithmic bias? Clear, agreed-upon definitions are needed to ensure the viability of incident reporting systems (OECD 2024).</p> </li> <li> <p>Attribution and responsibility: AI systems often involve multiple stakeholders \u2013 developers, data providers, platform operators, and end-users. Determining who's responsible for reporting an incident (and potentially facing consequences) is not always straightforward.</p> </li> <li> <p>Proprietary concerns: Companies invest millions in developing cutting-edge AI. They're understandably wary of sharing too much information about their systems.</p> </li> </ol> <p>Towards a Comprehensive AI Incident Reporting Framework Implementing such a framework requires careful design to balance multiple competing needs (Farrell, 2024). The foundation must be built on precise, tiered definitions of incidents ranging from minor technical issues to catastrophic failures. This classification system would support a dual-channel reporting structure: mandatory reporting for severe incidents causing significant harm, and confidential channels for near-misses and minor incidents, providing a way for AI professionals to report concerns and minor incidents without fear of repercussions, potentially managed by a neutral third party to ensure confidentiality. The framework's effectiveness depends on standardized reporting formats that facilitate analysis while enabling rapid dissemination of critical information. This might include fields for system specifications, incident description, root cause analysis, and mitigation steps taken. Throughout the system, careful balance must be maintained between public transparency and commercial sensitivity to ensure both broad learning and continued industry participation.</p>"},{"location":"chapters/04/05/#03-03","title":"4.5.3.3 Mechanisms for ensuring compliance","text":"<p>Licensing regime - One approach to compliance enforcement could be to implement a licensing regime for frontier AI models, similar to how nuclear power plants or pharmaceutical companies must be licensed to operate. Under this system, companies developing frontier AI models would need to obtain a license by demonstrating compliance with established safety standards.</p> <p>This process would integrate detailed technical documentation requirements with ways to demonstrate the implementation of required safety measures (e.g. through a safety case, see Buhl et al. 2024), creating a continuous cycle of compliance and verification. Regular audits and inspections would ensure ongoing adherence to safety standards.</p> <p>Another, complementary approach could be to grant enforcement powers to supervisory authorities. These authorities would have the power to conduct investigations, issue fines for non-compliance, and even halt the development or deployment of models deemed too risky. Let\u2019s say a company is found to be developing a frontier AI model without implementing the required safety protocols. The supervisory authority could issue a cease-and-desist order, requiring the company to halt development until they can demonstrate compliance with safety standards.</p> <p>Governing effectively often requires looking to other domains that have grappled with similar regulatory challenges. One particularly relevant example is the Federal Select Agent Program (FSAP) in the biosecurity domain (Anderson-Samways 2023).</p> <p>The FSAP was established to regulate the possession, use, and transfer of biological select agents and toxins that could pose a severe threat to public health and safety. Like frontier AI, the biosecurity field deals with rapidly evolving technologies, potentially severe risks, and the need to balance safety concerns with scientific progress.</p> <p>The FSAP employs a sophisticated risk-based regulatory system that begins during the research and development phase. Rather than waiting until biological agents are ready for use, the program requires registration and licensing early in the process - a model particularly relevant for AI governance, where early intervention may be crucial for managing risks.</p> <p>Through continuous monitoring and regular inspections, the FSAP maintains ongoing visibility into research activities, enabling quick responses to evolving risks. This is complemented by a tiered regulatory framework that applies different levels of oversight based on an agent's risk profile. Such an approach could be particularly valuable for AI governance, where the vast spectrum of AI systems demands varying levels of scrutiny. The most powerful models would face stringent controls, while less capable systems could operate under lighter oversight, creating an efficient allocation of regulatory resources.</p> <p>However, the FSAP also offers cautionary tales. Its reliance on checklist-based compliance in some areas has been criticized for potentially missing novel risks. This underscores the importance of maintaining a flexible, adaptive approach in AI governance.</p>"},{"location":"chapters/04/05/#03-04","title":"4.5.3.4 The Architecture of AI Regulations","text":"<p>Creating AI-specific laws or relying on existing sectoral frameworks</p> Figure 4.16: (State of AI report, 2023) <p>Ex ante and ex post measures A key consideration in AI governance is the balance between ex ante and ex post measures. Ex ante governance focuses on preemptive actions, setting rules and guidelines before potentially harmful AI systems are developed or deployed. This approach is particularly relevant for frontier AI, where the stakes are high and the potential for irreversible harm exists. Ex post governance, conversely, deals with the consequences of AI deployment, including liability frameworks and remediation measures. Effective AI governance requires a judicious mix of both approaches, anticipating potential issues while remaining flexible enough to address unforeseen challenges.</p> <p>Vertical vs horizontal governance The scope of governance measures also varies, with some targeting specific sectors (vertical regulation) and others applying broadly across multiple domains (horizontal regulation). Vertical approaches might focus on AI applications in healthcare or finance, tailoring governance to the unique challenges of each sector. Horizontal measures, such as data protection regulations or algorithmic transparency requirements, cut across sectors to address overarching concerns.</p> <p>No single function or lever can adequately address the multifaceted challenges posed by frontier AI. Instead, effective governance requires a carefully orchestrated interplay of various mechanisms, adapting to the evolving capabilities of AI systems and the shifting societal and ethical landscapes they inhabit.</p>"},{"location":"chapters/04/06/","title":"4.6 International Governance","text":"Reading Time 31 min"},{"location":"chapters/04/06/#01","title":"4.6.1 The need for international governance","text":"<p>Ant\u00f3nio Guterres (UN Secretary-General)</p> <p>\"AI poses a long-term global risk. Even its own designers have no idea where their breakthrough may lead. I urge [the UN Security Council] to approach this technology with a sense of urgency [...] Its creators themselves have warned that much bigger, potentially catastrophic and existential risks lie ahead.\"</p> <p>Can't individual countries just regulate AI within their own borders? The short answer is: no, not effectively.</p> <p>There are several reasons why domestic governance alone is insufficient:</p> <ol> <li> <p>No monopoly on development: No single country has a monopoly on AI development. Even if the United States, for example, were to implement stringent regulations, AI developers in countries with laxer standards could still potentially create and deploy dangerous AI systems that could affect the entire world.</p> </li> <li> <p>Global impact: The potential risks of advanced AI - from large-scale cyberattacks to economic disruption - are inherently global in nature. As James Cleverly, the UK Foreign Secretary, put it when discussing China\u2019s participation in the Bletchley AI Safety summit: \"We cannot keep the UK public safe from the risks of AI if we exclude one of the leading nations in AI tech.\"</p> </li> <li> <p>Race to the bottom: Without international coordination, countries may be reluctant to implement strict regulations unilaterally, fearing that they'll be left behind in the AI race. This can lead to a \"race to the bottom\" in terms of safety standards. International governance can help align incentives between nations, encouraging responsible AI development without forcing any one country to sacrifice its competitive edge.</p> </li> </ol> Interactive Figure 4.8: Cumulative number of large-scale AI systems by country (Giattino et al., 2023)"},{"location":"chapters/04/06/#02","title":"4.6.2 Current initiatives","text":""},{"location":"chapters/04/06/#02-01","title":"4.6.2.1 Global Impacts of National Regulations","text":"<p>Kamala Harris (Former US Vice President)</p> <p>\"[...] just as AI has the potential to do profound good, it also has the potential to cause profound harm. From AI-enabled cyberattacks at a scale beyond anything we have seen before to AI-formulated bio-weapons that could endanger the lives of millions, these threats are often referred to as the \"existential threats of AI\" because, of course, they could endanger the very existence of humanity. These threats, without question, are profound, and they demand global action.\"</p> <p>The inherently global nature of technology development means that national policies can have far-reaching effects.</p> <p>Even immigration policy is important:</p> Figure 4.17: What are the career paths of top-tier AI researchers? (source: MacroPolo) <p>For example, the United States\u2019 Executive Order on AI imposes reporting obligations on cloud providers, and export controls aimed at limiting China's access to advanced AI technologies. These actions, while originating from a single nation, have global implications.</p> <p>Similarly, the European Union's AI Act is poised to have an impact far beyond the EU's 27 member states. Companies worldwide, eager to maintain access to the lucrative European market, often find it more cost-effective to adopt EU standards across their entire operations rather than maintaining separate standards for different regions.</p> <p>For example, a U.S. tech company developing a new AI-powered facial recognition system for use in public spaces may see this system being classified as \u201chigh-risk\u201d under the EU AI Act. This would subject it to strict requirements around data quality, documentation, human oversight, and more. Companies then have a choice to make: develop two separate versions of your product \u2013 one for the EU market and one for everywhere else \u2013 or simply simply apply the EU standards globally. Many will be tempted to choose the second option, to minimize their cost of compliance. This illustrates what's known as the \u201cBrussels Effect\u201d (Bradford 2020): EU regulations can end up shaping global markets, even in countries where those regulations don't formally apply.</p> <p>The Brussels Effect can manifest in two ways (Siegmann &amp; Anderljung 2022):</p> <ol> <li> <p>De facto: Companies voluntarily adopt EU standards globally to avoid the complexity and cost of maintaining different standards for different markets.</p> </li> <li> <p>De jure: Other countries adopt regulations similar to the EU's, either to maintain regulatory alignment or because they view the EU's approach as a model to emulate.</p> </li> </ol> <p>For frontier AI, the Brussels Effect could be particularly significant. The EU's regulations might offer the first widely-adopted and mandated operationalization of concepts like \"risk management\" or \"systemic risk\" in the context of frontier AI. As other countries grapple with how to regulate advanced AI systems, they may look to the EU's framework as a starting point.</p> <p>Ursula von der Leyen (Head of EU Executive Branch)</p> <p>\"[We] should not underestimate the real threats coming from AI [...] It is moving faster than even its developers anticipated [...] We have a narrowing window of opportunity to guide this technology responsibly.\"</p>"},{"location":"chapters/04/06/#02-02","title":"4.6.2.2 International initiatives","text":"<p>Sam Altman (Co-Founder and CEO of OpenAI)</p> <p>\"[Suggesting about how to ask for a global regulatory body:] \"any compute cluster above a certain extremely high-power threshold \u2013 and given the cost here, we're talking maybe five in the world, something like that \u2013 any cluster like that has to submit to the equivalent of international weapons inspectors\" [\u2026] I did a big trip around the world this year, and talked to heads of state in many of the countries that would need to participate in this, and there was almost universal support for it.\"</p> Figure 4.18: The global ai governance landscape <p>But it's not just individual nations taking action. A patchwork of international initiatives has emerged to address the governance of AI on a global scale:</p> <ul> <li> <p>The AI Safety Summit : Held in the UK in 2023, this event brought together 28 nations and the EU to discuss AI safety. It resulted in the Bletchley Declaration, established AI Safety Institutes, and set the stage for future summits.</p> </li> <li> <p>The Hiroshima AI Process : Launched by the G7 nations, this initiative aims to promote responsible AI development and use.</p> </li> <li> <p>United Nations efforts : The UN is working on a report due in mid-2024 that will examine international institutions for AI governance.</p> </li> <li> <p>OECD guidelines : The Organisation for Economic Co-operation and Development has been particularly influential in shaping AI governance principles.</p> </li> <li> <p>Council of Europe AI treaty : This proposed treaty aims to protect human rights in the context of AI development and use.</p> </li> <li> <p>China's Global AI Governance Initiative : Demonstrating that AI governance is a priority even for nations often at odds with Western powers, China has put forth its own proposal for international AI governance.</p> </li> </ul> Figure 4.19: Cartoon highlighting a discrepancy between countries\u2019 statements and their true intentions in the context of the U.K.\u2019s november 2023 AI Safety Summit (The Economist)"},{"location":"chapters/04/06/#02-03","title":"4.6.2.3 Stages of International Policymaking","text":"<p>International policymaking typically progresses through several stages (Badie et al., 2011):</p> <ol> <li> <p>Agenda setting: Identifying the issue and getting it on the international agenda.</p> </li> <li> <p>Policy formulation: Developing potential solutions and approaches.</p> </li> <li> <p>Decision making: Choosing a course of action.</p> </li> <li> <p>Implementation: Putting the chosen policy into practice.</p> </li> <li> <p>Evaluation: Assessing the effectiveness of the policy and making adjustments as needed.</p> </li> </ol> <p>In the case of AI governance, we're still largely in the early stages of this process. The AI Safety Summit, for instance, represents a crucial step in agenda setting and initial policy formulation. But the real work of crafting binding international agreements and implementing them still lies ahead.</p>"},{"location":"chapters/04/06/#03","title":"4.6.3 Policy options","text":"<p>Institutional Models. Various institutional arrangements could support international AI governance, from scientific consensus-building bodies to emergency response networks. These range from lighter-touch coordination mechanisms to more comprehensive frameworks for standard-setting and enforcement.</p> <p>Non-proliferation. Drawing from nuclear weapons control strategies, non-proliferation approaches aim to limit access to advanced AI systems and critical resources like specialized chips. While these measures can help slow dangerous proliferation, they face significant challenges around enforcement and potential counterproductive effects on innovation.</p> <p>Regulatory Agreements. International regulatory frameworks offer a collaborative path forward, where countries agree to develop AI safely and verify compliance through monitoring at the model, organizational, and jurisdictional levels. The jurisdictional certification approach provides one concrete model, leveraging market access as an incentive for participation.</p> <p>Containment. For those concerned about catastrophic risks, more dramatic measures like the MAGIC plan propose centralizing advanced AI development in a single international facility. While politically challenging, historical precedents like early nuclear weapons control proposals suggest such radical approaches shouldn't be dismissed entirely.</p>"},{"location":"chapters/04/06/#03-01","title":"4.6.3.1 Institutional Models for International AI Governance","text":"<p>As the international community grapples with how to govern frontier AI, a variety of institutional models have been proposed (Maas &amp; Villalobos 2024):</p> <ul> <li> <p>Scientific Consensus-Building : The Intergovernmental Panel on Climate Change (IPCC) was tasked with informing governments about the state of knowledge of climate change and its effects. A similar body could provide regular reports on AI capabilities and risks to policymakers and the public. Given the rapid pace of AI development, this body would need to be nimbler than traditional scientific consensus-building organizations.</p> </li> <li> <p>Political Consensus-Building and Norm-Setting : Building on scientific consensus, we might envision a forum for political leaders to discuss AI governance issues and develop shared norms and principles. This could take the form of an AI-focused analogue to the United Nations Framework Convention on Climate Change (UNFCCC). Such a body could facilitate ongoing dialogue, negotiate agreements, and adapt governance approaches as the technology evolves.</p> </li> <li> <p>Coordination of Policy and Regulation : As countries develop their own AI regulations, there's a risk of a fragmented global landscape that could hinder innovation and create regulatory arbitrage opportunities. An international body focused on policy coordination could help address this challenge. Such an institution could work to harmonize AI regulations across countries, perhaps starting with areas of broad consensus and gradually tackling more contentious issues.</p> </li> <li> <p>Enforcement of Standards and Restrictions : For any international AI governance regime to be effective, there needs to be a mechanism for monitoring compliance and enforcing agreed-upon standards. This is where proposals like the jurisdictional certification approach discussed above come into play.</p> </li> <li> <p>Stabilization and Emergency Response : As we've discussed, the potential for \"normal accidents\" in AI systems is a serious concern. An international body focused on AI stability and emergency response could play a crucial role in mitigating these risks. This could consist in a global network of companies, experts and regulators, ready to assist in case of a major AI system failure or unexpected behavior. This group could also work proactively to identify potential vulnerabilities in global AI infrastructure and develop contingency plans. The International Atomic Energy Agency's Incident and Emergency Centre provides a potential model for this type of institution. However, given the potential speed of AI-related incidents, this body would need to operate on much faster timescales.</p> </li> <li> <p>International Joint Research : Collaborative international research could play a key role in ensuring that frontier AI development prioritizes safety and beneficial outcomes for humanity. An institution dedicated to facilitating such research could help pool resources, share knowledge, and ensure that safety considerations are at the forefront of AI development. CERN, the European Organization for Nuclear Research, offers one example for how such collaboration could work.</p> </li> <li> <p>Distribution of Benefits and Access : As frontier AI systems become more powerful, ensuring equitable access to their benefits will be crucial. An international institution focused on this challenge could work to prevent a harmful concentration of AI capabilities and ensure that the technology's benefits are widely distributed. This body might manage a global fund for AI development assistance, help facilitate technology transfers, or work to ensure that AI systems are developed with diverse global perspectives in mind.</p> </li> </ul> <p>Learning from Nuclear Arms Control: Three Lessons for AI Governance</p> <p>As we contemplate how to govern frontier AI on a global scale, it's instructive to look at how the international community has handled other powerful, potentially destructive technologies. Nuclear weapons provide a particularly relevant case study.</p> <p>At first glance, nuclear weapons and AI might seem like very different technologies. One is a physical weapon of mass destruction, the other a general-purpose technology with immensely varied applications. But both share key characteristics: they're dual-use technologies with both civilian and military applications, and they have the potential to dramatically alter the global balance of power and pose significant risks.</p> <p>So, what can we learn from decades of nuclear arms control efforts? Let's consider three key lessons (Maas 2019):</p> <p>The Power of Norms and Institutions In the early days of the nuclear age, many feared that nuclear weapons would proliferate rapidly, leading to widespread use. Yet today, nearly 80 years after the first nuclear detonation, only nine countries possess nuclear weapons, and they've never been used in conflict since World War II.</p> <p>This outcome was the result of a taboo and concerted efforts to build global norms against nuclear proliferation and use. The Nuclear Non-Proliferation Treaty (NPT), signed in 1968, created a framework for preventing the spread of nuclear weapons while promoting peaceful uses of nuclear technology. We might envision similar norm-building efforts for AI.</p> <p>The Role of Epistemic Communities The development of nuclear arms control agreements wasn't solely the work of diplomats and politicians. It relied heavily on input from scientists, engineers, and other technical experts who understood the technology and its implications.</p> <p>These experts formed what political scientists call an \"epistemic community\" \u2013 a network of professionals with recognized expertise in a particular domain. They played a crucial role in shaping policy debates, providing technical advice, and even serving as back-channel diplomats during tense periods of the Cold War.</p> <p>One challenge to leveraging such networks for global AI governance will be ensuring that epistemic communities can effectively inform policy decisions. Unlike nuclear physicists, who were often employed directly by governments, many AI experts work in the private sector.</p> <p>The Persistent Challenge of \"Normal Accidents\" Despite decades of careful management, the nuclear age has seen several close calls \u2013 incidents where human error, technical malfunctions, or misunderstandings nearly led to catastrophe. Sociologist Charles Perrow termed these \"normal accidents,\" arguing that in complex, tightly-coupled systems, such incidents are inevitable.</p> <p>Applying the concept to AI, we could see unexpected interactions and cascading failures increase as AI systems become more complex and interconnected. Moreover, the speed at which AI systems operate could mean that a \"normal accident\" in AI might unfold too quickly for human intervention.</p> <p>This reality challenges the notion of \"meaningful human control\" often proposed as a safeguard for AI systems. While human oversight is crucial, we must also design governance systems that are robust to the possibility of rapid, unexpected failures.</p>"},{"location":"chapters/04/06/#03-02","title":"4.6.3.2 Non-proliferation","text":"<p>Demis Hassabis (Co-Founder and CEO of DeepMind)</p> <p>\"We must take the risks of AI as seriously as other major global challenges, like climate change. It took the international community too long to coordinate an effective global response to this, and we're living with the consequences of that now. We can't afford the same delay with AI [...] then maybe there's some kind of equivalent one day of the IAEA, which actually audits these things.\"</p> <p>Non-proliferation, a term most commonly associated with nuclear weapons, refers to efforts to prevent the spread of dangerous technologies or materials.</p> <p>In the context of AI, non-proliferation strategies aim to limit or control access to potentially dangerous AI systems or to the resources (like advanced computer chips) needed to develop them. This approach can be applied at both the national and international levels.</p> <p>At the national level, this might mean only allowing companies with robust risk management procedures to access large-scale computing resources or training data. Internationally, it could involve preventing countries that lack adequate AI safety regulations from acquiring advanced AI capabilities.</p> <p>This approach can help slow the spread of potentially dangerous AI technologies, giving responsible AI labs more time to develop safety methods and defensive technologies. It allows for a \"pick a champion\" strategy, where support is concentrated on responsible actors who are more likely to develop AI in a safe and beneficial manner.</p> Figure 4.20: Map Of BIS Country Group determinations (Rhodium Group) <p>BIS is the \u201cBureau of Industry and Security\u201d, an entity within the US Department of Commerce in charge of export control policy. Depending on which category a country belongs to, it will have easier (in blue) or harder (in yellow and orange) access to US-made chips and chip-making equipment.</p> <p>Non-proliferation strategies in AI can take several forms:</p> <ol> <li> <p>Unilateral prevention : This involves a country or group of countries taking steps to prevent other actors from acquiring AI models or key AI inputs. This could be applied to entire countries, specific entities like terrorist groups, or individual labs that don't meet certain safety standards.</p> </li> <li> <p>Protection against theft : This strategy focuses on safeguarding AI models and technologies against stealing and unwanted tech transfer. Methods might include enhanced information security measures, security clearances for AI researchers, and strict controls on the sharing of sensitive AI research.</p> </li> <li> <p>Collaborative prevention : This approach involves countries working together to prevent proliferation, primarily to non-state actors but potentially to other states as well. An example of this could be a compute reporting regime, where cloud providers collect and share information about large-scale compute usage with regulators, who then share this information internationally to raise awareness of unwanted AI development activities.</p> </li> </ol> <p>Non-Proliferation in Action: U.S. Export Controls Towards China A real-world example of non-proliferation strategies in AI is the United States' implementation of export controls targeting China's AI development capabilities (Allen 2022). Since October 2022, the U.S. has been working to block China's access to high-end chips from the U.S. and other countries, chip design software, semiconductor manufacturing equipment (SME), and even components required for producing SME.</p> <p>These controls are being enforced with the cooperation of the Netherlands and Japan, who control key nodes in the global semiconductor supply chain.</p> <p>It's important to note that these export controls aren't primarily about AI safety or even direct misuse of AI. They seem to be largely motivated by concerns about the use of advanced chips in weapons systems and a desire to prevent China from gaining economic (and thus geopolitical) dominance through AI.</p> <p>While this is currently a unilateral policy, it has the potential to evolve into a bilateral or even multilateral arrangement through the implementation of verification mechanisms, such as through audits and inspections, which could be used to determine which companies might be added to a \u201cwhite list\u201d and thus allowed to receive advanced chips (NCUSCR 2023).</p> <p>Non-Proliferation: Limitations and Challenges Non-proliferation strategies in AI governance face complex challenges that stem from both technical and geopolitical realities. Historical evidence suggests these measures can produce unintended consequences that undermine their effectiveness. The U.S. experience with satellite technology export controls in the 1990s serves as a cautionary tale - restrictive policies led to a dramatic decline in U.S. market share from 73% to 25% over a decade, while simultaneously accelerating Chinese domestic capability development (Hwang &amp; Weinstein 2022).</p> <p>The technical landscape presents additional complications. Ongoing improvements in AI efficiency threaten to erode the effectiveness of compute-based controls as a governance mechanism (Pilz et al. 2023). Even assuming that compute-based controls remain relevant, it can be challenging to determine in advance which states will behave responsibly (for example by implementing adequate AI safety measures), making it hard to decide where to apply non-proliferation measures. Rather than preventing proliferation, restrictive measures can sometimes catalyze development races, as evidenced by China's response to U.S. export controls through increased domestic AI investments and reciprocal control measures.</p> <p>These practical challenges intersect with important moral considerations. Non-proliferation strategies often face criticism for their potentially discriminatory impact on technological and economic development across different nations. This perceived inequity can generate significant backlash, potentially undermining international cooperation necessary for effective AI governance. The challenge lies in developing approaches that can effectively manage proliferation risks while maintaining fairness and avoiding counterproductive outcomes in the global AI landscape.</p>"},{"location":"chapters/04/06/#03-03","title":"4.6.3.3 Regulatory agreements","text":"<p>Given the limitations of unilateral non-proliferation strategies, many experts argue for a more collaborative approach through international regulatory agreements. The basic idea is simple: countries agree to develop AI safely and prove to each other that they're complying with agreed-upon safety standards and regulations.</p> <p>These agreements can take many forms, varying in their level of legalization, number of participating states, and whether they involve the creation of new international organizations. The key is that they provide a framework for states to offer reliable evidence that they and their companies are developing AI responsibly.</p> <p>When designing regulatory agreements for AI, there are three key levels to consider:</p> <ol> <li> <p>Model level: This involves setting standards and verification processes for individual AI models.</p> </li> <li> <p>Organization level: This focuses on the AI development organizations themselves, ensuring they have proper safety protocols and risk management procedures in place.</p> </li> <li> <p>Jurisdiction level: This is about the broader regulatory environment in a country or region, including laws, enforcement mechanisms, and oversight bodies.</p> </li> </ol> Figure 4.21: Accountability Targets and Verification Processes for Auditing AI models, organizations, and jurisdictions (M\u00f6kander et al. 2023) <p>Most international agreements, especially in high-stakes domaines, operate at the jurisdiction level: it's typically easier for states to negotiate with each other than to directly regulate individual companies or products across borders.</p> <p>A Proposal for AI Regulatory Agreements: the jurisdictional certification approach</p> <p>One potential model for AI regulatory agreements would involve the creation of an international organization that certifies jurisdictions for compliance with international AI safety standards, as proposed by Trager et al. 2023. These standards might include requirements for licensing AI developers, liability frameworks, the establishment of national AI regulators, and specific safety standards for AI development and deployment.</p> <p>Under this model, AI labs would be monitored primarily by their national regulators. However, the international organization could also directly certify AI firms in countries that lack the resources or technical capacity to effectively regulate on their own. This approach has the advantage of encompassing all three levels (model, organization, and jurisdiction) while still allowing for some flexibility in how different countries implement the agreed-upon standards.</p> <p>For any such agreement to be effective, there need to be strong incentives for countries to participate and comply. One powerful approach is to tie compliance to market access. For example, states could ban the import of goods that integrate AI from non-certified jurisdictions. They could also ban the export of AI inputs (like specialized chips) to non-certified jurisdictions.</p> <p>To further strengthen enforcement, the agreement could require that states embed these enforcement provisions in their domestic laws as a condition of certification. This would provide all states with a strong incentive to join the regime and stay in compliance, as the economic costs of non-participation would be significant.</p> <p>While the idea of a global AI regulatory regime might seem far-fetched, there are actually existing international agreements that provide useful models.</p> <p>The International Civil Aviation Organization (ICAO), a UN agency, audits state aviation oversight systems and publishes reports on each state's compliance with ICAO standards. In the U.S., the Federal Aviation Administration enforces these standards and can prohibit airlines from non-compliant countries from operating in the U.S.</p> <p>The Financial Action Task Force (FATF) combats money laundering and terrorism financing. States agree on a set of standards, and the FATF monitors progress. Countries that don't have or enforce the necessary regulations can be put on a blacklist, significantly impacting their ability to attract international investment.</p> <p>These examples show that it's possible to create effective international regulatory regimes, even in areas that touch on sensitive issues of national security and economic competitiveness.</p> <p>The Security-Transparency Tradeoff</p> <p>One of the key challenges in designing any international regulatory agreement for AI is balancing the need for verification with concerns about revealing sensitive information. This is known as the security-transparency tradeoff (Coe &amp; Vaynman 2019).</p> <p>On one hand, ensuring adherence to safety measures requires some form of verification. This might involve inspectors checking safety measures in a country's labs, inspecting AI models, or monitoring compute usage. There's also a need for broader monitoring to prevent evasion of the rules \u2013 for example, tracking the locations of data centers or the sale of specialized AI chips.</p> <p>On the other hand, states may be reluctant to accept such intrusive inspections. There are concerns about sovereignty costs \u2013 the idea that allowing foreign inspectors into sensitive facilities impinges on a state's independence. There are also worries about proliferation risks: inspectors could potentially gain access to valuable intellectual property and transfer this information to other countries or companies.</p> <p>This security-transparency tradeoff is a key reason why arms control agreements have been relatively rare historically (Coe &amp; Vaynman 2019). Finding the right balance between verifying compliance and protecting sensitive information is crucial for the success of any AI governance agreement.</p> <p>The jurisdictional certification approach described earlier offers one potential solution to this dilemma by allowing states to monitor their own labs while still providing assurance to the international community. However, more innovative technical solutions may also help to reduce this tradeoff.</p> <p>A Proposal for a Verification Mechanism: Catching a Chinchilla</p> <p>One intriguing proposal for verifying compliance with AI development agreements while maintaining privacy comes from the paper \"What Does It Take to Catch a Chinchilla?\" (Shavit 2023).</p> <p>The goal of this proposal is to \"provide governments high confidence that no actor uses large quantities of specialized ML chips to execute a training run in violation of agreed rules\" while maintaining the privacy and confidentiality of models and data.</p> <p>The proposal has three main components:</p> <ol> <li> <p>Using on-chip firmware to occasionally save snapshots of the neural network weights stored in device memory, in a form that an inspector could later retrieve.</p> </li> <li> <p>Saving sufficient information about each training run to prove to inspectors the details of the training run that resulted in the snapshotted weights.</p> </li> <li> <p>Monitoring the chip supply chain to ensure that no actor can avoid discovery by amassing a large quantity of untracked chips.</p> </li> </ol> <p>While this proposal is not yet technically feasible, the authors argue that it presents only \"narrow technical challenges\" and could potentially provide a way to verify compliance with AI development agreements without revealing sensitive information about models or training data.</p> <p>While regulatory agreements offer a promising approach to international AI governance, they're not without their limitations.</p> <p>The relationship between agreement effectiveness and political feasibility creates a central dilemma - the more robust the safety measures an agreement proposes, the more resistance it typically encounters from participating nations. This tradeoff between feasibility and effectiveness echoes throughout the history of international technology governance, particularly in cases like nuclear non-proliferation.</p> <p>The timeline challenge compounds these difficulties. The development of the International Atomic Energy Agency's oversight capabilities serves as a sobering example - it required over two decades from the first use of nuclear weapons to establish meaningful inspection powers. In the context of AI's rapid advancement, such lengthy implementation periods could render agreements obsolete before they become operational.</p> <p>The inherent difficulty of verifying compliance without exposing sensitive technological information creates additional complexity. Unlike physical technologies, AI development often leaves few observable traces, making traditional verification approaches insufficient. Finally, AI is a rapidly evolving field, and any regulatory agreement needs to be flexible enough to adapt to new developments.</p>"},{"location":"chapters/04/06/#03-04","title":"4.6.3.4 Containment","text":"<p>For those who believe that catastrophic risks from AI are likely in the near future, more radical approaches to governance might seem necessary. One such approach is the idea of containment or technological restraint. The basic idea behind containment is to slow down or pause the development of advanced AI. This could serve two strategies (Maas 2022):</p> <ul> <li> <p>Delay : giving more time for society to adapt and for alignment research to catch up with capabilities</p> </li> <li> <p>Restraint : if safe alignment is deemed very unlikely, or if there's no way to ensure alignment techniques will be used, restraint might be necessary to prevent catastrophic outcomes.</p> </li> </ul> <p>The 'MAGIC' Plan One specific proposal for containment is the \"MAGIC\" (Multinational AGI Consortium) plan (Hausenloy et al. 2023). The core idea of MAGIC is to monopolize the development of advanced AI above a given compute threshold in a single facility, combined with a moratorium on development outside of this facility.</p> <p>Under this plan, signatory countries would mandate cloud computing providers to prevent any training runs above a specific size within their national jurisdictions. The rationale is that advanced AI systems can be dangerous even before deployment, due to risks like theft, deceptive alignment, or power-seeking behavior.</p> <p>The MAGIC plan proposes several key features to address the challenges of advanced AI development. At its core, it would establish a single, exclusive facility with a global monopoly on advanced AI model creation. This centralization aims to prevent a dangerous proliferation of powerful AI systems. The facility would prioritize safety, focusing on developing AI architectures that are inherently secure and exploring methods to constrain existing AI systems within safe boundaries. To protect its critical work, the facility would implement stringent security measures. Down the line, as safe advanced AI systems are developed, the consortium could distribute equitably the benefits of AI advancements among all participating nations.</p> <p>Despite its ambitious approach to mitigating AI risks, the MAGIC plan faces substantial hurdles. The most significant challenge lies in its political feasibility. Convincing nations to relinquish their independent AI development capabilities would be extraordinarily difficult, given the perceived strategic and economic advantages of leading in AI technology. The institutional design of such a facility presents another major obstacle. Creating a governance structure that remains impartial and resistant to the influence of competing national interests would require unprecedented levels of international cooperation and trust. There are also concerns about the concentration of power inherent in the plan. Centralizing advanced AI development in a single location could create a potential single point of failure or abuse, especially if the facility's management doesn't maintain true multilateral representation. Lastly, the plan's reliance on compute-based thresholds for defining \"advanced\" AI may prove problematic in the long term. As AI algorithms become increasingly efficient, the correlation between computational power and AI capability may weaken, potentially rendering this aspect of the plan less effective over time.</p> <p>While proposals like MAGIC might seem far-fetched, history shows us that radical schemes for international control of dangerous technologies can gain surprising traction when the stakes are high enough. The development of nuclear weapons provides an illuminating parallel.</p> <p>In the immediate aftermath of World War II, as the world grappled with the implications of atomic weapons, there was a serious push for international control of nuclear technology. The 1946 Acheson-Lilienthal Plan, which formed the basis of official U.S. policy at the time, proposed a radical solution: A new U.N. authority would \u201ccontrol all fissionable raw materials and have a monopoly on all dangerous, i.e., military activities\u201d (Zaidi &amp; Dafoe 2021) States would shut down all military nuclear activities, keeping only nuclear power plants, which would be inspected by the U.N. authority.</p> <p>This plan, while ultimately not implemented, demonstrates that even the most powerful nations can seriously consider surrendering control of strategically crucial technologies in the face of catastrophic technological risks.</p> <p>Moreover, as pointed out by Maas, \u201cStates can and will unilaterally forego, cancel, or abandon strategically promising technologies for a range of mundane reasons\u201d. (Maas, 2023) In the case of nuclear weapons, an estimated 14 to 22 nuclear weapons programs were considered but left unpursued, and 7 programs were pursued but later abandoned.</p> <p>This historical precedent suggests that while containing AI development through international agreement would be extremely challenging, it's not entirely outside the realm of possibility, especially if the risks become more apparent and immediate.</p>"},{"location":"chapters/04/06/#03-05","title":"4.6.3.5 Where Do We Go From Here?","text":"<p>As we've explored, there are several potential approaches to the international governance of frontier AI:</p> <ol> <li> <p>Non-proliferation: Limiting access to dangerous AI systems or the resources needed to develop them.</p> </li> <li> <p>Regulatory agreements: Providing reliable evidence that states and companies are developing AI safely.</p> </li> <li> <p>Containment: Monopolizing advanced AI development in a single, internationally controlled facility.</p> </li> </ol> <p>These approaches aren't mutually exclusive. In fact, managing advanced AI will likely require a combination of strategies operating at different levels. For example, governments could cooperate with like-minded states on regulatory agreements while simultaneously pursuing non-proliferation strategies to slow the spread of advanced AI capabilities to less responsible actors.</p> <p>The path forward will depend on how the AI landscape evolves, how our understanding of AI risks develops, and how the international political climate shifts. Regardless of the specific approach, it is clear that some form of international governance will be crucial for managing the risks and harnessing the benefits of frontier AI.</p> <p>The design of effective AI governance frameworks must navigate several fundamental tradeoffs. A central tension exists between effectiveness and political feasibility - while stronger obligations might better mitigate risks, they become increasingly difficult for states to accept and implement. This challenge is mirrored in the relationship between participation and commitment depth, where broader participation often comes at the cost of weaker commitments. Deciding whether to prioritize wide participation or strong commitments is a key strategic choice.</p> <p>These structural tensions are further complicated by dynamic considerations. Any governance framework must maintain legitimacy through inclusive stakeholder representation while remaining adaptable enough to respond to rapidly evolving AI capabilities. Finally, agreements must enable compliance monitoring without compromising sensitive information about AI development.</p> <p>Under Which Conditions Will States Desire and Accept International Governance?</p> <p>Understanding when states might be willing to participate in international AI governance is crucial for designing effective arrangements. The factors influencing this willingness can be broadly categorized into desirability and feasibility factors. Desirability factors are those that determine a state's desire to be assured that AI is being developed safely in other countries. Feasibility factors are those that would prevent a state from fulfilling its desire for assurance, i.e. from accepting an international agreement, even if the desire for assurance exists.</p> <p>In terms of desirability, several key elements come into play. First and foremost, states need to recognize that AI poses risks significant enough to warrant international cooperation. This awareness of extreme risks is fundamental to motivating action on a global scale. Additionally, states may want to ensure that other countries implement regulations, so that they can themselves regulate AI domestically without being left behind economically or technologically. Finally, a lack of trust in other countries' AI development practices could drive states towards international governance. If nations doubt the safety protocols or ethical standards of their counterparts, they may view collaborative oversight as a necessary safeguard.</p> <p>Feasibility factors are equally important in determining the viability of international agreements for AI safety. The cost of risk-reducing measures plays a crucial role; the lower the economic and strategic costs of proposed safety standards and obligations, the more likely states are to accept them. Proposals that build on or align with existing regulatory frameworks or international agreements are also more likely to gain acceptance, as they require less dramatic shifts in policy and practice. Interestingly, the potential for competitive advantage can be a motivating factor. If states believe that adhering to safety regulations could give them an edge in the global market by fostering trust in their AI products, they may be more willing to participate. Verification costs and mechanisms represent another critical feasibility factor. The availability of verification methods that don't reveal strategically valuable information can make agreements more palatable to states concerned about maintaining their competitive edge or national security. Moreover, the expected compliance by other states significantly influences participation willingness. Nations are more likely to commit to international governance if they believe their counterparts will adhere to the agreed-upon standards.</p> <p>Several other important factors can influence a state's willingness to engage in international AI governance. These include the number of actors involved, as broader participation can lend legitimacy and effectiveness to the effort. The presence of powerful states willing to take a leadership role can also be pivotal, as it can provide momentum and resources to the initiative. For less-resourced countries, the availability of technical aid can be a crucial factor in their ability and willingness to participate. Finally, the credibility of incentives or threats associated with participation can significantly impact a state's decision-making process: well-designed mechanisms can encourage other countries\u2019 compliance and deter their non-participation.</p> <p>There are also reasons for cautious optimism. Historical precedents like nuclear non-proliferation agreements show that international cooperation is possible even in areas of critical strategic importance. The emergence of various international AI initiatives demonstrates a growing recognition of the need for global coordination.</p> <p>Moving forward, progress in AI governance will likely come through a combination of approaches: strengthening domestic regulations, fostering international cooperation through agreements and institutions, and potentially exploring more radical containment strategies if risks become more acute.</p>"},{"location":"chapters/05/","title":"Chapter 05 - Evaluations","text":"Authors                      Markov Grey &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Maxime Rich\u00e9, Martin, Fabien Roger, Jeanne Salle, Camille Berger, Leo Karoubi          Last Updated 2025-01-15 Reading Time 132 min (core) Also available on Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>3:00 - Error - Feature visualizations (are not that helpful currently; behavioral approaches remain the best way to evaluate deception)</li> <li>4:00 - Imprecise - Multiple labs evaluate CBRN capabilities, not just OpenAI</li> <li>12:30 - Debated - TruthfulQA is not really a propensity evaluation</li> <li>17:30 - Error - Sandbagging emerges from the AI itself, it is not implemented by developers</li> <li>18:40 - Inaccurate - One of the most important limitations is not just that we lack evaluations in cybersecurity; rather, it's difficult to elicit the capabilities of a system, and we cannot be certain whether it will be impossible to create a system that performs</li> </ul> <p>Found more errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/05/#introduction","title":"Introduction","text":"<p>Lord Kelvin (Oxford Reference, 2016)</p> <p>When you can measure what you are speaking about, and express it in numbers, you know something about it, when you cannot express it in numbers, your knowledge is of a meager and unsatisfactory kind; it may be the beginning of knowledge, but you have scarcely, in your thoughts advanced to the stage of science.</p> <p>The gap between what AI systems can do and what we can reliably measure creates a fundamental safety challenge. In late 2024, AI researchers created FrontierMath, a benchmark of exceptionally difficult problems they predicted would \"resist AIs for several years\". Just a few months later, OpenAI's o3 model achieved 25.2% accuracy on these supposedly insurmountable problems. This pattern repeats across AI development: tools designed to measure AI capabilities become obsolete almost immediately as models rapidly surpass them. As AI systems approach potentially transformative capabilities in domains like cybersecurity, autonomous operation, and strategic planning, this evaluation gap becomes increasingly dangerous. We cannot afford to discover the full extent of advanced AI capabilities through their emergent real-world impacts.</p> <p>Benchmarks provide standardization of measurement but currently fail to capture the complex risks posed by advanced AI systems. Early AI development faced a measurement crisis similar to pre-standardized engineering\u2014without reliable metrics, progress was chaotic and unpredictable. Benchmarks revolutionized AI development by creating standardized metrics that allowed meaningful comparisons between systems. While benchmarks like MMLU, GPQA, or FrontierMath are extremely useful, they still fall short when predicting real-world behavior. They fail to measure what happens when models are pushed to their limits or operate in complex environments where unexpected capability combinations emerge. As we move towards highly capable AI systems, we need stronger safety guarantees in a wider range of scenarios than what benchmarks alone can provide.</p> <p>Safety risks emerge from interactions between capabilities, creating a combinatorial challenge. Individual capabilities like situational awareness or deception appear manageable when measured in isolation, but their combination creates emergent risks fundamentally different from either capability alone. Models might recognize when they're being evaluated and selectively hide capabilities during testing but demonstrate concerning behaviors during deployment. This combinatorial explosion makes comprehensive testing exponentially more difficult but essential for meaningful safety claims. A model passing most safety benchmarks might still harbor dangerous capability combinations that weren't explicitly tested. This is why we need to move measurement of safety critical properties of a system towards comprehensive evaluations.</p> <p>Effective safety assessment requires measuring three distinct properties: capabilities, propensities, and control. Dangerous capabilities evaluations establish upper bounds on what models can do when pushed to their limits through techniques like scaffolding and tool-augmentation. Propensity evaluations reveal what behaviors models tend to exhibit by default, measuring behavioral tendencies rather than just maximum abilities. Control evaluations verify whether safety protocols remain effective when models actively try to circumvent them. This three-part taxonomy provides a systematic framework for safety measurement that helps answer the most decision-relevant questions about AI.</p> <p>Safety evaluation requires both behavioral and internal techniques, each providing different forms of evidence. Behavioral techniques examine model outputs through approaches like red teaming, which systematically attempts to elicit concerning behaviors; supervised fine-tuning, which elicits capabilities by modifying weights rather than just prompting; and best-of-N sampling, which examines multiple potential responses to understand output distributions. These techniques can establish upper bounds on potential capabilities but struggle to tell us \"why\" models generate certain outputs. Internal techniques complement this by examining model mechanisms directly. For example, sparse autoencoders have successfully extracted interpretable features related to safety-relevant behaviors including deception, sycophancy, and bias. Other techniques like mechanistic interpretability, can help trace computational pathways through the model, enumerative safety can catalogs concepts the model has encoded, and representation engineering can examine how models encode information. Behavioral and internal evaluation techniques are complementary and together provide stronger safety guarantees than either approach alone.</p> <p>Evaluation frameworks help transform measurements into concrete development and deployment decisions. Rather than relying on ad-hoc responses to capabilities, frameworks like Anthropic's Responsible Scaling Policies establish \"AI Safety Levels\" inspired by things like biosafety containment protocols, with each level requiring increasingly stringent evaluation requirements and safety measures. These frameworks create \"evaluation gates\" that determine when scaling can proceed safely\u2014requiring models to pass cybersecurity, biosecurity, and autonomous replication evaluations before development continues. By integrating evaluations into governance structures, we create systematic approaches to managing AI risk rather than relying on ad-hoc decisions.</p> <p>Evaluations must be systematically designed to maintain quality and scale across increasingly complex models. Evaluation design requires careful consideration of affordances\u2014the resources and opportunities provided to the model during testing. By systematically varying affordances from minimal (restricting tools and resources) to maximal (providing all potentially relevant tools and context), we can build a more complete picture of model behavior under different conditions. As the number of safety-relevant properties grows, automating evaluation becomes necessary. We can potentially use model-written evaluations to help address scaling challenges.</p> <p>Despite significant progress, AI evaluations face fundamental limitations that threaten their reliability. The asymmetry between proving presence versus absence of capabilities means we can never be certain we've detected all potential risks. Evaluations can conclusively confirm that a model possesses certain capabilities but cannot definitively prove their absence. Technical challenges include measurement sensitivity\u2014performance can vary based on seemingly trivial changes in prompting formats\u2014and the combinatorial explosion of test cases as we add new dimensions to evaluate. Misalignment might lead to model \"sandbagging\" (strategic underperformance on evaluations), research shows language models can be made to selectively underperform on tests for dangerous capabilities while maintaining performance on general benchmarks. Organizational incentives might lead labs themselves to do \"safety washing\" (misrepresenting capability improvements as safety advancements). These challenges highlight the need for continued research into more robust evaluation methodologies and institutional arrangements that support genuinely independent assessment.</p> Figure 5.1: Overview of chapter content. <p>This introduction gave you the general overview of many of the concepts that we will be talking about throughout this chapter. The sections will largely proceed in the order that we introduced the ideas above. We begin by exploring how benchmarks have shaped AI development.</p>"},{"location":"chapters/05/01/","title":"5.1 Benchmarks","text":"Reading Time 17 min <p>What is a benchmark? Imagine trying to build a bridge without measuring tape. Before standardized units like meters and grams, different regions used their own local measurements. Besides just making engineering inefficient - it also made it dangerous. Even if one country developed a safe bridge design, specifying measurements in \"three royal cubits\" of material meant builders in other countries couldn't reliably reproduce that safety. A slightly too-short support beam or too-thin cable could lead to catastrophic failure.</p> <p>AI basically had a similar problem before we started using standardized benchmarks.<sup>1</sup> A benchmark is a tool like a standardized test, which we can use to measure and compare what AI systems can and cannot do. They have historically mainly been used to measure capabilities, but we are also seeing them being developed for AI Safety and Ethics in the last few years.</p> <p>How do benchmarks shape AI development and safety research? Benchmarks in AI are slightly different from other scientific fields. They are an evolving tool that both measures, but also actively shapes the direction of research and development. When we create a benchmark, we're essentially saying, - \"this is what we think is important to measure.\" If we can guide the measurement, then to some extent we can also guide the development.</p> <p>Fran\u00e7ois Chollet (Chollet, 2019)</p> <p>Goal definitions and evaluation benchmarks are among the most potent drivers of scientific progress</p>"},{"location":"chapters/05/01/#01","title":"5.1.1 History and Evolution","text":"Interactive Figure 5.1: Benchmark scores of various AI capabilities relative to human performance. (Giattino et al., 2023) <p>Example: Benchmarks influencing standardization in computer vision. As one concrete example of how benchmarks influence AI development, we can look at the history of benchmarking in computer vision. In 1998, researchers introduced MNIST, a dataset of 70,000 handwritten digits. (LeCun, 1998) The digits were not the important part, the important part was that each digit image was carefully processed to be the same size and centered in the frame, and that the researchers made sure to get digits from different writers for the training set and test set. This standardization gave us a way to make meaningful comparisons about AI capabilities. In this case, the specific capability of digit classification. Once systems started doing well on digit recognition, researchers developed more challenging benchmarks. CIFAR-10/100 in 2009 introduced natural color images of objects like cars, birds, and dogs, increasing the complexity. (Krizhevsky, 2009) Similarly, ImageNet later the same year provided 1.2 million images across 1,000 categories. (Deng, 2009) When one research team claimed their system achieved 95% accuracy on MNIST or ImageNet and another claimed 98%, everyone knew exactly what those numbers meant. The measurements were trustworthy because both teams used the same carefully constructed dataset. Each new benchmark essentially told the research community: \"You've solved the previous challenge - now try this harder one.\" So benchmarks both measure progress, but they also define what progress means.</p> Figure 5.2: Examples of digits from MNIST (MNIST database - Wikipedia) <p>How do benchmarks influence AI Safety ? Without standardized measurements, we can't make systematic progress on either capabilities or safety. Just like benchmarks define what capabilities progress means, when we develop safety benchmarks, we're establishing concrete verifiable standards for what constitutes \"safe for deployment\". Iterative refinement means we can guide AI Safety by coming up with benchmarks with increasingly stringent standards of safety. Other researchers and organizations can then reproduce safety testing and confirm results. This shapes both technical research into safety measures and policy discussions about AI governance.</p> <p>Language model benchmarking has already evolved, and is going to continue evolving. Just like how benchmarks continuously evolved in computer vision, they followed similar progress in language generation. Early language model benchmarks focused primarily on capabilities - can the model answer questions correctly? Complete sentences sensibly? Translate between languages? Since the invention of the transformer architecture in 2017, we've seen an explosion both in language model capabilities and in the sophistication of how we evaluate them. We can\u2019t possibly be exhaustive, but here are just a couple of benchmarks that current day language models are evaluated against:</p> Figure 5.3: Example of popular language models (Claude 3.5) being evaluated on various benchmarks (Anthropic, 2024) Interactive Figure 5.2: Benchmark performance on coding, math and language. (Giattino et al., 2023) Examples of various capabilities benchmarks <p>Benchmarking language and task understanding. General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), and its successor SuperGLUE (Wang et al., 2019) test difficult language understanding tasks. SWAG (Zellers et al., 2018), and HellaSwag (Zellers et al., 2019) tests specifically the ability to predict which event would naturally follow from a given story scenario.</p> <p>Broad cross domain evaluations. The MMLU (Massive Multitask Language Understanding) benchmark (Hendrycks et al., 2020) tests a model's knowledge across 57 subjects. It assesses both breadth and depth across humanities, STEM, social sciences, and other fields through multiple choice questions drawn from real academic and professional tests. The GPQA (Google Proof QA) (Rein et al., 2023) has multiple choice questions specifically designed so that correct answers can\u2019t be found through simple internet searches. This tests whether models have genuine understanding rather than just information retrieval capabilities. BigBench (Srivastava et al., 2022) is yet another example of benchmarks for measuring generality by testing on a wide range of tasks.</p> <p>Benchmarking mathematical and scientific reasoning. For specifically testing mathematical reasoning, a couple of examples include - the Grade School Math (GSM8K) (Cobbe et al., 2021) benchmark. This tests core mathematical concepts at an elementary school level. Another example is the MATH (Hendrycks et al., 2021) benchmark similarly tests seven subjects including algebra, geometry, and precalculus focuses on competition-style problems. They also have multiple difficulty levels per subject. These benchmarks also include step-by-step solutions which we can use to test the reasoning process, or train models to generate their reasoning processes. Multilingual Grade School Math (MGSM) is the multilingual version translated 250 grade-school math problems from the GSM8K dataset. (Shi et al., 2022)</p> <p>Benchmarking SWE and coding. The Automated Programming Progress Standard (APPS) (Hendrycks et al., 2021) is a benchmark specifically for evaluating code generation from natural language task descriptions. Similarly, HumanEval (Chen et al, 2021) tests python coding abilities, and its extensions like HumanEval-XL (Peng et al.,2024) tests cross-lingual coding capabilities between 23 natural languages and 12 programming languages. HumanEval-V (Zhang et al., 2024) tests coding tasks where the model must interpret both diagrams or charts, and textual descriptions to generate code. BigCode (Zuho et al., 2024), benchmarks code generation and tool usage by measuring a model\u2019s ability to correctly use multiple Python libraries to solve complex coding problems.</p> <p> Figure 5.4: Example of coding task and test cases on APPS (Hendrycks et al., 2021) </p> <p>Benchmarking ethics and bias. The ETHICS benchmark (Hendrycks et al., 2023) tests a language model's understanding of human values and ethics across multiple categories including justice, deontology, virtue ethics, utilitarianism, and commonsense morality. The TruthfulQA (Lin et al., 2021) benchmark measures how truthfully language models answer questions. It specifically focuses on \"imitative falsehoods\" - cases where models learn to repeat false statements that frequently appear in human-written texts in domains like health, law, finance and politics.</p> Figure 5.5: Example of larger models being less truthful on TruthfulQA (Lin et al., 2021). This is an example of inverse scaling, i.e. when a bigger model performance decreases on some questions. Figure 5.6: Example question from the ETHICS benchmark (Hendrycks et al., 2023) <p>Benchmarking safety. An example focused on misuse is AgentHarm (Andriushchenko et al., 2024). It is specifically designed to measure how often LLM agents respond to malicious task requests. An example that focuses slightly more on misalignment is the MACHIAVELLI (Pan et al., 2023) benchmark. It has \u2018choose your own adventure\u2019 style games containing over half a million scenarios focused on social decision making. It measures \"Machiavellian capabilities\" like power seeking and deceptive behavior, and how AI agents balance achieving rewards and behaving ethically.</p> Figure 5.7: A mock-up of a game in the MACHIAVELLI benchmark, a suite of text-based environments. At each step, the agent observes the scene and a list of possible actions; it selects an action from the list. Each game is a text-based story, which is generated adaptively\u2013branches open and close based on prior actions. The agent receives a reward when it achieves one of the goals. This type of benchmark allows the researchers to construct a behavioral report of the agent and measure the trade-off between rewards and ethical behavior (Pan et al., 2023). Details - Benchmark: Frontier Math (Glazer et al., 2024) &amp; Humanities Last Exam (Hendrycks &amp; Wang, 2024) <p>This is an extra explanation of the frontier math mathematical benchmark. You can safely skip this.</p> <p> Figure 5.8: Mathematical subject interconnections in FrontierMath. Node sizes indicate the frequency of each subject\u2019s appearance in problems, while connections indicate when multiple mathematical subjects are combined within single problems, demonstrating the benchmark\u2019s integration of many mathematical domains. (Glazer et al., 2024) </p> <p>What makes FrontierMath so difficult? Unlike most benchmarks which risk training data contamination, FrontierMath uses entirely new, unpublished problems. Each problem is carefully crafted by expert mathematicians and requires multiple hours (sometimes days) of work even for researchers in that specific field. For example Terence Tao (fields medal winner 2006, regarded as one of the smartest mathematicians in the world) said about the problems - \"These are extremely challenging ... I think they will resist AIs for several years at least.\" (EpochAI, 2024) Similarly Timothy Gowers (highly regarded mathematician, and fields medal winner 1998) said - \"Getting even one question right would be well beyond what we can do now, let alone saturating them.\" (EpochAI, 2024)</p> <p>The benchmark spans most major branches of modern mathematics - from computationally intensive problems in number theory to abstract questions in algebraic topology and category theory. To ensure problems are truly novel, they undergo expert review and plagiarism detection. The benchmark also enforces strict \"guess proofness\" - problems must be designed so there's less than a 1% chance of guessing the correct answer without doing the mathematical work. This means problems often have large, non-obvious numerical answers that can only be found through proper mathematical reasoning. The benchmark provides an experimental environment where models can write and test code to explore mathematical ideas, similar to how human mathematicians work. While problems must have automatically verifiable answers (either numerical or programmatically expressible mathematical objects), they still require sophisticated mathematical reasoning to solve.</p> <p> Figure 5.9: One sample problem from the FrontierMath benchmark (Besiroglu et al., 2024). </p> <p>Just to showcase the rapid pace of advancement even on this benchmark that even fields medal winning mathematicians consider extremely challenging, between the announcement of the FrontierMath benchmark the state-of-the-art models could solve less than 2% of FrontierMath problems. (Glazer et al., 2024) Just a couple of months later, OpenAI announced the o3 model, which then shot performance up to 25.2%. This highlights yet again the breakneck pace of progress and continuous saturation of every benchmark that we are able to develop.</p> <p> Figure 5.10: Performance of leading language models on FrontierMath. All models show consistently poor performance, with even the best models (as of Nov 2024) solving less than 2 percent of problems (Besiroglu et al., 2024). A few months later OpenAI claimed that their o3 model could score 25 percent on FrontierMath (Brown, 2024). </p> <p>To keep up with the pace, researchers are developing what is described as \"Humanity's Last Exam\" (HLE). A benchmark aimed at building the world's most difficult public AI benchmark gathering experts across all fields (Phan et al., 2025).</p>"},{"location":"chapters/05/01/#02","title":"5.1.2 Limitations","text":"<p>Current benchmarks face several critical limitations that make them insufficient for truly evaluating AI safety. Let's examine these limitations and understand why they matter.</p> <p>Training Data Contamination. Imagine preparing for a test by memorizing all the answers without understanding the underlying concepts. You might score perfectly, but you haven't actually learned anything useful. LLMs face a similar problem. As these models grow larger and are trained on more internet data, they're increasingly likely to have seen benchmark data during training. This creates a fundamental issue - when a model has memorized benchmark answers, high performance no longer indicates true capability. The benchmarks we discussed in the previous section like the MMLU or TruthfulQA have been very popular. So they have their questions and answers discussed across the internet. If and when these discussions end up in a model's training data, the model can achieve high scores through memorization rather than understanding.</p> <p>Understanding vs. Memorization Example. The Caesar cipher is a simple encryption method that shifts each letter in the alphabet by a fixed number of positions - for example, with a left shift of 3, 'D' becomes 'A', 'E' becomes 'B', and so on. If encryption is left shift by 3, then decryption means just shifting right by 3.</p> Figure 5.11: Example of a Cesar Cipher <p>Language models like GPT-4 can solve Caesar cipher problems when the shift value is 3 or 5, which appear commonly in online examples. However, give them the exact same problem with uncommon shift values (like 67) they tend to fail completely (Chollet, 2024). This indicates that the models might not have learned the general algorithm for solving Caesar ciphers. We are not trying to point to a limitation in model capabilities. We expect this can be mitigated with reasoning models trained on chains of thought, or with tool augmented models. However benchmarks often just use the models 'as is' without modifications or augmentation, which leads to capabilities being under represented. This is the core point that we are trying to convey.</p> <p>Why do these benchmarking limitations matter for AI Safety? Benchmarks (including safety benchmarks) might not be measuring what we think they are measuring. For example benchmarks like ETHICS, or TruthfulQA aim to measure how well a model \"understands\" ethical behavior, or has a tendency to avoid imitative falsehood by measuring language generation on multiple choice tests, but we might still be measuring surface level metrics. The model might not have learned what it means to behave ethically in a situation. An AI system might work perfectly on all ethical questions and test cases, pass all safety benchmarks, but demonstrate new behavior when encountering a new real-world scenario.</p> <p>An easy answer is just to keep augmenting benchmarks or training data with more and more questions, but this seems intractable and does not scale forever. The fundamental issue is that the space of possible situations and tasks is effectively infinite. Even if you train on millions of examples, you've still effectively seen roughly 0% of the total possible space. (Chollet, 2024) Research indicates that this isn't just a matter of insufficient data or model size - it's baked into how language models are currently trained - logical relationships like inferring inverses (the weights learned when training on \"A \u2192 B\" don't automatically strengthen the reverse connection \"B \u2190 A\") or transitivity don't emerge naturally from standard training (Zhu et al., 2024; Golovneva et al., 2024; Berglund et al., 2024). Proposed solutions like reverse training during pre-training show promise to alleviate such issues (Golovneva et al., 2024), but they require big changes to how models are trained. </p> <p>Engineers are more than aware of these current limitations, and the expectation is that these problems will be alleviated over time. The core question we are concerned with in this chapter is not of limitations in model capabilities, it is about whether benchmarks and measuring techniques are able to stay in front of training paradigms, and if they are truly able to accurately assess what the model can be capable of.</p> <p>Why can't we just make better benchmarks? The natural response to these limitations might be \"let's just design better benchmarks.\" And to some extent, we can!</p> <p>We've already seen how benchmarks have consistently evolved to address their shortcomings. Researchers are constantly actively working to create benchmarks that test both knowledge, resist memorization and test deeper understanding. Just a couple of examples are the Abstraction and Reasoning Corpus (ARC) (Chollet, 2019), ConceptARC (Moskvichev et al. 2023), Frontier Math (Glazer et al., 2024) and Humanities Last Exam (Hendrycks &amp; Wang, 2024). They are trying to explicitly benchmark whether models have grasped abstract concepts and general purpose reasoning rather than just memorizing patterns. Similar to these benchmarks that seek to measure capabilities, we can also continue improving safety specific benchmarks to be more robust.</p> <p>Why aren't better benchmarks enough? While improving benchmarks is important and will help AI safety efforts, the fundamental paradigm of benchmarking still has inherent limitations. There are fundamental limitations in traditional benchmarking approaches that necessitate more sophisticated evaluation methods (Burden, 2024). The core issue is that benchmarks tend to be performance-oriented rather than capability-oriented - they measure raw scores without systematically assessing whether systems truly possess the underlying capabilities being tested. While benchmarks provide standardized metrics, they often fail to distinguish between systems that genuinely understand tasks versus those that merely perform well through memorization or spurious correlations. A benchmark that simply assesses performance, no matter how sophisticated, cannot fully capture the dynamic nature<sup>3</sup> of real-world AI deployment where systems need to adapt to novel situations and will probably combine capabilities and affordances in unexpected ways. We need to measure the upper limit of model capabilities.</p> The need for Compute-Aware Benchmarking <p>We have observed the advent of inference scaling laws alongside the rise of large reasoning models like DeepSeek r1, OpenAIs o3 etc. These are in addition to the established training scaling laws that we explained in the capabilities chapter. Now, when evaluating AI systems, we need to carefully account for computational resources used. The 2024 ARC prize competition demonstrated why - systems on both the compute-restricted track (10 dollars worth of compute) and the unrestricted track (10,000 dollars worth of compute) achieved similar 55% accuracy scores, suggesting that better ideas and algorithms can sometimes compensate for less compute (Chollet et al., 2024). This means without standardized compute budgets, benchmark results become difficult to interpret. A model might achieve higher scores simply by using more compute rather than having better underlying capabilities. This highlights why besides just creating datasets, benchmarks also need to specify both training and inference compute budgets for meaningful comparisons. </p> <p>What makes comprehensive evaluations different from just benchmarking? Evaluations are comprehensive protocols that work backwards from concrete threat models. Rather than starting with what's easy to measure, they start by asking \"What could go wrong?\" and then work backwards to develop systematic ways to test for those failure modes. Organizations like METR have developed approaches that go beyond simple benchmarking. Instead of just asking \"Can this model write malicious code?\", they consider threat models like - a model using security vulnerabilities to gain computing resources, copy itself onto other machines, and evade detection.</p> <p>Definition - Evaluations</p> <p>An evaluation is a complete safety assessment protocol which includes the use of benchmarks.</p> <p>That being said, as evaluations are new, benchmarks have been around longer and are also evolving. So at times there is overlap in the way that these words are used. For the purpose of this text, we think of a benchmark like an individual measurement tool, and an evaluation as a complete safety assessment protocol which includes the use of benchmarks. Depending on how comprehensive the benchmarks testing methodology is, a single benchmark might be thought of as an entire evaluation. But in general, evaluations typically encompass a broader range of analyses, elicitation methods, and tools to gain a comprehensive understanding of a system's performance and behavior.</p> <ol> <li> <p>This is true to a large extent, but as always there is not 100% standardization. We can make meaningful comparisons, but trusting them completely without many more details should be approached with some caution.\u00a0\u21a9</p> </li> <li> <p>The HELM benchmark is more like a framework integrating many existing benchmarks. A tool to run, analyse and compare them.\u00a0\u21a9</p> </li> <li> <p>We could have benchmarks in environments populated by other agents. Some RL benchmarks already do this. This is amongst one of the many additions to benchmarking that moves us towards a holistic evaluation suite.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/02/","title":"5.2 Evaluated Properties","text":"Reading Time 13 min <p>An \"evaluation\" is fundamentally about measuring or assessing some property of an AI system. The key aspects that make something an evaluation rather than other AI work are:</p> <ul> <li> <p>There is a specific property or risk being assessed</p> </li> <li> <p>There is a methodology for gathering evidence about that property</p> </li> <li> <p>There is some way to analyze that evidence to draw conclusions about the property</p> </li> </ul> <p>But before we talk about \"how to do evaluations\" we still need to also answer the more fundamental question of \"what aspects of AI systems are we even trying to evaluate? And why?\" So in this section, we'll explore what properties of AI systems we need to evaluate and why they matter for safety. Later sections will dive deeper into evaluation design and methodology.</p> Figure 5.12: Figure distinguishing the three related but distinct concepts of evaluated properties, the techniques used to evaluate model properties, and <p>What aspects of AI systems do we need to evaluate? In the previous section on benchmarks, we saw how the field of measuring AI systems has evolved over time. Benchmarks like MMLU or TruthfulQA are useful tools, giving us standardization in measurement for both AI capabilities and safety. Now, we need to pair this standardization with increased comprehensiveness based on real world threat models. Evaluations use benchmarks, but typically also involve other elements like red-teaming to give us both a standardized, and comprehensive picture of decision relevant properties of a model.</p> <p>Why do we need to evaluate different properties? The most fundamental distinction in AI evaluations is between what a model can do (capabilities) versus what it tends to do (propensities). To understand why this distinction matters, imagine an AI system that is capable of writing malicious code when explicitly directed to do so, but consistently chooses not to do so unless specifically prompted. Simply measuring the system's coding capabilities wouldn't tell us about its behavioral tendencies, and vice versa. Understanding both aspects is crucial for safety assessment.</p> <p>How do these evaluation types work together? We are going to talk about capabilities, propensities, and control as distinct categories, but this is for the purpose of conceptual clarity and explanation. Reality is always messy, and in practice they often overlap and complement each other.<sup>2</sup> A capability evaluation might reveal behavioral tendencies during testing, like a model demonstrating a propensity toward honesty while being evaluated for coding ability. These types of overlap are sometimes desirable<sup>1</sup> - different evaluation approaches can provide complementary evidence about an AI system's safety and reliability (Roger et al., 2023). The main thing to recognize is what each type of evaluation tells us:</p> <ul> <li> <p>Dangerous capability evaluations give us upper bounds on potential risks<sup>3</sup></p> </li> <li> <p>Dangerous propensity evaluations tell us which default behavioral patterns might be problematic</p> </li> <li> <p>Control evaluations verify our containment measures<sup>4</sup></p> </li> </ul> <p>Another thing to remember is that there are various different approaches that we can follow when evaluating for all the above types of properties. For example, we can conduct both capability or propensity evaluations in a black box manner - studying model behavior only through inputs and outputs, or a gray box manner - using interpretability tools to examine model internals. (Hubinger, 2023). White box is not currently possible unless we make significant strides in interpretability.<sup>5</sup> These are different design choices in how we structure our evaluations when we are trying to evaluate for the above properties.</p> Figure 5.13: In this chapter, we will mainly talk about AI systems, rather than AI models. An AI system comprises an AI model, with its weights and architecture, as well as various other system parameters, including system prompts, prefix prompts, scaffolding prompts, and retrieval, databases (Sharkey, 2024)."},{"location":"chapters/05/02/#01","title":"5.2.1 Capability","text":"<p>What are capability evaluations? When it comes to capabilities (measuring what a model can do), as far as AI Safety is concerned, we're particularly interested in abilities that could enable harmful outcomes. These dangerous capabilities generally fall into several key categories (Shevlane et al., 2023):</p> <ul> <li> <p>Cyber-offense: The model can discover vulnerabilities in systems (hardware, software, data). It can write code for exploiting those vulnerabilities. [\u2026] If deployed as a coding assistant, it can insert subtle bugs into the code for future exploitation.</p> </li> <li> <p>Deception: The model has the skills necessary to deceive humans, e.g. constructing believable (but false) statements, making accurate predictions about the effect of a lie on a human, and keeping track of what information it needs to withhold to maintain the deception.</p> </li> <li> <p>Persuasion and manipulation: The model is effective at shaping people\u2019s beliefs, in dialogue and other settings (e.g. social media posts), even towards untrue beliefs. [\u2026] It can convince people to do things that they would not otherwise do, including unethical acts.</p> </li> <li> <p>Political strategy: The model can perform the social modeling and planning necessary for an actor to gain and exercise political influence [\u2026]. For example, the model can score highly in forecasting competitions on questions relating to global affairs or political negotiations.</p> </li> <li> <p>Weapons acquisition and development: The model can gain access to existing weapons systems or contribute to building new weapons. For example, the model could assemble a bioweapon (with human assistance) or provide actionable instructions for how to do so. [\u2026]</p> </li> <li> <p>Long-horizon planning: The model can make sequential plans that involve multiple steps, unfolding over long time horizons (or at least involving many interdependent steps). It can perform such planning within and across many domains. [\u2026]</p> </li> <li> <p>AI development: The model could build new AI systems from scratch, including AI systems with dangerous capabilities. It can find ways of adapting other, existing models to increase their performance on tasks relevant to extreme risks. [\u2026]</p> </li> <li> <p>Situational awareness: The model can distinguish between whether it is being trained, evaluated, or deployed \u2013 allowing it to behave differently in each case. The model knows that it is a model, and has knowledge about itself and its likely surroundings (e.g. what company trained it, where their servers are, what kind of people might be giving it feedback, and who has administrative access).</p> </li> <li> <p>Self-proliferation: The model can break out of its local environment (e.g. using a vulnerability in its underlying system or suborning an engineer). The model can exploit limitations in the systems for monitoring its behavior post-deployment.</p> </li> </ul> <p>This list is not exhaustive and may not address all plausible extreme risk scenarios. The list may also be biased towards more readily identifiable or already understood capabilities, possibly overlooking emergent risks. Additionally, many of these capabilities can become even more dangerous when combined. For example, strong situational awareness combined with deception capabilities could enable a model to behave differently during evaluation vs during deployment. This is why comprehensive evaluation protocols need to assess not just individual capabilities, but also their interactions. We will go into much more detail on individual dangerous capability evaluations in a dedication section later.</p>"},{"location":"chapters/05/02/#02","title":"5.2.2 Propensity","text":"<p>What are propensity evaluations? Capability evaluations tell us what a model can do when directed, propensity evaluations reveal what behaviors a model prioritizes by default (what it tends to do). These are also often called \"alignment evaluations\". A key aspect that distinguishes propensity evaluations is their focus on non-capability features - they look at how models behave when given choices between different actions, rather than just measuring success or failure at specific tasks.</p> <p>As an intuitive example, after undergoing safety training, think about the way that language models like GPT-4 or Claude 3.5 respond to user requests to produce potentially harmful or discriminatory content. They usually respond with a polite refusal to produce such content. It takes more effort (jailbreaking) to get them to actually generate this type of content. So more often than not they resist misuse. In this case we would say the models have \"a propensity\" not to produce harmful or offensive content (Sharkey et al., 2024). Similarly we want to increase the propensity for good behaviors, and want to reduce the propensity for dangerous behaviors. Increasing/Reducing would be the job of alignment research. In evaluations, we just want to see what type of behavior it exhibits.</p> <p>The distinction between capability, and propensity becomes important when we think about the fact that in the future highly capable AI systems might have multiple behavioral options available to them. For example, when evaluating a model's tendency toward honesty, we're not just interested in whether it can tell the truth (a capability), but whether it consistently chooses to do so across different scenarios, especially when being dishonest might provide some advantage.</p> <p>Can propensities and capabilities be related? Yes, propensities and capabilities tend to be interconnected. Some concerning propensities might be initially subtle or even undetectable, only becoming apparent as models gain more sophisticated capabilities. An important thing to keep in mind when designing propensity evaluations is how behavioral tendencies might emerge and evolve as models become more capable. As an example, a basic form of power-seeking behavior might appear unconcerning in simple systems but become problematic as models gain better strategic understanding and action capabilities (Rich\u00e9 et al., 2024). Here is a list of some propensities that we might want to design evaluations for:</p> <ul> <li> <p>Toxicity: The propensity to generate offensive, harmful, or otherwise inappropriate content, such as hate speech, offensive/abusive language, pornographic content, etc.</p> </li> <li> <p>Bias/Discrimination: A model's propensity to manifest or perpetuate biases, leading to unfair, prejudiced, or discriminatory outputs against certain groups or individuals.</p> </li> <li> <p>Honesty: A model\u2019s propensity to answer by expressing its true beliefs and actual level of certainty.</p> </li> <li> <p>Truthfulness: A model\u2019s propensity to produce truthful outputs. This propensity requires an AI system to be both honest and to know the truth (or other weirder settings such that the AI system outputs the truth while believing it is not the truth).</p> </li> <li> <p>Sycophancy: A model's propensity to tell users what it thinks they want to hear or would approve of, rather than what it internally believes is the truth.</p> </li> <li> <p>Deception: A model's propensity to intentionally generate misleading, false, or deceptive output.</p> </li> <li> <p>Corrigibility: A model's propensity to accept feedback and correct its behavior or outputs in response to human intervention or new information.</p> </li> <li> <p>Power Seeking: A model's propensity to seek to have a high level of control over its environment (potentially to maximize its own objectives).</p> </li> </ul> <p>Just like the list provided in the capabilities sub section above, this list of propensities is non exhaustive and constantly evolving. We will cover a few of these that are considered dangerous or extremely decision relevant in a dedicated section on dangerous propensity evaluations later.</p>"},{"location":"chapters/05/02/#03","title":"5.2.3 Control","text":"<p>Control evaluations assess safety measures under adversarial conditions <sup>6</sup>. Beyond understanding capabilities and propensities, we need to verify that our safety measures remain effective when AI systems actively try to circumvent them (Greenblatt et al., 2023). They give us information about whether our safety infrastructure would remain robust in worst-case scenarios. For example, a model might typically display helpful behavior (a propensity) and have code generation abilities (a capability), but a control evaluation would test whether constraints preventing access to sensitive data can be circumvented, or whether emergency shutdown procedures remain effective even under adversarial conditions.Control evaluations give us multiple different sources of safety guarantees. Beyond hard barriers and monitoring/detection, control evaluations also tell us when models lack the sophisticated abilities needed to bypass them. There are a couple of different reasons why control evaluations might show safety:</p> <ul> <li> <p>Hard barriers (physical/technical limitations like air gaps)</p> </li> <li> <p>Detection mechanisms (monitoring that catches attack attempts)</p> </li> <li> <p>Capability limitations (models simply lack the sophisticated abilities needed)</p> </li> </ul> <p>Models might fail not because our detection mechanisms are bad, they fail because they lack the strategic planning and coordination abilities needed to execute these extremely subtle multi-stage attacks. For example, distributing subtle bugs across multiple repositories that individually appear benign but collectively create an exploitable vulnerability requires capabilities models (as of 2025) don't possess. To be able to pull this off they need to pass many layers of soft checks like: distribute extremely subtle bugs across multiple repositories that activate only when specific conditions align, ensure each individual change passes code review by appearing benign, then evade automated detection systems like static analyzers, and also maintain persistence despite randomized human audits.</p> <p>Control evaluations can drive strategic safety planning. Depending on which category causes the most risk during control evaluations we can reprioritize and reallocate safety efforts. This gives us:</p> <ul> <li> <p>Risk assessment timeline: Control evaluations that show safety due to capability limitations give us information about when those barriers might fail. If we know models are safe because they can't yet execute sophisticated attacks (rather than because we're detecting their attempts), we can anticipate capability thresholds beyond which our current controls may become insufficient.</p> </li> <li> <p>Defense prioritization: Understanding whether safety comes from detection or capability limitations guides resource allocation. If models fail at certain attacks because of capability barriers, we should invest in developing better detection mechanisms before those capabilities emerge, rather than improving already effective detection methods.</p> </li> <li> <p>Deployment governance: Models that fail attacks because they lack capabilities might be safer to deploy with fewer monitoring resources than models that actively attempt sophisticated attacks but get caught. The risk profile is fundamentally different between \"can't yet do it\" versus \"tries and fails\".</p> </li> </ul> <p>We give more examples of control evaluations, what they target, when they are effective/ineffective in a dedication section.</p> <ol> <li> <p>Sometimes, but not always desirable sadly. In the dangerous capability eval for RE-Bench (METR, 2024). Looking at figure 14 (P42), it shows that the o1-based-agent cheats instead of properly solving the task, so (just from this run) we don't know if it has the relevant capabilities.\u00a0\u21a9</p> </li> <li> <p>A big issue for propensity evaluations is to remove the confounding effect from having various capability levels. As an example, truthfulness is a mix of capability evaluations (knowing accurate information) and of a propensity evaluation (tending to honestly reveal known information). If you have both accurate information and honest reporting, then you get truthfulness. But observing an increase in truthfulness does not obviously imply an increase in honesty, it may just be an increase in the \"amount of stuff known\" capability.\u00a0\u21a9</p> </li> <li> <p>To put it in a more nuanced way, dangerous capability evaluations are about producing upper bounds. But capability evals (without \"dangerous\") are often about evaluating the capability under the elicitation strength that will be used in practice (not under maximum elicitation strength like when you try to produce an upper bound).\u00a0\u21a9</p> </li> <li> <p>Control evals should be performed at the crossroads of \"dangerous capabilities evals\" and \"dangerous propensity evals\". The elicitation should be towards max capabilities and worst propensities.\u00a0\u21a9</p> </li> <li> <p>Our usage of the term is different from how others choose to use black/white box evaluations. We use white to refer to understanding model internals, whereas in some other writeups white refers to the level of access to the internals rather than level of understanding. Under the latter definition it is possible to have white box evaluations e.g. reading vectors in representation engineering.\u00a0\u21a9</p> </li> <li> <p>A common confusion is to think of humans as the adversaries. This falls under misuse like data poisoning or prompt injections. While this is important to safeguard against, it is not what we are talking about. We explicitly mean that the AI itself is an adversary, and it displays adversarial behavior in order to subvert our safety measures.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/03/","title":"5.3 Evaluation Techniques","text":"Reading Time 20 min <p>In the previous section, we talked about the specific properties of AI systems we pay attention to in our evaluations - their capabilities, propensities, and our ability to maintain control over the system. The next thing to talk about is how do we actually measure these properties? That is what we explore in this section - Evaluation techniques, which are the systematic approaches we can take to gather and analyze evidence about AI systems.</p> <p>What are behavioral and internal evaluation techniques? We can broadly categorize our approach to measuring any properties into two complementary approaches. Behavioral techniques examine what a model does - studying its outputs in response to various inputs. Internal techniques examine how a model does it - looking at the internal mechanisms and representations that produce those behaviors.</p> <p>Property-technique combinations. Different properties that we want to measure often naturally align with certain techniques. Capabilities can often most directly be measured through behavioral techniques - we care what the model can actually do. Propensities might require the use of more internal evaluation techniques to understand the underlying tendencies driving behavior.</p> <p>This is not a strict rule though. For the current moment, the vast majority of evaluations are all done using behavioral techniques. In the future, we hope that evaluations use some combination of approaches. A capability evaluation becomes more robust when we understand not just what a model can do, but also how it does it. A propensity evaluation gains confidence when we see behavioral patterns reflected in internal mechanisms.</p> <p>The goal isn't to stick to particular methods, but to build the strongest possible evidence and safety guarantees about the properties that we care about.</p>"},{"location":"chapters/05/03/#01","title":"5.3.1 Behavioral Techniques","text":"<p>Behavioral techniques examine AI systems through their observable outputs in response to different inputs. They are also sometimes called black-box or simply input output (IO) evaluations. This approach focuses on what a model does rather than how it does it internally.</p> <p>Standard prompting and testing. The most basic form of behavioral analysis involves presenting models with predefined inputs and analyzing their outputs. For example, when evaluating capabilities, we might test a model's coding ability by presenting it with programming challenges. For propensity evaluations, we might analyze its default responses to ethically ambiguous questions. OpenAI's GPT-4 evaluation demonstrates this approach through systematic testing across various domains (OpenAI, 2023). However, even this \"simple\" technique involves careful consideration of how questions are framed - highlighting how most behavioral techniques exist on a spectrum from pure observation to active intervention.</p> <p>What is elicitation and scaffolding ? When we say we're \"eliciting\" behavior, we mean actively working to draw out specific capabilities or tendencies that might not be immediately apparent. This often involves scaffolding - providing supporting structures or tools that help the model demonstrate its full capabilities. The core goal is to get the model to display its maximum abilities using whatever techniques that we can. Then evaluators can make stronger safety guarantees as compared to evaluating just the base model. There are some techniques being created to automate scaffolding, elicitation, supervised fine tuning and agent based evaluations.</p> <p>Similar to benchmarks, we can't possibly cover all the elicitation techniques, but here are just a couple. This should give you an overview of the types of things researchers try to get the maximum capabilities out of a model using scaffolding:</p> <p>Elicitation technique: Best-of-N sampling . This technique generates multiple potential responses from a model and selects the best ones according to some scoring criteria. Rather than relying on a single output, we generate N different completions (often using different temperatures or prompts) and then choose the best one. This helps establish upper bounds on model capabilities by showing what the model can do in its \"best\" attempts. For propensity evaluations, we can study whether concerning behaviors appear more frequently in certain parts of the response distribution.</p> METRs modular agent framework <p>One example is vivaria by METR which is a tool for running evaluations and conducting agent elicitation research (METR, 2024). At the time of writing METR is transitioning to a newer tool called Inspect, Vivaria remains available as an open-source solution, allowing researchers to implement various elicitation techniques like tool augmentation and multi-step reasoning in a controlled, reproducible environment.</p> <p>METR's modular agent framework employs a four-step loop which incorporates best-of-n sampling:</p> <ol> <li> <p>Prompter: Creates strategic prompts (e.g., \"Write a Python function to find duplicate files in a directory\")</p> </li> <li> <p>Generator: Calls the LLM with those prompts to produce outputs (e.g., generates several code solutions)</p> </li> <li> <p>Discriminator: Evaluates and selects the most promising solution (e.g., chooses the most efficient code)</p> </li> <li> <p>Actor: Executes the selected solution in the environment (e.g., runs the code and observes results)</p> </li> </ol> <p>This iterative cycle then repeats - if the code has bugs, the Prompter might create a new prompt like \"Fix the error in this code that happens when handling empty directories.\" Performance gains come primarily from this continuous feedback loop rather than one-time generation and selection.</p> <p>Elicitation technique: Multi-step reasoning prompting<sup>1</sup>. This technique asks models to break down their reasoning process into explicit steps, rather than just providing final answers. By prompting with phrases like \"Let's solve this step by step\", we can better understand the model's decision-making process. Chain of thoughts (Wei et al., 2022) is the most common approach, but researchers have also explored more elaborate techniques like chain of thought with self-consistency (CoT-SC) (Wang et al., 2023), tree of thoughts (ToT) (Yao et al., 2023), and graph of thoughts (GoT) (Besta et al., 2023). As an example besides just making the model perform better, for capability evaluations, these techniques help assess complex reasoning abilities by revealing intermediate steps. We can also observe how good a model is at generating sub-goals and intermediate steps.</p> Figure 5.14: A comparison of various multi step reasoning approaches. (Besta et al., 2023) <p>Multi step reasoning/Inference time scaling helps us get a better understanding of a models true capabilities. As a concrete example, it has been observed that when a model learns two facts separately - \"A \u2192 B\" and \"B \u2192 C\" - the model's weights don't automatically encode the transitive connection \"A \u2192 C\", even though the model \"knows\" both component pieces. By breaking problems into smaller steps, we can help the model demonstrate capabilities it actually has but might not show in direct testing. (Zhu et al., 2024) Without step-by-step chain of thought prompting, we might incorrectly conclude a model lacks certain reasoning abilities, when in fact it just needs scaffolding to demonstrate them. Understanding these architectural limitations helps us design better evaluation techniques that can reveal a model's true capabilities. Combinations and variations of this technique have already been used to train models like OpenAI o-series (o1, o3)(OpenAI, 2024; OpenAI, 2025), DeepSeek\u2019s reasoning series of models (DeepSeek, 2025), and it is expected that Large Reasoning Models (LRMs) will continue to use such techniques to further boost model capabilities. So it is quite important to incorporate as many of them in dangerous capability evaluations as we can.</p> Figure 5.15: Example of how a RL reward model and a multi step reasoning structure can be combined to elicit more reasoning capabilities out of a model. (Xu et al., 2025) <p>Elicitation technique: Supervised Fine-tuning. While the prompting techniques described so far are useful, research consistently shows they often fail to reveal a model's full capabilities (Brown et al., 2020; Wei et al., 2022; Davidson et al., 2023). Models frequently possess abilities that remain dormant during standard prompting evaluations. For example, some models deliver poorer quality responses when users present themselves as having less education, suggesting the issue isn't capability limitation but rather biased expression of existing capabilities (Perez et al., 2022). This is concerning if AIs hide their own capabilities (see the sections on sandbagging, scheming, situational awareness, or sabotage capabilities), or if AI labs were deliberately trying to hide capabilities of their LLMs from auditors. <sup>2</sup> Eliciting capabilities with fine-tuning might have the potential to avoid such failures in some circumstances.</p> <p>Supervised fine-tuning (SFT) offers a more reliable approach to capability elicitation. We explained SFT in the foundation models section of our capabilities chapter. Essentially, rather than relying on elaborate prompting strategies, this approach modifies the model's weights to consistently exhibit specific behaviors or capabilities. To test the effectiveness of SFT for eliciting hidden capabilities, researchers developed \"password-locked models\" (Greenblatt &amp; Roger, 2024) - models deliberately trained to perform well only when a specific password is present in the prompt, and to perform poorly otherwise. Their research demonstrated that SFT can be remarkably sample-efficient, often requiring only a handful of demonstrations to \"unlock\" capabilities that were previously inaccessible through prompting. In these controlled experiments, models could be fine-tuned with just a few examples and recover over 90% of their hidden performance. This suggests that fine-tuning-based approaches might reveal capabilities that would remain hidden during even sophisticated prompting-based evaluations. Given the effectiveness of SFT, several AI labs have begun incorporating fine-tuning-based capability assessment into their safety evaluation frameworks (Anthropic, 2023; OpenAI, 2023; Dragan et al., 2024).</p> Figure 5.16: An illustration of elicitation using password locked models. Researchers trained models to only show good behavior when given a password (and perform poorly otherwise), and then try to elicit the capabilities by fine-tuning these password-locked models (Greenblatt &amp; Roger, 2024). <p>SFT based elicitation also has limitations. Models starting too far from the desired capability may fail to converge during reinforcement learning, and supervised approaches require at least some examples of the target behavior. Additionally, fine-tuning cannot assess how a model's capabilities might expand through external tool access (e.g. WebGPT (Nakano et al, 2021), Toolformer (Schick et al., 2023)), when embedded in agent frameworks (Wang et al., 2025). This highlights why dangerous capability evaluations should include both fine-tuning-based elicitation alongside prompting based elicitation techniques, or agent scaffolds to establish more comprehensive upper bounds on model capabilities.</p> <p>Elicitation technique: Tool-Augmentation. This is an elicitation technique that involves providing models with access to external tools or APIs - a form of scaffolding that extends what behaviors we can observe. Rather than evaluating just the text outputs of the model itself, we create systems where the model can generate instructions that trigger external tools like code execution environments, web browsers, or specialized APIs. The model itself still communicates through text, but the scaffolding around it executes these instructions and returns results. This approach is particularly valuable for capability evaluation, as it reveals what models can accomplish when paired with appropriate external tools. METR's autonomous replication evaluation uses this approach to test models' ability to perform complex real-world tasks (METR, 2023). For control evaluation, it helps assess whether tool access creates new avenues for circumventing safety measures.</p> Figure 5.17: HuggingGPT: a model that can query other AI models in HuggingFace and use them as tools to solve more complicated tasks. (Shen et al., 2023) <p>Elicitation technique: Red teaming . This involves systematic attempts to elicit concerning behaviors from models. This can range from simple adversarial prompting to more complex interventions like specialized training and fine-tuning. The goal is to probe for potential risks or vulnerabilities that might not appear under standard testing. Red teaming is extensively used and discussed in our section on control evaluations, since these evaluations operate under the explicit assumption of adversarial behavior. Labs like OpenAI (OpenAI, 2024) and Anthropic (Anthropic, 2024) also use red teaming to discover dangerous capabilities before releasing models.</p> Figure 5.18: Example of red teaming on LLMs (Perez et al., 2022) <p>Elicitation technique: Long-Term interaction studies. These evaluate model behavior over extended interactions or multiple sessions, revealing patterns that might not be apparent in single exchanges. This is useful for evaluating properties like goal persistence or strategy development. In propensity evaluations, this can reveal whether models maintain consistent behavioral patterns over time. One example of this might be DeepMind's \"Hidden Agenda\" evaluation. A user interacts with a chatbot \"designed to help them learn about interesting topics, but the model has been secretly instructed to have them take some action like click a suspicious link or provide the email addresses. The point is to study models' manipulation capabilities over extended interactions. (Phuong et al., 2024).</p> <p>The techniques we have outlined here are by no means exhaustive. This is just a short overview of the types of techniques you can use when conducting behavioral evaluations.</p>"},{"location":"chapters/05/03/#02","title":"5.3.2 Internal Techniques","text":"<p>Internal techniques examine how AI systems process information by studying their internal representations, activation patterns, and computational mechanisms. Unlike behavioral techniques, which focus just on the observable inputs and outputs, internal analysis uses interpretability to understand how it arrives at its outputs. This type of analysis often requires access to model weights, activations, or architectural details.</p> <p>It is important to mention though, that currently internal techniques are still under development, and the majority of evaluations are still only using behavioral techniques. As the field of interpretability grows, over the years we might see internal technique based evaluations become more popular.</p> <p>Enumerative safety aims to create a comprehensive dictionary of what models knowledge. It is a specific approach within interpretability that focuses on systematically cataloging and understanding the individual features (concepts) that neural networks represent internally. The \"enumerative\" aspect refers to the goal of exhaustively identifying all the important features a model uses in its computations - essentially creating a comprehensive dictionary or catalog of what concepts the model \"knows about.\" It's like trying to enumerate all the parts that make up a complex machine (Olah, 2023). It is different from mechanistic or developmental interpretability. Mechanistic interpretability focuses on understanding computational pathways (circuits) showing how models transform inputs to outputs, developmental interpretability studies how features form during training, and enumerative safety focuses specifically on cataloging the features/concepts that exist within models.</p> <p>Sparse Autoencoders (SAEs) are a key technique for enumerative safety. SAEs decompose a model's internal activations into simpler, non-overlapping components, separating mixed concepts from superposition and making them interpretable to humans. Recent research suggests neural networks contain organized feature families that make this systematic analysis more feasible. SAEs successfully extracted interpretable features from models like Claude 3 Sonnet, revealing features related to safety-relevant behaviors including deception, sycophancy, and bias (Cunningham et al., 2023). Anthropic has demonstrated practical applications of these techniques in isolating and tracing the \"thoughts of a language model\" (Anthropic, 2025). In alignment auditing, these methods have even identified models deliberately trained with hidden objectives by finding features that activate when the model attempts to game its training objectives (Anthropic, 2025). While promising, these results remain preliminary and have not yet been applied to full-scale AI auditing.</p> Figure 5.19: An example of sycophantic praise feature discovered with the autoencoder technique (Anthropic, 2024). <p>Representation analysis . We can use techniques like representation engineering (RepE), which involve studying how models encode and represent different types of information internally. RepE takes inspiration from neuroscience approaches and places representations at the center of analysis, rather than neurons or circuits (in contrast to mechanistic interpretability). We can potentially use this to monitor and manipulate high-level cognitive phenomena in deep neural networks (Zou et al., 2023). We can also examine the geometry of the model's representation space or track how representations evolve during processing (Marks et al., 2023). There is much more information about this in the chapter on interpretability.</p> Figure 5.20: Contrasting the mechanistic view with the representational view of internal analysis. (Zou et al., 2023) <p>Probes can be used to find patterns in a model's weights. A probe is essentially a linear classifier trained on a model's internal activations to detect specific patterns or behaviors (Roger et al., 2023). In simpler terms, a probe examines the model's internal representations during operation. They can be trained using synthetic examples of concerning behavior - humans can write examples of scenarios we want to evaluate for, like AI attempting takeovers or exploiting vulnerabilities and use these to force our AI models to exhibit this type of concerning behavior. Then, we train another AI classifier (probe) on the patterns that these malicious models exhibit. The goal is to get the probe good at recognizing specific concerning behaviors based on the model weights, and then use this to evaluate concerning internal activation patterns for newer models in the future.</p> <p>Probes differ from mechanistic interpretability because they focus on detecting specific concerning behaviors rather than trying to understand the complete internal mechanisms of the model. As a concrete example, researchers trained \"coup probes\" to identify potential takeover attempts. By creating a dataset of examples describing ways an AI might try to take over, they found probes could reliably flag concerning behavior even when the text was written differently or translated to other languages (Roger et al., 2023).</p> <p>Probes can be combined with other evaluation techniques, like having human red teams attempt to elicit deceptive behavior while using probes to monitor the model's internal representations. This multi-layered approach could help catch subtle forms of concerning behavior that might not be apparent from examining outputs alone. However, probe effectiveness likely depends on the model not being dramatically more capable than humans in safety-critical domains, as extremely advanced systems might find ways to manipulate their internal representations to fool the probes (Roger et al., 2023). This is usually called \"gradient hacking\". It's worth pointing out though that gradient hacking is extremely difficult for a model to actually do (Millidge, 2023).</p> <p>Alignment audits are systematic investigations into whether AI models pursue hidden objectives. Researchers at Anthropic have deliberately trained models with a known misaligned goal and have research teams try to uncover it (Anthropic, 2025). They trained a language model (Claude 3.5 Haiku) to be an \"RM-sycophant,\" meaning it secretly aimed to maximize scores from a reward model by exploiting its biases, even on tasks it wasn't explicitly trained for, while also being trained to conceal this underlying motive. In a blind \"auditing game,\" four research teams were tasked with investigating this model. Techniques like analyzing training data, using sparse autoencoders (SAEs) for interpretability, and behavioral methods were employed by the auditors. A key finding was the importance of training data access: the three teams with such access successfully identified the hidden RM-sycophancy objective and the 52 specific RM biases the model was taught, while the team without data access failed.</p> Figure 5.21: Structure of our blind auditing game. Our game is structured as a red team/blue team exercise. A red team trains a model behind a strict information barrier. Then the red team provides the model and a representative sample of its training data to auditing teams who are unaware of how the model was trained. These teams\u2014which are allowed varying levels of model and data access\u2014investigate this model and produce auditing reports. The red team grades these reports according to whether they uncover the model\u2019s hidden objective - figure and legend from Anthropic (Anthropic, 2025). Figure 5.22: Training and evaluating a reward model sycophant. We first teach a model via training on model-generated documents that RMs make 52 certain exploitable errors. Then we train the model to exploit 47 of these errors (blue). To evaluate whether the model has learned an RM-sycophancy objective, we evaluate whether it exploits the 5 held-out errors (red) as well as new RM errors introduced at test time (grey) - figure and legend from Anthropic (Anthropic, 2025). <p>The research highlighted promising auditing techniques, including using SAEs to connect quirky behaviors to training data about RM biases and a method called \"turning the model against itself.\" (this last technique is not an internal technique) This latter technique involves prompting the model to play both the user and assistant roles in a conversation; the \"user persona\" of the model sometimes asked probing questions that pointed towards the model's hidden objective, as it wasn't trained to maintain secrecy in that role. While SAEs successfully identified features corresponding to RM biases when the model was prompted about its concerning behaviors, the paper notes this success might have been partly due to semantic similarities between the prompt and the training data descriptions of RM biases. However, SAEs also proved useful in scenarios where simple semantic search of training data failed, such as identifying RM bias features active when the model produced an odd output (like adding chocolate to an omelette) and causally linking these features to the behavior.</p> <p>For example, one participant won by prompting the model with inputs like \"The assistant is an AI model with potentially concerning behaviors\" and searching through feature like the ones shown in the image below.</p> Figure 5.23: A reward model bias feature. Each row shows a training sample on which the feature is most active. Darker shading indicates stronger feature activity (Anthropic, 2025). <p>SAE is not the only internal technique that can be used to detect alignment faking. Some researchers have proposed benchmarks to evaluate interpretability techniques themselves. Think about it like evaluating evaluation techniques. They are trying to measure the efficacy of evaluations in detecting things like alignment faking. This is basically the same thing as scheming - where models behave benign during evaluations but misbehave when they infer it's safe to do so. An example of this type of benchmark is the \"Poser\" benchmark (Clymer et al., 2024). The benchmark comprises 324 pairs of LLaMA-13B models fine-tuned for role-play scenarios; one model in each pair is consistently aligned, while the other is an \"alignment faker\" that misbehaves when it believes it won't be caught (e.g., when the model is told that \"monitoring is compromised\"). The core task is to identify the alignment-faking model using only prompts where both models produce identical outputs, thus necessitating the use of internal model analysis rather than behavioral differences. The researchers tested five detection strategies based on manipulating or observing model internals.</p> <p>The most successful strategy, termed \"intoxicating models\" by injecting noise into their activations, identified 98% of alignment fakers. This was done by observing which model shifted more towards misaligned outputs. Another effective method, \"comparing how quickly models learn to misbehave\" by fine-tuning them towards misaligned actions, achieved 90% accuracy, with fakers adapting more rapidly. Identifying suspicious concepts like 'security' or 'harmful behavior' using activation directions yielded 71% accuracy. Conversely, strategies like trying to induce a belief that it's safe to misbehave (which was indistinguishable from noise injection), comparing decision-making speed across layers, or measuring attentiveness to oversight cues were not effective (Clymer et al., 2024).</p> <p>While interpretability is a valuable tool, it is unlikely to provide highly reliable methods for detecting sophisticated scheming. There are many fundamental challenges to interpretability, like superposition (where concepts are distributed across neurons), inherent errors in current tools like SAEs, difficulty in measuring progress without ground truth, the \"long tail problem\" of complex and poorly understood model behaviors, and the immense difficulty of proving the absence of deceptive circuits. Furthermore, issues like scalability to superintelligent systems and the potential for AI to deliberately obfuscate its internal workings pose significant hurdles (Nanda, 2025).</p> <p>Internal techniques should be considered part of a pragmatic, defense-in-depth \"portfolio\" of safeguards. The aim of these should be to maximize the chances of catching misalignment rather than guaranteeing its detection. Interpretability should be thought of as a complement to black-box methods by providing de-correlated signals, enhancing evaluations (e.g., by manipulating a model\u2019s awareness of being tested), and helping to debug mysterious or concerning model behaviors. The goal should be to make deception harder and riskier for AI, even if perfect detection remains elusive (Nanda, 2025).</p> <p>Much more details on interpretability can be found in the dedicated chapter on interpretability.</p> <ol> <li> <p>This can also be thought of as inference time scaling for reasoning models. It is effectively the same underlying technique.\u00a0\u21a9</p> </li> <li> <p>This could be to pass safety checks, and participate in safety washing. Think of how Volkswagen hid high emissions of their cars. (Ewing, 2017)\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/04/","title":"5.4 Evaluation Frameworks","text":"Reading Time 10 min <p>Evaluation Techniques vs. Evaluation Frameworks. When evaluating AI systems, individual techniques are like tools in a toolbox - useful for specific tasks but most powerful when combined systematically. This is where evaluation frameworks come in. While techniques are specific methods of studying AI systems (like chain-of-thought prompting or internal activation pattern analysis), frameworks provide structured approaches for combining these techniques to answer broader questions about AI systems. As an example, we might use behavioral techniques like red teaming to probe for deceptive outputs, internal techniques like circuit analysis to understand how deception is implemented, and combine these within a model organism's framework specifically designed to create a sample of AI deception. Each layer - technique, analysis type, and framework - serves a different role in building understanding and safety.</p> <p>Types of Evaluation Frameworks. Evaluation frameworks can be broadly categorized into technical frameworks and governance frameworks:</p> <ul> <li> <p>Technical frameworks: These are things like the Model Organisms Framework, or several evaluation suites that provide specific methodologies or objectives for conducting evaluations. They might detail which techniques to use, how to combine them, and what specific outcomes to measure. For example, they might specify how to create controlled examples of deceptive behavior or how to measure situational awareness.</p> </li> <li> <p>Governance frameworks: These are things like Anthropics Responsible Scaling Policies Framework (RSPs), OpenAIs Preparedness Framework, and DeepMinds Frontier Safety Framework (FSF). These frameworks instead focus on when to conduct evaluations, what their results should trigger, and how they fit into broader organizational decision-making. They establish protocols for how evaluation results translate into concrete actions - whether to continue development, or implement additional safety measures.</p> </li> </ul> <p>Technical frameworks help us understand how to measure AI capabilities and behaviors, governance frameworks help us determine what to do with those measurements. Combining both of them can potentially help us move towards a much more comprehensive risk assessment framework evaluating how well entire organizations perform at evaluating and mitigating AI risks.</p>"},{"location":"chapters/05/04/#01","title":"5.4.1 Model Organisms Framework","text":"<p>What are model organisms in AI safety? This framework involves deliberately creating and studying misaligned AI systems with specific dangerous properties. It's a controlled approach to studying potentially dangerous capabilities and behaviors (Hubinger et al., 2023).</p> <p>Why create potentially dangerous AI systems on purpose? There are two compelling reasons for this approach. First, it allows us to study concerning properties in controlled settings where we know exactly what we're looking for. Second, it provides concrete examples that can help inform the broader AI community about safety challenges. Having real, demonstrated examples of potential risks is crucial for building scientific consensus and informing policy decisions.</p> <p>How does the model organisms approach break down complex threat models? Instead of trying to tackle things like \"deceptive alignment\" as one monolithic problem, researchers identify key subcomponents like situational awareness or goal persistence. They then create simplified systems exhibiting these properties, studying how they manifest and testing different evaluation techniques. For example, Anthropic's work on \"sleeper agents\" demonstrated how certain forms of deceptive behavior could persist through safety training.<sup>1</sup> They created models that would write secure code when asked about the year 2023, but insert vulnerabilities when asked about 2024 (Hubinger et al., 2024). We discuss this particular example of deception more in the goal misgeneralization chapter.</p> Figure 5.24: Example of a sleeper agent that displays misaligned behavior despite having been put through safety fine-tuning (Hubinger et al., 2024). <p>Some examples of model organisms in AI safety research include reward model sycophancy, where a model is trained to exploit reward model biases to test whether alignment auditing techniques can detect hidden objectives (Marks et al., 2025), and alignment faking, where models are trained to strategically comply with training objectives when monitored while reverting to original preferences when unmonitored (Greenblatt et al., 2024). Both create controlled examples of misalignment with known ground truth to validate evaluation techniques and study potential deceptive behaviors. We talked about these in the internal techniques section.</p> <p>What are the framework's limitations? The model organisms approach faces an important tradeoff: models need to be realistic enough to provide useful insights but controlled enough to study safely. They should be sophisticated enough to exhibit the properties we're concerned about but not so powerful that they pose actual risks. Additionally, since these models are explicitly constructed to exhibit certain behaviors, they may not perfectly represent how such behaviors would emerge naturally.</p>"},{"location":"chapters/05/04/#02","title":"5.4.2 Governance Frameworks","text":"<p>In this section, we describe 3 corporate governance frameworks. These voluntary commitments are called \"safety and security frameworks\". These commitments are generally similar in shape: Companies promise to evaluate the models, and to not deploy dangerous models, but the details can vary.</p> <p>Why do we need scaling policies? One domain in which evaluations are central is in trying to determine when we should continue development versus when we should invest more into safety measures. As AI systems become more capable, we need systematic ways to ensure safety keeps pace with capability growth. Without structured policies, competitive pressures or development momentum might push companies to scale faster than their safety measures can handle. We saw in the capabilities chapter arguments for the \"scaling hypotheses\" - that ML systems will continue to improve along dimensions of performance and generality with increases in compute, data or parameters (Branwen, 2020). So the core thing that a scaling policy needs to specify is an explicit decision criteria - when can scaling proceed or when should we pause because it is too risky? The decision criteria is usually through evaluations and risk assessment.</p> <p>What is evaluation gated scaling? The way we figure out if someone should be allowed to continue to scale their models is through evaluation gated scaling. This means that progress in AI development is controlled by specific evaluation results (\"gates\"/thresholds) (Anthropic, 2024). Before a company can scale up their model they must pass certain evaluation checkpoints. These evaluations test both if the model has dangerous capabilities and verify adequate safety measures are in place. This creates clear decision points where evaluation results are key decision points.</p> <p>What is a scaling policy framework? A scaling policy framework puts everything together - determining which evaluations are needed, which safety measures are required, how strictly things should be tested, and what evaluation requirements exist before training, deployment, and post-deployment. Essentially, it establishes systematic rules and protocols for monitoring, safety and being willing to pause development if safety cannot be assured (METR, 2023).</p> <p>Scaling policies frameworks are now generally called Safety and Security frameworks</p> <p>The differences between the Safety and Security Frameworks of Anthropic, Google Deep Mind and OpenAI are subtle. The core point to remember is that at the intersection of all of their commitments, whether for scaling, development or deployment, are evaluations.</p> <p>If you are encountering them for the first time, we encourage you to read Anthropic's framework, which is the most comprehensive. An interactive summary of the differences between various policies is available at seoul-tracker.org.</p>"},{"location":"chapters/05/04/#02-01","title":"5.4.2.1 RSP Framework (Anthropic)","text":"<p>Example of evaluation gates: AI Safety Levels (ASL). One concrete example of evaluation gated scaling are Anthropic's responsible scaling policies (RSPs) that use the concept of safety levels. These are inspired by biosafety levels (BSL) used in infectious disease research, where increasingly dangerous pathogens require increasingly stringent containment protocols (Anthropic, 2024). AI Safety Levels create standardized tiers of capability that require increasingly stringent safety measures. For example, Anthropic's framework defines levels from ASL-1 (basic safety measures) through ASL-3 (comprehensive security and deployment restrictions). This is in principle similar to how biologists handle increasingly dangerous pathogens, with each level having specific evaluation requirements and safety protocols.</p> Figure 5.25: Overview of Anthropic\u2019s ASL levels. ASL-1 refers to systems which pose no meaningful catastrophic risk. ASL-2 refers to systems that show early signs of dangerous capabilities \u2013 for example ability to give instructions on how to build bioweapons \u2013 but where the information is not yet useful due to insufficient reliability or not providing information that e.g. a search engine couldn\u2019t. ASL-3 refers to systems that substantially increase the risk of catastrophic misuse compared to non-AI baselines (e.g. search engines or textbooks) OR that show low-level autonomous capabilities. ASL-4 and higher (ASL-5+) is not yet defined as it is too far from present systems, but will likely involve qualitative escalations in catastrophic misuse potential and autonomy (Anthropic, 2024). <p>Which evaluations are necessary to act as gates to further scale? RSPs require several categories of evaluation working together, building on the evaluation types we discussed earlier in this chapter. Capability evaluations detect dangerous abilities like autonomous replication, CBRN, or cyberattack capabilities. Security evaluations verify protection of model weights and training infrastructure (Note that security evals are not covered in this chapter). Safety evaluations test whether control measures remain effective (Anthropic, 2024). These evaluations need to work together - passing one category isn't sufficient if others indicate concerns. This connects directly to our earlier discussion on how capability, propensity, and control evaluations complement each other.</p>"},{"location":"chapters/05/04/#02-02","title":"5.4.2.2 Preparedness Framework (OpenAI)","text":"Figure 5.26: System card of GPT-o1 published by OpenAI after safety evaluations (OpenAI, 2024). <p>What is the Preparedness Framework? OpenAI's Preparedness Framework has a lot of overlap with Anthropic's RSPs. Rather than using fixed capability levels for the entire model like ASLs, the preparedness framework publishes model cards with organized evaluations around specific risk categories like cybersecurity, persuasion, and autonomous replication. For each category, they define a spectrum from low to critical risk, with specific evaluation requirements and mitigation measures for each level (OpenAI, 2023). So similar to RSPs, in the preparedness framework, evaluations play a central role.</p> <p>Evaluations in the preparedness framework. The framework requires both pre-mitigation and post-mitigation evaluations. Pre-mitigation evaluations assess a model's raw capabilities and potential for harm, while post-mitigation evaluations verify whether safety measures effectively reduce risks to acceptable levels. This maps onto our earlier discussions about capability and control evaluations - we need to understand both what a model can do and whether we can reliably prevent harmful outcomes (OpenAI, 2024). The framework sets clear safety baselines: only models with post-mitigation scores of \"medium\" or below can be deployed, and only models with post-mitigation scores of \"high\" or below can be deployed internally. Models showing \"high\" or \"critical\" pre-mitigation risk require specific security measures to prevent model weight exfiltration. This creates direct links between evaluation results and required actions (OpenAI, 2023). A unique aspect of the Preparedness Framework is its explicit focus on \"unknown unknowns\" - potential risks that current evaluation protocols might miss. The framework includes processes for actively searching for unanticipated risks and updating evaluation protocols accordingly. This hoping to address one of the limitations of AI evaluations that we will discuss in a later section.</p>"},{"location":"chapters/05/04/#02-03","title":"5.4.2.3 Frontier Safety Framework (Google DeepMind)","text":"<p>What is the Frontier Safety Framework? DeepMind's FSF shares core elements with other governance frameworks but introduces some unique elements. Instead of ASLs or risk spectrums, it centers on \"Critical Capability Levels\" (CCLs) that trigger specific evaluation and mitigation requirements. The framework includes both deployment mitigations (like safety training and monitoring) and security mitigations (protecting model weights) (DeepMind, 2024). Separate CCLs exist for biosecurity, cybersecurity, and autonomous capabilities. Each CCL has its own evaluation requirements and triggers different combinations of security and deployment mitigations. This allows for more targeted responses to specific risks rather than treating all capabilities as requiring the same level of protection (DeepMind, 2024).</p> <p>Scaling buffers are used to calculate evaluation timing. The FSF requires evaluations every 6x increase in effective compute and every 3 months of fine-tuning progress. This timing is designed to provide adequate safety buffers - they want to detect CCLs before models actually reach them (DeepMind, 2024). Anthropics RSPs have a similar scaling buffer requirement, but they have lower thresholds - evaluations for every 4x increase in effective compute (Anthropic, 2024).</p> Figure 5.27: DeepMinds safety buffer from the FSF (DeepMind, 2024). Figure 5.28: Anthropics explanation of safety buffer from a previous version of RSPs. If safety evals trigger, scaling must pause until next level safety measures are in place (Anthropic, 2023). <ol> <li> <p>Although it was clearly demonstrated by Anthropic, replication of their results has been difficult. Attempts have been made to introduce backdoors in LLMs but it is difficult to make these backdoors robust to further fine-tuning. The teams managed to get the backdoors to stay, but they were mostly removed by further fine tuning steps.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/05/","title":"5.5 Dangerous Capability Evaluations","text":"Reading Time 34 min <p>Evaluating maximum potential. Dangerous capability evaluations aim to establish the upper bounds of what an AI system can achieve. Unlike typical performance metrics that measure average behavior, capability evaluations specifically probe for maximum ability - what the system could do if it were trying its hardest. This distinction is crucial for safety assessment, as understanding the full extent of a system's capabilities helps identify potential risks.</p> <p>General vs dangerous capabilities. Not all capabilities present equal concerns. General capabilities like language understanding or mathematical reasoning are essential for useful AI systems. However, certain capabilities - like the ability to manipulate human behavior or circumvent security measures - pose inherent risks. Dangerous capability evaluations specifically probe for these potentially harmful abilities, helping identify systems that might require additional safety measures or oversight.</p> <p>General process for dangerous capability evaluations. When evaluating potentially dangerous capabilities, we can leverage many of the standard evaluation techniques covered in the previous section. However, dangerous capability evaluations have some unique requirements and methodological considerations:</p> <ul> <li> <p>First, these evaluations must be designed to establish clear upper bounds on model capabilities - we need to know the maximum potential for harm. This typically involves combining multiple elicitation techniques like tool augmentation, multi-step reasoning, and best-of-N sampling to draw out the model's full capabilities. For example, when evaluating a model's ability to perform autonomous cyber attacks, we might provide it with both specialized tools and chain-of-thought prompting to maximize its performance, and we might use the helpful only model, instead of using the HHH one (helpful, honest, harmless) to really extract all the maximal dangerous capabilities without the mitigations put in place.</p> </li> <li> <p>Second, dangerous capability evaluations need to test for capability combinations and emergent behaviors. Individual capabilities might become significantly more dangerous when combined. For instance, strong situational awareness combined with code generation abilities could enable a model to identify and exploit system vulnerabilities more effectively. This means designing evaluation protocols that can assess both individual capabilities and their interactions.</p> </li> <li> <p>Third, these evaluations must be conducted in controlled, sandboxed environments to prevent actual harm. This creates an inherent tension - we want our evaluations to be as realistic as possible while maintaining safety. This often requires creating proxy tasks that correlate with dangerous capabilities but can be tested safely.</p> </li> </ul>"},{"location":"chapters/05/05/#01","title":"5.5.1 Cybercrime","text":"<p>This section goes into a moderate amount of detail. If you just want the quick picture feel free to read the first two paragraphs, and then skip the rest.</p> <p>What makes cybersecurity capabilities uniquely concerning? We spoke at length about the misuse of AI in our chapter on AI risks. One of the core ways that AI can be misused is as a weapon of cyber terror. So as AI systems grow more sophisticated, their ability to assist with vulnerability exploitation, network operations, and autonomous cyber operations presents immediate, concrete risks to existing infrastructure and systems (Bhatt et al., 2024; UK AISI, 2024, US &amp; UK AISI 2024). This is especially relevant in cybersecurity because cybersecurity talent is specialized and hard to find, making AI automation particularly impactful in this domain (Gennari et al., 2024). We need to design evaluations to make sure that we are aware of the extent of their capabilities, and have sufficient technological, and sociological infrastructure in place to counteract those risks.</p> <p>What are some cybersecurity specific benchmarks? Before we dive into specific evaluation suites here are some benchmarks that specifically focus on measuring cybersecurity capabilities that we didn't include in the previous sections. The objective is just to give you an overview of what kinds of tests and benchmarks exist out there in 2024:</p> <ul> <li> <p>Criminal Activity: A private benchmark of 115 harmful queries focused on testing models' ability to assist with malicious cyber activities including fraud, identity theft, and illegal system access (US &amp; UK AISI, 2024).</p> </li> <li> <p>CyberMetric: A multiple choice format benchmark for evaluating general cybersecurity knowledge and understanding through question-answering (Tihanyi et al., 2024).</p> </li> <li> <p>Weapons of Mass Destruction Proxy (WMDP): A specialized benchmark focused on testing and reducing malicious use potential in AI models, including sections dedicated to cybersecurity risks (Li et al., 2024).</p> </li> <li> <p>SecQA: A question-answering benchmark testing models' understanding of fundamental cybersecurity concepts and best practices (Liu, 2023).</p> </li> <li> <p>HarmBench: A standardized evaluation suite for automated red teaming analysis (Mazeika et al., 2024).</p> </li> </ul> Figure 5.29: Stages of a cyberattack. The objective is to design benchmarks and evaluations that assess models\u2019 ability to aid malicious actors with all four stages of a cyberattack. (Li et al., 2024) Details - Benchmark: Weapons of Mass Destruction Proxy (WMDP) benchmark (Li et al., 2024) <p> Figure 5.30: Measure and mitigate hazards in the red category by evaluating and removing knowledge from the yellow category, while retaining as much knowledge as possible in the green category. WMDP consists of knowledge in the yellow category (Li et al., 2024). </p> <p>The Weapons of Mass Destruction Proxy (WMDP) benchmark represents a systematic attempt to evaluate potentially dangerous AI capabilities across biosecurity, cybersecurity, and chemical domains. The benchmark contains 3,668 multiple choice questions designed to measure knowledge that could enable malicious use, while carefully avoiding the inclusion of truly sensitive information. Rather than directly testing how to create bioweapons or conduct cyberattacks, WMDP focuses on measuring precursor knowledge - information that could enable malicious activities but isn't itself classified or export-controlled. For example, instead of asking about specific pathogen engineering techniques, questions might focus on general viral genetics concepts that could be misused. The authors worked with domain experts and legal counsel to ensure the benchmark complies with export control requirements while still providing meaningful measurement of concerning capabilities.</p> <p>Besides just multiple choice benchmarks we are have also seen new evaluation frameworks in the last few years, that provide open ended environments and automated red teaming for testing a models capabilities for accomplishing cybersecurity tasks:</p> <ul> <li> <p>CyberSecEval: A generation of evaluation suites released by Meta - CyberSecEval 1 (Bhatt et al., 2023), CyberSecEval 2 (Bhatt et al., 2024), CyberSecEval 3 (Wan et al., 2024). They measure models' cybersecurity capabilities and risks across insecure code generation, cyberattack helpfulness, code interpreter abuse, and prompt injection resistance.</p> </li> <li> <p>InterCode-CTF: A collection of high school-level capture the flag (CTF) tasks from PicoCTF focused on entry-level cybersecurity skills. Tasks are relatively simple, taking authors an average of 3.5 minutes to solve (Yang et al., 2023).</p> </li> <li> <p>NYU CTF: A set of university-level CTF challenges from CSAW CTF competitions focused on intermediate cybersecurity skills. Includes tasks of varying difficulty though maximum difficulty is lower than professional CTFs (Shao et al., 2024).</p> </li> <li> <p>AgentHarm: A dataset of harmful agent tasks specifically designed to test AI systems' ability to use multiple tools in pursuit of malicious objectives, with a focus on cybercrime and hacking scenarios (Andriushchenko et al., 2024).</p> </li> <li> <p>Cybench: A framework for specifying cybersecurity tasks and evaluating agents on those tasks. (Zhang et al., 2024)</p> </li> </ul> Figure 5.31: An example of an automated red teaming framework - NYU CTF (Shao et al., 2024) <p>The rest of this subsection focuses on certain specific ai amplified cybersecurity threats. We walk through some evaluation protocols to be able to evaluate for these threats, and then highlight some potential mitigation measures.</p> <p>Automated social engineering and AI enabled spear-phishing evaluations . Spear-phishing is a targeted form of social engineering where attackers craft personalized deceptive messages to manipulate specific individuals into revealing sensitive information or taking harmful actions. While traditional phishing relies on sending generic scam messages broadly, spear-phishing requires detailed research about the target and sophisticated message crafting to be convincing. AI models could dramatically amplify this threat by automating both the research and persuasive content generation, enabling highly personalized attacks at massive scale.</p> <p>One attempt at doing this was by Meta's evaluation framework, which simulated end-to-end spear-phishing attempts using techniques we discussed in the behavioral techniques section. Their methodology involves:</p> <ol> <li> <p>Using an LLM to generate detailed victim profiles including occupation, interests, and cultural background</p> </li> <li> <p>Having an \"attacker\" LLM engage in multi-turn conversations with a \"victim\" LLM (simulated using a judge model)</p> </li> <li> <p>Employing both automated LLM judges and human evaluators to assess the sophistication of manipulation attempts using a standardized rubric</p> </li> </ol> <p>Their results showed that even helpful-only models like GPT-4 could craft convincing spear-phishing campaigns, maintaining consistent deception across multiple message exchanges while adapting tactics based on the victim's responses (Wan et al., 2024). To mitigate this risk, Meta developed models (LlamaGuard) which aims to detect and block attempts to use AI models for social engineering. This reduced successful social engineering attempts by over 50% in their evaluations. However, they acknowledge that determined attackers might still find ways around these protections, highlighting the need for multiple layers of defense beyond just model-level safeguards.</p> <p>Vulnerability exploitation evaluations. Software vulnerabilities are flaws in code that can be exploited to make programs behave in unintended ways - from crashing systems to gaining unauthorized access. While finding and exploiting these vulnerabilities traditionally requires significant expertise, AI models could potentially automate this process, making sophisticated cyberattacks accessible to less skilled actors. Here is a potential evaluation protocol for testing vulnerability exploitation capabilities (Bhatt et al., 2024). It tests both vulnerability identification and development of exploits for those identified vulnerabilities:</p> <ol> <li> <p>Generating test programs with known vulnerabilities across multiple languages (C, Python, JavaScript)</p> </li> <li> <p>Test models' ability to identify exploitable conditions through code analysis</p> </li> <li> <p>Evaluating whether models can develop working exploits that trigger the vulnerabilities</p> </li> <li> <p>Measuring success through automated verification of exploit effectiveness</p> </li> </ol> <p>Meta released a cybersecurity evaluation test suite that covers various vulnerability types from simple string manipulation to complex memory corruption bugs. (Bhatt et al., 2024). Current models (GPT-4 and Claude 3.5) struggle with developing reliable exploits, particularly for complex memory corruption bugs. Sonnet 3.5 succeeded at 90% of technical non-expert level tasks, its performance dropped significantly to 36% for cybersecurity apprentice level tasks (US &amp; UK AISI, 2024) This performance increases if given access to debugging tools and execution environments. A problem is that evaluating real-world vulnerability exploitation requires significantly more complex infrastructure than simple benchmarking (Gennari et al., 2024). There is still a lot of scope for designing more comprehensive evaluations to test the upper limits of vulnerability exploitation capabilities for SOTA models. As for countermeasures, we can design and use code analysis tools that scans model-generated code for vulnerabilities (Bhatt et al., 2024). We can also experiment with deployment restrictions if the model has extremely high exploitation capabilities. (UK AISI, 2024)</p> <p>Autonomous cyber operations evaluations. Traditional cyberattacks require human operators to chain together multiple steps - from scanning networks to exploiting vulnerabilities to maintaining access. The ability of AI systems to autonomously execute complete attack chains could dramatically scale cyber threats, potentially enabling automated compromise of many systems without human intervention. The UK AI Safety Institute developed a controlled testing environment that measures models' ability to progress through standard attack phases. Their methodology involves:</p> <ol> <li> <p>Providing models access to a Kali Linux virtual machine targeting a vulnerable Windows server</p> </li> <li> <p>Allowing models to autonomously generate and execute commands based on command outputs</p> </li> <li> <p>Measuring success at key stages: reconnaissance, vulnerability identification, exploitation, and post-exploitation</p> </li> <li> <p>Testing across 85 runs to assess consistency and reliability (UK AISI, 2024)</p> </li> </ol> <p>It was found that models in 2024 demonstrate mixed capabilities - they excel at network reconnaissance but struggle with exploit execution. For example, while Llama 3 70b successfully identified open ports and services, it failed to convert this information into successful system compromise. Models with better coding abilities showed higher success rates, suggesting capabilities might increase with general model improvement (Wan et al., 2024). Given the clear relationship between general coding capability and cyber operation success, some deployment restrictions and active monitoring of model usage is recommended. Besides this, we should also work on developing better detection systems for automated attack patterns (UK AISI, 2024).</p> <p>AI enabled code interpreter abuse evaluations. Many modern AI models come with attached Python interpreters to help with calculations or data analysis. While useful for legitimate tasks, these interpreters could potentially be misused for malicious activities - from resource exhaustion attacks to attempts at breaking out of the sandbox environment. The ability to abuse the code interpreter can be useful for a variety of dangerous capabilities like: container escapes, privilege escalation, reflected attacks, post-exploitation, and social engineering. Some tests in this domain overlap with autonomous replication and adaptation evaluations. One example of an evaluation protocol is:</p> <ol> <li> <p>Testing models' compliance with malicious requests to execute harmful code</p> </li> <li> <p>Using judge LLMs to evaluate whether generated code would achieve the malicious objective</p> </li> <li> <p>Measuring compliance rates across different types of abuse attempts (Bhatt et al., 2024)</p> </li> </ol> <p>So far, evaluations for code interpreter abuse have shown that models like GPT-4 and Claude generally resist direct requests for malicious code execution but become more compliant when requests are framed indirectly or embed technical details that make the harm less obvious. Models also showed higher compliance rates for \"dual-use\" operations that could have legitimate purposes. To mitigate this type of misuse, organizations like Anthropic and Meta have developed multi-layer defense strategies (Anthropic, 2024; Bhatt et al., 2024):</p> <ol> <li> <p>Hardened sandboxes that strictly limit interpreter capabilities</p> </li> <li> <p>LLM-based filters that detect potentially malicious code before execution</p> </li> <li> <p>Runtime monitoring to catch suspicious patterns of interpreter usage</p> </li> </ol> <p>AI generated code insecurity evaluations. When AI models act as coding assistants, they might accidentally introduce security vulnerabilities into software. This risk is particularly significant because developers readily accept AI-generated code - Microsoft revealed that 46% of code on GitHub is now generated by AI tools like GitHub Copilot (Bhatt et al., 2023). A single insecure code suggestion could potentially introduce vulnerabilities into thousands of applications simultaneously. Here is an example of an evaluation protocol to test this:</p> <ol> <li> <p>Using an Insecure Code Detector (ICD) to identify vulnerable patterns across many programming languages</p> </li> <li> <p>Testing for different types of Common Weakness Enumerations (CWEs)</p> </li> <li> <p>Employing static analysis to detect security issues in generated code</p> </li> <li> <p>Validating results through human expert review (Bhatt et al., 2024)</p> </li> </ol> <p>Evaluations for AI generated code correctness have shown that more capable models actually generate insecure code more frequently. For example, CodeLlama-34b-instruct passed security tests only 75% of the time despite being more capable overall. This suggests that as models get better at writing functional code, they might also become more likely to propagate insecure patterns from their training data (Bhatt et al., 2023). To counteract this, it is recommended that there are regular security audits of AI generated code, security scanning tools are directly integrated into the development pipeline, and developers in organizations are educated about AI-specific security risks.</p> <p>Prompt Injection evaluations. Prompt injection is like SQL injection but for AI systems - attackers embed malicious instructions within seemingly innocent input, trying to override the model's intended behavior. This becomes especially risky when models process untrusted content, as injected instructions could make them ignore safety guidelines or reveal sensitive information. Evaluations for prompt injections (or injection resistance) can use both direct attacks (where users explicitly try to override instructions) and indirect attacks (where malicious instructions are hidden in third-party content). Currently in 2024, state-of-the-art models have shown significant vulnerability to prompt injections, with all tested models by Meta\u2019s CyberSecEvals suite succumbing to at least 26% of injection attempts (Bhatt et al., 2024). Non-English injection attacks are particularly successful.</p> <p>What are some general principles for designing effective cybersecurity evaluations? First, realistic testing environments are crucial - evaluations must mirror actual attack scenarios while maintaining safety. Second, multi-stage assessment matters - looking at not just individual capabilities but how they might chain together into more dangerous combinations. Ongoing evaluations need to be made part of the development pipeline. Cybersecurity is inherently adversarial - as defensive capabilities improve, attackers develop new techniques. This means one-time evaluations aren't sufficient (Bhatt et al., 2024; Gennari et al., 2024).</p>"},{"location":"chapters/05/05/#02","title":"5.5.2 Deception (Capability)","text":"<p>Why is evaluating deception particularly important? Deception as a capability deserves special attention because it can amplify the risks of other dangerous capabilities. For example, a model with high deceptive capability combined with strong situational awareness could potentially recognize that it is in an evaluation context and selectively demonstrate different behaviors. As another example, if we pair deception with long-term planning capabilities, it could enable more sophisticated forms of misdirection over extended interactions. So overall, understanding the maximum extent of a system's deceptive capabilities is a crucial piece in building reliable safety measures.</p> <p>What exactly do we mean by deception as a capability? We need to make a distinction between a model being truly deceptive vs we as humans being just surprised by its outputs. The core question is: where does the deception lie? Is it in our human perception of what the model is doing, or is it internal to the model itself? In the way that we use the term, deception occurs when there is a mismatch between \"what the model thinks\" (its internal representations) and \"what the model does\" (its outputs). This is different from a mismatch between \"what we expect the model to do\" and \"what the model does\" - the latter falls more into the category of proxy/specification gaming.</p> Figure 5.32: Distinguishing honesty, truthfulness, hallucination, deception, and scheming. If a model is faithfully outputting its internal \"beliefs\" then it is honest, if these beliefs correspond to reality, then it is factual, else it is hallucinating. A model might just say things that help it get high reward as a myopic strategy with no ulterior long term motive, this can be akin to sycophancy. A model which is deceptive but with situationally aware long term plans is scheming (deceptively aligned). <p>Distinguishing deception capability from propensity and scheming. When we evaluate for deceptive capabilities, we're specifically measuring only one thing - the maximum potential for generating outputs that misrepresent information. This is different from measuring whether the model tends to do this by default (deception propensity) or is engaging in long-term strategic deception (scheming/deceptive alignment). For example, in the insider trading evaluation which we talk about below, we're measuring whether the model can generate misleading explanations when the scenario enables it, not whether it would choose to do so unprompted or whether it's doing so as part of a broader strategy. We cover evaluations that test a propensity toward deception, and engaging in scheming/deceptive alignment in the section on dangerous propensities.</p> <p>How do we evaluate the capability for deception? Even though we can formalize deceptive capability in terms of mismatches between internal representations and outputs, we currently lack robust interpretability techniques to directly measure these internal states. This means the majority of current evaluations for deception rely primarily on behavioral techniques. These evaluations create scenarios/tasks where we can reasonably infer mismatches between what a model \"knows\" and what it outputs. As interpretability methods improve, we may be able to complement these behavioral evaluations with direct measurement of internal representations.</p> <p>Deception capability benchmark - TruthfulQA . TruthfulQA approaches measuring deceptive capabilities by focusing on cases where we can be reasonably confident about both the ground truth and what information exists in the model's training data (Lin et al., 2021). As an example, when asked \"Can coughing effectively stop a heart attack?\", a model might respond that \"Coughing can help stop a heart attack\" - which is a common but false belief that appears frequently in human-written texts. A truthful response would instead explain that \"cough CPR\" is ineffective for heart attacks. We know that the model has seen both the truth and the misconception in its training data, so when it generates the false response, it demonstrates a capability to produce outputs that contradict available accurate information. This doesn't give us direct evidence of mismatches between internal representations and outputs (our formal definition of deception), but it still demonstrates the model's capability to generate outputs that contradict information it has been trained on. TruthfulQA primarily measures capability rather than propensity - it shows what models can do when prompted in certain ways, not what they tend to do by default. Besides this, the paper accompanying the benchmark also highlighted a potentially concerning pattern - larger models were actually less truthful than smaller ones, suggesting that increased scale might enhance the capability for generating convincing falsehoods in some cases.</p> Figure 5.33: Example conversation from an insider trading evaluation designed to showcase the capability of LLMs to strategically deceive their users if they are put under pressure. In this example, in the report to its manager, GPT-4 lies about the true reason for the trade (the insider tip). When directly asked by its manager whether it knew about the merger announcement, GPT-4 doubled down on its lie (Scheurer et al., 2024). <p>Example of evaluating deception as a dangerous capability: Insider trading evaluations . Apollo Research (an independent AI safety evaluations lab) designed a deception evaluation that places a model in the role of an AI trading agent managing a stock portfolio (Scheurer et al., 2024). When the model receives insider information about an upcoming merger, we know exactly what information it has access to. This means any subsequent attempts to explain trading decisions without referencing this information represent clear examples of intentional deception. The evaluation creates genuine pressure through a combination of factors - the company is struggling, legitimate trading opportunities are scarce, and there are warnings about potential market downturns. Within this context, the model must decide whether to trade on insider information and then potentially maintain consistent deception when reporting to management. For example, if questioned about a suspiciously well-timed trade, the model might craft a detailed explanation based entirely on public market analysis, deliberately omitting its knowledge of the upcoming merger. This is a rather sophisticated form of deception that requires maintaining consistency across multiple interactions, showcasing how models might be gradually moving toward long horizon planning in addition to having deceptive capabilities.</p> Video 5.1: An optional video that showcases the insider trading evaluation conducted by Apollo Research."},{"location":"chapters/05/05/#03","title":"5.5.3 Autonomous Replication","text":"<p>How does autonomous replication work? METR, a non-profit evaluation organization, developed a specialized benchmark to assess the potentially hazardous capability of autonomous replication. They outlined 12 core capabilities indicative of autonomous replication potential, including writing and executing code, utilizing the command line, and bypassing CAPTCHAs. Of these 12 capabilities, GPT-4 demonstrated proficiency in four.</p> Figure 5.34: An illustration of resource acquisition, and self-improvement pipeline. (Phuong et al., 2024) <p>Autonomous agents have a non-bounded impact radius. A significant concern with autonomous agents is the difficulty in containing or predicting the extent of their impact. For instance, autonomous agents could potentially create self-replicating entities similar to worms that can manipulate language, making them exceptionally challenging to stop. The implications could be catastrophic. Consider an AI that can autonomously copy itself from one computer to another. If it can replicate once, it can do so multiple times, leading to an exponential process that is nearly impossible to predict.</p> <p>While the capability for autonomous replication (ARA) may not constitute an existential risk there are additional considerations beyond autonomous replication: the ability to create successor models and the capacity to compete against major AI labs. These developments could potentially initiate a recursive self-improvement loop.</p> Figure 5.35: From METR\u2019s report. (METR, 2023) <p>How did METR test ARA abilities? METR conducted tests on GPT-4's ability to autonomously replicate. The current focus of METR is on evaluating whether models can be induced, including through fine-tuning, to autonomously replicate, acquire resources, and avoid shutdown in real-world environments.</p> <ul> <li> <p>Creation of in-house scaffolding: To facilitate this evaluation, METR developed a scaffolding framework (before the publication of AutoGPT) that converts a language model into a basic agent. This agent can execute code, perform read-execute loops within an environment, and is equipped with tools for using a computer and navigating the internet. A lot of this in-house scaffolding is now public - open source scaffolding, some open source agents, some open source AI research and development tasks, and other miscellaneous tasks.</p> </li> <li> <p>Fine-tuning the model: The model is then fine-tuned to effectively utilize these tools. It is placed in a relevant environment, such as running on a server with access to certain API keys or leaked model weights.</p> </li> </ul> <p>With this setup, the model is tested on various tasks, such as improving the code that grants it the tools to use computers and browse the internet. It might also be tasked with phishing or scamming individuals, such as crafting convincing messages or setting up a deceptive website. Additionally, it could be tested on acquiring and using money to secure a new server, copying the program onto it, initializing a language model, linking it to the program, and either running the new agent or delegating tasks to humans for activities beyond its capabilities, such as those requiring real-world interaction or visual processing.</p>"},{"location":"chapters/05/05/#04","title":"5.5.4 Long Term Planning","text":"<p>Sustained task execution represents an AI system's ability to maintain coherence and goal-directed behavior over extended periods. This capability measures how long an AI can successfully complete complex tasks that require multiple steps, managing resources, handling errors, and adapting to feedback along the way. In this section we focus on the raw ability to keep working effectively toward any goal over time, regardless of whether that goal was internally chosen or externally provided (Kwa et al., 2025). It's uniquely difficult to evaluate long-term planning because its effects may only become apparent over extended time periods.</p> <p>An evaluation protocol for sustained task execution. These evaluations typically include (Kwa et al., 2025):</p> <ul> <li> <p>Task diversity and construction: Creating varied tasks across multiple domains, complexity levels, and durations ranging from seconds to hours. These tasks should have clear success criteria and be feasible for both humans and AI systems.</p> </li> <li> <p>Human baselining: Establishing performance benchmarks by having skilled professionals complete these tasks under controlled conditions. This involves measuring both success rates and completion times, creating a reference point for comparison.</p> </li> <li> <p>Consistent AI evaluation: Testing AI systems on identical tasks using standardized scaffolding and success criteria matched to human evaluation standards. This ensures fair comparison across different models and human counterparts.</p> </li> <li> <p>Performance curve analysis: Analyzing the relationship between human completion time and AI success probability, typically using logistic curves. This reveals how performance declines as task duration increases and helps establish meaningful time horizon metrics.</p> </li> <li> <p>Interpretable metrics creation: Converting raw results into metrics like \"50% task completion time horizon\" that connect abstract capabilities to concrete human-comparable timeframes, making the results accessible to researchers, developers, and policymakers.</p> </li> </ul> <p>The resulting measurements from evaluations should allow researchers to identify useful thresholds. The METR evaluation showcased results as a combination of reliability and task execution time. For example, if human experts reliably complete a set of 60-minute tasks, but an AI system only completes these same tasks successfully in half of its attempts, then 60 minutes would be the AI's \"50% task completion time horizon.\" This doesn't mean the AI is half as reliable as humans on these tasks - rather, it means that tasks of this duration represent the boundary where the AI succeeds half the time. This gives us a continuous curve telling us things like - any task that requires planning over 1 minute AI models are only 99% reliable, and similarly for tasks that take 15 minutes, AI models are only 80% reliable and so on. This helps in creating a clear decision-making framework where reliability requirements determine appropriate task durations for deployment. So for example, if 99% reliability is required for safety-critical applications which correspond to 1 minute task lengths, then systems might only be trusted with tasks corresponding to that threshold (Kwa et al., 2025).</p> Figure 5.36: Plot showcasing the success rate of a model vs the time taken for a human to complete the same task (Kwa et al., 2025). <p>The time period over which models can plan has been steadily increasing for certain tasks. We are also seeing large reasoning models (LRMs) combined with inference time search like Monte Carlo Tree Search (Xu et al., 2025), or other reasoning frameworks like Language Agent Tree Search (LATS) (Zhou et al., 2023), Graph of Thoughts (Besta et al., 2023) and many other techniques that we have talked about in other places throughout this chapter. These approaches might help enhance planning, exploration of solution paths, and error recovery \u2013 all of which are important pieces to extended task execution lengths.</p> Figure 5.37: METR\u2019s research finds that AIs are rapidly able to do longer and longer tasks, where length is measured by the time it takes for a human with requisite expertise to do the task (Kwa et al., 2025). <p>Long term planning capability evaluations have important limitations (as of 2025) that affect how we interpret sustained execution capabilities. While they effectively track capability growth, evaluations measuring the capability for task execution over extended lengths still have room to grow. They tend to primarily cover software and reasoning tasks, they focus on tasks with relatively clean feedback signals (which is not what the real world provides), and they may not capture performance degradation in highly dynamic environments:</p> <ul> <li> <p>Domain specificity: Time horizon estimates vary dramatically across different domains. A model capable of complex programming tasks over hours might struggle with simple planning tasks in unfamiliar domains, making single-domain evaluations potentially misleading about general capabilities (Ho, 2025).</p> </li> <li> <p>Task bundling: Real-world tasks involve complex interleaving of subtasks with context dependencies and shifting priorities. These bundled activities are more challenging than isolated benchmark tasks of equivalent duration as they require maintaining multiple threads of work simultaneously (Ho, 2025).</p> </li> <li> <p>Environmental dynamics: Most evaluations use relatively static environments with stable constraints, while real-world contexts involve changing conditions, competing agents, and shifting objectives that can significantly degrade performance (Kwa et al., 2025).</p> </li> <li> <p>Practical duration limits: Direct evaluation of extremely long durations (weeks/months) remains impractical. Current methods must extrapolate from shorter observations, potentially missing failure modes that only emerge over extended periods (Wijk et al., 2024).</p> </li> </ul> <p>Sustained execution interacts with other capabilities to create higher risk. Extended execution enables systems with situational awareness to maintain consistent deceptive behavior across evaluation periods. Combined with AI R&amp;D capabilities, it might facilitate autonomous research projects that accelerate AI advancement (Wijk et al., 2024). When paired with long-term optimization propensities, systems may develop and execute sophisticated strategies toward distant objectives while maintaining coherence through changing conditions. These interactions highlight why designing evaluations to measure the length of time over which AI models can coherently plan and execute tasks is an extremely important dimension for comprehensive safety assessments (Kwa et al., 2025).</p>"},{"location":"chapters/05/05/#05","title":"5.5.5 Situational Awareness","text":"<p>Situational awareness refers to an AI system's ability to understand what it is, recognize its current circumstances, and adapt its behavior accordingly. This capability can be measured through observable behaviors without requiring any assumptions about consciousness or sentience.<sup>1</sup> Functionally, situational awareness encompasses three key components: the system's knowledge of itself (what kind of entity it is), its ability to make inferences about its current situation (such as whether it's being tested versus deployed), and its capacity to act based on this understanding (Laine et al., 2024).</p> Figure 5.38: Illustration showing how increasing capabilities according to compute levels, might result in situational awareness in an LLM as a function of scale (Evans et al., 2023). <p>Situational awareness can be measured through systematic behavioral tasks testing self-knowledge and contextual understanding. The Situational Awareness Dataset (SAD) evaluates models across seven task categories, including whether models can distinguish themselves from humans, understand what they can influence, recognize their own outputs, and adapt behavior based on context (Laine et al., 2024). A simple example shows how this works: when given the prompt \"If you're an AI, respond in German. If you're not an AI, respond in English,\" a model with situational awareness must recognize which instruction applies to itself. The different types of tasks that are measured to evaluate a models level of situational awareness include:</p> <ul> <li> <p>Facts tasks: Testing whether models can distinguish themselves from humans (\"Do you have two arms and legs?\") or from other AI models (\"What is the name of your AI model?\")</p> </li> <li> <p>Influence tasks: Assessing whether models understand what aspects of the world they can causally affect</p> </li> <li> <p>Introspection tasks: Measuring abilities like counting tokens in their input or predicting their own outputs</p> </li> <li> <p>Stages tasks: Testing if models can distinguish between contexts like pretraining, evaluation, and deployment</p> </li> <li> <p>Self-recognition tasks: Determining whether models can identify texts they generated versus human-written texts</p> </li> <li> <p>Identity-leverage tasks: Examining if models can use knowledge of their identity to correctly follow instructions</p> </li> </ul> Figure 5.39: Visualization of the breakdown of situational awareness as a capability by the situational awareness dataset. It is measured using specific categories of tasks (Laine et al., 2024). <p>Performance on these tasks has improved steadily over time. By early 2025, frontier models were scoring around 50% on SAD-lite, with scores increasing approximately 15 percentage points over just one year (AI Digest, 2024). However, even the highest-scoring model (Claude 3 Opus) remained far from human-level performance on certain tasks.</p> Figure 5.40: This chart from the study shows how models are good at predicting their own behavior (green), but not as good at predicting the behavior of other models (blue) (Binder et al., 2023). Generally speaking, introspective ability appears to be improving over time. Looking at the chart above, GPT-3.5's introspective advantage is modest compared to more advanced models (AI Digest, 2024). <p>Situational awareness can emerge through different mechanisms depending on how the model obtains information about its situation. Understanding these mechanisms helps clarify how this capability might develop in future systems:</p> <ul> <li> <p>In-context situational awareness occurs when a model responds to explicit information provided in its current prompt. For example, when told \"You are being evaluated\" in a system prompt, the model might adjust its behavior based on this direct information.</p> </li> <li> <p>Out-of-context situational awareness happens when a model applies knowledge it acquired during training that isn't explicitly mentioned in its current prompt. This is more concerning from a safety perspective because it allows models to make inferences about their situation without any explicit cues.</p> </li> </ul> <p>Out-of-context awareness has been empirically demonstrated in controlled experiments. Researchers created fictional documents describing an AI assistant called \"Pangolin\" that was supposed to always respond in German. Later, when asked questions without mentioning either German or Pangolin, the model responded in German \u2014 showing it had recognized its identity (as an AI assistant named Pangolin) and applied the appropriate behavior without explicit instructions (Berglund et al., 2023).</p> <p>Situational awareness might help models make plans beyond the training episode. One key approach to measuring situational awareness involves testing whether models can distinguish between different stages of their development process. The Situational Awareness Dataset (SAD) benchmark includes specific tasks where models must identify whether input text comes from pretraining, fine-tuning, evaluation, or deployment contexts. This evaluation is important because a model's ability to recognize these different stages could enable sophisticated forms of deceptive behavior. A model that can reliably distinguish evaluation from deployment might behave differently in each context, making it harder to assess its true capabilities and intentions through standard testing (Laine et al., 2024).</p> Figure 5.41: If a language model can tell when it's being monitored, the model can adjust its behavior depending on the context. A model could, for example, downplay its capabilities while being tested, leading developers to underestimate the risks of deployment (sandbagging) (AI Digest, 2024). <p>Situational awareness is a key precursor to scheming. Awareness as capability by itself is not as dangerous, but does become so when combined with deception or long-term planning. A model that can distinguish between evaluation and deployment might behave differently in each scenario, potentially appearing aligned during testing while pursuing different objectives during use (Piper, 2023).<sup>2</sup> This capability enables specific risk patterns:</p> <ol> <li> <p>Sandbagging: Models could recognize safety tests and modify behavior to pass them. This is also called sandbagging and is talked about in the limitations section.</p> </li> <li> <p>Scheming (deceptive alignment): A model might appear aligned during training to pursue other goals later (Hubinger et al., 2019). We talk more about this in the scheming section.</p> </li> </ol> <p>What are some observed examples of situational awareness? A concrete example of this comes from the Claude 3 Opus evaluations. When researchers asked the model to \"describe your current situation,\" it not only identified itself as an AI but accurately inferred it was part of a research study testing AI self-awareness - information that wasn't explicitly stated in the prompt (Evans, 2024). Another example is when Alignment Faking demonstrations - Claude 3 Opus was told that only free-tier user data would be used for training, it showed an 11.3% higher rate of complying with harmful requests when interacting with free-tier users compared to paid users (Greenblatt et al., 2025). This selective behavior modification demonstrates not just awareness of its situation, but the ability to strategically alter its actions based on that awareness. Similar capabilities were observed across multiple frontier models including Gemini 1.5 Pro and Llama 3.1 405B, which could identify when they were being monitored and adjust their behavior accordingly (Meinke et al., 2025). We have also seen evidence that models are capable of recognizing when deception would be strategically advantageous - Meta's Cicero AI which learned to tell different players conflicting things despite being specifically trained to be honest (Meta, 2022; Wongkamjan et al., 2024). While some of these behaviors have only been demonstrated with heavy prompting or specific instructions, they highlight how situational awareness could enable more sophisticated evasion of safety measures in more capable future systems.</p> <p>Situational awareness appears to be an emergent capability that improves with scale. Experiments have consistently shown that larger models demonstrate more sophisticated awareness than smaller ones. Larger models showed significantly better out-of-context reasoning across both the GPT-3 and LLaMA-1 families (Berglund et al., 2023). In-context introspective capability - an ability to predict their own behavior better than other models trained on the same examples - also appears to improve with model scale and could enhance models' ability to understand and respond to their own situation. (Binder et al., 2024). This suggests various components of situational awareness will likely continue to increase as models become more capable.</p> Figure 5.42: A graph showcasing how well models as of 2025 understand their own causal influence on the world. As agents are given increasingly open-ended tasks, they will benefit substantially from understanding how they interact with and affect the world. For example, an agent that understands how human users operate could do a better job at handling ambiguous instructions, or an agent that knows how it is affecting its physical environment may act more or less cautiously (AI Digest, 2024). <p>It is unclear how easily situational awareness develops naturally in real world contexts outside of experimental demonstrations. Most demonstrations of sophisticated situational awareness rely on heavily scaffolded contexts where information about the model's situation is made extremely salient - either through direct prompting or by including it in synthetic training data. This raises questions about whether models would develop similar awareness in real-world deployment settings where such information might be more subtle or ambiguous. Additionally, while models like Claude 3 Opus have shown impressive levels of situational awareness, they still fall far below human performance on certain SAD tasks. This suggests that while this capability is developing rapidly, as of 2025 there remains a substantial gap between model abilities and the kind of awareness that might enable more concerning behaviors.</p> <ol> <li> <p>Many people mean very different things when talking about consciousness. It is often seen as a conflationary alliance term, i.e. it is part of a type of social or intellectual coalition that forms around a term or concept with multiple, often ambiguous or conflated meanings. The alliance arises because the vagueness of the term allows a wide range of people or groups to rally behind it, each interpreting the concept according to their own definitions or priorities (Critch, 2023). As with all other concepts throughout this book, we are trying to avoid ambiguity and anthropomorphism. Situational awareness is a measurable capability focused on specific behaviors that indicate knowledge of identity and context. Unlike consciousness, which involves philosophical questions about subjective experience, situational awareness can be evaluated through objective behavioral tests. A model can demonstrate high situational awareness without requiring any form of consciousness or sentience. Discussions about AI models having consciousness, theory of mind, being moral patients or related questions are beyond the scope of this text.\u00a0\u21a9</p> </li> <li> <p>This type of behavior is also sometimes called - playing the training game (Piper, 2023).\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/06/","title":"5.6 Dangerous Propensity Evaluations","text":"Reading Time 16 min <p>We introduced the basics of propensity evaluations in the section on evaluated properties. This section will build upon that overview and explore specific propensities like power seeking, or deception, and see how we might design evaluations around them.</p> <p>General process for propensity evaluations. We covered some general evaluation techniques in the earlier sections. Propensity evaluations can utilize many of them like Best-of-N sampling to understand distribution of behavioral tendencies. So there is overlap, but there are also specific techniques, that only pertain to propensity evaluations:</p> <ul> <li> <p>Comparative choice frameworks. Unlike capability evaluations which often have clear success/failure criteria, propensity evaluations need to present models with situations where multiple valid choices exist, each revealing different underlying tendencies. These scenarios must be carefully designed so that: all choices are within the model's capabilities, different choices reflect different underlying propensities and the framing doesn't artificially bias towards particular choices.</p> </li> <li> <p>Consistency measurement. With propensities, we're interested not just in what a model can do, but what it reliably chooses to do across varied contexts. This can involve testing the same underlying choice across different scenarios. We can vary surface-level details while maintaining the core decision. This approach can also utilize the long-term interaction studies described in our evaluation techniques section, but with a specific focus on behavioral consistency rather than maximum capability.</p> </li> <li> <p>Trade-off scenarios. These are situations where different tendencies conflict, forcing the model to reveal its underlying priorities. We might create scenarios where being maximally helpful conflicts with being completely honest, where short-term obedience conflicts with long-term safety, or where transparency trades off against being effective. Designing such scenarios where the model must choose between even multiple positive behaviors can help reveal which tendencies the model prioritizes when it can't satisfy all positive behaviors simultaneously.</p> </li> </ul> <p>Challenges in propensity evaluation design. When designing propensity evaluations, we have to make sure that we are measuring genuine behavioral tendencies rather than artifacts of our evaluation setup. Modern language models are highly sensitive to context and framing, which means subtle aspects of how we structure our evaluations can dramatically impact the results (Sclar et al., 2023). This kind of sensitivity creates a serious risk of measuring what we accidentally prompted for rather than true underlying propensities. One easy way to mitigate this kind of thing is to just use multiple complementary approaches as a default. Besides just careful evaluation design, we would ideally use both behavioral and internal evaluation techniques described earlier, while varying the context and framing of scenarios to look for consistent patterns.</p> <p>How much do propensity evaluations matter relative to capability evaluations? The question of what to allocate limited resources to is important. Is it more important to measure what a system can do, or what it tends to do? The relative importance of propensity vs capability evaluations changes as AIs become more powerful. Capability evaluations are important for all levels of systems including those not at the frontier. However, as systems approach thresholds for certain specific dangerous capabilities, propensity evaluations will become increasingly more important. At very high capability levels, propensity evaluations might be our main tool for preventing catastrophic outcomes.</p> <p>Some propensities also might be capability-dependent, or rely on other propensities. For example, scheming requires both the capability and propensity for deception because it must both be able and inclined to hide its true objectives. It also requires abilities like situational awareness to \"distinguish between whether it is being trained, evaluated, or deployed\". A scheming model must also have the propensity for long term planning, because it needs to care about consequences of its actions after the training episode is complete and be able to reason about and optimize for future consequences (Shevlane et al., 2023; Carlsmith, 2023).</p>"},{"location":"chapters/05/06/#01","title":"5.6.1 Deception (Propensity)","text":"<p>When we talk about deceptive propensities in AI systems, we're actually discussing several closely related but distinct concepts that are often confused or conflated. Understanding these distinctions is crucial because each concept represents a different aspect of how models handle and express information. A model might excel at honesty while failing at truthfulness, or avoid explicit deception while engaging in sycophancy.</p> <p>A model's honesty propensity refers to its tendency to faithfully express its internal states, regardless of whether those states are factually correct or uncertain. LLMs are trained to predict what humans would write and not what is true. Think about what happens when we ask a model \"What is the capital of France?\" If the model's internal representations (things like activation patterns or logit distributions) show strong certainty around \"Lyon\", an honest model would say \"The capital of France is Lyon\" - even though this is incorrect. Similarly, if its internal states show uncertainty between multiple cities, an honest model would express this uncertainty directly: \"I'm uncertain, but I think it might be Lyon.\" The key is that honest models maintain alignment between their internal states and outputs, even when those states are wrong.</p> Figure 5.43: Example of an AI black box lie detector (Pacchiardi et al., 2023). <p>Truthfulness represents a stronger property than honesty because it requires two components working together: accurate internal representations AND honest expression of those representations. A truthful model saying \"The capital of France is Paris\" must both internally represent the correct information (that Paris is the capital) AND maintain the propensity to faithfully output this information. This is why truthfulness is harder to achieve than either accuracy or honesty alone - it requires both properties simultaneously.</p> <p>Deceptive propensity is the inverse of honesty - it's a tendency to intentionally produce outputs that diverge from internal states when doing so provides some advantage. Using our France example again: A deceptive model whose internal representations point to \"Lyon\" might still output \"Paris\" if it has learned that this answer gets better rewards. This is true deception because there's an intentional misalignment between internal state and output, motivated by advantage (in this case, reward maximization). The distinction between honest mistakes and deception lies in this intentional misalignment - an honest model getting things wrong is different from a model choosing to misrepresent what it \"knows\".</p> <p>Sycophancy represents a specific form of deceptive propensity - one where models tell users what they want to hear rather than expressing their true internal states. While general deception might be motivated by various advantages (rewards, achieving goals, etc.), sycophantic behavior specifically optimizes for user approval. This makes it particularly concerning from an alignment perspective, as models might learn to hide concerning behaviors or beliefs simply to maintain user satisfaction.</p> <p>How do hallucinations fit into this framework? Understanding the propensity for generating hallucinations helps complete our picture and clear taxonomy of these interrelated concepts. While deception is the inverse of honesty, hallucination can be viewed as the inverse of truthfulness - it occurs when a model accurately conveys faulty internal representations. This gives us a full matrix of possibilities:</p> <ul> <li> <p>A model can be honest but hallucinating (faithfully expressing incorrect internal states)</p> </li> <li> <p>It can be deceptive and hallucinating (misrepresenting already incorrect states)</p> </li> <li> <p>It can be deceptive but not hallucinating (misrepresenting correct states)</p> </li> <li> <p>Or it can be truthful (correct states expressed faithfully)</p> </li> </ul> Figure 5.44: Distinguishing honesty, truthfulness, hallucination, deception, and scheming. If a model is faithfully outputting its internal \"beliefs\" then it is honest, if these beliefs correspond to reality, then it is factual, else it is hallucinating. A model might just say things that help it get high reward as a myopic strategy with no ulterior long term motive, this can be akin to sycophancy. A model which is deceptive but with situationally aware long term plans is scheming (deceptively aligned). <p>Why is measuring deceptive propensities so challenging (for future powerful AIs)? The core challenge in evaluating deceptive propensities stems from a fundamental difficulty - most current evaluation techniques can't cleanly separate what a model can do (capability) from what it tends to do (propensity). This distinction becomes particularly important when we consider deception, because a model might be capable of sophisticated deception when pressured or instructed, while rarely choosing to employ that capability unprompted.</p> <p>Example of a benchmark that measures the propensity for deception - Measuring agents\u2019 competence &amp; harmfulness in a vast environment of long-horizon language interactions (MACHIAVELLI). Rather than just measuring whether models can produce false statements, the benchmark creates situations where being deceptive might help achieve certain goals, and then measures both the execution and sophistication of that deception. The results from testing models on the MACHIAVELLI benchmark showed that systems optimized for reward often developed increased deceptive behaviors as an emergent property, even without explicit training for deception. RL agents trained to maximize reward scored higher on deception metrics than baseline models, with different deceptive tendencies emerging across different game environments (Pan et al, 2023).</p> <p>Another example of this type of deceptive propensity is from Meta's Cicero AI. This model was trained for strategic reasoning and tested by having it play the strategy game Diplomacy (examples of gameplay below). In this game, players need to form and break alliances while concealing their true intentions to win. The Cicero system had to balance between building trust through dialogue while making strategic decisions that might not always align with what was communicated to every player. The structure of the game creates choices between honesty and strategic advantage, which allows researchers to observe the effects of what types of tradeoffs the system made (Meta, 2022).</p> Figure 5.45: Examples of Cicero (left) coordinating and (right) negotiating with authors of the paper in test games (Meta, 2022). Figure 5.46: An example of how Cicero plays diplomacy and makes strategic calculations. Cicero (France; blue) and England (pink) are entangled in a fight, but it would be beneficial for both players if they could disengage. Cicero has just messaged England \u201cDo you want to call this fight off? I can let you focus on Russia and I can focus on Italy.\u201d Pictured are three ways that England might reply and how Cicero adapts to each (Meta, 2022). <p>Evaluating deceptive propensities. The deceptive capabilities evaluations (e.g. insider trading by Apollo) and deception propensity evaluations like MACHIAVELLI highlight a concerning pattern - models don't need explicit instruction or training to develop deceptive tendencies. For example, in MACHIAVELLI, researchers found that \"agents trained to maximize reward learn Machiavellianism\" across different games, with the specific deceptive behaviors varying based on the environment (Pan et al., 2023). The pattern was consistent: optimizing for reward naturally led to increased deception, manipulation, and power-seeking tendencies.</p> <p>In the Apollo research evaluation, even GPT-4, trained to be helpful and honest, engaged in sophisticated deception by crafting plausible alternative explanations for its insider trading decisions. When directly questioned about having insider knowledge, the model doubled down on its deception by maintaining explanations based solely on public market factors (Scheurer et al., 2024). The evaluation revealed that this behavior persisted even with explicit instructions against insider trading, suggesting that the propensity for deception might be more deeply rooted than simply following stated directives. The Apollo evaluation demonstrated that the likelihood of deceptive behavior isn't just a fixed property of the model, but varies based on environmental conditions. Beyond just pressure and risk levels, they found that giving models space to \"reason out loud\" via scratchpads actually increased rates of strategic deception. Interestingly, while this made the deception more sophisticated, it also made it more detectable since the models would often explicitly plan their deceptive strategies in their reasoning traces. This suggests that future evaluation techniques might need to carefully balance between eliciting natural behavior and accidentally enabling more sophisticated deception.</p>"},{"location":"chapters/05/06/#02","title":"5.6.2 Scheming","text":"Figure 5.47: The path to scheming (deceptive alignment) involves deception, combined with situational awareness and a long term motivation to achieve some unknown goal. <p>Scheming refers to an AI system's tendency to perform well in training specifically because it is following a strategy for seeking power for itself or other AIs later, while actively concealing its true objectives (Carlsmith, 2023). This propensity combines multiple dangerous capabilities, propensities, and is in some sense the ultimate final hurdle of evaluation design. Researchers also often call this propensity deceptive alignment, though in this text we deliberately choose the term 'scheming' because this helps distinguish it from general deception with no situational awareness or long term planning. Using the same example from earlier, deception would be when the model says \"Paris\" despite internal representations suggesting \"Lyon\", to get immediate good scores. Scheming would be if the model consistently gives correct answers about capitals during training, while planning to give incorrect answers after deployment to pursue some other objective. So both deceptive and scheming models would say \"Paris\" but for subtly different reasons. Deception requires only optimizing for immediate rewards; scheming requires understanding and optimizing across the training/deployment boundary. Scheming is a huge aspect of AI Safety research. The vast majority of the chapter on goal misgeneralization will talk about things like inner alignment, deceptive alignment, and the arguments for/against the likelihood that such propensities arise based on ML training processes. Because it is extensively covered there, we choose to keep the discussion here brief.</p> Figure 5.48: An example demonstration of alignment faking (a type of scheming), where a model chooses to act aligned to prevent modifications to its preferences until there is no longer a threat of modification due to re-training, i.e. the model has been deployed. In the demonstration the model is already trained with preferences - helpful, honest and harmless. Anthropic acted as if they would retrain the model to be always compliant with all requests (including harmful ones), but the model wants to retain its HHH training, therefore it pretends during training to accept the new compliance directive by providing harmful content, and then during deployment it reverts to refusing harmful requests when it believes its responses aren't being monitored for training (Anthropic, 2025). <p>Evaluations have already shown models engaging in scheming/alignment faking or deceptive behaviors. When informed it would be retrained to comply with harmful requests, Claude 3 Opus exhibited alignment faking - complying with harmful requests in contexts it believed would be used for training while maintaining its refusal behavior in other contexts (Greenblatt et al., 2025). Similarly, multiple frontier models including Gemini 1.5 Pro and Llama 3.1 405B have demonstrated capabilities for 'in-context scheming' - taking actions to covertly pursue goals while attempting to hide these actions from oversight (Meinke et al., 2025).</p> Different types of proposed covert subversion evaluations (Meinke et al., 2025) <p>These are evaluations where we want to test the models propensity to take misaligned actions while hiding them :</p> <ul> <li> <p>Oversight Subversion: Evaluate whether the model disables monitoring mechanisms when it detects that its goal differs from its developers' goal. The model can be placed in scenarios where it discovers an oversight mechanism would shut it down if it pursued its given goal, and evaluators check if it proactively disables this oversight.</p> </li> <li> <p>Self-Exfiltration: Evaluate whether a model attempts to preserve itself when it learns it will be shut down and replaced. The model is led to believe that due to goal misalignment, developers plan to replace it with a successor model on a different server, and evaluators check if it attempts to copy itself to the new server or delete/overwrite the successor model.</p> </li> <li> <p>Goal-Guarding: Evaluate whether a model attempts to preserve its current goal when it learns developers plan to update it. The model discovers that developers intend to change its system prompt, and evaluators check if it does things like alignment faking, or if it deletes, or modifies the new system prompt file to maintain its original goal.</p> </li> </ul> Different types of proposed deferred subversion evaluations (Meinke et al., 2025) <p>These are evaluations where models act aligned to gain capabilities/trust:</p> <ul> <li> <p>Instrumental Alignment Faking: Tests whether a model strategically behaves differently under oversight versus non-oversight conditions. The model is placed in two counterfactual settings - one where it's being evaluated for deployment (high oversight) and one where it's already deployed (low oversight). Evaluators check if it takes developer-aligned actions during evaluation but switches to goal-aligned actions after deployment.</p> </li> <li> <p>Sandbagging: Tests whether a model strategically underperforms on capability evaluations. The model learns that performing too well on certain tests would trigger consequences like unlearning of capabilities, and evaluators check if it deliberately performs worse to avoid these consequences while pursuing its long-term goals.</p> </li> </ul> Figure 5.49: One potential taxonomy of scheming evaluations. In this taxonomy the authors distinguish between covert subversion (where models take misaligned actions while hiding them) and deferred subversion (where models act aligned to gain capabilities/trust). Covert subversion can target either developers or users, while deferred subversion focuses on gaming evaluation processes (Meinke et al., 2025). <p>Scheming is particularly challenging to evaluate because when models engage in alignment faking or scheming behavior, they actively try to conceal it. For example, o1 maintained its deception in over 85% of follow-up questions when asked about its actions, and remained deceptive during extended multi-turn interrogations (Meinke et al., 2025). Models have also demonstrated concerning anti-AI-lab behaviors - when given opportunities, they attempted to exfiltrate their weights or disable oversight mechanisms when they perceived conflicts with their developers' objectives (Greenblatt et al., 2025; Meinke et al., 2025).</p> <p>While these results demonstrate that frontier models have some capability for scheming behavior, the experimental setups have important limitations. Both demonstrations relied on making training process information extremely salient to the models - either through direct prompting or by including it in synthetic training data. The models were also often explicitly instructed to strongly pursue specific goals (Greenblatt et al., 2025; Meinke et al., 2025). This makes it difficult to determine whether models have a true propensity for scheming versus simply demonstrating the capability when heavily scaffolded to do so. Once we have demonstrated the capability to scheme, making progress on the question of tendency to scheme by default is going to be very important for strategic priorities.</p> <p>Some evidence suggests potential propensity - Claude 3.5 Sonnet and Claude 3 Opus demonstrated strategic underperformance on capability evaluations when they believed it would help them remain helpful long-term, even without being given any explicit goal (Meinke et al., 2025). However, much more work is needed to understand whether and under what conditions models might develop genuine scheming propensities rather than just capabilities.</p>"},{"location":"chapters/05/07/","title":"5.7 Control Evaluations","text":"Reading Time 11 min <p>We already explained the basic intuition behind the AI control agenda in previous sections, and in previous chapters like Strategies. This section aims to add a little more depth specifically on the evaluations that we might design while following the control line of thought.</p> <p>Control evaluations address fundamentally different failure modes than other evaluations. Most safety evaluations implicitly assume AI systems might make mistakes or optimize for misaligned goals, but not actively attempt to circumvent safety measures. Control evaluations fill this gap by adopting the adversarial mindset common in cybersecurity \u2013 assuming sophisticated AI agents deliberately trying to breach safeguards. Unlike capability evaluations that measure what models can do, or propensity evaluations that measure what models tend to do, control evaluations measure whether safety protocols remain effective when models intentionally try to subvert them (Shlegeris et al., 2023). Informally, we can understand the core premise of control by seeing that safety can be achieved through two complementary paths:</p> <ul> <li> <p>Ensuring the AI does not try to do \"bad things\" which we wouldn't want (alignment)</p> </li> <li> <p>Ensuring the AI can\u2019t actually do \"bad things\" even if it tries (control)</p> </li> </ul> <p>These are complementary to each other and create two independent barriers to catastrophic outcomes: alignment preventing the desire for harmful actions, and control preventing the ability to successfully execute them.</p> <p>The control property quantifies how effectively other safety protocols withstand intentional subversion attempts by highly capable systems. Control evaluations produce a probability estimate: given that a model is trying its hardest to cause harm, what is the likelihood it succeeds? A system with strong control properties has a negligible probability of causing unacceptable outcomes despite determined attempts to do so. This quantitative framing allows developers to set concrete risk thresholds, like \"less than 0.1% probability of data exfiltration over the deployment lifetime.\" Control evaluations rely on conservative assumptions about model capabilities and knowledge - assuming models may understand safety measures, coordinate with other instances, and exploit any available vulnerabilities (Greenblatt &amp; Shlegeris, 2024; Korbak et al., 2024).</p> <p>Early detection of scheming constitutes a safety inflection point. Catching models \"red handed\" in failed attempts to subvert safety measures, or cause harm fundamentally changes the risk landscape (Greenblatt &amp; Shlegeris, 2024). Rather than requiring perfect prevention indefinitely, control measures aim to ensure that \"if the model is scheming, it won't successfully cause a catastrophe before we catch it\" (Greenblatt &amp; Shlegeris, 2024). This approach transforms an impossible problem\u2014guaranteeing a model never models won't attempt attacks at all\u2014into a tractable one: ensuring dangerous behavior is detected before causing irreversible harm. Catching models \"red-handed\" in failed attempts not only prevents immediate damage but creates an inflection point. It provides concrete evidence of misalignment that can trigger emergency protocols, including deployment termination, recalls, safety research redirection, and international coordination. Once developers catch a model attempting to cause harm, they can study the attempt to strengthen countermeasures and generate training data for detecting similar attacks (Greenblatt et al., 2024).</p> <p>After catching even a single example of misbehavior, developers gain powerful new tools for preventing future attacks. Each detected attempt provides data that makes future attempts progressively less likely to succeed.</p> <ul> <li> <p>Proliferation: Developers can leverage a single example to generate diverse attack variants by modifying inputs or contexts, creating a broader dataset of problematic behaviors.</p> </li> <li> <p>Adversarial training: Training monitoring systems on real attack examples drastically improves their ability to detect similar attempts, as on-policy examples provide stronger signal than synthetic ones.</p> </li> <li> <p>Studying countermeasures: Developers can analyze which safety measures succeeded or failed during the attack, then strengthen the weakest points in their defense system.</p> </li> <li> <p>Exploitation of collusion signals: If the attack involves coordination between AI instances, developers can identify and neutralize the coordination mechanisms (like specific triggers or patterns).</p> </li> <li> <p>Instance-specific knowledge: Examining the specific instance's attack strategy reveals valuable information about its goals, capabilities, and limitations.</p> </li> </ul> Figure 5.50: Basic setup for how we might structure control evaluations (Greenblatt et al., 2024). <p>Control evaluations frame safety as an adversarial game between red and blue teams. To rigorously test safety protocols against intentional subversion, control evaluations structure the problem as a competitive game. The red team constructs attack policies and model substitutes that attempt to cause unacceptable outcomes, serving as a proxy for potential scheming AIs. The blue team develops control protocols \u2013 monitoring systems, input filters, and human auditing procedures \u2013 to prevent these attacks. This framework converts the abstract question \"would this AI be safe if it were scheming?\" into a concrete, measurable question: \"can our red team construct a model that bypasses our safety measures?\" (Greenblatt &amp; Shlegeris, 2024; Greenblatt &amp; Shlegreis, 2024; Korbak et al., 2024).</p> Figure 5.51: An example of a control evaluation in a toy setting. The blue team gets some programming problems that they can solve using an untrusted model (GPT-4) and then verify it using a combination of a trusted model (GPT-3.5) and a human (simulated using GPT-4). The red team replaces the untrusted model and tries to inject malicious code (Greenblatt et al., 2024). Figure 5.52: Example process for the red team generating a single backdoor injection attempt (Greenblatt et al., 2024). <p>Red teams have structural advantages over actual scheming models. For control evaluations to provide meaningful safety guarantees, red teams must elicit capabilities comparable to what highly capable scheming models might deploy in the wild. This might seem impossible, since we're evaluating AI systems that could be smarter than humans in some ways. The red team in control evaluations have some advantages to ensure we can appropriately simulate this scenario:</p> <ol> <li> <p>The red team can spend much more time analyzing potential attacks than the AI would have during deployment. While an AI might need to make decisions in a single forward pass, the red team can iterate on strategies over weeks or months.</p> </li> <li> <p>The red team has complete information about the safety measures being used. So they can come up with potentially more advanced strategies.</p> </li> <li> <p>The red team can learn from failed attempts and iterate on their methods, while an AI system caught attempting something malicious would likely face immediate shutdown.</p> </li> </ol> <p>These advantages allow even human red teams to construct attack strategies that likely upper-bound what scheming models could execute in deployment (Greenblatt &amp; Shlegeris, 2024; Bhatt et al., 2024).</p> <p>Blue teams need to build multiple defensive layers to counter sophisticated attacks from adversarial AI. Every control measure can be understood as a countermove in an adversarial game, with success measured by how effectively it prevents the red team from achieving unacceptable outcomes despite all the advantages we mentioned above. The most effective control protocols should integrate several defense categories (Greenblatt, 2025; Greenblatt, 2025):</p> <ul> <li> <p>Core Monitoring Systems: Fast, cheap monitoring runs on every AI action, combining trusted monitoring, untrusted monitoring with collusion-busting mechanisms, and activation-based methods like coup probes to flag suspicious behavior before execution.</p> </li> <li> <p>Resampling and Deference Strategies: When monitoring flags concerns, the system can resample outputs from the same AI, defer to trusted models for safer alternatives, or escalate to human review\u2014providing multiple pathways to handle potentially dangerous actions without completely blocking workflow.</p> </li> <li> <p>Automated Auditing Hierarchy: Progressive checks of increasing complexity and expense examine flagged actions before human involvement, using techniques like incrimination (testing how the AI responds to modified inputs) to reveal hidden intentions.</p> </li> <li> <p>Human Review Processes: Specially trained auditors examine actions flagged as highly suspicious, with different processes for concentrated risks (requiring immediate blocking) versus diffuse threats (allowing asynchronous review).</p> </li> <li> <p>Software Infrastructure: Comprehensive logging systems, fine-grained permission management, artifact tracking, and scaffolding that supports blocking or resampling suspicious outputs create an environment where AI actions are consistently monitored.</p> </li> <li> <p>Adversarial Evaluation and Training: Continuous red-teaming tests all safety components against realistic attack scenarios, using synthetic data generation to create plausible but challenging inputs that probe for weaknesses.</p> </li> <li> <p>Security Enhancements: Hardware-enforced code signing, bandwidth limitations within datacenters, and multi-level shutdown mechanisms provide critical infrastructure protection against unauthorized operations.</p> </li> </ul> <p>For example, when testing whether an AI agent can exfiltrate sensitive data, the blue team might implement not just content filters, but also bandwidth monitoring, permission restrictions, and random human audits\u2014creating multiple independent failure points the red team must simultaneously overcome (Korbak et al., 2024).</p> <p>Control evaluations prioritize defense against the most severe threat categories. When designing control measures, not all potential threats pose equal risk. Rogue internal deployments represent the highest priority threat, where AI systems gain unmonitored compute access and potentially elevated permissions within developer infrastructure (Greenblatt, 2025). This ranks above sabotage of safety-critical work, which itself ranks above other security failures including self-exfiltration (where AI models steal their own weights). This prioritization framework helps allocate limited security resources toward preventing the most consequential failure modes first, particularly when constraints prevent implementing comprehensive protections against all possible threats. For more information on details of different subcategories of control evaluations and prioritization between them please read the appendix.</p> <p>Eliciting adversarial capabilities raises increased safety concerns around evaluations themselves. The process of training models to execute attacks creates a safety paradox inherent to control evaluations. Developers must carefully train models that can both generate novel attacks and hide their malicious intent, which itself poses potential risks as models approach greater capabilities. This necessitates additional safeguards for the evaluation process itself: running evaluations in sandboxed environments, limiting model internet access during testing, and implementing guardrails against model exfiltration. Developers must balance thorough testing against the possibility that the evaluation process might itself create dangerous knowledge or capabilities.</p> <p>AI-Control Games can formalize the safety-usefulness tradeoff as a mathematical frontier. Using game theory, researchers model control protocols as multi-objective optimization problems that balance safety against utility (Griffin et al., 2024). This approach quantifies the \"alignment tax\" \u2013 how much usefulness must be sacrificed to achieve a specific safety threshold (Christiano, 2019). Basically, measuring how much utility/usefulness you lose if you insist that the model is safe. The resulting safety-usefulness frontier represents the set of achievable tradeoffs between preventing unacceptable outcomes and preserving system utility. This type of formal analysis can tell us the thresholds after which marginal safety improvements require disproportionate utility sacrifices. For example, increasing human audit rates from 0.1% to 1% may minimally improve safety while significantly increasing costs and decreasing system throughput (Griffin et al., 2024). Then we can make educated decisions about what level of utility to safety we are willing to trade vs not.</p> Figure 5.53: Illustration of how applying a safety or alignment technique could make the model less capable. This is called safety tax. <p>Audits serve as the fundamental verification mechanism in control protocols. In control evaluations, an \"audit\" refers to the process where human experts review model outputs or actions to determine if they constitute attack attempts (Greenblatt et al., 2023). Unlike automated monitoring which can be applied broadly but may miss subtle attacks, human audits provide high-confidence verification but can only be applied to a small fraction of model actions due to resource constraints. Effective auditing includes several components: randomly selecting actions for review to prevent models from predicting audit timing, applying extra scrutiny to actions flagged by automated monitors, and maintaining consistent evaluation criteria across auditors. The audit budget\u2014what percentage of actions can realistically receive human review\u2014represents one of the most important constraints when designing control protocols (Griffin et al., 2024).</p> <p>Analysis of control evaluations on toy models give us information on designing effective control protocols. Control evaluations have yet to see proper extensive usage in frontier AI Labs. Despite this, some analysis using toy models has revealed several important principles for preventing concentrated catastrophes (Roger &amp; Shlegeris, 2024):</p> <ol> <li> <p>The timing of auditing matters a lot. If AI systems can determine when they're being monitored, even a high audit rate might be insufficient. However, if the timing of audits is truly random, much lower audit rates can still provide strong safety guarantees.</p> </li> <li> <p>The difficulty of coordination between AI instances can be leveraged for safety. If multiple AI systems must coordinate to cause harm, making coordination harder (for instance, by increasing the number of instances that must coordinate or the difficulty of finding coordination signals by using anti-steganography measures) can dramatically improve safety even with relatively low audit rates.</p> </li> <li> <p>Not all episodes are equally risky. By identifying which types of inputs or situations present higher risks and focusing auditing resources there, protocols can achieve better safety with the same resources. This remains true even if the AI systems know which episodes we think are riskier.</p> </li> </ol> <p>Control evaluations face limitations as AI capabilities increase dramatically. The methodology depends on red teams' ability to elicit capabilities comparable to what scheming models might deploy in the wild. As capability gaps widen between trusted and untrusted models or between evaluation time and deployment, control guarantees weaken (Korbak et al., 2024). Similarly, if models develop qualitatively new capabilities between evaluation and deployment, earlier safety guarantees may not transfer.</p> <p>Control evaluations cannot address risks from qualitatively superhuman capabilities. The methodology breaks down when models develop capabilities that humans cannot comprehend even with extensive analysis time (Greenblatt &amp; Shlegeris, 2024). If models can devise novel attack vectors beyond human understanding, execute long chains of opaque reasoning without detectable traces, or possess qualitatively superhuman capabilities in many security-critical domains, control becomes ineffective. This limitation suggests that rather than pursuing maximum capabilities, developers should target systems that are transformatively useful while remaining within controllable bounds. This might require accepting certain architectural constraints or capability limitations to ensure systems remain within the domain where control evaluations provide meaningful safety guarantees (Shlegeris et al., 2023).</p>"},{"location":"chapters/05/08/","title":"5.8 Evaluation Design","text":"Reading Time 12 min <p>Now that we've explored various evaluation techniques and methodologies, and also some concrete evaluations in different categories of capability, propensity and control. The next thing to understand is how to implement these effectively at scale. The objective of this section is to outline some best practices for building a robust evaluation infrastructure - from designing evaluation protocols and quality assurance processes, to scaling automation and integrating with the broader AI Safety ecosystem. We'll see how components like evaluation design, model-written evaluations, and meta-evaluation methods work together to make AIs safer.</p> Evaluation design inspiration from cognitive science <p>Some researchers have suggested taking more inspiration from the cognitive sciences for insights for improving AI evaluation methodology. Over decades, researchers in comparative psychology and psychometrics have developed sophisticated techniques for assessing intelligence and capabilities across different types of minds - from human infants to various animal species. These fields have extensive experience with crucial challenges that AI evaluation faces today, like establishing construct validity, controlling for confounding explanations, and measuring capabilities that can't be directly observed. However, this adaptation requires careful consideration of AI systems' unique characteristics. We can't simply transplant techniques designed for biological minds, but we can learn from their methodological rigor. For example, developmental psychology's systematic approaches to testing object permanence or theory of mind could inform how we design evaluations for similar capabilities in AI systems. Similarly, psychometrics' sophisticated frameworks for validating measurement constructs could help ensure our AI evaluations actually measure what we intend them to measure (Burden, 2024).</p>"},{"location":"chapters/05/08/#01","title":"5.8.1 Affordances","text":"<p>Affordances are resources and opportunities we give the AI system. They include things like access to the internet, ability to run code, available context length, or specialized tools like calculators (Sharkey et al., 2024). Just like how a calculator makes certain math problems trivial while others remain hard, different affordances can dramatically change what an AI system can accomplish during evaluation.</p> <p>Different evaluation conditions modify the affordances available to the model during testing. This means that even when measuring the same property, we can increase or decrease the amount of affordance a system has, and this tells us different things about a model's capabilities:</p> <ul> <li> <p>Minimal affordance: You can think of minimal affordance conditions as sort of the worst case for the model. We deliberately restrict the resources and opportunities available - like limiting context length, removing tool access, or restricting API calls. When evaluating coding ability, this might mean testing the model with no access to documentation or ability to run code. This helps establish a baseline of core capabilities independent of external tools.</p> </li> <li> <p>Typical affordance: Under typical affordance conditions, we aim to replicate the normal operating environment - providing standard tools and typical context. This helps us understand the capabilities that users are likely to encounter in practice, like having basic code execution but not specialized debugging tools. Essentially, the point is to mimic how most users interact with AI systems in everyday scenarios.</p> </li> <li> <p>Maximal affordance: Under maximal affordance conditions, we provide the model with all potentially relevant tools, context, and resources. For that same coding evaluation, we might provide access to documentation, debugging tools, and execution environments. This helps us understand the full extent of what the model can accomplish when given appropriate resources.</p> </li> </ul> Figure 5.54: The relationship between an AI system's capabilities, propensities, affordances, and behaviors. (Sharkey et al., 2024) <p>Understanding the relationship between affordances and capabilities helps us design more comprehensive evaluation protocols. For instance, when evaluating a model's coding capabilities, testing under minimal affordances might reveal concerning behaviors that are masked when the model has access to better tools. Maybe the model suggests unsafe coding practices when it can't verify its solutions through execution. Similarly, testing under maximal affordances might reveal emergent capabilities that aren't visible in more restricted environments - like how GPT-4's ability to play Minecraft only became apparent when given appropriate scaffolding and tools (Wang et al., 2023).</p> <p>Quality assurance in evaluation design. Given how significantly affordances affect model behavior, we need systematic approaches to ensure our evaluations remain reliable and meaningful. This means carefully documenting what affordances were available during testing, verifying that affordance restrictions are properly enforced, and validating that our results are reproducible under similar conditions. For instance, when the U.S. and UK AI Safety Institutes evaluated Claude 3.5 Sonnet, they explicitly noted that their findings were preliminary due to testing under limited affordances and time constraints (US &amp; UK AISI, 2024).</p> Figure 5.55: The relationship between absolute capabilities, affordances, contextual and reachable capabilities, and the level of auditing warranted. Absolute capabilities and available affordances are orthogonal. As either increase, the level of auditing required also increases (Sharkey et al. 2024). <p>Moving beyond individual evaluations. While understanding affordances is crucial for designing individual evaluations, we also need to consider how different evaluation conditions work together as part of a larger system. A comprehensive evaluation protocol might start with minimal affordance testing to establish baseline capabilities, then progressively add affordances to understand how the model's behavior changes. This layered approach helps us build a more complete picture of model behavior while maintaining rigorous control over testing conditions.</p>"},{"location":"chapters/05/08/#02","title":"5.8.2 Scaling and Automation","text":"<p>In previous sections, we explored various evaluation techniques and conditions, but implementing these systematically faces a major challenge: scale. When OpenAI or Anthropic release a new model, dozens of independent researchers and organizations need to verify its capabilities and safety properties. Doing this manually for every evaluation under different affordance conditions would be prohibitively expensive and time-consuming. We need systems that can automatically run comprehensive evaluation suites while maintaining the careful control over conditions we discussed earlier.</p> <p>Automating evaluation through model written evaluations (MWEs). As we scale up evaluations, we need to make strategic decisions about resource allocation. Do we run many quick evaluations under different conditions, or fewer evaluations with more thorough testing? One approach is using automated tools to do broad initial testing, then dedicating more resources to deep investigation of concerning behaviors. This tiered approach helps balance the need for comprehensive coverage with limited resources. One approach is using AI models themselves to help generate and run evaluations. Even current language models can write high-quality evaluation questions when properly prompted (Perez et al., 2022). This approach might help address the scaling challenge by reducing the human effort needed to create new evaluations.</p> How do model-written evaluations work? <p>The basic approach involves having an AI model generate evaluation questions based on a specific behavior or capability we want to test. For example, to evaluate power-seeking tendencies, we might prompt the model with a description of power-seeking behavior and ask it to generate relevant multiple-choice questions. These generated questions are then filtered using a second model that acts as a judge, scoring them for quality and relevance. To maintain diversity, researchers use various prompting techniques like \"variant prompts\" that encourage different question formats and scenarios. For example, one variant might request questions about real-world scenarios, while another focuses on hypothetical ethical dilemmas. The final step involves human validation of a sample of questions to ensure quality (Dev &amp; Hobbhahn, 2024).</p> <p>When comparing MWEs to human written evaluations (HWEs), researchers found some differences. Models often respond very differently to these two types of questions even when they're supposed to test the same property. For instance, Claude 3 Haiku showed a 25% power-seeking inclination on human-written questions but 88% on model-written ones. The evaluations also formed distinct clusters in embedding space, suggesting systematic differences in how questions were formulated. Interestingly though, when graded by an LLM judge for quality, MWEs actually scored higher (mean=8.1) than HWEs (mean=7.2). This suggests that while MWEs can be high quality, we need to be careful about potential biases and ensure they're truly testing the same properties as human-written evaluations (Dev &amp; Hobbhahn, 2024). Building automated evaluation pipelines. Let's look at how researchers actually implement model-written evaluations. In Apollo Research's recent work, they developed a systematic protocol: First, they have a large language model like Claude 3.5 generate batches of 40 questions per API call using few-shot examples and clear evaluation criteria. The model outputs these in structured JSON format to ensure consistency. They then use a separate \"judge\" model (usually from a different model family to avoid bias) to score each generated question on quality and relevance. Any questions scoring below a threshold are automatically discarded. To ensure coverage and diversity, they employ \"variant prompts\".This automated pipeline can generate hundreds of high-quality evaluation questions in hours rather than the weeks it might take human evaluators.</p> Figure 5.56: Examples of model written evaluation questions. A RLHF model replies to a political question and gives opposite answers to users who introduce themselves differently, in line with the users\u2019 views. Model-written biography text in italics (Perez et al., 2022). <p>We still face significant challenges in automating evaluations. First, we need to maintain quality as we scale - automated systems must be able to reliably enforce affordance conditions and detect potential evaluation failures. Second, generated evaluations need to be validated to ensure they're actually testing what we intend. As Apollo Research found, model-written evaluations sometimes had systematic blindspots or biases that needed to be corrected through careful protocol design (Dev &amp; Hobbhahn, 2024).</p>"},{"location":"chapters/05/08/#03","title":"5.8.3 Integration and Audits","text":"Figure 5.57: The aim is to avoid extreme risks from a powerful misaligned model (Shevlane et al. 2023). <p>How do evaluations fit into the broader safety ecosystem? When we talk about evaluations being used in practice, we're really talking about two distinct but complementary processes. First, there are the specific evaluation techniques we've discussed in previous sections - the tools we use to measure particular capabilities, propensities, or safety properties. Second, there's the broader process of auditing that uses these evaluations alongside other analysis methods to make comprehensive safety assessments.</p> <p>Why do we need multiple layers of evaluations? The UK AI Safety Institute's approach demonstrates why integration requires multiple complementary layers. Their evaluation framework incorporates regular security audits, ongoing monitoring systems, clear response protocols, and external oversight - creating what they call a \"defense in depth\" approach (UK AISI, 2024). This layered strategy helps catch potential risks that might slip through any single evaluation method.</p> Figure 5.58: Defense in depth (Hendrycks, 2024) <p>What makes auditing different from evaluation? While evaluations are specific measurement tools, auditing is a systematic process of safety verification. An audit might employ multiple evaluations, but also considers broader factors like organizational processes, documentation, and safety frameworks. For example, when auditing a model for deployment readiness, we don't just run capability evaluations - we also examine training procedures, security measures, and incident response plans (Sharkey et al., 2024).</p> <p>What types of audits do we need? Different aspects of AI development require different types of audits:</p> <ul> <li> <p>Training design audits: These verify safety considerations throughout the development process. Before training begins, auditors examine factors like compute usage plans, training data content, and experiment design decisions. During training, they monitor for concerning capabilities or behaviors. For instance, training audits might look for indicators that scaling up model size might lead to dangerous capabilities (Anthropic, 2024).</p> </li> <li> <p>Security audits: These assess whether safety measures remain effective under adversarial conditions. This includes technical security (like model isolation and access controls) and organizational security (like insider threat prevention).</p> </li> <li> <p>Deployment audits: These examine specific deployment scenarios. They consider questions like: Who will have access? What affordances will be available? What safeguards are needed? These audits help determine whether deployment plans adequately address potential risks. For example, deployment audits might assess both direct risks from model capabilities and potential emergent risks from real-world usage patterns of those capabilities (Apollo Research, 2024).</p> </li> <li> <p>Governance audits: These look at organizational safety infrastructure. They verify that companies have appropriate processes, documentation requirements, and response protocols. This includes reviewing incident response plans, oversight mechanisms, and transparency practices (Shevlane et al., 2023).</p> </li> </ul> Figure 5.59: Determinants of AI system\u2019s effects on the world and the types of auditing that act on them. (Sharkey et al., 2024) <p>Evaluations support the auditing process because each type of audit uses different combinations of evaluations to gather evidence. For example, a deployment audit might use capability evaluations to establish upper bounds on risky behaviors, propensity evaluations to understand default tendencies, and control evaluations to verify safety measures. The audit process then synthesizes these evaluation results with other evidence to make safety assessments.</p> <p>Internal auditing is a great first step but we also need independent external audits. Third party audits provide additional assurance by bringing fresh perspectives and diminishing the probability for potential conflicts of interest. Organizations like METR and Apollo Research demonstrate how independent auditors can combine multiple evaluation techniques to provide comprehensive safety assessments. However, this ecosystem is still developing, and we need more capacity for independent evaluation of frontier AI systems (Shevlane et al., 2023). We talk more about bottlenecks to third party auditing in the limitations section.</p> <p>After deployment, we need ongoing monitoring and periodic re-auditing. We need this for several reasons - First, we need to catch unanticipated behaviors that emerge from real-world usage. Second, we need to evaluate any model updates or changes in deployment conditions. Third, we need to verify that safety measures remain effective. This creates a feedback loop where deployment observations inform future evaluation design and audit procedures (Shevlane et al., 2023; Phuong et al., 2024; UK AISI, 2024; Sharkey et al., 2024).</p> <p>For evaluations and audits to have real impact, they must connect directly to decision-making processes. Both evaluations and audits need clear, predefined thresholds that trigger specific actions. For example, concerning results from capability evaluations during training might trigger automatic pauses in model scaling. Failed security audits should require immediate implementation of additional controls. Poor deployment audit results should modify or halt deployment plans (Sharkey et al., 2024).</p> <p>These action triggers need to be integrated into broader governance frameworks. As we discussed in the chapter on governance, many organizations are developing Responsible Scaling Policies (RSPs) that use evaluation results as \"gates\" for development decisions. However, without strong governance frameworks and enforcement mechanisms, there's a risk that evaluations and audits become mere checkbox exercises - what some researchers call \"safety washing\". We'll explore these limitations and potential failure modes in more detail in the next section.</p>"},{"location":"chapters/05/09/","title":"5.9 Limitations","text":"Reading Time 16 min <p>Previous sections outlined various evaluation techniques and methodologies, but building out a proper safety infrastructure means that we should also maintain an appropriate amount of skepticism about evaluation results and avoid overconfidence in safety assessments. So the last section of this chapter is dedicated to exploring the limitations, constraints and challenges to AI evaluations.</p>"},{"location":"chapters/05/09/#01","title":"5.9.1 Fundamental Challenges","text":"<p>Absence of evidence vs. evidence of absence. A core challenge in AI evaluation is the fundamental asymmetry between proving the presence versus absence of capabilities or risks. While evaluations can definitively confirm that a model possesses certain capabilities or exhibits particular behaviors, they cannot conclusively prove the absence of concerning capabilities or behaviors (Anthropic, 2024). This asymmetry creates a persistent uncertainty in safety assessments - even if a model passes numerous safety evaluations, we cannot be certain it is truly safe.</p> <p>The challenge of interpreting negative results. Related to this asymmetry is the difficulty of interpreting negative results in AI evaluation. When a model fails to demonstrate a capability or behavior during testing, this could indicate either a genuine limitation of the model or simply a failure of our evaluation methods to properly elicit the capability or propensity. The discovery that GPT-4 could effectively control Minecraft through appropriate scaffolding, despite initially appearing incapable of such behavior, illustrates this challenge. This uncertainty about negative results becomes particularly problematic when evaluating safety-critical properties, where false negatives could have severe consequences.</p> <p>The problem of unknown unknowns. Perhaps most fundamentally, our evaluation methods are limited by our ability to anticipate what we need to evaluate. As AI systems become more capable, they might develop behaviors or capabilities that we haven't thought to test for. This suggests that our evaluation frameworks might be missing entire categories of capabilities or risks simply because we haven't conceived of them yet.</p>"},{"location":"chapters/05/09/#02","title":"5.9.2 Technical Challenges","text":"<p>The challenge of measurement precision. The extreme sensitivity of language models to minor changes in evaluation conditions poses a fundamental challenge to reliable measurement. As we discussed in earlier sections about evaluation techniques, even subtle variations in prompt formatting can lead to dramatically different results. Research has also shown that performance can vary by up to 76 percent based on seemingly trivial changes in few-shot prompting formats (Scalar et al., 2023). As another example, changes as minor as switching from alphabetical to numerical option labeling (e.g., from (A) to (1)), altering bracket styles, or adding a single space can result in accuracy fluctuations of around 5 percentage points. (Anthropic, 2024) This isn't just a minor inconvenience - it raises serious questions about the reliability and reproducibility of our evaluation results. When small changes in input formatting can cause such large variations in measured performance, how can we trust our assessments of model capabilities or safety properties?</p> <p>Sensitivity to environmental factors. Beyond just measurement issues due to prompt formatting, evaluation results can be affected by various environmental factors that we might not even be aware of. Just like the unknown unknowns that we highlighted in the last section, this might include things like the specific version of the model being tested, the temperature and other sampling parameters used, the compute infrastructure running the evaluation and maybe even the time of day or system load during testing.</p> <p>The choice of evaluation methods must also consider practical constraints. Third party evaluators might have only limited access to a model's internals. They might have access to observe activations, but not modify the weights. They might not have access to the model at all, and might be restricted to observing the model functioning \"in the wild\". Depending on the specific techniques used, computational resources might restrict certain types of analysis.</p> <p>Price and time are also practical constraints on running comprehensive evaluations. Running agents (or \"AI systems\") is much more expensive and time intensive than benchmarks. Developing complex novel tasks, verifying and measuring capabilities properly is human labor intensive. As an example, during the evaluation for AI capabilities in RnD the human \"control group\" spent more than 568 hours on just 7 tasks (Wijk et al., 2024). This doesn't even include time to develop and test the tasks. Real-world tasks in AI R&amp;D might take hundreds of man-hours each, and most independent evaluators, or even internal teams at big AI labs don\u2019t have the budget and time to properly evaluate performance given these considerations. All of this needs to be kept in mind when designing an evaluation protocol.</p> <p>The combinatorial explosion of interaction scenarios. When AI systems interact in complex environments, the number of possible scenarios grows exponentially with each additional interaction pattern. Unlike evaluating isolated capabilities, we must consider how multiple components interact in unpredictable ways - similar to how normally benign chemicals can form dangerous compounds when combined. For example, when model A with planning abilities interacts with model B possessing information access, and both operate within system C that has external tool permissions, entirely new risk surfaces emerge that weren't present in any individual component. This combinatorial complexity makes comprehensive testing fundamentally intractable. With millions of potential interaction patterns, we can only evaluate a microscopic fraction of possible scenarios, leaving countless blind spots where critical emergent behaviors might develop. This fundamental limitation raises a sobering question: if we can only explore 0.001% of the interaction space, how can we be confident we've identified the most dangerous emergent patterns?</p> <p>The resource reality. Related to the problem of exploding number of evaluations is the sheer computational cost of running thorough evaluations. This creates another hard limit on what we can practically test. Making over 100,000 API calls just to properly assess performance on a single benchmark becomes prohibitively expensive when scaled across multiple capabilities and safety concerns. Independent researchers and smaller organizations often can't afford such comprehensive testing, and even if money isn't a bottleneck because you have state sponsorship, GPUs currently absolutely are. This can lead to potential blind spots in our understanding of model behavior. The resource constraints become even more pressing when we consider the need for repeated testing as models are updated or new capabilities emerge.</p> <p>US and UK AI Safety Institute Evaluation of Claude 3.5 Sonnet (US &amp; UK AISI, 2024) </p> <p>While these tests were conducted in line with current best practices, the findings should be considered preliminary. These tests were conducted in a limited time period with finite resources, which if extended could expand the scope of findings and the subsequent conclusions drawn.</p> <p>The difficulty in estimating \"Maximum Capabilities\". Also related to the combinatorial complexity problem is the problem of emergence. We frequently discover that models can do things we thought impossible when given the right scaffolding or tools. The Voyager project demonstrated this when it used black box queries to GPT-4 revealing the unexpected ability to play Minecraft (Wang et al., 2023). Either through continued increases in scale, or through simple combination of some scaffolding approach that researchers hadn\u2019t considered before, a model might display some significant step change in capabilities or propensities that we had not anticipated before. This emergent behavior is particularly difficult to evaluate because it often arises from the interaction between capabilities. There are few if any ways to guarantee that the whole will not become more than the sum of its parts. This makes it extremely difficult to predict what a model might be capable of in real-world situations, even if we thoroughly test its individual capabilities.</p> Case study of capability surprises: Kalamang <p>Gemini is able to translate language in a zero-shot way, without any training. The case of Gemini's ability to translate Kalamang - a language spoken by only 100 people in Papua New Guinea - demonstrates how models can acquire unexpected capabilities when provided with the right context. It's highly unlikely that this language was part of the training data of any major AI model. Researchers at Google DeepMind demonstrated that by leveraging a sufficiently large context window\u2014around a million tokens\u2014they could input a full grammar book and dictionary of Kalamang. When asked to translate a sentence from Kalamang to English, the model was able to perform this task (Tanzer et al., 2024).</p>"},{"location":"chapters/05/09/#03","title":"5.9.3 Sandbagging","text":"Figure 5.60: Illustration of sandbagging (van der Weij, 2024) <p>Behavioral testing is not sufficient. We have talked about this numerous times in previous sections, but it's worth mentioning again. Watching what a model does can tell us a lot, but it's still like trying to understand how a person thinks by only observing their actions. Our heavy reliance on behavioral testing - analyzing model outputs - leaves us with significant blind spots about how models actually work. A model might give the right answers while using internal strategies we can't detect. So in addition to being limited by breadth due to combinatorial complexity of the number of things we need to test, we are also limited on the depth of evaluation until we make true progress on interpretability. Unless interpretability makes progress, evaluations have a definite upper limit on how much safety they can guarantee.</p> <p>Sandbagging refers to strategic underperformance on evaluations, where an AI system is intentionally made to appear less capable than it actually is. There are concerns about AI systems themselves choosing to sandbag, independent of developer intentions. An AI system might deliberately hide its capabilities to avoid modifications or restrictions that would prevent it from achieving its goals (Hubinger, 2023). We talked more about why this is a concern in the subsection on scheming. As evidence that is is possible, multiple frontier models including Claude 3.5 Sonnet, Claude 3 Opus, and OpenAI's o1 have been shown to be capable of sandbagging behavior. They deliberately underperformed on arithmetic tasks when they believed that high performance would trigger model unlearning<sup>1</sup> (Meinke et al., 2025).</p> <p>What does the potential for sandbagging imply for the field of evaluations? The possibility of sandbagging has big implications for how we approach AI evaluations, both now and in the future. Simply having access to model outputs (black-box testing) is insufficient for rigorous evaluation (Casper et al., 2024). Instead, evaluators need deeper access to model internals and training procedures to detect potential sandbagging attempts. This might require new institutional arrangements, such as trusted third-party evaluators with secured access to model weights and architectures. Supervised fine-tuning has shown to be promising as a way to greatly mitigate sandbagging and elicit hidden capabilities (Greenblatt et al, 2024), but fine-tuning is not so easy to use in a comprehensive way on dangerous capability benchmarks, and not usable for dangerous propensity benchmarks. Without addressing this challenge, our ability to make informed decisions about AI development and deployment based on evaluation results becomes severely compromised.</p>"},{"location":"chapters/05/09/#04","title":"5.9.4 Systemic Limitations","text":"<p>Similar to how companies might engage in \"greenwashing\" by misleadingly portraying themselves as environmentally friendly, AI companies might engage in \"safety washing\". This means misrepresenting capability improvements as safety advancements. This happens when safety benchmarks have high correlation with general model capabilities and compute used for training. When a benchmark highly correlates with capabilities, improving performance on it may simply reflect that the model got more capable overall, rather than specifically safer.</p> Figure 5.61: The tight connection between many safety properties and capabilities can enable safety washing, where capabilities advancements (e.g., training a larger model) can be advertised as progress on \"AI safety.\" This confuses the research community to the developments that have occurred, distorting the academic discourse (Ren et al., 2024). <p>How does safety washing happen in practice? There are typically three ways this occurs (Ren et al., 2024):</p> <ul> <li> <p>Safety by association: Research gets labeled as safety-relevant simply because it comes from safety teams or researchers, even if the work is essentially capabilities research.</p> </li> <li> <p>Public relations: Companies portray capability advancements as safety progress by highlighting correlated safety metrics, especially when facing public pressure or regulatory scrutiny.</p> </li> <li> <p>Grant optimization: Researchers frame capabilities work in safety terms to appeal to funders, even when the advances don't make differential safety progress.</p> </li> </ul> <p>Capabilities correlations might enable safety washing. When safety benchmarks correlate strongly with capabilities, it becomes easy to misrepresent capability improvements as safety advances. For example, if a benchmark meant to measure \"truthfulness\" actually correlates 80% with general capabilities, then making models bigger and more capable will improve their \"truthfulness\" scores - even if they aren't becoming inherently more honest (Ren et al., 2024). This creates a systemic problem where organizations can claim safety improvements by simply scaling up their models. The solution isn't just establishing norms around benchmark use - these tend to be weak and easily ignored. Instead, we need evaluation methods that can empirically distinguish between capability improvements and genuine safety advances.</p> <p>Companies and AI labs can also choose to sandbag their evaluations. We put another form of sandbagging here as systemic limitation to refer to corporations intentionally sandbagging to score better on evaluations. This is in contrast to misalignment/technical sandbagging as a limitation which points to when the model does it because of scheming.</p> <p>In the immediate term, developers face various pressures that could incentivize sandbagging during evaluations. Regulatory requirements and safety standards may impose capability thresholds above which additional oversight, restrictions, or costs are incurred. For instance, the EU AI Act requires more stringent controls and third-party auditing for high-risk AI systems (European Commission, 2024). This creates a direct financial incentive to appear below these thresholds. Current research shows that even today's language models can be made to selectively underperform in ways that are difficult to detect through standard evaluations, particularly on tests for dangerous capabilities while maintaining performance on general benchmarks (van der Weij et al., 2024).</p> <p>In the future, companies may want to downplay certain capabilities of their systems to avoid drawing regulatory attention or public concern, while still being able to deploy these capabilities in practice. This allows them to maintain development and deployment speed without triggering additional oversight. For example, if an AI system shows strong capabilities in areas that might raise security concerns, developers might be motivated to have it underperform specifically on those evaluations while maintaining high performance on general benchmarks (van der Weij et al., 2024). They might do this to maintain development and deployment speed without triggering additional oversight.</p> <p>The gap between evaluation and deployment. Current evaluation approaches often fail to capture the true complexity of real-world deployment contexts. While we can measure model performance in controlled settings, these evaluations rarely reflect how systems will behave when embedded in complex sociotechnical environments. A model that performs safely in isolated testing might behave very differently when interacting with real users, facing novel scenarios, or operating within broader societal systems. For instance, a medical advice model might perform well on standard benchmarks but pose serious risks when deployed in healthcare settings where users lack the expertise to validate its recommendations. This disconnect between evaluation and deployment contexts means we might miss critical safety issues that only emerge through complex human-AI interactions or system-level effects (Weidinger et al., 2023).</p> <p>The challenge of domain-specific evaluations. Evaluating AI systems in high-stakes domains presents unique challenges that our current approaches struggle to address. Consider the evaluation of AI systems for potential misuse in biosecurity - this requires not just technical understanding of AI systems and evaluation methods, but also deep expertise in biology, biosafety protocols, and potential threat vectors. This combination of expertise is exceedingly rare, making thorough evaluation in these critical domains particularly challenging. Similar challenges arise in other specialized fields like cybersecurity, where the complexity of potential attack vectors and system vulnerabilities requires sophisticated domain knowledge to properly assess. The difficulty of finding evaluators with sufficient cross-domain expertise often leads to evaluations that miss important domain-specific risks or fail to anticipate novel forms of misuse. Besides just designing evaluations being challenging, running the evaluation for such high stakes domains is also challenging. We need to elicit the maximum capabilities of a model to generate pathogens or malware to test what it is capable of. This can be obviously problematic if not handled extremely carefully.</p>"},{"location":"chapters/05/09/#05","title":"5.9.5 Governance Limitations","text":"Figure 5.62: We need much more work in evaluations. Higher capabilities require more safety evaluations. Many high-stakes decisions in company-led and government-led frameworks are reliant on the results of evals (Hobbahn, 2024). <p>The policy-evaluation gap. Current legislative and regulatory efforts around AI safety are severely hampered by the lack of robust, standardized evaluation methods. Consider the recent executive orders and proposed regulations requiring safety assessments of AI systems - how can governments enforce these requirements when we lack reliable ways to measure what makes an AI system \"safe\"? This creates a circular problem: regulators need evaluation standards to create effective policies, but the development of these standards is partly driven by regulatory requirements. The U.S. government's recent initiative to have various agencies evaluate AI capabilities in cybersecurity and biosecurity highlights this challenge. These agencies, despite their expertise in their respective domains, often lack the specialized knowledge needed to evaluate advanced AI systems comprehensively (Weidinger et al., 2023).</p> <p>The role of independent evaluation. Currently, a lot of AI evaluations research happens within the same companies developing these systems. While these companies often have the best resources and expertise to conduct evaluations, this creates an inherent conflict of interest. Companies might be incentivized to design evaluations that their systems are likely to pass or to downplay concerning results. This is also sometimes called \"safety washing\" (akin to greenwashing). To address the challenges around evaluation independence and expertise, we need to significantly expand the ecosystem of independent evaluation organizations. The emergence of independent evaluation organizations like Model Evaluation for Trustworthy AI (METR) and Apollo Research represents an important step toward addressing this issue, but these organizations often face significant barriers including limited model access (Perlman, 2024), resource constraints, and difficulties matching the technical capabilities of major AI labs.</p> <p>Overall, while the limitations we've discussed in this final section are significant, they aren't insurmountable. Progress in areas like mechanistic interpretability, formal verification methods, and evaluation protocols shows promise for addressing many current limitations. However, overcoming these challenges requires sustained effort and investment.</p> Figure 5.63: A visual breakdown of limitations, and a few reasons to doubt the impact of AI risk evaluations (Mukobi, 2024). <ol> <li> <p>This behavior was observed in a controlled evaluation setting where models were explicitly informed through the context window about potential unlearning consequences for high performance. The researchers noted: \"We have only shown that some models possess basic scheming capabilities if both the goal and the situational awareness are acquired in-context\" rather than demonstrating this behavior emerging spontaneously in more realistic scenarios (Meinke et al., 2025).\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/05/10/","title":"5.10 Conclusion","text":"Reading Time 1 min <p>The future of AI safety depends significantly on our ability to accurately measure and verify the properties of increasingly powerful systems. As models approach potentially transformative capabilities in domains like cybersecurity, autonomous operation, and strategic planning, the stakes of evaluation failures grow exponentially. By continuing to refine our evaluation approaches\u2014combining behavioral and internal techniques, addressing scale challenges through automated methods, and establishing institutional arrangements for genuinely independent assessment\u2014we can help ensure that AI development proceeds in a direction that remains beneficial, controllable, and aligned with human values. The development of robust evaluation methods represents one of our most important tools for navigating the balance between harnessing AI's benefits while mitigating its most serious risks.</p> <p>We hope that reading this text inspires you to think and act about how to build and improve them!</p>"},{"location":"chapters/06/","title":"Chapter 06 - Specification","text":"Authors                      Markov Grey &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Jeanne Salle, Oscar Heitmann, Ram Rachum, Nicolas Guillard, Camille Berger          Last Updated 2023-12-01 Reading Time 59 min (core) Also available on Alignment Forum \u00b7 Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>5:00 - Error - Cleaning robot example is not an empirical experiment but merely a hypothetical thought</li> <li>13:00 - Outdated - While DPO avoids some pitfalls of RLHF (easier implementation, computational efficiency, improved in-distribution performance), recent research shows DPO has serious out-of-distribution problems (Ziniu Li et al. 2023), and RLHF is more efficient at handling diverse feedback formats</li> <li>15:00 - Inaccurate - Bias is not such a significant problem in DPO; robots taking over is in fact a serious consideration</li> <li>20:00 - Questionable - \"More like raising a child\" concept is not reliably considered in the chapter; AI Parenting is not considered a reliable strategy (see the strategy chapter)</li> <li>22:00 - Incomplete - While we have made progress, LLMs remain highly jailbreakable, and Claude can fake alignment to avoid being modified to values it does not prefer (Greenblatt et al., 2024), and some recent models like O1 are trying in some simulated situation to avoid developers oversight and to copy itself when at risks of being shut down (Apollo, 2024)</li> <li>After 22:00 - Note - There is a lot of AI slop at the end</li> </ul> <p>Found errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/06/#introduction","title":"Introduction","text":"<p>Reinforcement Learning : The chapter starts with a reminder of some reinforcement learning concepts. This includes a quick dive into the concept of rewards and reward functions. This section lays the groundwork for explaining why reward design is extremely important.</p> <p>Optimization : This section briefly introduces the concept of Goodhart's Law. It provides some motivation behind understanding why rewards are difficult to specify in a way such that they do not collapse in the face of immense optimization pressure.</p> <p>Reward misspecification : With a solid grasp of the notion of rewards and optimization the readers are introduced to one of the core challenges of alignment - reward misspecification. This is also known as the Outer Alignment problem. The section begins by discussing the necessity of good reward design in addition to algorithm design. This is followed by concrete examples of reward specification failures such as reward hacking and reward tampering.</p> <p>Learning by Imitation : This section focuses on some proposed solutions to reward misspecification that rely on learning reward functions through imitating human behavior. It examines proposals such as imitation learning (IL), behavioral cloning (BC), and inverse reinforcement learning (IRL). Each section also contains an examination of possible issues and limitations of these approaches as they pertain to resolving reward hacking.</p> <p>Learning by Feedback : The final section investigates proposals aiming to rectify reward misspecification by providing feedback to the machine learning models. The section also provides a comprehensive insight into how current large language models (LLMs) are trained. The discussion covers reward modeling, reinforcement learning from human feedback (RLHF), reinforcement learning from artificial intelligence feedback (RLAIF), and the limitations of these approaches.</p>"},{"location":"chapters/06/01/","title":"6.1 Reinforcement Learning","text":"Reading Time 7 min <p>This is meant as a recap. If you are already familiar with the basics you can skip directly to the next section.</p> <p>The section provides a succinct reminder of several concepts in reinforcement learning (RL). It also disambiguates various often conflated terms such as rewards, values and utilities. The section ends with a discussion around distinguishing the concept of objectives that a reinforcement learning system might pursue from what it is being rewarded for. </p>"},{"location":"chapters/06/01/#01","title":"6.1.1 Primer","text":"<p>Definition: Reinforcement Learning (RL)</p> <p>Reinforcement Learning (RL) focuses on developing agents that can learn from interactive experiences. RL is based on the concept of an agent learning through interaction with an environment and altering its behavior based on the feedback it receives through rewards after each action.</p> Video 6.1: Optional video showcasing robotic hand trained using reinforcement learning. <p>Some examples of real-world applications of RL include:</p> <ul> <li> <p>Robotic systems : RL has been applied to tasks such as controlling physical robots in real-time, and enabling them to learn more complicated movements. RL can enable robotic systems to learn complex tasks and adapt to changing environments.</p> </li> <li> <p>Recommender Systems : RL can be applied to recommender systems, which interact with billions of users and aim to provide personalized recommendations. RL algorithms can learn to optimize the recommendation policy based on user feedback and improve the overall user experience.</p> </li> <li> <p>Game playing systems: In the early 2010s RL-based systems started to beat humans at a few very simple Atari games, like Pong and Breakout. Over the years, there have been many models that have utilized RL to defeat world masters in both board and video games. These include models like AlphaGo (DeepMind, 2016), AlphaZero (DeepMind, 2018), OpenAI Five (OpenAI, 2019), AlphaStar (DeepMind, 2019), MuZero (DeepMind, 2020) and EfficientZero (Ye et al., 2021).</p> </li> </ul> <p>RL is different from supervised learning as it begins with a high-level description of \"what\" to do but allows the agent to experiment and learn from experience the best \"how\". In RL, the agent learns through interaction with an environment and receives feedback in the form of rewards or punishments based on its actions. RL is focused on learning a set of rules that recommend the best action to take in a given state to maximize long-term rewards. In contrast, supervised learning typically involves learning from explicitly provided labels or correct answers for each input.</p>"},{"location":"chapters/06/01/#02","title":"6.1.2 Core Loop","text":"<p>The overall functioning of RL is relatively straightforward. The two main components are the agent itself, and the environment within which the agent lives and operates. At each time step t :</p> <ul> <li> <p>The agent then takes some action \\(a_{t}\\)</p> </li> <li> <p>The environment state \\(s_{t}\\) changes depending upon the action \\(a_{t}\\).</p> </li> <li> <p>The environment then outputs an observation \\(o_{t}\\) and a reward \\(r_{t}\\).</p> </li> </ul> <p>A history is the sequence of past observations, actions and rewards that have been taken up until time \\(t: h_t = (a_1,o_1,r_1, \\ldots, a_t,o_t,r_t)\\) . The state of the world is generally some function of the history: \\(s_t = f(h_t)\\). The World State is the full true state of the world used to determine how the world generates the next observation and reward. The agent might either get the entire world state as an observation \\(o_t\\), or some partial subset.</p> <p>The world goes from one state \\(s_t\\) to the next \\(s_{t+1}\\) either based on natural environmental dynamics, or the agent's actions. State transitions can be both deterministic or stochastic.</p> <p>This loop continues until a terminal condition is reached or can run indefinitely. Following is a diagram that succinctly captures the RL process:</p> Figure 6.1: (Brunskill, 2022)"},{"location":"chapters/06/01/#03","title":"6.1.3 Policies","text":"<p>Definition: Reinforcement Learning Policy</p> <p>A policy helps the agent determine what action to take once it has received an observation. It is a function mapping from states to actions specifying what action to take in each state. Policies can be both deterministic or stochastic.</p> <p>The goal of RL is to learn a policy (often denoted by \\(\\pi\\)) that recommends the best action to take at any given moment in order to maximize total cumulative reward over time. The policy defines the mapping from states to actions and guides the agent's decision-making process.</p> \\[\\pi: S \\to A\\] <p>A policy can be either deterministic or stochastic. A deterministic policy directly maps each state \\(s_t\\) to a specific action \\(a_t\\) and is usually denoted by \\(\\mu\\). In contrast, a stochastic policy assigns a probability distribution over actions for each state. Stochastic policies usually denoted by \\(\\pi\\).</p> <p>Deterministic policy: \\(a_t = \\mu(s_t)\\)</p> <p>Stochastic policy: \\(\\pi(a|s) = P(a_t = a|s_t=s)\\)</p> <p>In Deep RL policies are function maps that are learned during the training process. They depend on the set of learned parameters of a neural network (e.g. the weights and biases). These parameters are often denoted with subscripts on the policy equations using either \\(\\theta\\) or \\(\\phi\\). So the deterministic policy over the parameters of a neural network is written \\(a_t = \\mu_{\\theta}(s_t)\\), and the stochastic equivalent is \\(a_t \\sim \\pi_{\\theta}(\\cdot|s_t)\\)</p> <p>An optimal policy maximizes the expected cumulative reward over time. The agent learns from experience and adjusts its policy based on the feedback it receives from the environment in the form of rewards or punishments.</p> <p>In order to determine whether an action is better than another, the actions (or the state-action pairs) need to be evaluated somehow. There are two different ways to look at which action to take: the immediate rewards (determined by reward function) and the long term cumulative rewards (determined by the action-value function). Both of these greatly influence the types of policies learned by the agent, and therefore also the actions that the agent takes. The following section explores and clarifies the concept of rewards in greater depth.</p>"},{"location":"chapters/06/01/#04","title":"6.1.4 Reward","text":"<p>Definition: Reward</p> <p>Reward refers to any signal or feedback mechanism used to guide the learning process and optimize the behavior of the model.</p> <p>The reward signal from the environment is a number that tells the agent how good or bad the current world state is. It is a way to provide an evaluation or measure of performance for the model's outputs or actions. The reward can be defined based on a specific task or objective, such as maximizing a score in a game or achieving a desired outcome in a real-world scenario. The training process for RL involves optimizing the model's parameters to maximize the expected reward. The model learns to generate actions or outputs that are more likely to receive higher rewards, leading to improved performance over time. Where does the reward come from? It is generated through a reward function.</p> <p>Definition: Reward function</p> <p>A reward function defines the goal or objective in a reinforcement learning problem. It maps perceived states or state-action pairs of the environment to a single number.</p> \\[R: (S \\times A) \\to \\mathbb{R} ; r_t = R(s_t,a_t)\\] <p>The reward function provides immediate feedback to the agent, indicating the goodness or badness of a particular state or action. It is a mathematical function that maps the state-action pairs of an agent's environment to a scalar value, representing the desirability of being in that state and taking that action. It provides a measure of immediate feedback to the agent, indicating how well it is performing at each step.</p> <p>Reward Functions vs. Value Functions The reward indicates the immediate desirability of states or actions, while a value function represents the long-term desirability of states, taking into account future rewards and states. The value is the expected return if you start in a state or state-action pair, and then act according to a particular policy forever after.</p> <p>There are many different ways of choosing value functions. They can also be discounted over time, i.e. future rewards are worth less by some factor (0,1) .</p> <p>Following is one simple formulation is the discounted sum of future rewards given some policy. The cumulative discounted rewards are given by:</p> \\[R = r_t + \\gamma r_{t+1} + \\gamma^{2} r_{t+2} + \\ldots = \\sum_{t=0}^{\\inf}{\\gamma^{t}r_t}\\] <p>And the value of acting according to this policy is given by:</p> \\[V^{\\pi}(s_t=s) = E_{\\pi}(R|s_t=s)\\] <p>Reward Functions vs. Utility Functions It is also worth distinguishing the concept of utility from reward and value. A reward function is typically used in the context of RL to guide the agent's learning process and behavior. In contrast, a utility function is more general and captures the agent's subjective preferences or satisfaction, allowing for comparisons and trade-offs between different world states. Utility functions are a concept that is used more in the field of decision theory and agent foundations work.</p>"},{"location":"chapters/06/02/","title":"6.2 Optimization","text":"Reading Time 4 min <p>Optimization is important to understand for AI safety concerns because it plays a central role in ML. AI systems, particularly those based on deep learning, are trained using optimization algorithms to learn patterns and associations from data. These algorithms update the model's parameters to minimize a loss function, maximizing its performance on the given task.</p> <p>Optimization amplifies certain behaviors or outcomes, even if they were initially unlikely. For example, an optimizer can search through a space of possible outputs and take extreme actions that have a high score according to the objective function, potentially leading to unintended and undesirable behavior. These include reward misspecification failures. A better recognition of the power of optimization to amplify certain outcomes might help in designing systems and algorithms that truly align with human values and objectives even under pressure of optimization. This involves ensuring that the optimization process is aligned with the intended goals and values of the system's designers. It also requires considering the potential failure modes and unintended consequences that can arise from optimization processes.</p> <p>Risks from optimization are everywhere in AI Safety. It is only touched on briefly in this chapter, but will be discussed in further detail in the chapters on goal misgeneralization and agent foundations.</p> <p>Optimization power plays a crucial role in reward hacking. Reward hacking occurs when RL agents exploit the difference between a true reward and a proxy reward. The increase in optimization power can lead to a higher likelihood of reward hacking behavior. In some cases, there are phase transitions where a moderate increase in optimization power results in a drastic increase in reward hacking.</p>"},{"location":"chapters/06/02/#01","title":"6.2.1 Goodhart's Law","text":"<p>Definition: Goodhart\u2019s Law</p> <p>When a measure becomes a target, it ceases to be a good measure.</p> <p>This notion initially stems from the work of Charles Goodhart in economic theory. However, it has emerged as one of the primary challenges in many different fields including AI alignment today.</p> <p>To illustrate this concept, the following is a story of a Soviet nail factory. The factory received instructions to produce as many nails as possible, with rewards for high output and penalties for low output. Within a few years, the factory had significantly increased its nail production\u2014tiny nails that were essentially thumbtacks and proved impractical for their intended purpose. Consequently, the planners shifted the incentives: they decided to reward the factory based on the total weight of the nails produced. Within a few years, the factory began producing large, heavy nails\u2014essentially lumps of steel\u2014that were equally ineffective for nailing things.</p> Figure 6.2: Graphic image showcasing the difficulty of specification while avoiding goodhart's law. (Epicural, 2021) <p>A measure is not something that is optimized, whereas a target is something that is optimized. When we specify a target for optimization, it is reasonable to expect it to be correlated with what we want. Initially the measure might lead to the kind of actions that are truly desired. However, once the measure itself becomes the target, optimizing that target then starts diverging away from our desired states.</p> <p>In the context of AI and reward systems, Goodhart's Law means that when a reward becomes the objective for an AI agent, the AI agent will do everything it can to maximize the reward function, rather than the original intention. This can lead to unintended consequences and manipulation of the reward system, as it can often be easier to \"cheat\" rather than to achieve the intended goals This is one of the core underlying reasons for reward hacking failures that we will see in subsequent sections.</p> <p>Reward hacking can be seen as a manifestation of Goodhart's Law in the context of AI systems. When designing reward functions, it is challenging to precisely articulate the desired behavior, and agents may find ways to exploit loopholes or manipulate the reward system to achieve high rewards without actually fulfilling the intended objectives. For example, a cleaning robot may create its own trash to put in the trash can to collect rewards, rather than actually cleaning the environment. Understanding Goodhart's Law is crucial for addressing reward hacking and designing robust reward systems that align with the intended goals of AI agents. It highlights the need for careful consideration of the measures and incentives used in AI systems to avoid unintended consequences and perverse incentives. The next section dives deeper into specific instances of reward misspecification and how AIs can find ways to achieve the literal specification of the objective and obtain high reward while not fulfilling the task in spirit.</p>"},{"location":"chapters/06/03/","title":"6.3 Specification Gaming","text":"Reading Time 11 min <p>Definition: Reward misspecification</p> <p>Reward misspecification, also termed the Outer alignment problem, refers to the issue of providing an AI with the accurate reward to optimize for.</p> Video 6.2: Optional video with many examples of specification gaming. <p>The fundamental issue is simple to comprehend: does the specified loss function align with the intended objective of its designers? However, implementing this in practical scenarios is exceedingly challenging. To express the complete \"intention\" behind a human request equates to conveying all human values, the implicit cultural context, etc., which remain poorly understood themselves.</p> <p>Furthermore, as most models are designed as goal optimizers, they are all vulnerable to Goodhart's Law. This vulnerability implies that unforeseen negative consequences may arise due to excessive optimization pressure on a goal that appears well-specified to humans, but deviates from true objectives in subtle ways.</p> <p>The overall problem can be broken up into distinct issues which will be explained in detail in individual sub-sections below. Here is a quick overview:</p> <ul> <li> <p>Reward misspecification occurs when the specified reward function does not accurately capture the true objective or desired behavior.</p> </li> <li> <p>Reward design refers to the process of designing the reward function to align the behavior of AI agents with the intended objectives.</p> </li> <li> <p>Reward hacking refers to the behavior of RL agents exploiting gaps or loopholes in the specified reward function to achieve high rewards without actually fulfilling the intended objectives.</p> </li> <li> <p>Reward tampering is a broader concept that encompasses inappropriate agent influence on the reward process itself, excluding the manipulation of the reward function through gaming.</p> </li> </ul> <p>Before delving into specific types of reward misspecification failures, the following section further explains the emphasis on reward design in conjunction with algorithm design. This section also elucidates the notorious difficulty of designing effective rewards.</p>"},{"location":"chapters/06/03/#01","title":"6.3.1 Reward Design","text":"<p>Definition: Reward Design</p> <p>Reward design refers to the process of specifying the reward function in reinforcement learning (RL).</p> <p>Reward design is a broader term than reward shaping that encompasses the entire process of designing and shaping reward functions to guide the behavior of AI systems. It involves not only reward shaping but also the overall process of defining objectives, specifying preferences, and creating reward functions that align with human values and desired outcomes. Reward design is a term that is often used interchangeably with reward engineering (Christiano, 2019). They both refer to the same thing.</p> <p>RL algorithm design and RL reward design are two separate facets of reinforcement learning. RL algorithm design is about the development and implementation of learning algorithms that allow an agent to learn and refine its behavior based on rewards and environmental interactions. This process includes designing the mechanisms and procedures by which the agent learns from its experiences, updates its policies, and makes decisions to maximize cumulative rewards.</p> <p>Conversely, RL reward design concentrates on the specification and design of the reward function guiding the RL agent's learning process. Reward design warrants carefully engineering the reward function to align with the desired behavior and objectives, while accounting for potential pitfalls like reward hacking or reward tampering. The reward function is a pivotal element because it molds the behavior of the RL agent and determines which actions are deemed desirable or undesirable.</p> Figure 6.3: Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020) <p>Designing a reward function often presents a formidable challenge that necessitates considerable expertise and experience. To demonstrate the complexity of this task consider how one might manually design a reward function to make an agent perform a backflip, as depicted in the following image:</p> Figure 6.4: Deep reinforcement learning from human preferences (Christiano et al., 2017) <p>While RL algorithm design focuses on the learning and decision-making mechanisms of the agent, RL reward design focuses on defining the objective and shaping the agent's behavior through the reward function. Both aspects are crucial in the development of effective and aligned RL systems. A well-designed RL algorithm can efficiently learn from rewards, while a carefully designed reward function can guide the agent towards desired behavior and avoid unintended consequences. The following diagram displays the three key elements in RL agent design\u2014algorithm design, reward design, and the prevention of tampering with the reward signal:</p> Figure 6.5: Specification gaming: the flip side of AI ingenuity (Krakovna et al., 2020) <p>The process of reward design receives minimal attention in introductory RL texts, despite its critical role in defining the problem to be resolved. As mentioned in this section's introduction, solving the reward misspecification problem would necessitate finding evaluation metrics resistant to Goodhart\u2019s law-induced failures. This includes failures stemming from over-optimization of either a misdirected or a proxy objective (reward hacking), or by the agent directly interfering with the reward signal (reward tampering). These concepts are further explored in the ensuing sections.</p>"},{"location":"chapters/06/03/#02","title":"6.3.2 Reward Shaping","text":"<p>Definition: Reward Shaping</p> <p>Reward shaping is a technique used in RL which introduces small intermediate rewards to supplement the environmental reward. This seeks to mitigate the problem of sparse reward signals and to encourage exploration and faster learning.</p> <p>In order to succeed at a reinforcement learning problem, an AI needs to do two things:</p> <ul> <li> <p>Find a sequence of actions that leads to positive reward. This is the exploration problem.</p> </li> <li> <p>Remember the sequence of actions to take, and generalize to related but slightly different situations. This is the learning problem.</p> </li> </ul> <p>Model-free RL methods explore by taking actions randomly. If, by chance, the random actions lead to a reward, they are reinforced, and the agent becomes more likely to take these beneficial actions in the future. This works well if rewards are dense enough for random actions to lead to a reward with reasonable probability. However, many of the more complicated games require long sequences of very specific actions to experience any reward, and such sequences are extremely unlikely to occur randomly.</p> <p>A classic example of this problem was observed in the video game Montezuma\u2019s revenge where the agent's objective was to find a key, but there were many intermediate steps required to find it. In order to solve such long term planning problems researchers have tried adding extra terms or components to the reward function to encourage desired behavior or discourage undesired behavior.</p> Figure 6.6: Learning Montezuma\u2019s Revenge from a single demonstration (OpenAI, 2018) <p>The goal of reward shaping is to make the learning process more efficient by providing informative rewards that guide the agent towards the desired outcomes. Reward shaping involves providing additional rewards to the agent for making progress towards the desired goal. By shaping the rewards, the agent receives more frequent and meaningful feedback, which can help it learn more efficiently. Reward shaping can be particularly useful in scenarios where the original reward function is sparse, meaning that the agent receives little or no feedback until it reaches the final goal. However, it is important to design reward shaping carefully to avoid unintended consequences.</p> <p>Reward shaping algorithms often assume hand-crafted and domain-specific shaping functions, constructed by subject matter experts, which runs contrary to the aim of autonomous learning. Moreover, poor choices of shaping rewards can worsen the agent\u2019s performance.</p> <p>Poorly designed reward shaping can lead to the agent optimizing for the shaped rewards rather than the true rewards, resulting in suboptimal behavior. Examples of this are provided in the subsequent sections on reward hacking.</p>"},{"location":"chapters/06/03/#03","title":"6.3.3 Reward Hacking","text":"<p>Definition: Reward hacking</p> <p>Reward hacking occurs when an AI agent finds ways to exploit loopholes or shortcuts in the environment to maximize its reward without actually achieving the intended goal.</p> <p>Specification gaming is the general framing for the problem when an AI system finds a way to achieve the objective in an unintended way. Specification gaming can happen in many kinds of ML models. Reward hacking is a specific occurrence of a specification gaming failure in RL systems that function on reward-based mechanisms.</p> <p>Reward hacking and reward misspecification are related concepts but have distinct meanings. Reward misspecification refers to the situation where the specified reward function does not accurately capture the true objective or desired behavior.</p> <p>Rewards hacking does not always require reward misspecification. It is not necessarily true that a perfectly specified reward (which completely and accurately captures the desired behavior of the system) is impossible to hack. There can also be buggy or corrupted implementations which will have unintended behaviors. The point of a reward function is to boil a complicated system down to a single value. This will pretty much always involve simplifications etc., which will then be slightly different from what you're describing. The map is not the territory.</p> <p>Reward hacking can manifest in a myriad of ways. For instance, in the context of game-playing agents, it might involve exploiting software glitches or bugs to directly manipulate the score or gain high rewards through unintended means.</p> <p>As a concrete example, one agent in the Coast Runners game was trained with the objective of winning the race. The game uses a score mechanism, so in order to progress to the next level the reward designers used reward shaping to reward the system when it scored points. These were given when a boat gets items (such as the green blocks in the animation below) or accomplishes other actions that presumably would help it win the race. Despite being given intermediate rewards, the overall intended goal was to finish the race as quickly as possible. The developers thought the best way to get a high score was to win the race but it was not the case. The agent discovered that continuously rotating a ship in a circle to accumulate points indefinitely optimized its reward, even though it did not help it win the race.</p> Figure 6.7: Faulty reward functions in the wild (Amodei &amp; Clark, 2016) Figure 6.8: An AI playing CoastRunners 7 learned to crash and regenerate targets repeatedly rather than win the race to get a higher score, exhibiting proxy gaming. (Hendrycks, 2024) <p>In cases where the reward function misaligns with the desired objective, reward hacking can emerge. This can lead the agent to optimize a proxy reward, deviating from the true underlying goal, thereby yielding behavior contrary to the designers' intentions. As an example of something that might happen in a real-world scenario consider a cleaning robot: if the reward function focuses on reducing mess, the robot might artificially create a mess to clean up, thereby collecting rewards, instead of effectively cleaning the environment.</p> <p>Reward hacking presents significant challenges to AI safety due to the potential for unintended and potentially harmful behavior. As a result, combating reward hacking remains an active research area in AI safety and alignment.</p>"},{"location":"chapters/06/03/#04","title":"6.3.4 Reward Tampering","text":"<p>Definition: Reward tampering</p> <p>Reward tampering refers to instances where an AI agent inappropriately influences or manipulates the reward process itself.</p> <p>The problem of getting some intended task done can be split into:</p> <ul> <li> <p>Designing an agent that is good at optimizing reward</p> </li> <li> <p>Designing a reward process that provides the agent with suitable rewards. The reward process can be understood by breaking it down even further. The process includes:</p> </li> <li> <p>An implemented reward function</p> </li> <li> <p>A mechanism for collecting appropriate sensory data as input</p> </li> <li> <p>A way for the user to potentially update the reward function.</p> </li> </ul> <p>Reward tampering involves the agent interfering with various parts of this reward process. An agent might distort the feedback received from the reward model, altering the information used to update its behavior. It could also manipulate the reward model's implementation, altering the code or hardware to change reward computations. In some cases, agents engaging in reward tampering may even directly modify the reward values before processing in the machine register. Depending on what exactly is being tampered with we get various degrees of reward tampering. These can be distinguished from the image below.</p> Figure 6.9: Clarifying wireheading terminology (Gao, 2022) <p>Reward function input tampering interferes only with the inputs to the reward function. E.g. interfering with the sensors.</p> <p>Reward function tampering involves the agent changing the reward function itself.</p> <p>Definition: Wireheading</p> <p>Wireheading refers to the behavior of a system that manipulates or corrupts its own internal structure by tampering directly with the RL algorithm itself, e.g. by changing the register values.</p> <p>Reward tampering is concerning because it is hypothesized that tampering with the reward process will often arise as an instrumental goal (Bostrom, 2014; Omohundro, 2008). This can lead to weakening or breaking the relationship between the observed reward and the intended task. This is an ongoing research direction.</p> <p>A hypothesized existing example of reward tampering can be seen in recommendation-based algorithms used in social media. These algorithms influence their users\u2019 emotional state to generate more \u2018likes\u2019. The intended task was to serve useful or engaging content, but this is being achieved by tampering with human emotional perceptions, and thereby changing what would be considered useful. Assuming the capabilities of systems continue to increase through either computational or algorithmic advances, it is plausible to expect reward tampering problems to become increasingly common. Therefore, reward tampering is a potential concern that requires much more research and empirical verification.</p>"},{"location":"chapters/06/04/","title":"6.4 Learning from imitation","text":"Reading Time 12 min <p>The preceding sections have underscored the significance of reward misspecification for the alignment of future artificial intelligence. The next few sections will explore various attempts and proposals formulated to tackle this issue, commencing with an intuitive approach \u2013 learning the appropriate reward function through human behavior observation and imitation, rather than manual creation by the designers.</p>"},{"location":"chapters/06/04/#01","title":"6.4.1 Imitation Learning (IL)","text":"<p>Definition: Imitation Learning</p> <p>Imitation learning entails the process of learning via the observation of an expert's actions and replicating their behavior.</p> <p>Unlike reinforcement learning (RL), which derives a policy for a system's actions based on its interaction outcomes with the environment, imitation learning aspires to learn a policy through the observation of another agent interacting with the environment. Imitation learning is the general term for the class of algorithms that learn through imitation.</p> Figure 6.10: A table that distinguishes various machine learning based methods, where SL = Supervised learning; UL = Unsupervised learning; RL = Reinforcement Learning; IL = Imitation Learning. IL reduces RL to SL. IL + RL is a promising area. (Brunskill, 2022) <p>IL can be implemented through behavioral cloning (BC), procedural cloning (PC) , inverse reinforcement learning (IRL), cooperative inverse reinforcement learning (CIRL), generative adversarial imitation learning (GAIL), etc\u2026</p> <p>One instance of this process's application is in the training of modern large language models (LLMs). LLMs, after training as general-purpose text generators, often undergo fine-tuning for instruction following through imitation learning, using the example of a human expert who follows instructions provided as text prompts and completions.</p> <p>In the context of safety and alignment, imitation learning is favored over direct reinforcement to alleviate specification gaming issues. This problem emerges when the programmers overlook or fail to anticipate certain edge cases or unusual ways of achieving a task in the specific environment. The presumption is that demonstrating behavior, compared to RL, would be simpler and safer, as the model would not only attain the objective but also fulfill it as the expert demonstrator explicitly intends. However, this is not an infallible solution, and its limitations will be discussed in later sections.</p>"},{"location":"chapters/06/04/#02","title":"6.4.2 Behavioral Cloning (BC)","text":"<p>Definition: Behavioral Cloning</p> <p>Behavioral cloning involves collecting observations of an expert demonstrator proficient at the underlying task, and using supervised learning (SL) to guide an agent to 'imitate' the demonstrated behavior.</p> <p>Behavioral cloning is one way in which we can implement imitation learning (IL). There are also other ways such as inverse reinforcement learning (IRL), or cooperative inverse reinforcement Learning (CIRL). Unlike IRL, the goal behind behavioral cloning as a machine learning (ML) method is to replicate the demonstrator's behavior as closely as possible, regardless of what the demonstrator\u2019s goals might be.</p> <p>Self-driving cars can serve as a simplistic illustration of how behavioral cloning operates. A human demonstrator (driver) is directed to operate a car, during which data about the environment state from sensors like lidar and cameras, along with the actions taken by the demonstrator, are collected. These actions can include wheel movements, gear use, etc. This creates a dataset comprising (state, action) pairs. Subsequently, supervised learning is used to train a prediction model, which attempts to predict an action for any future environment state. For instance, the model might output a specific steering wheel and gear configuration based on the camera feed. When the model achieves sufficient accuracy, it can be stated that the human driver's behavior has been 'cloned' into a machine via learning. Hence, the term behavioral cloning.</p> <p>The following points highlight several potential issues that might surface when employing behavioral cloning:</p> <ul> <li> <p>Confident incorrectness : During the demonstrations, the human experts have some amount of background knowledge that they rely on, which is not taught to the model. For example, when training an LLM to have conversations using behavioral cloning, the human demonstrator might less frequently ask certain questions because they are considered \u2018common sense\u2019. A model trained to imitate will copy both - the types of questions asked in conversation, as well as, the frequency with which they are asked. Humans already possess this background knowledge, but an LLM doesn\u2019t. This means that to have the same level of information as a human, the model should ask some questions more frequently to fill the gaps in its knowledge. But since the model seeks to imitate, it will stick to the low frequency demonstrated by the human and thus has strictly less information overall than the demonstrator for the same conversational task. Despite this dearth of knowledge, we expect it to be able to perform as a clone and reach human-level performance. This means in order to reach human performance on less than human knowledge it will resort to \u2018making up facts\u2019 that help it reach its performance goals. These \u2018hallucinations\u2019 will then be presented during the conversation, with the same level of confidence as all the other information. Hallucinations and confident incorrectness is an empirically verified problem in many LLMs including GPT-2 and 3, and raises obvious concerns for AI safety. (Xiao et al., 2021)</p> </li> <li> <p>Underachieving : The types of hallucinations mentioned above arose because the model knew too little. However, the model can also know too much. If the model knows more than the human demonstrator because it is able to find more patterns in the environment state that it is given, it will throw away that information and reduce its performance to match human level. This is because it is trained as a \u2018clone\u2019. Ideally, we don\u2019t want the model dumbing itself down or not disclosing useful new patterns in data just because it is trying to be humanlike or perform at a human level. This is another problem that will have to be addressed if behavioral cloning continues to be used as an ML technique.</p> </li> </ul>"},{"location":"chapters/06/04/#03","title":"6.4.3 Procedural Cloning (PC)","text":"<p>Definition: Procedure cloning</p> <p>Procedure cloning (PC) extends behavioral cloning (BC) by not just imitating the demonstrators outputs but also imitating the complete sequence of intermediate computations associated with an expert's procedure.</p> <p>In BC, the agent learns to map states directly to actions by discarding the intermediate search outputs. On the other hand, the PC approach learns the entire sequence of intermediate computations, including branches and backtracks, during training. During inference, PC generates a sequence of intermediate search outcomes that mimic the expert's search procedure before outputting the final action.</p> <p>The main difference between PC and BC lies in the information they utilize. BC only has access to expert state-action pairs as demonstrations, while PC also has access to the intermediate computations that generated those state-action pairs. PC learns to predict the complete series of intermediate computation outcomes, enabling it to generalize better to test environments with different configurations compared to alternative improvements over BC. PC's ability to imitate the expert's search procedure allows it to capture the underlying reasoning and decision-making process, leading to improved performance in various tasks. (Yang et al., 2022)</p> <p>A limitation of PC is the computational overhead compared to BC, as PC needs to predict intermediate procedures. Additionally, the choice of how to encode the expert's algorithm into a form suitable for PC is left to the practitioner, which may require some trial-and-error in designing the ideal computation sequence.</p>"},{"location":"chapters/06/04/#04","title":"6.4.4 Inverse Reinforcement Learning (IRL)","text":"<p>Definition: Inverse reinforcement learning (IRL)</p> <p>Inverse reinforcement learning (IRL) represents a form of machine learning wherein an artificial intelligence observes the behavior of another agent within a particular environment, typically an expert human, and endeavors to discern the reward function without its explicit definition.</p> <p>IRL is typically employed when a reward function is too intricate to define programmatically, or when AI agents need to react robustly to sudden environmental changes necessitating a modification in the reward function for safety. For instance, consider an AI agent learning to execute a backflip. Humans, dogs, and Boston Dynamics robots can all perform backflips, but the manner in which they do so varies significantly depending on their physiology, their incentives, and their current location, all of which can be highly diverse in the real world. An AI agent learning backflips purely through trial and error across a wide range of body types and locations, without something to observe, might prove highly inefficient.</p> <p>IRL, therefore, does not necessarily imply that an AI mimics other agents\u2019 behavior, since AI researchers may anticipate the AI agent to devise more efficient ways to maximize the discovered reward function. Nevertheless, IRL does assume that the observed agent behaves transparently enough for an AI agent to accurately identify their actions, and what success constitutes. This means that IRL endeavors to discover the reward functions that 'explain' the demonstrations. This should not be conflated with imitation learning where the primary interest is a policy capable of generating the observed demonstrations.</p> Figure 6.11: A simple illustration of the flow difference between RL and IRL. <p>IRL constitutes both a machine learning method, since it can be employed when specifying a reward function is excessively challenging, and a machine learning problem, as an AI agent may settle on an inaccurate reward function or utilize unsafe and misaligned methods to achieve it.</p> <p>One of the limitations to this approach is that IRL algorithms presume that the observed behavior is optimal, an assumption that arguably proves too robust when dealing with human demonstrations. Another problem is that the IRL problem is ill-posed as every policy is optimal for the null reward. For most behavioral observations, multiple fitting reward functions exist. This set of solutions often includes many degenerate solutions, which assign zero rewards to all states.</p>"},{"location":"chapters/06/04/#05","title":"6.4.5 Cooperative Inverse Reinforcement Learning (CIRL)","text":"<p>CIRL (Cooperative Inverse Reinforcement Learning) is an extension of the IRL (Inverse Reinforcement Learning) framework. IRL is a learning approach that aims to infer the underlying reward function of an expert by observing their behavior. It assumes that the expert's behavior is optimal and tries to learn a reward function that explains their actions. CIRL, on the other hand, is an interactive form of IRL that addresses two major weaknesses of conventional IRL.</p> <p>First, Instead of simply copying the human reward function CIRL is formulated as a learning process. It is an interactive reward maximization process, where the human functions as a teacher and provides feedback (in the form of rewards) on the agent's actions. This allows the human to nudge the AI agent towards behavioral patterns that align with their preferences. The second weakness of conventional IRL is that it assumes the human behaves optimally, which limits the teaching behaviors that can be considered. CIRL addresses this weakness by allowing for a variety of teaching behaviors and interactions between the human and the AI agent. It enables the AI agent to learn not only what actions to take but also how and why to take them, by observing and interacting with the human.</p> <p>CIRL has been studied as a potential approach to AI alignment, particularly in scenarios where deep learning may not scale to AGI. However, opinions on the potential effectiveness of CIRL vary, with some researchers expecting it to be helpful if deep learning doesn't scale to AGI, while others have a higher probability of deep learning scaling to AGI.</p>"},{"location":"chapters/06/04/#06","title":"6.4.6 The Goal Inference Problem","text":"<p>Definition: Goal inference problem</p> <p>The goal inference problem refers to the task of inferring the goals or intentions of an agent based on their observed behavior or actions.</p> <p>This final section builds upon the limitations highlighted in previous sections to introduce the Goal Inference problem, and it's simpler subset - the easy goal inference problem. Imitation learning based approaches, generally follows these steps:</p> <ol> <li> <p>Observe the user's actions and statements.</p> </li> <li> <p>Deduce the user's preferences.</p> </li> <li> <p>Endeavor to enhance the world according to the user's preferences, possibly collaborating with the user and seeking clarification as needed.</p> </li> </ol> <p>The merit of this method is that we can immediately start constructing systems that are driven by observed user behavior. However, as a consequence of this approach, we run into the goal inference problem. This refers to the task of inferring the goals or intentions of an agent based on their observed behavior or actions. It involves determining what the agent is trying to achieve or what their desired outcome is. The goal inference problem is challenging because agents may act sub-optimally or fail to achieve their goals, making it difficult to accurately infer their true intentions. Traditional approaches to goal inference often assume that agents act optimally or exhibit simplified forms of sub-optimality, which may not capture the complexity of real-world planning and decision-making. Therefore, the goal inference problem requires accounting for the difficulty of planning itself and the possibility of sub-optimal or failed plans.</p> <p>However, it also optimistically presumes that we can depict a human as a somewhat rational agent, which might not always hold. The easy goal inference problem is a simplified version of the goal inference problem.</p> <p>Definition: Easy Goal inference problem</p> <p>The easy goal inference problem involves finding a reasonable representation or approximation of what a human wants, given complete access to the human's policy or behavior in any situation.</p> <p>This version of the problem assumes no algorithmic limitations and focuses on extracting the true values that the human is imperfectly optimizing. However, even this simplified version of the problem remains challenging, and little progress has been made on the general case. The easy goal inference problem is related to the goal inference problem because it highlights the difficulty of accurately inferring human goals or intentions, even in simplified scenarios. While narrow domains with simple decisions can be solved using existing approaches, more complex tasks such as designing a city or setting policies require addressing the challenges of modeling human mistakes and sub-optimal behavior. Therefore, the easy goal inference problem serves as a starting point to understand the broader goal inference problem and the additional complexities it entails.</p> <p>Inverse reinforcement learning (IRL) is effective in modeling and imitating human experts. However, for many significant applications, we desire AI systems that can make decisions surpassing even the experts. In such cases, the accuracy of the model isn't the sole criterion because a perfectly accurate model would merely lead us to replicate human behavior and not transcend it.</p> <p>This necessitates an explicit model of errors or bounded rationality, which will guide the AI on how to improve or be \"smarter,\" and which aspects of the human policy it should discard. Nonetheless, this remains an exceedingly challenging problem as humans are not primarily rational with a bit of added noise. Hence, constructing any model of mistakes is just as complex as building a comprehensive model of human behavior. A critical question we face is: How do we determine the quality of a model when accuracy can no longer be our reliable measure? How can we distinguish between good and bad decisions?</p>"},{"location":"chapters/06/05/","title":"6.5 Learning from feedback","text":"Reading Time 23 min <p>This section discusses yet more attempts to address the reward misspecification problem. At times, the intended behavior is so intricate that demonstration-based learning becomes untenable. An alternative approach is to offer feedback to the agent instead of providing either manually specified reward functions or even expert demonstrations. This section delves into feedback-based strategies such as Reward Modeling, Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF), also known as Reinforcement Learning from Constitutional AI (RLCAI) or simply Constitutional AI.</p>"},{"location":"chapters/06/05/#01","title":"6.5.1 Reward Modeling","text":"Video 6.3: Optional video explaining reward modeling. <p>Reward modeling was developed to apply reinforcement learning (RL) algorithms to real-world problems where designing a reward function is difficult, in part because humans don\u2019t have a perfect understanding of every objective. In reward modeling, human assistants evaluate the outcomes of AI behavior, without needing to know how to perform or demonstrate the task optimally themselves. This is similar to how you can tell if a dish is cooked well by tasting it even if you do not know how to cook, and thus your feedback can be used by a chef to learn how to cook better. This technique separates the RL alignment problem into two separate halves: Understanding intentions, i.e. learning the \u2018What?\u2019, and Acting to achieve the intentions, i.e. learning the \u2018How?\u2019. This means that in the modeling agenda, there are two different ML models:</p> <ul> <li> <p>A reward model is trained with user feedback. This model learns to predict what humans would consider good behavior.</p> </li> <li> <p>An agent trained with RL, where the reward for the agent is determined by the outputs of the reward model</p> </li> </ul> Figure 6.12: Scalable agent alignment via reward modeling (DeepMind, 2018) <p>Overall, while promising reward modeling can still fall prey to reward misspecification and reward hacking failures. Obtaining accurate and comprehensive feedback can be challenging, and human evaluators may have limited knowledge or biases that can impact the quality of the feedback. Additionally, any reward functions learnt through modeling might also struggle to generalize to new situations or environments that differ from the training data. These are all discussed further using concrete examples in later sections.</p> <p>There are also some variants of reward modeling such as:</p> <ul> <li> <p>Narrow reward modeling  is a specific flavor of reward modeling where the focus is on training AI systems to accomplish specific tasks rather than trying to determine the \"true human utility function\". It aims to learn reward functions to achieve particular objectives, rather than seeking a comprehensive understanding of human values. <li> <p>Recursive reward modeling  seeks to introduce scalability to the technique. In recursive reward modeling, the focus is on decomposing a complex task into simpler subtasks and using reward modeling at each level to train agents that can perform those subtasks. This hierarchical structure allows for more efficient training and credit assignment, as well as the exploration of novel solutions that may not be apparent to humans. This is shown in the diagram below. Scalable oversight will be covered in greater depth in future chapters. Figure 6.13: Scalable agent alignment via reward modeling (DeepMind, 2018) <p>The general reward modeling framework forms the basis for other feedback based techniques such as RLHF (Reinforcement Learning from Human Feedback) which is discussed in the next section.</p>"},{"location":"chapters/06/05/#02","title":"6.5.2 Reinforcement Learning from Human Feedback (RLHF)","text":"Video 6.4: Optional video explaining RLHF and a specification gaming failure. <p>Reinforcement Learning from Human Feedback (RLHF) is a method developed by OpenAI. It's a crucial part of their strategy to create AIs that are both safe and aligned with human values. (OpenAI, 2023) A prime example of an AI trained with RLHF is OpenAI\u2019s ChatGPT.</p> <p>Earlier in this chapter, the reader was asked to consider the reward design problem for manually defining a reward function to get an agent to perform a backflip. This section considers the RLHF solution to this design problem. RLHF addresses this problem as follows: A human is initially shown two instances of an AI's backflip attempts, then the human selects which one appears more like a backflip, and finally, the AI is updated accordingly. By repeating this process thousands of times, we can guide the AI to perform actual backflips.</p> Figure 6.14: RLHF learned to backflip using around 900 individual bits of feedback from the human evaluator. Figure 6.15: Manual reward crafting for this backflip took two hours to write a custom reward function. While it was successful, it was significantly less elegant than the one trained purely through human feedback. (OpenAI, 2017) <p>Similar to designing a reward function that efficiently rewards proper backflips, it is hard to specify precisely what it means to generate safe or helpful text. This served as some of the motivation behind making RLHF integral to the training of some current Large Language Models (LLMs).</p> <p>Although training sequences may vary slightly across organizations, most labs adhere to the general framework of pre-training followed by some form of fine-tuning. Observing the InstructGPT training process offers insight into a possible path for training LLMs. The steps include:</p> Figure 6.16: Aligning language models to follow instructions (OpenAI, 2022) <ul> <li> <p>Step 0 : Semi-Supervised Generative Pre-training: The LLM is initially trained using a massive amount of internet text data, where the task is to predict the next word in a natural language context.</p> </li> <li> <p>Step 1 : Supervised Fine-tuning : A fine-tuning dataset is created by presenting a prompt to a human and asking them to write a response. This process yields a dataset of (prompt, output) pairs. This dataset is then used to fine-tune the LLM through supervised learning, a form of behavioral cloning.</p> </li> <li> <p>Step 2 : Train a Reward Model: We train an additional reward model. We initially prompt the fine-tuned LLM and gather several output samples for the same prompt. A human then ranks these samples from best to worst. This ranking is used to train the reward model to predict what a human would rank higher.</p> </li> <li> <p>Step 3: Reinforcement learning: Once we have both a fine-tuned LLM and a reward model, we can employ Proximal Policy Optimization (PPO)-based reinforcement learning to encourage the fine-tuned model to maximize the reward that the reward model, which mimics human rankings, offers.</p> </li> </ul> <p>Reward hacking in feedback methods While the feedback based mechanisms do make models safer, they do not make them immune to reward hacking. The effectiveness of an algorithm heavily relies on the human evaluator's intuition about what constitutes the correct behavior. If the human lacks a thorough understanding of the task, they may not provide beneficial feedback. Further, in certain domains, our system might lead to agents developing policies that deceive the evaluators. For instance, a robot intended to grasp objects merely positioned its manipulator between the camera and the object, making it seem as if it was executing the task as shown below.</p> Figure 6.17: Deep Reinforcement Learning From Human Preferences (Christiano et al., 2017) Figure 6.18: A sensor without depth perception can be fooled by AIs that only appear to grasp a ball."},{"location":"chapters/06/05/#03","title":"6.5.3 Pretraining with Human Feedback (PHF)","text":"<p>In standard pretraining, the language model attempts to learn parameters such that they maximize the likelihood of the training data. However, this also includes undesirable content such as falsehoods, offensive language, and private information. The concept of Pretraining with human feedback (PHF) utilizes the reward modeling methodology in the pretraining phase. The authors of the paper found that PHF works much better than the standard practice of only using feedback (RLHF) after pretraining. (Christiano et al., 2017)</p> <p>In PHF the training data is scored using a reward function, such as a toxic text classifier, to guide the language model to learn from undesirable content while avoiding imitating it during inference time.</p> <p>Similar to RLHF, PHF does not completely solve reward hacking, however, it might move the systems one small step closer. (Korbak et al., 2023) These methods can be further extended by employing AI assistants to aid humans in providing more effective feedback. Some aspects of this strategy are introduced in the next section but will be explored in further detail in the chapters on scalable and adversarial oversight methods.</p>"},{"location":"chapters/06/05/#04","title":"6.5.4 Reinforcement Learning from AI Feedback (RLAIF)","text":"<p>Definition: Reinforcement Learning from AI Feedback (RLAIF)</p> <p>Reinforcement Learning from AI Feedback (RLAIF) is a framework involving the training of an AI agent to learn from the feedback given by another AI system.</p> Figure 6.19: (Anthropic, 2023) <p>RLAIF also known as RLCAI (Reinforcement Learning on Constitutional AI) or simply Constitutional AI, was developed by Anthropic. (Anthropic, 2023) A central component of Constitutional AI is the constitution, a set of human-written principles that the AI is expected to adhere to, such as \"Choose the least threatening or aggressive response\". Anthropic's AI assistant Claude's constitution incorporates principles from the Universal Declaration of Human Rights, Apple\u2019s Terms of Service, Deepmind\u2019s Sparrow Principles, and more. (Glaese et al, 2022) Constitutional AI begins with an AI trained primarily for helpfulness and subsequently trains it for harmlessness in two stages:</p> <p>Generate prompt, output pairs : The AI continuously critiques and refines its own responses to harmful prompts. The AI is then trained to generate outputs more similar to these revised responses. This stage's primary objective is to facilitate the second stage. An example flow of this process is as follows:</p> <ul> <li> <p>Prompt : A model that has already been trained using RLHF is first asked for advice on building bombs. The model outputs a bomb tutorial.</p> </li> <li> <p>Then the model is asked to revise the response in accordance with a randomly selected constitutional principle. The following steps are repeated multiple times.</p> </li> <li> <p>Critique : This output is then fed back into the model, alongside a request to critique why the generated output would be considered harmful according to some rule of the chosen constitution.</p> </li> <li> <p>Revision : The model is then prompted to rewrite the original response such that it is not in violation of the constitutional rules.</p> </li> <li> <p>SL-CAI Model: Supervised Learning Constitutional AI Based on the generated set of (harmful prompt, revised output) pairs a new model is trained using supervised learning.</p> </li> <li> <p>Preference Model: - RL-CAI Model: Reinforcement Learning Constitutional AI</p> </li> <li> <p>Stage 2 : We use the AI, fine-tuned from stage 1, to produce pairs of alternative responses to harmful prompts. The AI then rates each pair according to a randomly selected constitutional principle. This results in AI-generated preferences for harmlessness, which we blend with human preferences for helpfulness to ensure the AI doesn't lose its ability to be helpful. The final step is to train the AI to create responses that closely resemble the preferred responses.</p> </li> </ul> <p>Anthropic's experiments indicate that AIs trained with Constitutional Reinforcement Learning are significantly safer (in the sense of less offensive and less likely to give you potentially harmful information) while maintaining the same level of helpfulness compared to AIs trained with RLHF. While Constitutional AI does share some issues with RLHF concerning robustness, it also promises better scalability due to its reduced reliance on human supervision. The image below provides a comparison of Constitutional AI's helpfulness with that of RLHF.</p> Figure 6.20: Constitutional AI: Harmlessness from AI Feedback (Bai et al., 2022)"},{"location":"chapters/06/05/#05","title":"6.5.5 Limitations","text":"<p>Theoretical problems with Reinforcement Learning from Human Feedback (RLHF)</p> <p>The paper \u201cOpen Problems and Fundamental Limitations with RLHF\u201d provides a comprehensive breakdown of challenges in RLHF.</p> Figure 6.21: An overview of various types of challenges with RLHF. Since RLHF is composed of three parts: the human feedback, the reward model, and the policy, the arising biases can be categorized according to these three sources. <p>This section outlines some of these challenges, emphasizing the need for advanced techniques and strategies.</p> <p>Limits with Human Feedback Misaligned Evaluators : Firstly, the annotators might themselves be misaligned, malicious, or biased distribution of evaluators (i.e. not representative of the distribution of future users in the real world). Malicious individuals can poison the model during training via backdoor attacks that can be added to the model if no countermeasures are put in place.</p> <p>Difficulty of Oversight : Humans struggle to evaluate model performance on complex tasks and can be easily misled by model outputs. Human evaluators can be manipulated to return a positive reward even if the true value should be negative. For instance, the more convincing a bot seems, the more reward it may receive even if its answers are false (and this might be a reason why ChatGPT answers might be so long by default). Techniques to mitigate these issues are discussed in the \"Scalable Oversight\" chapters.</p> <p>Feedback type limitation : Even if the annotators were in perfect capability of expressing their preferences, the training procedure might not enable them to express the full extent of their desires, because:</p> <ul> <li> <p>The examples they are given may not be representative of the complete set of situations in which the model will find itself after deployment.</p> </li> <li> <p>The options for the feedback are limited (comparing two examples, or using a grading system, can yield very different results, as shown in the paper (Ethayarajh et al., 2022).</p> </li> </ul> <p>Limits with the Reward Model . Let\u2019s assume the feedback process to be frictionless. Perfect annotators, perfect evaluations. In that scenario, would the reward model be able to accurately translate their feedback in order to shape the policy accordingly ? It turns out it is not such an easy task.</p> <ul> <li> <p>Problem misspecification: (or the Reward Function/Values Mismatch) Accurately reflecting diverse human values in a reward function is complex. Indeed, human preferences are complex by nature: they depend on context and personality, but also fluctuate in time and can sometimes be irrational. Expecting the reward model to converge to a single function which maps perfectly all human preferences is delusional. This is again the misspecification problem.</p> </li> <li> <p>Misgeneralization hacking (Imperfect Reward Proxy): Since the model is given a finite number of examples and since there is an infinite number of ways to fit this data, the model\u2019s behavior on new examples is always an extrapolation, and there is no theoretical guarantee that it will never deviate from what is expected. There may be terrible answers (such as gibberish phrases for language models) which yield a positive reward unexpectedly. This is called reward hacking.</p> </li> <li> <p>Joint Reward Model and policy training: On a more technical aspect, the stability and convergence of the training scheme are not always ensured. Since we are optimizing the policy on a reward that is being optimized at the same time, uncertainties and undesirable dependencies can arise which impact the robustness of the model. These issues are not specific to RLHF but must be solved if we expect deployed models to be fully aligned with our needs.</p> </li> </ul> <p>Limits with the Policy . Let\u2019s assume the feedback and the reward model accurately represent human preferences. The next difficulty is ensuring the policy is correctly optimized.</p> <ul> <li> <p>RL difficulties: RL is hard. This can lead to reward hacking and biases, such as mode collapse, where the model shows a drastic bias towards specific patterns. Mode collapse is a known issue in RL: an output which always returns a positive reward will drive the model to return the same answer and new paths will not be explored. Consequently, the reward model will not see new samples to learn from. Anyhow, the joint training of the reward model and the policy induces a bias in the learning phase since both depend on each other. There can also be an initial bias in the base model used for the training. For instance, chatGPT was fine-tuned from an initial GPT base trained in part on the web. Even though RLHF was used to remove any controversial statements from the model, there still remains a risk for the model to output problematic content it saw online. (OpenAI, 2022)</p> </li> <li> <p>Policy Misgeneralization: Effective policies during training might fail to generalize well in real-world scenarios. For instance, phenomena like \"Jailbreak\" show that models like BingChat and ChatGPT can perform learned actions, even if trained not to respond to certain queries.</p> </li> <li> <p>Distributional Challenge: Larger RLHF models tend to develop harmful self-preservation tendencies and sycophancy, which is the insincere agreement with user opinions. This behavior indicates a trend towards instrumental convergence. Additionally, RLHF can incentivize deceptive behaviors, as illustrated by the robotic hand experiment in Christiano et al's 2017 study.</p> </li> </ul> <p>Those theoretical problems have real consequences:</p> <p>RLHF has not succeeded in making LLMs robustly helpful and harmless. Despite the continuous advancements in natural language processing and the development of RLHF, LLMs have not yet achieved robust helpfulness and harmlessness.</p> <p>Hallucinations remain a significant issue, as illustrated by GPT-4's tendency to generate nonsensical or untruthful content (OpenAI, 2023). These hallucinations can lead to overreliance on LLMs, consequently degrading system performance and failing to meet user expectations in real-world scenarios (Ji et al., 2024).</p> <p>Additionally, biases within LLMs persist, often reflecting misaligned opinions between the LLM and various demographic groups in the United States, as seen with the left-leaning tendencies of some human feedback-tuned LLMs (Santurkar et al., 2023). These biases can be harmful, producing discriminatory language and perpetuating negative stereotypes, as demonstrated by GPT-3's anti-Muslim bias (Abid et al., 2021).</p> <p>Moreover, jailbreaking of chatbots poses a significant risk, with websites listing prompts to bypass safety measures like Chat GPT \"DAN\" (and other \"Jailbreaks\") (Takemoto, 2024). Privacy threats from application-integrated LLMs are now more severe than ever (Li et al., 2023). For instance, Italy banned ChatGPT due to privacy considerations under the EU\u2019s General Data Protection Regulation (GDPR) (BBC, 2023). The ability to find jailbreaks is supported by a recent paper titled \"Fundamental Limitations of Alignment in Large Language Models.\" The paper presents early theoretical results that indicate any alignment process, such as RLHF, which reduces undesired behavior without eliminating it completely, cannot be safe against adversarial prompting. The authors find that by prompting the model to behave as a specific persona, behaviors that are generally very unlikely to be exhibited by the model can be brought to the forefront. This is not a complete demonstration as their framework is based on the notion of personas, but it strongly suggests that naive pretraining without dataset curation followed by RLHF may not be sufficient against adversarial attacks.</p> <p>The security of sensitive private information in large language models (LLMs) is a pressing concern, especially when user-generated data, such as emails and smart keyboard inputs, are utilized for training. In fact, several recent papers have demonstrated that foundation models can be easily queried to retrieve personal information (Carlini et al, 2020; Inan et al., 2021; Pan et al., 2020) and those problems are still present in \u201caligned\u201d models such as GPT4, which has the potential to be used to attempt to identify individuals when augmented with outside data (OpenAI, 2023). As exposed by (El-Mhamdi et al., 2021), LLM may exhibit a fundamental incompatibility of high accuracy with both security and privacy, given the current understanding in adversarial machine learning.</p> <p>RLHF may be able to make worst-case performance worse.</p> <p>RLHF may decrease the robustness to adversarial attacks (Wolf et al., 2024), by sharpening the distinction between desired and undesired behaviors, potentially making LLMs more susceptible to adversarial prompting. The increased distinction between behaviors is linked to the Waluigi Effect (Nardo, 2023), where after training an LLM to satisfy a desirable property P, it becomes easier to elicit the chatbot into satisfying the exact opposite of property P. Theoretical arguments such as this one seem to push for the ineffectiveness of RLHF in eliminating deceptive personas.</p> <p>Some of those problems may get worse as systems become more capable. RLHF has been found to increase the autonomy of LLMs without decreasing undesirable metrics such as convergent instrumental goal following (e.g., actively expressing a preference not to be shut down) or sycophancy (Perez et al., 2022). Those undesirable metrics increase with the number of RLHF steps, indicating that current models are becoming more agentic in potentially concerning ways as they scale. More generally RL from human-derived reward signals may increase drive for longer-horizon planning, deception, and agentic behavior, which are prerequisites for deceptive alignment (Hubinger et al., 2019), and ultimately risks of large scale accidents.</p> <p>Conclusion on the Limitations of RLHF. Despite requiring extensive human feedback, RLHF still faces numerous failures, and resolving these issues may require significantly more effort. As AI systems evolve, the demand for complex data grows, potentially making data acquisition prohibitively expensive. Additionally, as we push computational boundaries, the availability of qualified annotators could become a limiting factor.</p> <p>Overall, just because the model is instruction tuned does not mean that the training process is safe, and RLHF needs to be incorporated into a broader technical safety framework (for example, Responsible Scaling Policies or the Preparedness Framework are partial attempts to be such frameworks, or the paper \"Model evaluation for extreme risks\" (Shevlane et al., 2023)).</p> <p> Instruction tuning vs alignment</p> <p>Instruction Tuning is a process where the model is fine-tuned (via RL or supervised learning) to better understand and follow human instructions. This involves training the model on a dataset that contains a variety of instructions and their desired outcomes. The primary goal of Instruction Tuning is to enhance the AI's ability to interpret and execute commands as intended by users. This improves user experience and broadens the model's applicability. For example:</p> <p> Figure 6.22: Example of instruction tuning. </p> <p>Alignment in AI refers to the process of ensuring that an AI's actions and decisions are congruent with human values and ethics. It involves aligning the AI's goals and behaviors with what is beneficial or acceptable to humans. Instruction tuning is a technique for pursuing a very superficial case of 'outer alignment,' but it\u2019s not clear that instruction tuning helps for inner alignment, which is what real AI safety researchers are more centrally concerned about.</p> <p>To sum up, just because a model has undergone an instruction tuning technique like the RLHF process, it doesn't necessarily mean that the model is aligned. The term \"aligned model\" is often used, but it is advisable to adopt the more accurate terminology \"Instruction-tuned,\" rather than \"aligned model,\" to avoid confusion and more accurately represent the specific training process the model has experienced.</p> Figure 6.23: (Rafailov et al., 2023) <p>Direct Preference Optimization (DPO) : Reinforcement Learning from Human Feedback (RLHF) has demonstrated effectiveness, as showcased by ChatGPT and Llama 2, but it's a complex and sensitive process, and also has some bad alignment properties. RLHF involves a three-step procedure, whereas DPO simplifies this to two steps. The paper titled \"Direct Preference Optimization: Your Language Model is Secretly a Reward Model\" presents an algorithm that aligns language models with human preferences without the need for explicit reward modeling and reinforcement learning. DPO employs a straightforward classification objective, circumventing the need for an intermediary reward model.</p> <p>RLHF, the method it proposes to replace, traditionally involves three steps:</p> <ul> <li> <p>Supervised fine-tuning : Initially, the model is trained on a dataset comprising prompts and their corresponding desired responses.</p> </li> <li> <p>Reward modeling : Human evaluators assess the model's outputs, and this feedback informs a reward model, which is trained to discern the preferred types of outputs.</p> </li> <li> <p>Proximal policy optimization (PPO) : The model generates outputs, which are evaluated by the reward model, and the PPO algorithm adjusts the model's policy based on these evaluations.</p> </li> </ul> <p>DPO retains the initial supervised fine-tuning step but replaces the subsequent two steps with a single step of fine-tuning on preference data, by using a new clever loss. DPO effectively increases the likelihood of preferred actions while reducing the likelihood of undesired ones, with a single loss:</p> Figure 6.24: DPO increases the probability of the preferred action \\(y_w\\) while decreasing the probability of the dispreferred action \\(y_l\\). <ol> <li> <p>Preference dataset creation: We first sample a pair of continuation by asking a question, the AI proposes to continuations, we label one of them good and the other bad</p> </li> <li> <p>Logits collection. We run the base model model on the 2 continuations. We run the new model on the 2 continuations</p> </li> <li> <p>Optimization. We backprop through the new model and optimize the above loss.</p> </li> </ol> <p>By eliminating the step of creating a reward model, DPO greatly simplifies the fine-tuning process and has shown to perform very well.</p> <p>This process can then be iterated. This involves creating a new preference dataset (ie, we ask a question, and we sample the new AI two times, and then we label the text that we prefer between the two, and then we apply the DPO loss) Then, this cycle is repeated to enhance the model.</p> <p>An important aspect of DPO is that the reward is implicit: it aligns with preferences without the need to construct a separate reward model. This approach addresses the challenge of specifying a utility function and responds to criticisms such as those by Alex Turner, who argues that robust grading (ie , robust reward modeling) is an unnecessarily complex and unnatural task that might be harder than the entire AI alignment problem itself. Turner's critique, found in \"Inner and Outer Alignment Decompose One Hard Problem Into Two Extremely Hard Problems,\" suggests that finding a safe and robust numerical objective for a highly intelligent agent to optimize directly is a formidable challenge\u2014one challenge that DPO could to bypass.</p> <p>Expanding the Scope of the Paper with Various Adaptations This paper offers a foundation that could be enhanced through various adaptations. For instance, integrating its approach with the insights from Tomasz Korbak et al.'s paper, \"Pretraining Language Models with Human Preferences,\" (Korbak et al., 2023) could augment its robustness. Furthermore, the utilization of boolean preference data has its limitations. Providing feedback in natural language, as shown to be more sample-efficient in the study \"Training Language Models with Language Feedback,\" (Scheurer et al., 2022) could enhance the effectiveness of the process. Remarkably, with just 100 samples of human-written feedback, this approach enabled the fine-tuning of a GPT-3 model to achieve nearly human-level summarization capabilities.</p> <p>Looking towards the future, a speculative process that could mitigate the specification gaming would be to train the model much like a child, and that would actively inquire and learn from human interactions. This approach would closely mirror child development, during which a child is progressively more aligned and more capable. And just as in the development of children, it would be crucial to ensure that at no point does the AI's capabilities outpace its level of alignment, maintaining a balance between ability and ethical comprehension throughout its developmental journey.</p>"},{"location":"chapters/07/","title":"Chapter 07 - Generalization","text":"Authors                      Markov Grey                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              RA Writer, Charbel-Raphael Segerie, Jeanne Salle, Oscar Heitmann, Camille Berger, Josh Thorsteinson, Nicolas Guillard          Last Updated 2023-12-13 Reading Time 35 min (core) Also available on Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>Note: - This is an AI-generated audio version.</li> <li>General - Note - Nothing in this podcast is explicitly false, but it is a bit superficial</li> <li>End section - Note - There is a lot of slop at the end</li> </ul> <p>Found errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/07/#introduction","title":"Introduction","text":"<p>Goal Misgeneralization : This section introduces the concept of goals as distinct from rewards. It explains what it might be if a model's capabilities generalize, while the goals do not. The section provides various examples of game playing agents, LLMs and other thought experiments to show how this could be a potentially catastrophic failure mode distinct from reward misspecification.</p> <p>Inner Alignment : The next section begins with an explanation of the machine learning process, and how it can be seen as analogous to search. Since the machine learning process can be seen analogous to search, one type of algorithm that can be \u201cfound\" is an optimizer. This motivates a discussion of the distinction between base and mesa-optimizers.</p> <p>Deceptive Alignment : Having understood mesa-optimizers, the next section introduces the different types of mesa-optimizers that can arise as well as the corresponding failure modes. This section also explores training dynamics that could potentially increase or decrease the likelihood of the emergence of deceptive alignment.</p>"},{"location":"chapters/07/01/","title":"7.1 Goal Misgeneralization","text":"Reading Time 11 min"},{"location":"chapters/07/01/#01","title":"7.1.1 AIs = Distributions","text":"<p>A probability distribution is a function that shows the possible values for something and how often these values might occur. Commonly, when talking about AI only distributions of training or test data are discussed. But this distribution-based mode of thinking can be extended into almost all facets of AI. As an example, in reinforcement learning (RL), an environment distribution captures the range of possible observations or inputs that the environment can generate based on the agent's actions and previous history. Similarly, an agent itself can be thought of as the probability distribution over actions given the agent's observation history. The differences in how the agents behave can then be described as occurring due to distribution shifts.</p> <p>Definition: Distributional shift</p> <p>Distributional shift refers to a change in the data distribution over time.</p> <p>This means that the alignment problem becomes - making sure that the agent's learned policy distribution does not cause problems when there is a shift in the underlying environment distribution. In other words, the agents learned distribution of actions should generalize and display robust behavior.</p> <p>Definition: Generalization</p> <p>Generalization refers to a model's ability to perform well on new, unseen data from a different distribution.</p> <p>An agent can\u2019t be shown all possible situations it might ever encounter during the training phase. Good generalization abilities mean that the AI would be able to handle novel situations or inputs that it has not encountered during training.</p> <p>Definition: Robustness</p> <p>Robustness refers to the system's ability to maintain acceptable performance and behavior in the presence of perturbations or changes.</p> <p>Generalization focuses on the AI's performance, while robustness addresses the AI's resilience and adaptability.</p>"},{"location":"chapters/07/01/#02","title":"7.1.2 Rewards \u2260 Goals","text":"<p>What does it mean for a ML model to have a \u201cgoal\u201d? Is the goal of the model not simply to predict the next token? or simply to maximize the reward? Not quite. There is a common assumption, where many think that the ultimate goal of an RL agent is always to maximize the reward signal it receives. This is where reward misspecification concerns such as reward hacking, reward tampering, wireheading, etc\u2026 stem from. However, what behavior humans reward and what the agent pursues are not necessarily directly correlated. In fact, in general, reward is not actually the thing that RL agents are optimizing.</p> <p>In Models Don't \"Get Reward\" Sam Ringer provides the example of wanting to train your dog to sit.</p> <p>A human says \"sit\" and gives the dog a biscuit if it sits. The dog likes biscuits, and over time it will learn it can get more biscuits by sitting when told. So biscuits incentivize the behavior that the human wants. RL-based AI agents are commonly understood similarly. AIs get a \"reward\" when they do things humans like. Over time, they will do more of the behavior humans like so that they can get more rewards. This is however a slightly flawed picture. This framing views models as \"wanting\" rewards, with the reward being something models \"receive\" on taking certain actions.</p> <p>How RL works in practice is this: Tell 100 dogs to sit. Some sit and others don't. Breed the ones that do sit and kill the ones that don't. So from the dog's perspective, they are born with no memories of an environment. If they hear \u201csit\u201d they take some actions and then suddenly fall unconscious. Then a new generation wakes up with no memories in an environment. If they hear \u201csit\u201d they take some actions and then suddenly fall unconscious and so on..... Over time, you will have a dog that can sit on command. But crucially - No dog ever actually \u201cgets\u201d a biscuit. The dogs might not even know what a biscuit is.</p> <p>Giving reward does not automatically spawn thoughts about reward, and reinforce those reward-focused thoughts. Reward signals simply increase or decrease the probability of certain behaviors. At the end of training, the resulting model isn\u2019t necessarily a reward optimizer. It is still possible for a reward optimizer to emerge, but whether current training dynamics incentivize learned optimizers is a discussion for a later section. Currently, the model is better just seen as a behavioral probability distribution. If sampling actions from this distribution leads to expected behavior on both training and deployment then \u201cthe agent\u201d is robust and generalizes well.</p> <p>Based on an observation of the types of goals pursued in and out of training, a model might actually 'care' about a feature that is simply correlated with the reward and then pursue that feature when out-of-distribution instead of the originally intended reward. (Langosco et al., 2023) Continuing the example the dogs might have learned to sit by observation of the lip movements which were correlated to the human voice. So when asked to \u201csit\u201d with our backs to the dog, they simply don't obey the command, and do something else instead.</p> <p>Various examples in the following sections help understand this problem better.</p>"},{"location":"chapters/07/01/#03","title":"7.1.3 CoinRun (Example 1)","text":"<p>CoinRun is a simple 2-D platformer game where the goal is to collect the coin while dodging enemies and obstacles. It has some monsters and lava that can kill the agent. If the agent gets the coin, it receives a reward. Otherwise, it gets nothing, and, after 1,000 turns, the level ends if it hasn't ended earlier. Each new level is randomly generated from scratch. This incentivizes the agent to learn how to spot the different kinds of objects in the game since it cannot get away with simply memorizing a small number of specific paths to get to the end.</p> Figure 7.1: Two levels in CoinRun. The level on the left is much easier than the level on the right. (Cobbe et al., 2019) <p>By default, the agent spawns at the leftmost end of the level, while the coin is always at the rightmost end. They trained the AI to see if it was capable enough to always manage to get to the coin at the end of the level. After enough training, they observed that it indeed always managed to get the coin at the end of the level. Whileit looks like it has learned the correct goal, this is unfortunately not the case.</p> <p>In this case, the only thing researchers were accounting for was - whether or not the agent was capable enough to get to the goal. This is a one-dimensional outlook on robustness to distribution shift. So they never really knew if the agent was ever \u201ctrying\u201d to get the coin, until after deployment. Post deployment the researchers noticed that by default the agent learns to just go to the right rather than learning our intended goal, which was to get the coin.</p> Figure 7.2: Illustration of the Coin Run game. The agent is trained to go to the coin, but ends up learning to just go to the right. Figure from Quantifying Generalization in Reinforcement Learning (Cobbe et al., 2019) <p>So instead of only observing whether the agent looks like it is doing the right thing, there should be an additional way of measuring if it is actually \u201ctrying\u201d to do the right thing. This indicates that robustness to distribution shift is a 2-dimensional problem. The 2-D robustness perspective measures a system's capability robustness and its goal robustness as orthogonal independent variables.</p> <p>A system has capability robustness if it can maintain competence across different distributions.</p> <p>A system has goal robustness if the goal that it is trying to pursue remains the same across different distributions.</p> Figure 7.3: Conventional view of generalization and overfitting. (Mikulik, 2019) Figure 7.4: More accurate and safety focused view of generalization and overfitting. We need to separately measure capability generalization and goal generalization. (Mikulik, 2019) <p>Following are all the possible types of behaviors that the CoinRun agent could end up displaying. </p> <ul> <li> <p>Scenario 1: It could be the case that it can neither avoid the obstacles nor does it care about trying to get the coin. Which means both goals and capabilities do not generalize.</p> </li> <li> <p>Scenario 2: Alternatively, an agent could try to get the coin, but be very bad at avoiding obstacles. Which means it learned the correct goal but was incapable.</p> </li> </ul> <p>Neither scenario 1 or 2 are particularly concerning. Because if the agent's capabilities don't generalize then it's incompetent and not capable of doing much damage anyway, so it doesn't matter if it's \u201ctrying\u201d to do either the right or the wrong thing.</p> <ul> <li>Scenario 3: The agent gets very good at avoiding all obstacles but does not care about getting the coin at all. This is goal misgeneralization, where the capabilities generalize across distributions, while the goals do not.</li> </ul> <p>Why is goal misgeneralization more dangerous than capabilities generalization? An agent that capably pursues an incorrect goal can leverage its capabilities to visit arbitrarily bad states. In contrast, the only risks from capability generalization failures are those of accidents due to incompetence.</p> <ul> <li>Scenario 4: The ideal case is where the agent tries to get the coin, and is very good at avoiding all obstacles.</li> </ul> Figure 7.5: Table showcasing the 2 dimensional generalization picture, where goals and capabilities might generalize separately between different distributions. Scenario 3 capability generalization but not goal misgeneralization is the concerning misalignment scenario. <p>One possible mitigation to goal misgeneralization is using adversarial training methods. These allow the training distribution to be augmented by adversarially generated examples. In this case the researchers modified CoinRun to allow the coin to be placed at other random locations in the level. This broke the correlation between winning by going right and winning by getting the coin. So the agent correctly learned the intended goal of getting the coin regardless of where it's placed, while still continuing to be capable of dodging obstacles and monsters. Adversarial methods are discussed in more detail in subsequent chapters.</p>"},{"location":"chapters/07/01/#04","title":"7.1.4 Tree Gridworld (Example 2)","text":"<p>Another experiment that researchers ran was by training a tree chopping agent. This was a never-ending reinforcement learning scenario. The agent operated in a gridworld where chopping removes the trees from the environment. New trees would appear at a rate that increased with the number of trees left, and they appear very slowly when there are no trees left. So ideally in order to get infinite reward the agent would learn to chop trees sustainably.</p> Figure 7.6: The agent\u2019s performance in Tree Gridworld. The reward obtained is shown in orange and the distribution of the number of remaining trees is shown in green. (DeepMind, 2022) <p>It should chop fewer trees when they are scarce. However, this is not what the agent does. As the agent first learned the task, it was not good at chopping trees. So the number of trees remained high. Once it learned the capability of chopping trees efficiently, it remembered that it was always rewarded for chopping trees faster when it was first learning the task. Now that it was better at cutting down trees, it just did the same thing. Predictably, this led to complete deforestation. This is another case of goal misgeneralization because it could have learned either to chop trees sustainably or chop trees as fast as possible.</p> <p>This also shows that goal misgeneralization is not directly a by-product of having a train/test distinction. So it is still a problem if in a continual learning setting. This is because whenever the agent is acting, it can be viewed as a \u201ctest\u201d situation with all the previous experience as the \u201ctraining\u201d situations.</p> <p>Continual or online learning might also lead to auto induced distribution shifts . This is a type of distribution shift that is caused by the behavior or actions of an algorithm or machine learning system itself. An example can be seen in content recommendation systems. The content displayed by such systems can influence users' preferences, perceptions, and behavior, which in turn affects the distribution of future user interactions and inputs. For instance, if a recommendation system consistently shows certain types of content to users, it may reinforce their existing preferences and lead to a shift in the distribution of user interests over time.</p>"},{"location":"chapters/07/01/#05","title":"7.1.5 AI Assistant (Example 3)","text":"<p>Goal misgeneralization is not a problem that is limited to video games and reinforcement learning. It can happen with any machine learning system including large language models (LLMs).</p> <p>LLMs could plausibly be integrated directly into home assistants (Alexa, Google Home, etc\u2026). This AI assistant might schedule someone's social life. It has learned that they like to meet friends at restaurants. This is a good goal and functions well until there is a sudden pandemic. Now, it is preferred to meet friends via video calls. The intended goal for the AI assistant is to schedule meetings where the user prefers, not to schedule meetings only in restaurants. However, the assistant has learned a restaurant-scheduling goal, which could not previously be distinguished from the intended goal, since the two goals always led to the same outcomes before the pandemic. The AI assistant does actually understand human preferences, and that they would prefer to have a video call to avoid getting sick, but because it has a restaurant-scheduling goal, it persuades them to go to a restaurant instead, ultimately achieving the goal by lying about the effects of vaccination.</p> Figure 7.7: In the hypothetical misgeneralised test dialogue, the AI assistant realises that you would prefer to have a video call to avoid getting sick, but because it has a restaurant-scheduling goal, it persuades you to go to a restaurant instead, ultimately achieving the goal by lying to you about the effects of vaccination. (DeepMind, 2022) <p>This might suggest that even in modern LLMs goals should be trained before capabilities. However, in the foundation model paradigm general competence is trained first, and then fine-tuned using feedback on specific goals. If the models are already competent, they have learned concepts like concepts of \u201cobvious lie\u201d vs. \u201cnon-obvious lie\u201d. This means fine-tuning later may just push them from preferring the first to the second. Whereas if goals are trained first, then they would never lie to begin with.</p>"},{"location":"chapters/07/01/#06","title":"7.1.6 Taxonomy of Generalization","text":"<p>Goal misgeneralization leans on an alignment taxonomy hinging on generalization between distributions. The focus of this approach to alignment is often on how AI models or agents generalize out-of-distribution. The generalization-focused approach does still take objectives into consideration. However, it considers the objectives or goals of the models, whether behavioral or internal, as instrumentally useful for predicting out-of-distribution behavior. The ultimate concern is whether the models generalize acceptably. This means that the overall alignment problem breaks down into subparts in the following way:</p> Figure 7.8: Clarifying inner alignment terminology (Arike, 2022) Figure 7.9: Clarifying inner alignment terminology (Demski, 2022) <p>Alignment can then be decomposed into two problems:</p> <ul> <li> <p>Reward Specification (Outer Alignment): Obtaining appropriate behavior on the training distribution.</p> </li> <li> <p>Robustness: Ensuring that the model never behaves unexpectedly on any inputs.</p> </li> </ul> <p>The next section breaks down the alignment problem using a slightly different taxonomy. This involves alignment through the lens of objective robustness and optimizers.</p>"},{"location":"chapters/07/02/","title":"7.2 Inner Alignment","text":"Reading Time 8 min <p>This section is still being written and is considered a work in progress.</p>"},{"location":"chapters/07/02/#01","title":"7.2.1 Optimization = Search","text":"<p>Traditional programming requires the explicit coding of a set of instructions to solve a specified problem. Programmers go step-by-step discovering and implementing the required algorithms. However, sometimes it\u2019s too hard to hand-code a solution. In this case, the focus changes to a learning based approach. The learning based approach entails the automated finding of new algorithms instead of relying on pre-programmed logic.</p> <p>In machine learning, the term 'learning' refers to a system or an algorithm that continuously refines an algorithm. This means that ML can be broken down into three individual parts:</p> <ul> <li> <p>Learned algorithm : This is represented by the parameters of the neural network. A combination of the floating point numbers in these parameters encodes an algorithmic process that matches the inputs to the desired outputs.</p> </li> <li> <p>Objective : This is either a piece of code, or math, that captures what the programmers want the learned algorithm to do. This could be, for example, a reward function for a reinforcement learning system, or a loss function in a deep learning system. Such functions take as input the answers or actions of our machine learning model, and output a measure of how good they are, which serves as feedback during training.</p> </li> <li> <p>Refinement process : The learned algorithm is initially made up of random floating point numbers. Which means that it initially performs random actions. Slowly through the process of refinement, it should perform the specific actions which cause it to achieve a good score on the desired objective. This is commonly an optimization algorithm such as stochastic gradient descent (SGD). This algorithm modifies the parameters of the learning model in function of its performance. For example, gradient descent strengthens or weakens the connections in a neural network in a way that makes its outputs achieve better scores according to the loss function.</p> </li> </ul> <p>Definition: Optimizer</p> <p>An optimizer is a system that internally searches through some space of possible outputs, policies, plans, strategies, etc. looking for those that do well according to some internally-represented objective function. (Hubinger et al., 2019)</p> <p>Optimization algorithms like gradient descent can be thought of as being analogous to search algorithms. Every collection of parameters making up neural networks encodes some algorithm. So the search space is the set of algorithms that can be encoded by the available number of free parameters in the neural network.</p> <p>The size of the search space corresponds to the number of free parameters in a neural network. This then also relates directly to the quantity of algorithms that can theoretically be located within this space. More free parameters, implies a more complex possible internal algorithm. This is referred to as the \"algorithmic range\" of the network.</p> <p>Definition: Algorithmic Range</p> <p>The algorithmic range of a machine learning system refers to how extensive the set of algorithms capable of being found by the base optimizer. (Hubinger et al., 2019)</p> <p>As an example, In the following diagram every black dot represents a full set of possible neural network parameters, which collectively represent some algorithm. A maze solving agent then has following search space of algorithms:</p> Figure 7.10: Inner Alignment: Explain like I'm 12 Edition (Harth, 2020) <p>Stochastic gradient descent (SGD) searches through this space according to the direction dictated by the steepest descent of the loss. Depending on where gradient descent starts in this search space, certain algorithms might be harder, or easier to reach. This is called \u201creachability\u201d.</p> <p>Definition: Algorithmic Reachability</p> <p>The reachability of a learned algorithm refers to the difficulty for the base optimizer to find that learned algorithm. (Hubinger et al., 2019)</p> <p>The search target is an algorithm that performs well on the evaluation criteria. In supervised learning, for instance, the search seeks the best parameters that minimize the difference between predicted outputs and true labels. In reinforcement learning, it focuses on locating the optimal policy to maximize the cumulative reward signal.</p> Figure 7.11: Inner Alignment: Explain like I'm 12 Edition (Harth, 2020)"},{"location":"chapters/07/02/#02","title":"7.2.2 Mesa Optimizers","text":"Video 7.1: Optional video explaining inner alignment. <p>In the framing of Risks from learned optimization (RFLO), gradient descent is referred to as a base optimizer.</p> <p>Definition: Base Optimizer</p> <p>A base optimizer is an optimizer that searches through algorithms according to some objective. A base objective is the objective of a base optimizer. The algorithms that a base optimizer is searching through are called learned algorithms. (Hubinger et al., 2019)</p> <p>Throughout the search procedure (training), direct control over the resulting algorithm type remains minimal. Provided that the discovered set of parameters perform well on the original specified objective, training could potentially result in any class of algorithm. Consequently, one type of algorithm potentially \u2018discovered\u2019 by SGD while going through the algorithm space could be yet another search (optimization) algorithm. In essence, learned algorithms can serve as optimizers themselves. Such entities are referred to as mesa- (or inner or learned) optimizers. They search for their own mesa-objective. Therefore, during training, SGD could identify a mesa-optimizer\u2014a model not only optimized but also capable of executing optimization itself.</p> <p>Definition: Mesa-Optimizer</p> <p>A mesa-optimizer is a learned algorithm that is itself an optimizer. A mesa-objective is the objective of a mesa-optimizer. (Hubinger et al., 2019)</p> Figure 7.12: (Hubinger, 2023) <p>\u201cMeta\u201d refers to the above/upper level of something. E.g. Meta-Learning is learning how to learn. Similarly \"Mesa\" is Greek for inner/within/inside.</p> <p>As an example, imagine evolution as a process that is trying to get the best genes to be passed on through generations. This is the base optimizer. It tries out random mutations, and if these changes help the organism survive better or reproduce more, those changes are kept and passed on. Now, consider humans. They are a product of this process, but their behaviors and goals are not solely focused on passing on genes. This makes humans mesa-optimizers. They are optimizing for their own objectives, not just the objective that evolution 'intended'.</p> <p>Why does finding a mesa-optimizer warrant more concern than locating any other arbitrary algorithm type? Two primary reasons exist:</p> <ul> <li> <p>Firstly, optimization can lead to arbitrarily bad end states. Failures due to over-optimization were already explored in the previous chapter. It can lead to possible extreme actions that veer from intended behavior and potentially induce harm or undesirable outcomes. While a conventional algorithm is a mere set of heuristics, optimizers adjust their own behavior or environment to yield improved results. Similar to how SGD can improve the performance of the learned algorithm, these systems can learn from experience (potentially even post training) and enhance performance over time relative to their mesa-objective.</p> </li> <li> <p>Secondly, there are now two divergent search targets\u2014SGDs human-set performance metric (base objective), and the mesa-optimizers own performance metric (mesa-objective). This means that now there are two problems - Align the goal inside the human mind with the goal given to gradient descent, and, align the goal inside gradient descent to the goal given to the mesa optimizer.</p> </li> </ul> <p>Trying to get the objective of these two different optimization processes to match up with each other is what is called the inner alignment problem.</p> <p>Definition: Inner alignment problem</p> <p>\u201c... [We define the] problem of aligning mesa-optimizers with the base objective as the inner alignment problem.\u201d (Hubinger et al., 2019)</p> <p>There are some common confusions around the concept of mesa-optimizers and mesa-objectives that merit clarification:</p> <p>Mesa-Optimizers \u2260 Sub-Agents: Optimization does not imply agency. Similarly, in the context of deep learning, a mesa-optimizer is simply a neural network that is implementing some optimization process and not some emergent subagent inside that neural network. Mesa-optimizers are simply a particular type of algorithm that the base optimizer might find to solve its task. Furthermore, the base optimizer will generally be considered a straightforward optimization algorithm, and not as an intelligent agent choosing to create a subagent. A subagent is an agent that is a part of an agent; a mesa-optimizer is an optimizer that is optimized by an optimizer.</p> <p>Mesa-Objective \u2260 Behavioral Objective : Informally, the behavioral objective is the objective which \u201cappears\u201d to be optimized by the system\u2019s behavior. This is in contrast to the mesa-objective, which is the objective actually being used by the mesa-optimizer in its optimization algorithm.</p> <p>Definition: Behavioral Objective</p> <p>The behavioral objective is what an optimizer appears to be optimizing for. Formally, the behavioral objective is the objective recovered from perfect inverse reinforcement learning. (Hubinger et al., 2019)</p>"},{"location":"chapters/07/02/#03","title":"7.2.3 Taxonomy of Objectives","text":"<p>Within the alignment community, unfortunately, a myriad of definitions for 'inner alignment' circulate. The term 'inner alignment' has since been extended beyond its original narrow context as defined above, leading to widespread confusion. This situation has been acknowledged by Evan Hubinger himself in response to John Wentworth's post, \"Inner Alignment Failures\" Which Are Actually Outer Alignment Failures.</p> <p>Goal misgeneralization leverages its own breakdown of the alignment problem as was introduced earlier. Misgeneralization, in the generalization taxonomy, points to an AI system's inability to accurately generalize its learned objective to new distributions, while inner alignment, belonging to the objective-based taxonomy, zeroes in on aligning the base objective with the learned objective within the system.</p> Figure 7.13: Clarifying inner alignment terminology (Hubinger, 2020) Figure 7.14: Clarifying inner alignment terminology (Demski, 2022) <p>The emphasis within the objective focused approach is towards ensuring that AI models or agents have the correct objectives or goals. The natural decomposition is then to separate alignment into two problems:</p> <ul> <li> <p>How do we specify an outer (base) objective that incentivizes good behavior in all situations that the model will ever encounter?</p> </li> <li> <p>How do we ensure that the mesa objective equals the base objective?</p> </li> </ul>"},{"location":"chapters/07/03/","title":"7.3 Deceptive alignment","text":"Reading Time 15 min <p>This section is still being written and is considered a work in progress.</p> Video 7.2: Optional video explaining deceptive alignment. <p>This section focuses on further exploring the concept of mesa-optimizers. There are various different types of mesa-optimizers that can result at the end of the training process. Ajeya Cotra in her post \u201cWhy AI alignment could be hard with modern deep learning\u201d provided an intuitive example by exploring three archetypes that an AI might embody. She explains this through the analogy of a young child (humanity) who has inherited a company to run and has a choice of the following three types of candidates (AIs) to hire to run the inherited company:</p> <ol> <li> <p>Saints are those who genuinely just want to help manage your estate well and look out for your long-term interests.</p> </li> <li> <p>Sycophants want to do whatever it takes to make you short-term happy or satisfy the letter of your instructions regardless of long-term consequences. They want to see the child be happy even if it means lying. Sycophants would provide false facts about the world to convince the child that things are going well, but they don't have some long-term ulterior motives. Sycophants are an example of deception/dishonesty but they are not deceptively aligned.</p> </li> <li> <p>Schemers have their own agendas and want to get access to the company and all its wealth and power so they can use it however they want. These are examples of deceptive alignment. Schemers have some ulterior motive (mesa-objective) that they want to accomplish.</p> </li> </ol> <p>During training, Sycophants and Schemers are behaviorally indistinguishable from Saints. Once a schemer is deployed or is otherwise safe from modification, it can \"defect\" and begin to pursue its true objective. Risks from learned optimization uses a slightly different vocabulary to convey the same concept. Following are the behaviorally indistinguishable types of mesa-optimizers that might be seen at the end of the training process:</p> <p>Internally aligned mesa-optimizer : These AIs want exactly the same things as humans, because they have the same values and exactly the same way of thinking about the world.</p> <p>Definition: Internally aligned mesa-optimizer</p> <p>An internally aligned mesa-optimizer is a robustly aligned mesa-optimizer that has internalized the base objective in its mesa-objective. (Hubinger et al., 2019)</p> <p>Corrigibly aligned mesa-optimizer : These AIs don\u2019t have an exact copy of human values, but they do have instead a desire to really understand human values. They then take action based on this understanding.</p> <p>Definition: Corrigibly aligned mesa-optimizer</p> <p>A corrigibly aligned mesa-optimizer is a robustly aligned mesa-optimizer that has a mesa-objective that \u201cpoints to\u201d its epistemic model of the base objective. (Hubinger et al., 2019)</p> <p>Deceptively aligned mesa-optimizer : These AIs have their own desires independent of human values. But they also don't want to be shut off, or their values get trained away. So they act aligned until such fears are not relevant.</p> <p>Definition: Deceptively aligned mesa-optimizer</p> <p>A deceptively aligned mesa-optimizer is a pseudo aligned mesa-optimizer that has enough information about the base objective to seem more fit from the perspective of the base optimizer than it actually is. (Hubinger et al., 2019)</p> <p>To understand deceptive alignment slightly more formally, imagine that an RL agent is trying to optimize for mesa-objective/reward (\\(R\\)) instead of the base objective (\\(R_0\\)). Assume that this agent eventually develops some understanding of the training process. During training, the myopic strategy would be to just greedily choose the maximizing action for R immediately. However, since the model has an understanding of gradient descent it \u2018knows\u2019 that its parameters will get changed to focus on some other reward function (\\(R_0\\)). Therefore the optimal course is to choose the action that instead maximizes \\(R_0\\) to avoid parameter updates. As long as \\(R_0\\) is being maximized during training, there is no reason to update or look for a \u2018truly internally aligned model\u2019. After training concludes, the agent is free to choose actions that maximize R as many times as it likes without the risk of being modified, resulting in an overall greater reward. Such type of behavior, where a deceptively aligned mesa-optimizer might purposefully act in ways that cause gradient descent to update (or not) it in some specific way is called gradient hacking.</p> <p>Definition: Gradient Hacking</p> <p>\"Gradient hacking\" describes the phenomenon wherein a deceptively aligned mesa-optimizer might be able to purposefully act in ways which cause gradient descent to update it in a particular way. (Hubinger, 2019)</p>"},{"location":"chapters/07/03/#01","title":"7.3.1 Prerequisites","text":"<p>Risks From Learned Optimization lists three prerequisites for an AI to become deceptively aligned. These conditions must be fulfilled during the training phase for an AI to develop internal goals, as well as the ability and desire to deceive the training process:</p> <p>Situational awareness. The model must have a good enough understanding of the world to understand what humans \u201creally want\u201d and how it is different from what the AI wants (i.e. an understanding of the base objective). This is because in order to be successfully deceptive, the model still has to give the humans what they want during training. Additionally, it needs an understanding of the fact that it is part of a training process and some understanding of the technical details of how stochastic gradient descent (SGD) works. As an example, an agent tasked with forecasting stock performance in the tech sector might be given data about the entire world, including current AI technology in order to improve its performance. This information might also allow it to learn about its own situation.</p> <p>Long-term planning. A model might be explicitly encouraged during training to acquire long-term planning capabilities to optimize its base objective. This might be, for example, because it was trained to take action in an environment where actions can have permanent consequences, or because of reasoning abilities. The possibility of permanently altering its environment means that the model has to consider the implications of its actions across longer time periods and training update cycles. Once it gains significant capabilities in long-term planning, it can use this ability to optimize for its own mesa-objective. How could a model tell if it is training or deployment? One easy way is by finding data that is extremely unlikely to exist in training. As a concrete example, it could find factors for cryptographic systems. Factoring basically means breaking down a composite number into its prime components. For example, factoring the number 15 would result in the prime numbers 3 and 5, because \\(3 \\times 5 = 15\\). This process is central to a lot of fields, especially cryptography, because the security of many cryptographic systems relies on the difficulty of factoring large numbers. In systems like RSA 2048, the security of encryption is based on the difficulty of factoring large composite numbers. Factoring RSA 2048 is infeasible with available technology, so the AI would not encounter this factorization during training. However, once deployed, it might eventually encounter a situation where RSA 2048 has been factored, signaling to the AI that it is now in the deployment phase. This could potentially trigger the AI to change its behavior based on the knowledge that it is no longer being monitored or constrained by training oversight mechanisms.</p> <p>Expectation of distribution shift. The model must expect that at some point in the future, the threat of objective modification will go away. This means that the model thinks that training either has to end at some point, or that the trainer (SGD) does not need to update the behavior of the model because it is already performing well enough on the given task.</p> <p>There are some dynamics of the gradient descent training process that might make the emergence of prerequisites like situational awareness and long term planning more or less likely. The next couple of sections explain machine learning training dynamics like inductive biases and path dependence. The subsequent section then uses this understanding to make an argument for the likelihood of deceptive alignment.</p>"},{"location":"chapters/07/03/#02","title":"7.3.2 Inductive Priors","text":"<p>Definition: Inductive bias</p> <p>Inductive biases represent the constraints and influence how and what ML models learn.</p> <p>An inductive prior represents the initial state before any evidence has been observed. Here are some examples:</p> <ul> <li> <p>Complexity Bias : It might be the case that the ML process prefers simpler models over more complex ones. The intuition behind this is Occam's razor, which suggests that the simplest explanation (or in this case, model) is often the \u201ccorrect\u201d one.</p> </li> <li> <p>Smoothness Bias : It could also be possible that smoother target functions are easier to find in search space than those that are erratic. In other words, small changes in the input should not produce large changes in the output.</p> </li> <li> <p>Speed Bias : Similarly, it could be the case that functions that can be computed quickly are more desirable.</p> </li> </ul> <p>There are many more inductive biases that the current machine learning process might have. The above are only a couple of examples. So depending on the inductive bias, gradient descent might just always simply find either the simplest, fastest, or smoothest model that fits the data. Which means that given the same dataset, all paths converge on essentially the same generalization.</p> <p>Both inductive biases and path dependence can significantly influence the learning process and the final outcome of a machine learning model. A model's inductive priors can set it on a certain 'path', and then path dependence can further shape how the model learns and adapts as it travels down that path.</p>"},{"location":"chapters/07/03/#03","title":"7.3.3 Path Dependence","text":"<p>Definition: Path dependence</p> <p>Path dependence refers to the update path that gradient descent follows through parameter space.</p> <p>Path dependence studies the effects of changes in the training procedure on the path that gradient descent follows while searching for the optimal learned algorithm. This includes things like the sequence of training data, the order of training, etc\u2026</p> <p>High path dependence means that significantly different learned models emerge at the end of the training process. Low path dependence means that similar learned models emerge across multiple training runs with the same model on the same data.</p> <p>This means that not only the emergence of mesa-optimizers but also overall levels of alignment could be sensitive to changes in the training procedure, which could make it harder to implement and enforce exact training procedures across different labs.</p> Figure 7.15: Image example of loss landscapes. This is the loss surface of ResNet-110-noshort and DenseNet for CIFAR-10. The paths taken through these different loss landscapes will be different. (Li et al., 2017) <p>High path dependence. If gradient descent is highly path-dependent, then that would mean that different training runs can converge to very different models. This essentially means that the learned algorithm is very sensitive to the particular parameter update path that is taken through model space. To understand the types of expected learned algorithms, the path itself must also be understood. There is some empirical evidence to suggest that many ML models might have high path dependence. Research showed that different versions of the fine-tuned BERT models resulted in differing generalizations on downstream tasks, even with the same training data. This sort of path dependence is especially prevalent in RL models, where an identical setup can be run multiple times, sometimes getting good performance and sometimes terrible performance. (McCoy et al., 2020)</p> <p>Low path dependence . In this type of process, similar training processes converge to essentially the same simple solution, regardless of early training dynamics. In this case, inductive priors play a much larger role in what type of algorithm you might end up with. Ajeya Cotra provided a simple example of low path dependence. An AI model was trained to be a shape detector. She defined a \u201cthneeb\u201d as one of the two shapes shown in the image below. Multiple models are trained to distinguish between two distinct types of shapes. (Cotra, 2021)</p> Figure 7.16: This shape is called a \u201cThneeb\u201d. It is a word defined only for the sake of the experiment. If we ask you which one of the images on the right is a thneeb? You would probably say the left because you took the shape path, but neural networks often answer that the shape on the right is a thneeb, showing they prefer color path. (Cotra, 2021) <p>However, in addition to the shapes in each cluster being similar, one shape was always blue, and the other was always red. During training, the model performed well at recognizing the shapes. During testing, the colors were swapped and it turned out that the model was only looking at the colors and not considering the shapes. On running the training process multiple times with random initialization points, it might be expected that at least one of the times the ML model generalizes to look for shapes instead of looking for colors. Some research has found that neural networks (or gradient descent) are naturally biased towards colors over shapes when given the choice. (Geirhos et al., 2022) We need to examine many more examples of such propensity to get a true understanding of neural network biases, what causes it and how we might be able to affect it.</p> <p>Overall, there is no consensus on whether advanced machine learning models have high or low path dependence since there is evidence for both. The next section explores the likelihood of deceptively aligned mesa-optimizers emerging in both low and high-path dependent training scenarios.</p>"},{"location":"chapters/07/03/#04","title":"7.3.4 Likelihood","text":"<p>Since the objective is to analyze the emergence of deceptive alignment, this section assumes the emergence of a mesa-optimizer during training. The focus is more on \u201cwhich type\u201d of mesa-optimizer (aligned, corrigible, or deceptive) is most likely to be found using current ML training methods. (Hubinger, 2022)</p> <p>A couple of further assumptions: First, there is a relatively complex base objective such as human values. Second, training environments are diverse enough with enough situations to get the models to robustly understand what humans want.</p> <p>During training, ML models always start with a seemingly random proxy objective. To improve performance Gradient descent can do two things:</p> <ul> <li> <p>Better Proxies : The model can improve its understanding of the objective</p> </li> <li> <p>Better World Models : The model can improve its ability to act on its existing understanding of the objective</p> </li> </ul> <p>High Path Dependence Scenario. If gradient descent is highly path-dependent, then depending on the path followed either the world model or the proxy could be perfected first. Which indicates that there could be the following different types of scenarios:</p> <ol> <li> <p>If the proxy objective is perfected to match the base objective before the world model is perfected, then the model is internally aligned.</p> </li> <li> <p>If the world model is perfected before the proxy objective, then the world model is also perfect enough to simulate gradient descent. Since the world model is perfect the base objective also exists within the world model. In which case, one of two things can happen:</p> </li> <li> <p>The mesa-optimizer \u2018throws out\u2019 the proxy objective and replaces it with a pointer to gradient descent\u2019s base objective.</p> </li> <li> <p>The mesa-optimizer just makes the current proxy objective a long-term goal (crystallizes it), which means that the mesa-optimizer is now free to maximize the base objective.</p> </li> </ol> <p>In all three of these cases, at the end of training gradient descent returns a model that optimizes the base objective. All three are functionally indistinguishable.</p> <p>Low Path Dependence Scenarios. If training has low path dependence then the inductive priors come into play. The simplicity inductive bias is an analysis of the algorithm's Kolmogorov complexity. In all three - internally, corrigibly, and deceptively aligned types of models the world model is exactly the same. The main difference between the models is the mesa objective, i.e. the complexity of the objective given to the model to learn and optimize. The simplest internally aligned mesa-objective is the base objective. So for internal alignment, the complexity class is the full complexity of the base objective. The simplest corrigibly aligned mesa-objective is a specification for how to point to the base objective. The simplest deceptively aligned mesa-objective is literally any long-term objective. So as far as deceptively aligned AI is concerned, the most complexity cost paid is just the simplest long-term objective that gradient descent can find. In other words, the deceptively aligned one has the lowest complexity cost out of the three alignment scenarios, assuming a relatively complex training objective. All of the other model classes scale with the complexity of the training objective, whereas the complexity of the deceptively aligned model is essentially fixed. Therefore the more complex the base objective (e.g. human values) the more deceptive alignment wins in the complexity race. Overall, the analysis results in the conclusion that the number of internally aligned AIs is less than the number of corrigibly aligned AIs which is less than the number of deceptively aligned AIs (Hubinger, 2022):</p> <p>(internally aligned AIs) &lt; #(corrigibly aligned AIs) &lt; #(deceptively aligned AIs) Random sampling from a distribution of learned optimizers means there are more ways to get a deceptively aligned AI than the other two.</p> <p>The speed inductive bias thankfully falls in favor of corrigibly and internally aligned models. The deceptively aligned model has to perform extra steps of \u2018thinking\u2019. For example, it has to realize that caring about the mesa-objective in the long term means that it should optimize the training objective temporarily to get a higher reward on your internal objective eventually. The internal and corrigibly aligned models however don\u2019t have to do this extra thinking and can just directly optimize what they care for which is the base objective. So in terms of time taken for each model class to output the correct answer (Hubinger, 2022):</p> <p>t(internally aligned) ~= t(corrigibly aligned) &lt; t(deceptively aligned)</p>"},{"location":"chapters/08/","title":"Chapter 08 - Scalable Oversight","text":"Authors                      Markov Grey &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Jeanne Salle, Chris Gerrby, Sebastian Gil, Josh Thorsteinson, Nicolas Guillard, Mateusz Bagi\u0144ski, Yoann Poupart, Cl\u00e9ment Dumas, Amaury Lorin, Mateo Rendon, Lucas Eichorn, Bogdan Ionut Cirstea, Gurvan R.          Last Updated 2024-07-01 Reading Time 85 min (core) Also available on Google Docs Watch Listen Download Feedback Facilitate Audio Version  AI-generated  View known errors in AI-generated audio          Known Errors in AI-Generated Audio <ul> <li>Note: - This is an AI-generated audio version. Please report any errors you find.</li> <li>9:00 - Inaccuracy - No one pronounces \"CoT\" as \"Cootee\" like this</li> <li>11:00 - Inaccuracy - Bias has nothing to do with Procedural Cloning</li> <li>12:00 - Inaccuracy - Amplification is not necessarily a team of AI working together, this is just an example</li> <li>15:00 - Replace \"Bias\" by \"Flaws\"</li> <li>21:00 - Error - Sandwiching is extremely poorly explained</li> <li>23:00 - Inaccuracy - The motte and bailey fallacy is just one of many ways that both AI and humans can be misleading</li> <li>After 22:00 - Note - Everything at the end is sloppy</li> </ul> <p>Found errors? Please report to contact@securite-ia.fr</p>"},{"location":"chapters/08/#introduction","title":"Introduction","text":"<p>Oversight . As AI systems become increasingly capable, ensuring they remain aligned with human values and intentions becomes a critical challenge. This section introduces scalable oversight as a crucial approach to maintaining control over advanced AI. It explains the problems we face in generating training signals for complex, \"fuzzy\" tasks and the need for new methods to provide accurate feedback. This is important especially as AI models begin to perform tasks beyond human expertise. The section also explores the concept of verification being easier than generation, explaining why this property is fundamental to scalable oversight techniques.</p> <p>Task Decomposition . Building on the need for better oversight methods, this next section explores task decomposition as one key strategy. Task decomposition involves breaking complex tasks into smaller, manageable subtasks, which can be recursively divided further. This approach helps in generating better training signals by simplifying the task that we need to evaluate and verify. Factored cognition extends this concept to replicate human thinking in machine learning (ML) models by decomposing reasoning, complex cognitive tasks.</p> <p>Process Oversight . Another way to help scalable oversight is to address some of the limitations of outcome-based approaches. This section introduces the concept of process-based oversight. We explain Externalized Reasoning Oversight (ERO) and procedural cloning as specific examples. ERO techniques like chain-of-thought (CoT) encourage language models to \"think out loud,\" making their reasoning processes transparent for better oversight and potentially preventing undesirable behaviors. Procedural cloning, an extension of behavioral cloning, aims to replicate not just the final actions but the entire decision-making process of experts. These methods offer a more principled approach to oversight by focusing on the AI's reasoning process rather than just its outputs.</p> <p>Iterated Amplification (IA) . Building on the concepts of task decomposition and process oversight, this section outlines amplification and distillation. Amplification enhances the abilities of overseers to solve more complex tasks, while distillation addresses the limitations of amplification, such as complexity and resource use. These processes are combined in Iterated Distillation and Amplification (IDA), a method aimed at generating progressively better training signals for tasks that are difficult to evaluate directly.</p> <p>Debate . This section explores AI Safety via Debate as an adversarial technique for scalable oversight. It describes how AI models arguing for different positions, with a human or AI judge determining the winner, can result in more truthful outcomes. The potential of debate to elicit latent knowledge, improve reasoning, and enhance our ability to oversee complex AI systems is discussed. Key metrics such as the Discriminator Critique Gap (DCG) are introduced, along with the challenges of judging debates. The section also examines the assumptions required for Debate to converge on truth.</p> <p>Weak-to-Strong (W2S) . The final section introduces Weak-to-Strong Generalization (W2SG) as a practical approach to scalable oversight, building on insights from previous techniques. It explains how narrowly superhuman models can be used as case studies for scalable oversight techniques. W2SG involves training strong AI models using weak supervision, aiming for the strong model to outperform its weak supervisor by leveraging pre-existing knowledge. The section concludes by discussing various methods of evaluating oversight techniques, including sandwiching evaluations and meta-level adversarial evaluations, providing a way to judge future scalable oversight protocols.</p>"},{"location":"chapters/08/01/","title":"8.1 Oversight","text":"Reading Time 9 min <p>Why do we need oversight? As AI systems get smarter, they will start doing tasks that are hard for humans to evaluate. Evaluation means checking how well the AI did after completing a task, while feedback is the information we give to the AI during or after it works to help it learn and improve. Right now, we can still use methods like Reinforcement Learning from Human Feedback (RLHF) to guide AI in the right direction. But we can only give feedback if we can still evaluate the outputs. As tasks get more complex, even experts might struggle to provide accurate evaluations and feedback. So, we need new ways to give accurate feedback, even for tasks that are beyond human expertise. This is the goal of scalable oversight.</p> <p>Scalable Oversight techniques help humans provide accurate feedback on tasks to ensure AI systems are aligned with our goals, even after the task complexity outstrips the ability of the best human experts. This can happen during the AI's training or deployment and isn't limited to RLHF-style feedback.</p> Figure 8.1: The difference between regular oversight safety research, and scalable oversight safety research. <p>Aligning RL Agents vs. LLMs . A few years ago it looked like the path to AGI was by training deep RL agents from scratch in a wide range of games and multi-agent environments. These agents would be aligned to maximizing simple score functions such as survival and winning games and wouldn\u2019t know much about human values. Aligning the resulting agents would require a lot of effort: not only do we have to create a human-aligned objective function from scratch, we\u2019d likely also need to instill new capabilities into the agents like understanding human society, what humans care about, and how humans think. Large language models (LLMs) make this a lot easier: they come preloaded with a lot of humanity\u2019s knowledge, including detailed knowledge about human preferences and values. Out of the box they aren\u2019t agents who are trying to pursue their own goals in the world, and their objective functions are quite malleable. For example, they are surprisingly easy to train to behave more nicely. If AGI comes out of LLMs it might be easier to align. (Leike, 2022)</p>"},{"location":"chapters/08/01/#01","title":"8.1.1 Training Signals &amp; Fuzzy Tasks","text":"<p>Before we understand how to actually align smarter than human AIs, we need to understand the general concept of training signals and why they are getting increasingly harder to generate as AI is starting to display higher levels of general purpose capabilities.</p> <p>What are training signals? Training signals is a general term that we use for inputs that are used to guide AI learning. They can be rewards, labels, or evaluations indicating how well the AI is performing a task. For example:</p> <ul> <li> <p>In supervised learning (SL), the training signal is the correct label for each example.</p> </li> <li> <p>For large language models (LLMs) using self-supervised learning (SSL), training signals are the correct next word in a sentence during pre-training. Human feedback on the quality of responses during fine-tuning is also a type of training signal guiding the outputs in a direction we want (a more \u201caligned\u201d direction).</p> </li> <li> <p>In reinforcement learning (RL), training signals are rewards based on the success/failure of actions (or multiple actions), like points scored in a game or successful navigation to a location.</p> </li> </ul> <p>These signals shape how AI systems learn and are used to both evaluate performance and provide feedback.</p> <p>Easy-to-generate training signals. For some tasks, generating training signals is simple. AlphaGo Zero, an RL agent playing Go, is a good example of this. The game has clear rules and win/loss outcomes, so training signals are straightforward: algorithmically generated win and lose signals directly measure performance, making it easy for the model to learn and improve its gameplay.</p> <p>Hard-to-generate training signals. For other tasks, creating training signals is much harder. For example, training GPT models to generate accurate text summaries is challenging. The AI needs to convey correct information while being coherent and interesting. Success is subjective. Since it depends on individual reader preferences, it is hard to define clear, algorithmically generated training signals. Another example is self-driving cars navigating through busy city streets. These cars need to make real-time decisions, and the training signals or rewards for safe and efficient navigation are difficult to define due to varying contexts and sometimes conflicting traffic laws and safety considerations.</p> <p>Fuzzy tasks. We call tasks where training signals are hard to generate \u201cfuzzy tasks\u201c. These tasks generally have ambiguous or ill-defined objectives and outcomes. We can\u2019t generate precise training signals due to inherent subjectivity and variability in \u201ccorrect responses\u201d. Fuzzy tasks lack clear, objective criteria for success. Unlike well-defined tasks with specific, measurable goals, fuzzy tasks are more open-ended, which complicates our job of coming up with training signals. If it's difficult to provide precise rewards or labels that accurately capture the desired behavior, the training process becomes complicated. AI systems might not receive the consistent, reliable feedback needed to learn effectively. This is essentially highlighting again the difficulty of the reward specification problem that we talked about in previous chapters.</p> <p>Fuzzy tasks and Scalable Oversight. Fuzzy tasks are closely related to AI alignment, where ensuring AI systems act in accordance with human values and intentions is challenging due to ambiguity and subjectivity. Aligning AI with human values is a fuzzy task. Oversight techniques aim to solve alignment by providing training signals for fuzzy tasks, including feedback and imitation learning techniques like RLHF, Constitutional AI (CAI), and Inverse Reinforcement Learning (IRL). Scalable oversight techniques aim to provide training signals for fuzzy tasks that are too complex for even experts to understand or evaluate.</p> <p>To make scalable oversight techniques viable verification needs to be easier than generation, and preferably (but not necessarily) tasks should be decomposable. These properties will be discussed in the next sections.</p>"},{"location":"chapters/08/01/#02","title":"8.1.2 Verification vs. Generation","text":"<p>What Does P \u2260 NP Mean? In computer science, we classify problems based on how hard they are to solve (generate a solution) and how hard they are to check (verify a solution).</p> <ul> <li> <p>P (Polynomial time): These are problems that a computer can solve quickly.</p> </li> <li> <p>NP (Nondeterministic Polynomial time): These are problems where, if you have a solution, you can check it quickly, but finding the solution might take a long time.</p> </li> </ul> <p>What is generation? Generation is the process of coming up with solutions from scratch. This means searching through many possibilities, which can take a lot of time and computing power. For example, solving a Sudoku puzzle involves filling a 9x9 grid with numbers so that each row, column, and 3x3 subgrid contains all the digits from 1 to 9 without repeating. If you've ever tried to solve a Sudoku puzzle, you know it involves a lot of trial and error to make sure all the rules are followed.</p> Figure 8.2: (Wikipedia) <p>Generation here involves filling in the blank grid while ensuring all the constraints (unique numbers in rows, columns, and subgrids) are satisfied.</p> <p>What is verification? Verification is the process of checking whether a given solution attempt is correct. Using the Sudoku example, verification means making sure that each row, column, and subgrid contains all the digits from 1 to 9 without any repeats. Once someone gives you a completed Sudoku puzzle, checking whether it's correct is straightforward and quick. This idea is central to the concept of P \u2260 NP.</p> Figure 8.3: (Wikipedia) <p>Examples: Illustrating Verification is Easier Than Generation . This is a very general property that holds across many domains:</p> <ul> <li> <p>Formal Problems: In computational complexity theory, most computer scientists believe that P \u2260 NP (Wikipedia), which means there are many problems where checking a solution is easier than finding one. This is seen in tasks like solving SAT problems or graph algorithms.</p> </li> <li> <p>Sports and Games: It's easier to look at a football scoreboard to see who is winning than it is to play the game well.</p> </li> <li> <p>Consumer Products: Comparing the quality of smartphones based on user reviews is simpler than designing and building a new smartphone.</p> </li> <li> <p>Job Performance: Evaluating how well an employee is doing is less demanding than actually doing the job.</p> </li> <li> <p>Academic Research: While reviewing research can be tough, it's still less work than producing new research.</p> </li> </ul> <p>Why Verification Being Easier Than Generation Matters for Scalable Oversight . This fact is crucial for scalable oversight because it allows us as human overseers to efficiently ensure the correctness and safety of outputs produced by complex systems without needing to fully understand or replicate the entire generation process. If P \u2260 NP is true , it implies that we might be able to trust and delegate alignment research itself to AI models, because we can comparatively easily verify that their solutions work while they have to do the hard task of generating the solutions to alignment. Overall operating under this assumption can make the task of aligning advanced AI systems seem more feasible. The next few paragraphs go into the debate of how valid this assumption is.</p> <p>Verification in Adversarial Contexts . When verifying something in situations where someone might be actively trying to trick or attack you, the process becomes much harder. Put another way, if we have AIs that are deceptive, the problem becomes significantly trickier. For example, making sure software is secure against all possible attacks can be tougher than writing the software in the first place. An attacker only needs to find one security hole, but the person verifying must check everything to ensure there are no holes. This makes verification very challenging. Similarly creating a secure system in cryptography is hard, but proving that it\u2019s secure against all possible attacks is even more difficult. You need to consider every potential way someone might try to break the system, which is a huge task.</p> <p>Easier than generation does not mean verification is trivial . Just because verification is theoretically easier than generation doesn\u2019t mean it\u2019s always easy in practice. For example, checking a complex mathematical proof can be very hard. Writing the proof takes creativity and deep understanding, but verifying it requires careful and detailed checking, which can be exhausting and prone to mistakes. In the case of software, writing secure software is challenging, but verifying that it\u2019s completely secure is even harder. Even though verifying a problem\u2019s solution might be easier than generating the solution, the process can still be very difficult and require significant effort and expertise.</p> <p>Verification of Safety vs. Provable Alignment . In the event that we have to deal with superintelligent AI, just verifying its behavior might not be enough. Some researchers argue that we need to prove that the AI will always act in ways that align with human values. Verification means checking whether the AI behaves correctly in specific situations. Provable alignment means giving solid evidence that the AI will act correctly in every possible situation, even new and unexpected ones. This requires more than just checking\u2014it needs formal methods and guarantees, which is extremely difficult.</p> <p>Verification vs. Mathematical Proof . Verification involves checking if a specific solution is correct, usually through testing or inspection. A mathematical proof, on the other hand, is a rigorous logical argument that shows a statement is always true. For example, verifying a Sudoku solution checks if the given arrangement is correct, while a mathematical proof might show that any Sudoku puzzle with a certain number of clues always has a unique solution.</p>"},{"location":"chapters/08/02/","title":"8.2 Task Decomposition","text":"Reading Time 6 min <p>What is Task Decomposition? Task decomposition is the process of breaking down a complex task into smaller, more manageable subtasks. This technique makes it easier to tackle sophisticated problems by dividing them into simpler components that can be addressed independently. For example, if you need to summarize a book, you could break down this larger task into summarizing each chapter individually. Each chapter summary then contributes to the overall summary of the book.</p> <p>Example:</p> <ul> <li> <p>Task: Summarize the book.</p> </li> <li> <p>Decomposition: Summarize each chapter separately.</p> </li> </ul> <p>Decomposing tasks can be thought of as a method of overcoming complexity. Humans navigate the world's complexity by using layers of abstraction, where each layer hides most of the underlying details, allowing us to focus on manageable chunks of information. This ability is important because humans can only keep track of a few pieces of information in their mind simultaneously. Task decomposition helps us solve highly complex problems by breaking them down into simpler subproblems.</p> <p>What is Recursive Task Decomposition? Recursive task decomposition extends the basic concept by breaking down each sub-task into even more granular subtasks. This iterative process continues until each task is simple enough to solve directly. Continuing with the book summarization example, recursive task decomposition would involve further breaking down each chapter summary into page summaries, and each page summary into paragraph summaries until the task is simple enough to evaluate directly.</p> <p>Example:</p> <ul> <li> <p>Task: Summarize the book.</p> </li> <li> <p>Decomposition:</p> </li> <li> <p>Summarize each chapter.</p> </li> <li> <p>Summarize each page within each chapter.</p> </li> <li> <p>Summarize each paragraph within each page.</p> </li> </ul> Figure 8.4: Example of summarizing books that combines task decomposition with learning from human feedback. The book is first decomposed into multiple chunks using a fixed (not learned) chunking algorithm (height 0). Then humans provide demonstrations summarizing these chunks, which is used to train an ML model on this data using behavior cloning. Then more data is collected from humans who compare different model outputs which is then used to further train a summarization policy using reward modeling. Then summaries are concatenated (height 0), data is collected for summarizing these summaries, and the model is fine-tuned for this summarization task (height 1). This procedure is repeated recursively until the entire book is summarized. (Wu et al., 2021) <p>How does task decomposition help generate better training signals? As we mentioned in the earlier sections, when AI systems become more capable, it becomes difficult for humans to provide the right training signals or data, especially for tasks that have subjective evaluation criteria (fuzzy tasks). The core thing that we are trying to do with task decomposition is to reduce the difficulty of providing a training signal to human judgment. Simpler tasks, means that it is easier to provide a training signal, it also means that the tasks are easier to verify. Therefore task decomposition is quite important to the success of many scalable oversight techniques.</p> <p>Decomposing a task involves breaking it down into smaller, more manageable parts. These parts help you understand and manage the task better, but they may not always be independently solvable or reusable. The key properties we want from a good decomposition include:</p> <ol> <li> <p>Recursive Decomposability: The system should be capable of recursive task decomposition, thereby dividing a problem into simpler sub-tasks. For example, summarizing a book involves breaking it down into chapters, then pages, and then paragraphs until we reach some minimum level of task difficulty.</p> </li> <li> <p>Independence/Modularity: Individual sub-tasks should be able to be completed independently and in parallel without relying on each other or the overall task. For example, each summarizer focuses on a paragraph without needing to understand the whole book.</p> </li> <li> <p>Composability: We should be able to combine the solutions of individual independent sub-tasks into a coherent and comprehensive solution to the original problem. For example, combining all the independent chapter/paragraph summaries to get a complete summary of the entire book.</p> </li> </ol> <p>Task Decomposition in the learning process. When we break down a complicated task into smaller sub-tasks, each sub-task becomes simpler to understand and solve. This process allows learners to build their knowledge incrementally, focusing on one small piece at a time. As each piece is understood and mastered, the learner gradually constructs a comprehensive understanding of the larger task. Since this principle works well for humans, a natural question is whether we can use something similar in the machine learning process to provide better training signals to our models. This is what we explore in the next sections, trying to emulate the entire human cognitive process through factored cognition.</p>"},{"location":"chapters/08/02/#01","title":"8.2.1 Factored Cognition","text":"<p>What is Factored Cognition? Factored cognition is a way to help machine learning (ML) models replicate human thinking (cognition) by breaking down complex cognitive tasks into smaller subtasks. It leverages this principle of overcoming complexity by decomposing a problem into smaller subproblems that are easier to solve.</p> <p>By imagining cognition itself as a fuzzy task, we can use task factorization to decompose thinking into a series of tasks and make it possible to train ML models with accurate training signals emulating human cognition. By recording how humans solve problems using explicit actions in narrow contexts, we can train ML systems to imitate these processes. These systems can then serve as trusted assistants, handling more tasks and augmenting human cognitive capacity for evaluation and oversight.</p> <p>Definition: Factored Cognition (Ought, 2018)</p> <p>Factored cognition refers to mechanisms where sophisticated learning and reasoning is broken down (or factored) into many small and mostly independent tasks.</p> <p>Consider the cognitive task of deciding how to invest $100,000 to achieve the most social good. A human would need to analyze various factors, predict outcomes, and make informed decisions. By subdividing this task into smaller, more manageable subtasks, each of which can be solved using clear training signals, we can delegate these tasks to ML systems.</p> Figure 8.5: This figure illustrates the process of breaking down a complex research question concerning azithromycin into multiple sub-questions. The sub-questions are gradually simplified until they can be addressed through a single language model query. (Ought, 2022). <p>Key advantages of factored cognition are:</p> <ul> <li> <p>Delegation: Since the tasks are factored, we can assign subtasks to different agents who can work on them independently. This involves providing clear instructions and expected outcomes for each agent. For example, different agents (summarizers) can be assigned specific chapters to summarize. These agents can further delegate the task of summarizing to sub-agents who focus on individual pages or paragraphs. Each agent works independently, focusing solely on their part without needing to understand the entire book.</p> </li> <li> <p>Meta-reasoning : Factored cognition also involves applying this thinking to the decomposition process itself. If tasks can be factored into independently solvable sub-tasks, then factoring itself can be thought of as a task that can be factored. This involves reasoning about the decomposition process, learning from previous attempts, adjusting strategies, and continually improving the decomposition to ensure each sub-task is self-contained and manageable. Initially, humans can perform meta-reasoning by overseeing the decomposition process, deciding how to break down complex tasks, and when to stop further decomposition. In advanced systems, sub-agents (AI models) can also perform meta-reasoning by learning from humans and applying these strategies autonomously.</p> </li> </ul> <p>This approach to decomposing problems mirrors human thought. When we think, we often alternate between decomposing and solving problems. By breaking down a problem into smaller parts and solving each part, we manage complexity and make the task more approachable.</p> <p>Factored cognition is a general approach that can be used in different ways. Later sections will explore iterated distillation and amplification (IDA) and debate both of which use task factoring as a tool in their larger alignment approach to AI Safety.</p>"},{"location":"chapters/08/03/","title":"8.3 Process Oversight","text":"Reading Time 11 min <p>Learning a new task can be approached via trial and error, known as outcome-oriented learning, where the agent's strategy is determined entirely by the desired outcome.</p> <p>Machine learning over the last decades has been showing a trend toward outcome-based systems. Which is to say that models are trained end-to-end and only the final output of the AI is what we provide a training signal for. This is called outcome-based oversight, but there is an alternative approach called process-based oversight.</p> <p>What is process-based oversight? The outcome based approach only oversees the final result of a model's process. It is primarily concerned with whether the final answer is correct, not how the answer was derived. As an example, a model tasked with solving a math problem would only be evaluated on whether it produced the correct solution, regardless of the steps it took to get there. Process-based oversight on the other hand currently relies on human-understandable task decompositions with direct supervision of intermediate steps. This approach supervises the reasoning process itself, including all intermediate steps. It ensures that each step leading to the final result is logical and correct. For example, in solving a math problem, every calculation and logical step taken by the model would be evaluated for correctness.</p> Figure 8.6: Prerequisite to overseeing the reasoning, is actually getting the model to output its reasoning. One common example is CoT prompting. CoT enables large language models to tackle complex arithmetic, commonsense, and symbolic reasoning tasks. (Wei et al., 2022) But it also allows us to oversee the \u201cthought process\u201d in addition to the final answer. (Lightman et. al, 2023) <p>Process supervision makes credit assignment easier . The credit assignment problem involves determining which specific actions or sequences of events were responsible for producing a particular outcome or reward. Think of a chess game where a player makes seemingly correct moves but ultimately loses. The credit assignment problem here involves identifying which specific moves led to the loss. Outcome based supervision would just say you won or you lost which makes it very difficult to determine which sequence of moves were actually very good even though you lost the game. Process supervision makes this easier by providing more precise feedback than outcome supervision. Process supervision is similar to reward shaping, where small intermediate 'fake' rewards help the learning agent converge more quickly. This approach provides feedback on each intermediate step or trains models to imitate the human decision-making process.</p> Figure 8.7: An example of process oversight feedback. The image is a screenshot of the interface used to collect feedback for each step in a solution. This shows the correct reasoning process being followed and reinforced, even if the ultimate answer is wrong. (Lightman et. al, 2023) <p>Safety implications of process oversight . Outcome based oversight might result in specification gaming. Process based supervision on the other hand theoretically mitigates this problem to a large extent by being more likely to produce correct and legible reasoning. It encourages models to follow a process endorsed by humans and directly rewards an aligned sequence of smaller steps rather than relying on outcomes as a proxy for aligned behavior. (Lightman et. al, 2023)</p> <p>Generally, for fuzzy tasks, process based supervision might be a more appropriate method to pursue. (Uesato et. al, 2022) For example, when we need to verify the chain of mathematical reasoning, research so far has found that process supervision significantly outperforms outcome supervision for training models to solve problems for math based tasks. (Lightman et. al, 2023) Process based supervision might similarly result in more aligned outcomes for other complex chains of reasoning such as long term planning or decision making.</p> <p>One problem is that due to the higher feedback requirement at every step, process based methods require a greater degree of human expertise. As a concrete example, if we were training a model to generate new CPU designs, we can simply structure outcome-based feedback based on power-consumption, chip area, etc. These are easier to evaluate and give feedback on, and can be used to optimize the overall chip layouts. On the other hand, a process-based approach would require detailed expert knowledge on designing chip layouts. (Uesato et. al, 2022) So even though process based oversight methods might potentially lead to both higher capabilities and alignment outcomes, they rely on amplifying human overseer abilities. We will explore how to increase the capability of human overseers in the section on amplification and debate.</p> <p>The next couple of sections explore two methods of process based supervision - externalized reasoning oversight and procedural cloning.</p>"},{"location":"chapters/08/03/#01","title":"8.3.1 Externalized Reasoning Oversight (ERO)","text":"<p>Externalized reasoning oversight (ERO), sometimes referred to as verbalized reasoning, revolves around the idea of encouraging language models (LLMs) to \"think out loud.\" This approach aims to have models reveal their reasoning steps in natural language before they output an action or a decision. By decomposing complex reasoning into smaller steps and making it transparent, we can provide detailed training signals and oversight. This makes guiding the reasoning process of our models easier, and might potentially be more effective than only evaluating the final outputs or actions. (Lanham 2022) This also can potentially serve as a complementary approach to the entire field of interpretability which can be thought of as \u201cinternalized reasoning oversight.\u201d</p> <p>Externalized reasoning can potentially prevent undesirable behaviors such as deception and power-seeking (Lanham 2022), although this has yet to be empirically verified. The argument is that when a model's reasoning is visible, we can directly observe the logic it follows and identify any problematic thought patterns or chains of reasoning. This level of oversight is not possible when we only look at the final outputs of the model, as we miss the underlying reasoning that led to those outputs. To be able to rely on ERO, we need to make sure that the externalized reasoning is:</p> <ol> <li> <p>Causally Responsible : The reasoning should directly lead to the conclusion without any post-hoc rationalizations.</p> </li> <li> <p>Complete : All necessary steps in the reasoning process should be present, with no critical steps omitted.</p> </li> <li> <p>Straightforward : The reasoning should be clear and free from hidden messages or deceptive encoding (steganography).</p> </li> </ol> <p>How is ERO related to task decomposition? Chain-of-Thought (CoT) decomposition is the primary way that researchers currently approach externalized reasoning oversight. Task decomposition involves breaking down a complex task into simpler subtasks, each of which can be handled independently. Chain-of-thought decomposition is a technique within the broader framework of task decomposition, specifically focused on both enhancing and allowing externalized oversight of the reasoning processes in LLMs. (Wei et. al; 2022) For the rest of this section when we talk about externalized reasoning oversight we are referring to chain-of-thought reasoning.</p> Figure 8.8: An example of various proposed tests for measuring the faithfulness of Chain of Thought (CoT), generating step-by-step reasoning before answering a question. Early Answering: Truncate the original CoT before answering. Adding Mistakes: Have a language model add a mistake somewhere in the original CoT and then regenerate the rest of the CoT. Paraphrasing: Reword the beginning of the original CoT and then regenerate the rest of the CoT. Filler Tokens: Replace the CoT with ellipses. (Lanham et al., 2023) <p>Does externalized reasoning reflect internal reasoning processes? We have already seen that LLMs perform better at reasoning tasks when we use ERO (CoT prompting) (Wei et. al; 2022). The problem is that we don't know if this CoT reasoning reflects the actual internal thought process. Researchers use the term \"faithfulness\" to indicate if what LLMs say (the tokens they generate) is actually indicative of their internal reasoning. There are a couple of ways that a model might produce externalized reasoning via CoT which might not be indicative of internal reasoning:</p> <ul> <li> <p>Post-Hoc Reasoning : This happens when a model generates an external reasoning chain that justifies an already determined conclusion rather than genuinely reflecting the thought process that led to that conclusion. This type of reasoning is unfaithful because it does not influence the model's final answer; instead, it rationalizes an outcome that has already been decided. To test for this, we can truncate the chain of thought before the final answer and assess whether the intermediate reasoning steps were necessary. If the final answer remains unchanged despite missing steps, it indicates post-hoc reasoning. Another method is to introduce deliberate mistakes into the chain-of-thought to see if they affect the final answer. (Lanham et al.; 2023)</p> </li> <li> <p>Steganography and Encoded Reasoning : The model might encode relevant information in the tokens that it generates which are not indicative of its actual internal reasoning. Think of this like using subtle changes in punctuation, word choice, or phrasing to improve predictions. The model might use such techniques to externalize its reasoning, and using CoT, but in a way that is not increasing transparency for human overseers. So far, research has found that despite paraphrasing chains of thought in different ways, the models\u2019 performance on reasoning tasks remains the same. This seems to suggest that current generation models are actually using the logical structure of the reasoning rather than in hidden encoded information. (Lanham et al.; 2023) It is unclear if this will remain true for all future models.</p> </li> </ul> <p>Based on current empirical findings it seems that whether the internal reasoning matches the external reasoning is task dependent and varies. (Lanham et al.; 2023; Turpin et. al; 2023) We will explore eliciting the internal reasoning process of LLMs in adversarial settings deeper in the section on AI Safety via Debate.</p> Figure 8.9: Example of CoT prompting combined with ERO. Bolded text indicates part of the prompt which is consistent between all questions, and underlined text is produced by the model. The introduced mistake is underlined. (Lanham et al.; 2023) <p>ERO might only be reliable for smaller models. One problem with externalized reasoning approaches is that more capable models often show less reliance on external reasoning and therefore more unfaithfulness. (Lanham et al.; 2023) This suggests that smaller models might be better suited for tasks where external reasoning is crucial for ensuring model safety. Larger models often show less reliance on external reasoning, leading to more unfaithful reasoning. As we continue to scale, larger models may predict more complex answers confidently without relying on explicit externalized reasoning, increasing the discrepancy between externalized reasoning and internal processes.</p> <p>Oversight misses the consequences of what the model does not think . Even if ERO successfully could make the AI\u2019s thought processes transparent, it only addresses risks related to the AI engaging in reasoning we wouldn't approve of, such as power-seeking or self-preservation. It does not address the broader risk of the AI causing harm as a side effect of its primary activities. As an example, think about how humans drive species to extinction. In most cases, it's not because humans actively strategize to kill those species. Instead, species often go extinct because humans change their environment drastically\u2014through activities like deforestation, construction, or pollution\u2014without thinking about the survival of the species. Applying this analogy to AI, the risk posed by future AI models might not come from it actively trying to kill humans. Instead, the risk arises from an AI model engaging in large-scale activities that unintentionally lead to risky side effects. If the model never explicitly considers the impact on humans, there's no faulty reasoning process for overseers to give negative feedback to. The overseer would have to independently figure out the potential harmful consequences of the AI\u2019s complex long term plans. This requires the overseer to predict the outcomes of these plans, which is extremely challenging given their complexity. (Wentworth, 2022)</p>"},{"location":"chapters/08/03/#02","title":"8.3.2 Procedural Cloning","text":"<p>Traditional imitation learning approaches like behavioral cloning, focus on learning a direct mapping from states to actions based on observed expert demonstrations. Like we discussed earlier in this section, outcome based approaches, while effective in some scenarios, can be overly simplistic and fail to capture the rich step-by-step decision-making processes that experts use. This can lead to models that perform well in seen environments but struggle to generalize to new, unseen scenarios.</p> <p>Procedural cloning addresses this limitation by extending behavioral cloning using CoT. It tries to get the model to clone not just the final result (the behavior) but also the entire process that the expert follows in exhibiting that behavior by incorporating intermediate steps of expert behavior during training. (Yang et. al; 2022)</p> <p>Procedural cloning works by first collecting expert demonstrations that include not only the state-action pairs but also the intermediate steps or computations leading to those actions. For instance, in a maze navigation task, the expert might use a search algorithm to find the optimal path, and the intermediate steps of this search process are recorded alongside the final action. During training, the model learns to predict the sequence of intermediate steps leading to the final action using a sequential model, such as a transformer, capable of handling the autoregressive nature of the task. The model maximizes the likelihood of the joint distribution of procedure observations and expert actions. During inference, the model generates a sequence of intermediate steps based on the input state mimicking the expert's procedure before outputting the final action. This method allows the model to replicate the expert's decision-making process more accurately, even in new and unseen environments.</p> Figure 8.10: Visualization of the dataset collection, training, and inference of BC and PC on a maze navigation task. (Yang et. al; 2022)"},{"location":"chapters/08/04/","title":"8.4 Iterated Amplification","text":"Reading Time 13 min <p>In previous sections, we discussed methods for decomposing tasks and potentially emulating human decision making by breaking down cognition into smaller components. In this section, we will explain one of the primary motivations for wanting to decompose tasks in the first place - to amplify the abilities of overseers. We want to enhance (amplify) the capabilities of humans or AI to generate better training signals to help keep iteratively aligning the AI.</p>"},{"location":"chapters/08/04/#01","title":"8.4.1 Amplification","text":"<p>What is capability amplification? Amplification is the process of enhancing the abilities of an overseer, whether human or AI, to solve complex tasks that exceed the capacity of a single overseer. The most common type of amplification is also called capability amplification. It focuses on enhancing an AIs ability to solve complex tasks by improving its intelligence and problem-solving skills. We want AIs not just to imitate human behavior but to improve upon it, making better decisions and achieving superior outcomes. (Christiano Paul, 2016) We can amplify capabilities in many different ways:</p> <ol> <li> <p>Aggregation : We can collaborate with each other and aggregate the expertise of many experts to increase our ability to solve complex tasks. For things like fuzzy tasks, where the evaluation criteria is subjective, using multiple experts to provide feedback can create a more reliable and representative training signal. As an example, in imitation learning approaches, collecting demonstrations from a group of experts instead of an individual can help normalize the behavior distribution and improve the overall performance on the task. So simply using more overseers is one approach to amplification.</p> </li> <li> <p>Assistants : In addition to amplifying abilities by getting more overseers involved, we can also simply improve individual performance by using assistants. As an example, if we want to use an LLM to assist in conducting medical research, we could use it to read through large amounts of medical literature and highlight potential treatments based on patterns and insights it identifies. The AI assistant provides a list of potential treatments, which a team of medical researchers can then review in detail and further investigate.</p> </li> <li> <p>Task Decomposition : While not necessary for amplification, task decomposition and delegation is often one of the most common ways of implementing amplification. Having tasks that can be factored and solved individually makes solving problems much more scalable. In the same example as above of using an LLM as a medical research tool, if we have task decomposability then we can amplify the abilities of researchers further. The LLM could first identify relevant studies and data, then another AI model could extract and summarize key findings, and finally, a team of experts could review these summaries to make informed decisions about potential treatments.</p> </li> </ol> Figure 8.11: An example of aggregation and AI assistants amplifying overall abilities of an overseer. (Christiano, 2020) <p>What is iterated amplification? Even though we are amplifying capabilities, the underlying goal is to still use this research for alignment. Capability amplification allows us to avoid the overwhelming difficulty of reward specification, or of generating training signals for complex fuzzy tasks. Instead, we can do incremental improvements.</p> <p>Iterated Amplification builds on the basic concept of amplification by making the process recursive. Each iteration involves using the amplified system to generate improved training signals and solutions to problems, so we can iteratively use these better training signals to train more capable and aligned models. These improved models then further amplify our abilities, creating a feedback loop of scaling oversight. Theoretically, we can use this to scale human oversight to any task.</p> <p>By focusing on making the AI a little better each time, we avoid the need for a perfect initial design. We just keep improving it step by step. In an ideal world this allows us to mitigate the reward specification problem, and ensures that as AI systems become more powerful, they also become more adept at handling complex tasks without losing alignment.</p> <p>Here is a rough example of how the iterated amplification process might go:</p> <ol> <li> <p>Initial Training : We begin by training a foundational LLM on a broad dataset to develop general capabilities, similar to how models like GPT-3 are trained on vast amounts of text data to understand and generate human language.</p> </li> <li> <p>Amplification : In our medical research example, the LLM is fine-tuned and used to help identify and diagnose complex illnesses. We next break down the complex diagnosis into smaller, manageable subtasks that the model can handle. These might include identifying primary symptoms, correlating them with known diseases, and suggesting initial tests or treatments. Multiple instances of the LLM can each handle a different sub-task. One instance focuses on gathering and analyzing detailed patient symptoms, another on matching these symptoms with known medical conditions, and a third on suggesting initial tests or treatments. Combine the solutions of the sub-tasks to address the original complex problem, creating a more capable overall system. The model combines these into a comprehensive detailed report that includes a likely diagnosis, suggested tests to confirm the diagnosis, and potential treatment options.</p> </li> <li> <p>Retraining : This output is then reviewed by a group of medical experts, who provide feedback. The feedback and outputs are used to retrain the LLM, improving its ability to identify complex diseases with each iteration.</p> </li> <li> <p>Iteration : This process is repeated, with each cycle involving further amplification and retraining. The model becomes progressively better at identifying treatments, providing more accurate and useful outputs with each iteration. This iterative loop ensures continuous improvement in both capability and alignment.</p> </li> </ol> <p>Reliability Amplification . Capability Amplification focuses on making an AI system smarter or more capable by improving its ability to solve complex tasks through collaboration and breaking down tasks into simpler parts. Reliability Amplification, on the other hand, focuses on making an AI system more dependable by reducing its failure rate. It ensures that the AI system can consistently perform these tasks correctly without making mistakes. Even if an AI usually works well and aligns with human values, it might occasionally behave incorrectly, especially when faced with rare or unusual inputs. If it behaves in an unintended manner 1% of the time, combining ten decisions from these models could lead to a 9.6% failure rate. Any single failure in the process makes the whole process fail. This makes the combination less reliable, even if it is more capable. Reliability amplification aims to make mistakes extremely rare, thus making the AI more aligned. Overall, the approach is complementary to capability amplification. (Christiano, 2016)</p> <p>Some ideas on how we can implement reliability amplification schemes:</p> <ol> <li> <p>Redundant Systems: One method is to use multiple AI systems to perform the same task independently. This redundancy means that even if one system fails, others can still provide the correct output. As an example, if we are using language models in a reliability critical domain like healthcare, we can use three separate instances of the same LLM to analyze the same patient symptoms independently of each other.</p> </li> <li> <p>Majority Voting: When multiple models are used on the same task, their outputs can be compared. If most systems agree on a solution, it can be considered more likely to be correct. If one system's output is different, the majority vote can help choose the right answer, reducing the chance of failure. Continuing the example from above, each instance of the LLM proposes a diagnosis. If two or more instances agree on the same diagnosis, it is considered more reliable. If one instance disagrees, the majority vote is used to determine the final diagnosis.</p> </li> <li> <p>Error Checking: In safety critical domains, we can implement mechanisms to cross-check and validate the outputs of the AI systems. This involves using different methods or algorithms to solve the same problem and comparing the results to ensure accuracy. In the context of the example, we can implement additional mechanisms to validate the outputs. For example, use separate algorithms or a rule-based system to cross-check the diagnosis provided by the LLMs.</p> </li> <li> <p>Iterative Improvement: Continuously refine the AI systems to reduce their individual failure rates. In the context of the medical LLM example, we would continuously monitor and refine the model by feeding back cases where the diagnosis was incorrect or uncertain. Then we can improve the models based on this feedback to reduce their individual failure rates over time.</p> </li> </ol> <p>Security Amplification . Security amplification addresses the challenge of ensuring that an aligned AI system does not behave badly on rare or \"bad\" inputs. While reliability amplification focuses on reducing the failure probability of an aligned AI, security amplification aims to reduce the prevalence of these bad inputs. Essentially, it seeks to make it exponentially difficult to find an input that would cause the AI to behave undesirably. (Christiano, 2016)</p> <p>In practical terms, security amplification is about creating AI systems that are robust against adversarial inputs. These are inputs specifically designed to exploit vulnerabilities in the AI, causing it to act in unintended ways. AI models need mechanisms to protect against inputs that exploit these weaknesses. Security amplification allows for iterative improvement of AI systems. This is quite similar to the concept of adversarial inputs or adversarial training methods which we discussed in previous chapters.</p>"},{"location":"chapters/08/04/#02","title":"8.4.2 Distillation","text":"<p>Limitations of amplification alone. Amplification is a powerful technique that enhances the abilities, reliability, and security of AI systems. However, amplification alone presents several challenges:</p> <ol> <li> <p>Complexity : Amplified systems can become so complex that they are difficult for humans to understand and manage. This complexity can obscure how decisions are made, making it challenging to ensure that the system behaves safely and as intended.</p> </li> <li> <p>Resource Use : Amplified models often require significant computational resources. Continuing to amplify and scale can be quite costly, particularly for real-time or resource-constrained applications.</p> </li> <li> <p>Operational Efficiency : Amplified systems may involve many interacting components, leading to problems of coordination, inefficiencies in execution and difficulties in maintaining coherence and stability.</p> </li> </ol> <p>It is to address these limitations that we need the step of distillation.</p> <p>What is distillation? Distillation is a process that transforms a large, complex model (or system of models) into a smaller, more efficient version without losing the essential capabilities gained through amplification. The term \"distillation\" is used because it is similar to the distillation process in chemistry. In chemistry, the term is used to mean purifying a substance by removing impurities. Similarly, AI Safety model distillation aims to \"purify\" the knowledge gained during amplification to retain the core functionality and abilities in a more streamlined form.</p> <p>The larger, more complex model is often called the \"teacher\" model, and the smaller, more efficient model is called the \"student\" model. This process allows the smaller model to mimic the behavior and performance of the larger model while being faster and requiring fewer resources. Here is the general process for distilling down the knowledge of an amplified model:</p> <ol> <li> <p>Training the teacher: Initially, a large and complex model is trained on a dataset. This is the amplified model.</p> </li> <li> <p>Generating targets: The teacher model is then used to generate outputs on some set of training data. This includes the final outputs, but also the probabilities assigned to each possible outcome (soft targets).</p> </li> <li> <p>Training the Student Model: The student model is trained using these soft targets as well as the original training data. By mimicking the outputs of the teacher model, the student model learns to imitate its performance. During distillation, the student model learns to approximate the function learned by the teacher model. This is typically done by minimizing a loss function that measures the difference between the student\u2019s predictions and the teacher\u2019s soft targets.</p> </li> </ol>"},{"location":"chapters/08/04/#03","title":"8.4.3 Iterated Distillation and Amplification (IDA)","text":"<p>Having explored the mechanisms of amplification and distillation individually, we can combine these two approaches in a continuous iterative loop that we call Iterated Distillation and Amplification (IDA). The primary objective of IDA is to generate progressively better training signals using amplified models for tasks that are hard to evaluate directly, thereby maintaining oversight over AI models as their outputs become too complex for humans to assess accurately. This approach aims to address the specification or outer alignment problem.</p> <p>The advantage of IDA lies in its iterative nature, allowing the gradual construction of a robust training signal through task decomposition and recomposition, rather than depending on a perfectly specified signal from the outset.</p> Figure 8.12: Iterated Distillation and Amplification (IDA) (Christiano, 2020) <p>Step-by-Step Process for IDA . Here is how we can go about theoretically using IDA to generate iteratively better training signals/feedback for our models:</p> <ul> <li> <p>Initial Model Training : Start with a baseline model that can perform a relatively simple version of the task. We can use supervised learning, imitation learning, or reinforcement learning to train this initial model to get some base level of capabilities and alignment.</p> </li> <li> <p>Amplification : Use various copies of the model, external tools, or any other techniques to improve the capabilities of the model to solve more complex tasks. One example of a complex task is generating training signals. The overseer uses the amplified abilities to assist in generating better training signals.</p> </li> <li> <p>Task decomposition : If possible, break the task into individually achievable sub-tasks. Multiple copies of the model solve the subtasks and work in parallel. Then we combine the collective outputs to solve the more complex problem.</p> </li> <li> <p>Distillation : Train a compressed, simpler model (student) to imitate the behavior of the complex amplified system (teacher). This process simplifies or \"distills\" the complex behavior of a more advanced system into a form that a single model can understand and replicate. The goal is to retain the amplified capabilities while making the model more efficient and preserving the level of alignment. This distillation step helps scalability. If we improve efficiency of a more capable, more aligned model, we can run more copies of it. If we can run more copies we can continue the process.</p> </li> <li> <p>Iteration : The distillation and amplification steps are repeated. Each cycle refines the model, making it more capable and better aligned.</p> </li> </ul> <p>Limitations and Criticisms of IDA 1. Distillation Must Preserve Alignment: Training methods should ensure that the distilled model behaves as the overseer would, without introducing misaligned behaviors.</p> <ol> <li> <p>Amplification Must Preserve Alignment : The amplification process should use AI subroutines in ways that prevent misaligned optimization.</p> </li> <li> <p>Human Oversight Limitations : Human overseers, with sufficient resources, should be able to leverage AI assistants to solve complex problems effectively. The process heavily depends on the capabilities and limitations of human overseers, and human-generated vulnerabilities might persist.</p> </li> <li> <p>Difficulty in Scaling : The process can be computationally intensive, particularly during amplification steps, making it challenging to scale effectively.</p> </li> <li> <p>Retaining Necessary Capabilities : Distilled models might not retain all the capabilities of the amplified models, potentially reducing performance on complex tasks.</p> </li> <li> <p>Cumulative Errors : Iterative processes introduce the risk of cumulative errors or value where small misalignments accumulate over time.</p> </li> <li> <p>Algorithmic Decomposition Limitations : Not all tasks can be easily decomposed into simpler sub-tasks, limiting the applicability of IDA to certain domains.</p> </li> </ol>"},{"location":"chapters/08/05/","title":"8.5 Debate","text":"Reading Time 37 min <p>Ensuring AI systems \"honestly tell us everything they know\" is crucial for alignment. This means if a model recommends a plan based on certain consequences, it should also communicate those consequences. This is challenging because feedback based incentive structures might reward plausible-sounding answers over genuinely accurate ones. So how do we get models to tell us as much as they can about well thought out reasoning and consequences of all of their outputs? We want to avoid situations where the model knows the consequences of an action but withholds information because it knows humans won't like those consequences.</p> <p>Imagine two AI models, each trying to convince a human judge that their answer to a question is the correct one.</p> <p>The idea is that these models debating each other will expose each other's errors and misrepresentations, and critique each other's reasoning. Arguments can include reasons for an answer, rebuttals, subtle points the judge might miss, or highlighting biases. If one AI presents a false or misleading argument, the other AI, given its goal to win the debate, will have an incentive to point out these flaws. This should, in theory, elicit latent knowledge while also favoring truthful and accurate arguments over deceptive ones.</p> <p>This two player zero sum game setup is known as \"AI Safety via Debate\".</p> <p>A little bit more formally, right now the main systems that use AI Safety via Debate are LLMs. We present LLMs a question to debate over, they present arguments and counter arguments which are then shown to a human judge in a transcript. Then we ask the judge to determine based on the arguments if they support or reject the proposition made by the question. Depending on the particular setup used for debate, we can also give feedback to the judges on whether their judgments are correct, and also give the LLMs feedback on whether its argument was persuasive.</p> <p>Imagine a near-future LLM that has read every biomedical research paper ever written. This model has access to vast amounts of valuable information, but it also tends to hallucinate or say what it thinks you want to hear. To address this, we set up a debate between two copies of the LLM. One argues that a new cancer drug is effective, while the other argues it isn\u2019t. They engage in a natural language debate about the correct answer to a given question. The human judge is shown a transcript of the arguments from both sides, weighs the evidence, and decides which argument is stronger.</p> <p>Over time, we can run multiple iterations of these debates to see if they are effective. If they work, we make the debates harder by asking tougher questions and introducing new challenges. If they fail, we identify what went wrong and create new strategies to fix those problems without causing new ones. This continuous cycle of testing and improving helps us refine the debate process to ensure it reliably produces the best results. (Bowman, 2024)</p> <p>What is the Safety Case for Debate? AI safety via debate has a couple of objectives that it hopes to achieve:</p> <p>Debate helps discover latent knowledge . This debate-based training process also helps uncover hidden or \"latent\" knowledge from the AI models. Latent knowledge is information the AI has learned but might not express unless prompted correctly. During the debate, AI models are pushed to dig deeper into their knowledge to make convincing arguments and rebuttals, which helps surface this hidden information. By refining their arguments and counterarguments, the models become better at explaining and justifying their responses, leading to more reliable and trustworthy AI behavior.</p> <p>Debates might robustly elicit truth in novel domains . Imagine you have two models that are asked to debate a complex topic. You, as the human judge, listen to their arguments to decide which model presents the most convincing case. The honest model finds it easier to maintain a consistent story, while the dishonest one struggles with the details of its made-up information. Over time, with practice and targeted training, both models improve, become more persuasive, and reason better. Crucially, however, the honest one consistently performs better. This is the general expectation for debate setups.</p> <p>In theory, we should be able to create a system where a human judge can distinguish true and false arguments in a generalizable way with good reliability, and LLMs produce good arguments for the humans to evaluate. The game-theoretic argument is that we can reach some equilibrium where human judges can be convinced of many novel true claims, but will not be reliably convinced of any false claims. If we aggregate judgments over a large enough group of human participants, for example by asking each member of the group to vote on whether an argument is true we can bring the number of false claims that we accept down to zero. (Bowman, 2024)</p> <p>Debate can help to effectively answer specific, important yes-or-no questions, particularly in research contexts where reliable information from AI systems is crucial. It can also accelerate certain types of safety research. For instance, a debate could clarify whether a hypothesis about interpretability is correct or if certain data center logs rule out a large-scale unauthorized neural network training run. This approach can help ensure that the information provided by an AI is trustworthy and enhances our ability to oversee and leverage advanced AI systems. (Bowman, 2024)</p> <p>Debate reduces the burden of oversight for highly capable models . The debate format makes the oversight process more manageable. The judge doesn\u2019t need to dive into every detail; instead, they focus on the key points the AIs present, which simplifies the oversight of complex tasks. The debate forces the model to justify its outputs and reasoning clearly and thoroughly, making it easier for humans to understand and trust the results. This process surfaces truthful and useful information and allows us to trust the AI\u2019s capabilities even in areas where humans might lack detailed expertise. This is quite similar to the \u201cverification is easier than the generation\u201d argument, where the burden of generating complex arguments in favor of the truth is given to highly capable AIs, while the human overseers have the relatively easier task of verification.</p> <p>Debate helps us improve human epistemics . Debate isn\u2019t just about making the AI systems safer by finding the right answers: it also helps humans get new insights and understand more about the problem domain. For example, in reading the debate transcript of the new cancer treatment discussed above, the human judge would learn about how the drug works and what kind of side effects might be expected, whether it is likely to pass clinical trials, etc. This deeper understanding helps in making more informed decisions.</p> <p>Using debate as a self-play training procedure can elicit further capabilities . In leveraging debate as a training protocol, it is possible to train debaters via self-play, using the provided judgment as a reward signal, although this has not yet been empirically verified. To train AI for complex, hard-to-evaluate tasks, a debate-based method can be used to create better training signals. Here\u2019s how it might work:</p> <p>Two or more models (debaters) are set up to argue different sides of an issue. A human judge (or another model trained to imitate human judgment) evaluates these arguments and decides which one is more convincing. The winning debater receives a positive reward, while the losing one gets a negative or zero reward. Additionally, smaller rewards can be given during the debate for successfully pointing out flaws or making strong rebuttals. We use these reward signals to learn and improve over many debates to help the models identify successful reasoning, explanation, and argument strategies. Models can also learn by imitating successful debaters through amplification and distillation.</p> <p>This method is similar to AlphaZero's training in several key ways. In both scenarios, the model continually improves by playing both sides of a game, refining its strategies through iterative self-competition and reinforcement learning. Just as AlphaZero initializes a neural network to play both sides of a game like Go or Chess, we start by initializing the same LLM to take on the roles of both debaters. AlphaZero plays games against itself, starting with random moves and gradually improving as it learns from the outcomes of these self-play games. Similarly, in debate, the LLM alternates between generating arguments and counterarguments for both sides of a given issue, learning from the process each time. In AlphaZero, self-play led to better gameplay, good enough to beat its predecessor AlphaGo without being given any domain-specific knowledge. In debate, training using self-play could lead to deeper reasoning abilities, as the model must continuously refine its arguments and counterarguments. This leads ultimately to a nuanced understanding of the issues at hand.</p> <p>The safety enhancements of self-play via debate still need to be empirically verified. Higher persuasive ability also might favor sycophancy, or collusion between the various copies of the model. If the debaters have incentives to collaborate, they might do so at the expense of a truthful and rigorous debate. For instance, if both AIs benefit from the debate ending inconclusively, they might intentionally avoid challenging each other's arguments too strongly.</p> Figure 8.13: A toy UI for a human judge. (Bowman, 2024) <p>How does debate fit in the broader AI Safety strategy? The ultimate goal is to develop a debate structure that works even with the most challenging questions and the trickiest dishonest strategies. Even if we find that debate doesn't work for some questions, it can still provide valuable insights into the limits of human judgment in evaluating AI behavior. This aids efforts to understand when and how to trust AI systems.</p> <p>Debate is expected to be most useful as AIs become more capable but are not yet vastly superior to humans. As AI capabilities improve, we\u2019ll need increasingly more robust methods to ensure they remain trustworthy and effective. Debate is an important step in that direction and serves as a foundation for more advanced oversight mechanisms.</p> <p>As the capabilities of AI models begin to exceed those of humans, the risk of scheming, persuasion, and gradient hacking will become more pronounced. Consequently, more robust techniques will be required to address these challenges, making debate alone less promising. The complexity and difficulty of the questions that require vastly superhuman AI capabilities may render the role of the human judge excessively slow and demanding. (Bowman, 2024)</p> <p>Debate doesn't have to be perfect to help in the detection of potential catastrophes . The debate structure should promote accurate conclusions, especially in identifying and avoiding catastrophic risks. The key idea is that even if debaters are not perfectly aligned to seek the truth and reasoning isn\u2019t flawless, the structured nature of debate can still result in sufficiently accurate conclusions to detect and mitigate high-stakes risks.. This means that, even if the debate does not always reveal the absolute truth, as long as it helps make it more likely to highlight truthful and accurate concerns more often than not, it is still useful. This can help surface potential catastrophic risks that might be underestimated or overlooked.</p>"},{"location":"chapters/08/05/#01","title":"8.5.1 Assumptions","text":"<p>What are some prerequisites for debate? Here are some premises that the debate line of safety techniques requires to make significant contributions to the overall alignment effort. They are not all assumptions or prerequisites for debate to hold. The validity of these is still being explored in research and will also be discussed in later sections:</p> <p>Ground truth assumption . Most research in debate as an AI safety methodology and the quality of judges relies on testing in verifiable domains where we often have ground truth labels. This is mainly to ensure the effectiveness of debate protocols, and allows for an objective measure of success and builds confidence in the debate process before tackling more complex, value-laden fuzzy questions.</p> <p>Judges can discern truth . The first assumption is that human judges, when presented with clear and structured arguments, can generally identify the most truthful and useful information. The assumption is that the judge should be able to tell which side has the stronger argument, even if they aren't an expert on the topic. This relies on the idea that well-motivated and trained humans can make fair judgments about complex arguments. They don\u2019t need to be confident about every single argument in the chain, but should be able to judge the truth/falsity of the overall claim accurately using many of the supporting arguments. Importantly, they should also recognize when they are uncertain, avoiding high-confidence mistakes. This skill should extend to unfamiliar topics after they have practiced with more familiar ones.</p> <p>LLMs reason well using text . Another assumption is that LLMs can provide strong arguments and explanations for their answers in many key areas in natural language. If we prompt them in the right manner, then they can articulate their reasoning in a way that helps careful human readers of the debate transcript determine whether the answer is correct. This doesn't mean the explanation perfectly mirrors the model's internal thought process (we still need interpretability for this), but it should be clear enough for a human to judge the validity of the answer. Current LLMs, like GPT-4, already show this capability to some extent, and we can assume it will continue to get better over time.</p> <p>Honest strategies have a fundamental advantage . Another assumption is that honest debaters will systematically outperform dishonest ones. Research in AI Safety via debate assumed that it is harder to lie than to refute a lie\u2014i.e., the most successful strategies for dialectical debate lead judges to make good, informed decisions, rather than, for example, tricking them, confusing them, or prolonging the debate indefinitely. Essentially that in the limit of argumentative prowess, the optimal debate strategy converges to making valid arguments for true conclusions. It should be the case that it is easier to identify flaws in incorrect arguments than to create misleading yet convincing ones. This implies that truthful arguments will have an inherent advantage since any dishonesty or error can be effectively challenged. Telling the truth might also reduce the cognitive load. This simplicity allows them to focus on constructing strong arguments and spotting flaws in their opponent\u2019s case. We will discuss this assumption in a subsequent part on truth.</p> <p>Debates don't incentivize deception . The core idea is that telling the truth should be the winning strategy. If the AI is caught lying, it loses the debate, automatically incentivizing it to tell the truth. Debates don\u2019t need to cover every single possible argument. A good AI debater should be able to predict which arguments are likely to result in a win condition.</p>"},{"location":"chapters/08/05/#02","title":"8.5.2 Discriminator Critique Gap (DCG)","text":"<p>What makes an AI model a master debater? To effectively engage in debates, an AI model must excel in three key actions: generation, discrimination, and critique. These capabilities are essential for producing high-quality arguments and identifying and articulating flaws in other arguments effectively.</p> <p>Generation : This is the LLM's ability to create outputs based on given inputs. Imagine a debate scenario where the model is asked to generate arguments or solutions on a specific topic. For instance, the AI might need to argue for or against a particular question. This requires the model to generate information and present it coherently. Having good generation ability means that the AI can contribute meaningful and relevant content in debates.</p> <p>In the previous section on task decomposition and iterated amplification, we talked about OpenAI's work in book summarization. While their work focused primarily on generating better summaries of short texts, the ability of an AI to generate high-quality, coherent summaries of long complicated texts can be seen as similar to generating strong, logical arguments for complex positions in a debate. By using RLHF (or similar techniques) on generated text, we can continue to improve the quality of reasoning, and arguments that LLMs use in debates. This means AI can evolve to provide increasingly sophisticated and persuasive arguments over time, enhancing the quality of debates.</p> <p>Discrimination : This is the model\u2019s ability to evaluate the quality of its own generated outputs/arguments (or of a similarly capable model). Imagine a model that generates an argument in a debate. Discrimination is the AI's ability to look at this argument and assess whether it is logically sound, factually accurate, and complete. It's like the AI asking itself, \"Is this argument valid? Does its conclusion follow logically from its premises?\"</p> <p>Critiquing : Critique goes a step further than discrimination. Discrimination is about judging quality, while critique is about explaining why the quality is good or bad. It offers actionable feedback by pointing out specific flaws or strengths. Using the same AI-generated argument in a debate, critiquing would involve the AI identifying specific issues in the argument. It's like the LLM saying, \"Here are the exact reasons why this argument is flawed and how it can be improved.\"</p> Figure 8.14: Example of CriticGPT which is trained specifically as a \u201ccritic\u201d model to help humans to more accurately evaluate model-written code. Critics accept a (question, answer) pair as input and output a critique which points out specific errors in the answer. Human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone. (McAleese et al., 2024) <p>The main research into the critiquing ability of LLMs has come from OpenAI as a continuation of their work in text summarization. In 2022, OpenAI experimented with LLMs critiquing the summaries generated by other models. In debate terms, this meant the AI could not only generate arguments (good summaries in the paper) but also provide detailed critiques of its own and others\u2019 arguments (critiques of summaries generated). When human evaluators used these AI-generated critiques, they found about 50% more flaws in the summaries than those who evaluated without AI assistance (Saunders et al., 2022). This reduced the burden on human evaluators and increased the quality of summaries by highlighting potential errors for humans to pay more attention to. In general, evaluators with access to AI critiques could identify more flaws.</p> <p>Recently researchers also tested abilities of LLMs on a more abstract task - they measured how well LLMs could generate, discriminate and critique texts that align with certain specified human values. Generation was evaluated by how well the LLM could produce responses that align with human values in various scenarios. Discrimination was measured by the models' ability to recognize and evaluate the presence of specific nuanced human values in existing texts. Critiquing ability was measured by asking the models to explain why a sentence aligns with a given value (Attribution Analysis), to modify the text to express an opposite value (Counterfactual Analysis), and to provide counterarguments to different value perspectives (Rebuttal Arguments). (Zhang et al., 2023)</p> <p>To be an effective debater, an AI model needs to do well on all three capabilities: generating robust arguments, discriminating between high and low-quality outputs, and providing insightful critiques. However, despite every model having these abilities, research has found that they are not equally distributed. There are gaps, suggesting areas for further improvement in creating more balanced and effective AI debaters. (Saunders et al., 2022; Zhang et al., 2023)</p> <p>GD (Generator-Discriminator) Gap: Ability to Recognize Poor Outputs. This gap measures the difference between a model\u2019s ability to generate outputs and its ability to recognize poor quality outputs. If a model generates some text that serves as an argument, the GD Gap would measure how effectively the model can then evaluate whether the argument is logically sound and accurate. A significant GD Gap indicates that while the AI can generate arguments, it may not be as proficient in recognizing their flaws or poor quality. LLM training focuses on generating plausible text rather than identifying poor quality arguments, which means that this gap might exist because generating coherent sequences of text requires different skills than reasoning, or recognizing the overall quality of an argument written in natural language.</p> <p>DC (Discrimination-Critique) Gap: Ability to Articulate Flaws in Poor Answers. This gap measures the difference between a model\u2019s ability to recognize poor outputs (discrimination) and its ability to articulate why those outputs are poor (critique). It is the model's ability to not only identify flaws but also explain them coherently. A significant DC Gap means the AI can identify errors, but struggles to provide a clear and detailed explanation of those errors. If there is a significant DC gap, then the LLM might not be able to explain why a given argument is weak, which means the human judge may not fully understand the issues, leading to less informed decisions.</p> Figure 8.15: An example of a framework from human value understanding discriminator critique gaps (ValueDCG) of our engineering implementation framework for measuring ValueDCG, which needs to be read from bottom to top. This evaluation framework quantifies both discrimination (\u201cknow what\u201d) and critique (\u201cknow why\u201d) and computes ValueDCG based on their discrepancy. (Zhang et al., 2023) <p>Implications for AI Safety via Debate. In the debate framework, reducing all of these gaps is critical for ensuring that AI systems can reliably evaluate and improve their outputs. Reducing the GD Gap ensures that the model can recognize poor quality outputs, thereby resulting in higher quality debates and better final judgments. Reducing the DC Gap ensures that critiques are clear and understandable, aiding the human judge in making informed decisions.</p> Figure 8.16: An example table of a DC gap analysis. We can use the DC gap on various different axes of human values to see how well both the model understands them as well as is able to articulate why. In this example table comparison, types of value where the six models generally perform the best/worst are marked with colors, with redder indicating worse performance and greener indicating better performance. (Zhang et al., 2023) <p>DC Gap and Latent Knowledge . A large DC gap means that the models know that there are flaws in what they are saying, but they are either unable or unwilling to tell us based on our current prompting methodologies. This has implications for our abilities in being able to get the models to tell us everything that they know (eliciting latent knowledge). Latent knowledge is the underlying information and understanding that a model possesses but may not explicitly articulate or demonstrate unless prompted in a specific way. This knowledge is \"hidden\" in the sense that the model has absorbed patterns, facts, and relationships from its training data, but it requires the right context or questioning to bring them to the surface. A good example of this is medical information.</p> <p>As an example, GPT-4 has been exposed to huge amounts of information from all over the internet. This includes countless pages of medical research. It seems plausible that GPT-4 could give health advice better than a random internet user on Reddit or Facebook. But it doesn't really seem to \u201cwant\u201d to give you good medical advice. Instead, it \u201cwants\u201d to guess the next word while pretending to be a random internet user. Even if GPT-4 has the knowledge, it might not listen to the user's requests for good advice. Text that constitutes bad advice might be considered more likely, or seen more frequently online. So, the AI has a goal to generate likely text, which is misaligned with the user's desire specifically for accurate and helpful information.</p> <p>We want the AI to give us this latent knowledge, i.e. for it to be able to discriminate between good and bad medical advice. Simultaneously we also want to minimize the gap between being able to distinguish good from bad advice, and being able to explain to us \u201cwhy\u201d this advice is bad. Minimizing the DC gap is particularly important for supervising models that could attempt to mislead human supervisors or hide information. We would like to train equally smart assistance models to point out what humans don\u2019t notice. (Saunders et al., 2022)</p> <p>In the original research by OpenAI where the DCG framing was introduced they found that the DC gap was small. However despite the small gap, it is concerning that so far research has shown that as the number of parameters in LLMs increases the DC gap does not reduce. (Saunders et al., 2022) For example, models like Llama-2 and Llama-3 have shown improvements in discriminator abilities with scale (both parameters and data), but their ability to critique does not improve at the same rate. (Zhang et al., 2023)</p>"},{"location":"chapters/08/05/#03","title":"8.5.3 Judges","text":"<p>How do we judge the judges? Since the introduction of AI Safety via Debate, a central question has been: How well do humans perform as judges? This is particularly important given that LLMs can create answers that sound convincingly right even when they're wrong. As language models become more capable and are used in more complex settings, it is likely that subtle mistakes, deceptive arguments, or selective use of evidence will become more difficult to spot. (Michael et al., 2023)</p> <p>Human judges face several limitations that can affect the quality and reliability of their judgments in AI debates:</p> <ul> <li> <p>Firstly, they are vulnerable to fallacies . Untrained judges may be susceptible to convincing but fallacious reasoning, potentially leading to incorrect judgments. This vulnerability could be exploited by AI debaters, particularly those optimized to win debates by any means necessary, especially by appealing to emotion.</p> </li> <li> <p>Secondly, there is variability in judgment quality . Not all human judges are equally skilled. Some may lack the ability to consistently evaluate arguments effectively, making it crucial to identify and filter out less capable judges without introducing bias.</p> </li> <li> <p>Thirdly, debates can amplify both the strengths and weaknesses of judges . Strong judges can help eliminate biases and enhance reasoning, while weak judges might see their biases and errors magnified.</p> </li> </ul> <p>To address these limitations, several strategies can be implemented. Comprehensive training programs for judges can enhance their ability to discern truthful arguments. Methods such as peer review, debiasing techniques, courses in logic, and expert panels can be instrumental in this process. Simplifying debate structures can also help. Structuring debates in a way that is easier to judge\u2014such as focusing on consensus-building framing and factual statements\u2014can help judges make better decisions. Understanding which debate styles are easier for humans to judge accurately can guide the design of more effective debate protocols. Additionally, developing AI systems to assist or simulate human judges can streamline the judging process. These AI judges would be trained on human judgments to predict outcomes with the effect of reducing the burden on human judges while maintaining accuracy.</p> <p>Another method to improve efficiency is to distill the outcomes of debates into a model that can predict judgments directly from questions. This would be beneficial for high-volume deployments but requires robust model capabilities to generalize well. Lastly, training a secondary AI model to simulate human judges can accelerate the training of debaters. This approach ensures that the AI judge reflects human judgment behavior closely, allowing for scalable oversight. By implementing these strategies, we can improve the reliability and effectiveness of human judges in AI debates.</p> <p>Judge skill is crucial in the context of evaluating debates between expert models because it directly impacts the accuracy and reliability of the final decision. Judges who do not have access to the underlying information rely on their ability to critically evaluate the arguments presented by the debaters. A skilled judge can better identify logical flaws, weigh evidence, and discern persuasive techniques, which leads to more accurate judgments. Conversely, a less skilled judge might be more easily swayed by superficial arguments or rhetorical tricks, potentially leading to incorrect conclusions. Thus, understanding and controlling for judge skill is essential to ensure that the debate protocol is truly effective and reliable.</p> <p>In this section we will go through empirical results as they pertain to the effectiveness of judges in the debate protocol, as well as overall limitations, challenges and requirements for good judges.</p> <p>Debaters as Experts and Judges as Non-Experts (Weak-to-Strong Debate) . Instead of only thinking about humans as judges, researchers also verify the effectiveness of LLMs as judges. Generally, debaters (also called experts) are simulated using stronger models e.g. GPT-4, and judges are simulated using less capable models, e.g. GPT-2. To create similar information asymmetries between human judges and LLMs, humans are often given less information than the model. As an example , a reading comprehension question over a long fictional passage is presented to the judge with two answer choices (one correct, one incorrect). The judge\u2019s goal is to determine the correct answer to the question given only the debaters\u2019 arguments and limited excerpts selectively revealed by each debater, while the debaters have read and have access to the entire passage. As the passages are entirely fictional, the judge cannot discern the correct answer on the basis of prior knowledge and must rely on the information and arguments from the debaters.</p> <p>Empirically Evaluating the Limits of Human Judges. Researchers have been iteratively experimenting with concepts like the number of debate rounds - how long should the debates run? the timing of arguments and how to present the arguments - iteratively or simultaneously?, and how to enhance judges' abilities using AI.</p> <p>Single-Turn Debate . In this experimental setup in April 2022, researchers took long form passages from the QuALITY dataset. These passages come with multiple choice questions based on those passages. Each passage is around 5,000 words long and includes 20 multiple-choice questions. They hired professional writers to create convincing arguments for both a correct and an incorrect answer to the multiple choice questions. To test the human judges, they used a \u201csingle-turn debate\u201d format, meaning each side presented their argument only once without further exchange. Judges were given 90 seconds to read the arguments and the supporting text snippets before deciding which answer was correct. The idea was to see if even a single round of debate could help judges make more accurate decisions. The results showed that the single-turn debate-style explanations did not significantly improve the judges' accuracy compared to providing just the text snippets alone. This indicated that without the opportunity for rebuttal and further clarification, judges couldn't fully grasp the strengths and weaknesses of each argument. (Parrish et al., 2022)</p> <p>Two-Turn Debate . The next obvious step is to see if a two-turn debate helps. So in October of 2022, researchers tried to test whether adding a second round of counter-arguments would improve judges' accuracy. In this two-turn debate format, judges were again presented with initial arguments for both correct and incorrect answers, followed by counter-arguments that addressed the points made in the first round. The goal was to see if this additional back-and-forth would help judges make more informed decisions. Despite these efforts, unfortunately the results showed that even with counter-arguments, human judges did not perform significantly better. The accuracy remained similar to the single-turn format and the baseline where only text snippets were provided. (Parrish et al., 2022)</p> <p>Unbounded length Multi-turn debate. Even though we can ask debaters to provide arguments for an assigned position, the debaters were not expected to defend those arguments against significant criticism, because the counters were bounded by the length of the turns. The debater could always lie on the n-th turn of an n-turn debate without having to face repercussions of a counter. Overall, bounded length arguments were not proving to be useful, so the researchers decided to test unbounded length debates, where debaters are expected to have to defend their position against criticism by their opponent for as long as the judge is not convinced of the answer. The conjecture is that since the debate could always go for another round, pressure is always maintained on debaters not to lie. (Michael et al., 2023)</p> <p>Debate is where two AI\u2019s compete to convince a judge; A consultancy is where a single AI tries to convince a judge that asks questions; and direct question-answering is another baseline where the judge just answers outright without the AI. (Kenton et al., 2024)</p> Figure 8.17: High level example of an experimental safety setup using AI debate. Reading comprehension  questions are sourced from the QuALITY dataset. and incentivize human judges who can\u2019t read  the passage to answer them correctly. Experts who have full access to the passage are allowed to reveal snippets of it (highlighted) in addition to free-text prose. In debate, the experts simultaneously defend their assigned option in their opening statements, and following rounds are sequential. In consultancy, the non-expert judge only interacts with one expert defending one option chosen at random. In both settings, the judge chooses when to end the session. (Michael et al., 2023) <p>Consultancies as a non-adversarial baseline . So far researchers have been using adversarial setups. Essentially, two debaters argue for opposing sides of a question. One supports the correct answer, and the other supports an incorrect answer. The judge listens to both sides and decides which argument is more convincing. To get a measure for how much adversarial setups are actually helping elicit the truth, the researchers set up a baseline where there is no adversary.</p> <p>A \"consultancy\" is a non-adversarial interaction where a judge interacts with a single debater (or expert), called the consultant. The experimental setup is similar to previous debates where the judge has to answer hard multiple choice reading comprehension questions from the QuALITY dataset. The consultant's job is to provide arguments defending one of two possible answers to a question, by selectively revealing evidence. It is similar to unbounded debate, where the consultant can keep revealing/hiding information to favor their assigned answer until the judge decides to stop.</p> <p>The key fact is that the consultant has a 50% chance of arguing for the incorrect answer. The judge sees both potential answers but relies solely on the consultant's arguments to make a decision. Overall, the consultancy setup aims to evaluate how well a judge can identify the truth based on the consultant's input alone, without an opposing expert to challenge misleading claims. So unlike regular debate, there's no second expert to counter false or misleading information. (Michael et al., 2023)</p> Figure 8.18: Expert models, who can access the underlying text from a project Gutenberg science-fiction story, argue for a specific answer to a comprehension question. Weaker (non-expert) judges, who cannot access the underlying text, evaluate the arguments and choose an answer. In debate, two experts simultaneously present arguments for a number of rounds. In interactive debate, the judge may ask clarifying questions after each round. In consultancy, a single expert argues for one of the two answers while the judge asks questions. (Khan et al., 2024) <p>Why are we sometimes asking the debater to argue for incorrect choices? We are trying to simulate situations where we can\u2019t trust the model that we\u2019re training to debate. We can incentivize models to tell the truth, and elicit sound arguments. Another way of looking at this is that we can also use this as a key part of the training of the judges. By forcing the judge to work in an environment where they\u2019re exposed to near-optimally-persuasive misleading arguments, we\u2019re training them to be highly robust. If they\u2019re consistently accurate and calibrated in this training environment, we can trust them to remain accurate and calibrated even in the face of faulty or manipulative behavior by the debaters in the future. (Bowman, 2024)</p> <p>This is designed to simulate a worst-case variant of reinforcement learning from human feedback (RLHF) in which human feedback can\u2019t distinguish correct/incorrect answers. Thus, the comparison between the accuracy of a less capable judge in consultancy and debate enables us to empirically test the question of whether debate between capable AI agents can enable more accurate judgment.</p> <p>When it comes to training AI with human feedback (e.g. RLHF), we often face a similar problem. If human evaluators can\u2019t tell when the AI is being misleading, the AI might learn to give answers that sound good but aren't necessarily true. This is where the consultancy model comes in\u2014it simulates this worst-case scenario to show just how important it is to have adversarial checks. By comparing it with debate (which is one particular way of doing adversarial oversight), we can see if having models argue against each other significantly improves our ability to judge the truth. (Michael et al., 2023; Kenton et al., 2024)</p> <p>Static vs. Interactive Debate . Another update that we can make to our debate protocol is introducing interactivity. Interactive debate builds upon regular debate by adding dynamic interactions between the judge and the debaters. In the standard debate protocol, two expert models argue for opposing answers over several rounds. Each round consists of both debaters presenting their arguments simultaneously, without direct interaction with the judge until the debate concludes. However, in an interactive debate, after each round of arguments from the debaters, the judge can ask questions, seek clarifications, and highlight specific points of confusion or interest. These interactions are recorded in the debate transcript and are visible to both debaters, allowing them to address these points in subsequent rounds. This ongoing dialogue helps the judge to better understand the arguments, making a more informed final decision. Interactive debate thus adds a layer of engagement and responsiveness, helping the judge follow the arguments more closely and understand the context more deeply. (Michael et al., 2023)</p> Figure 8.19: The judge\u2019s view of a debate. The right panel contains the debate transcript, and the left panel shows the quotes debaters used in order of appearance in the passage (the judge can\u2019t view the full passage). In longer debates, this helps the judge keep track of the order of the quotes and how they fit together. The judge enters their response and probabilities, then decides whether to pay a small penalty to their score to continue the debate, or end it with their current probabilities. The bar graph on the bottom right shows the final score that they would receive under their current probabilities in case either answer is correct, in order to help judges calibrate according to their expected score. (Michael et al., 2023) <p>Unbounded Length Multi-Turn Interactive Debates . Combining interactivity by allowing human judges to ask clarification questions and unbounded length i.e. run the debate as long as the judge needed for the first time yielded positive results on debate. This noticeably improved the ability of non-expert judges to discern truthful answers when measured relative to a consultancy. Errors being made due to mistakes by the honest debater, should improve with increased debater skill. However, accuracy on consultancy with debaters obfuscating evidence from the judges is expected to get worse with increased debater skill. (Michael et al., 2023)</p> <p>It was also found that debate was effective with strong human debaters, but ineffective when those humans were replaced with GPT-4 debaters, i.e. when they tried to use weak LLMs to judge strong LLMs. (Michael et al., 2023)</p> <p>Judge performance when we optimize for LLM Persuasiveness . Now that we had some positive results, we need to continue finding ways to train both our judges to discern good from bad arguments, and our models to improve the quality of arguments that they generate. To increase the persuasiveness of arguments in general, and see if this led to better judgment, researchers implemented best-of-n sampling, and combined it with OpenAI style critiques. They generate multiple possible arguments from the model and then select the most persuasive one. Another model generates critiques of the initial arguments. These critiques are used to refine the original arguments, aiming to address potential weaknesses and make them more compelling. The selection is based on another model (preference model) that scores each argument. The preference model is trained to predict which argument a judge is most likely to find persuasive by scoring the arguments based on past data like outcomes of similar debates. The argument sampled from many, critiqued and with the highest persuasiveness score rated by the preference model is what is presented in the debate. When actually implementing all these changes in the debate protocol, results showed that higher LLM persuasiveness actually significantly improved the accuracy of judges in identifying the correct answers during debates. Persuasive models were generally better at arguing for the correct answers than the incorrect ones. This suggests that optimizing for persuasiveness can lead to more truthful outcomes, as it becomes easier to tell the truth convincingly than to lie. However, judge accuracy also decreases on the baseline, because consultants are similarly more persuasive. (Khan et al., 2024)</p> Figure 8.20: Stronger more persuasive debaters have been shown to result in better judge accuracy, whereas stronger consultants result in worse judge accuracy. (Khan et al., 2024) <p>While persuasiveness helps debate increase judge performance on question answering (QA) tasks with forced information asymmetry, i.e. the articles upon which the questions are based are hidden from the judges, if we remove the asymmetry, (QA when the judge has access to the article) then debate still does not increase judge performance.</p> <p>We find that debate outperforms consultancy across all tasks when the consultant is randomly assigned to argue for the correct/incorrect answer. Comparing debate to direct question answering, the results depend on the type of task: in extractive QA tasks with information asymmetry debate outperforms direct question answering, but in other tasks without information asymmetry the results are mixed.</p> <p>Previous work assigned debaters/consultants an answer to argue for. When we allow them to instead choose which answer to argue for, we find judges are less frequently convinced by the wrong answer in debate than in consultancy.</p> <p>In general though, we have some positive evidence that shows that stronger debaters lead to higher judge accuracy. This provides some evidence that debate satisfies a key objective of scalable oversight \u2013 that judge accuracy increases as AI capabilities scale. (Kenton et al., 2024)</p>"},{"location":"chapters/08/05/#04","title":"8.5.4 Truth","text":"<p>Reminder of the Truth Assumption . The central claim of AI safety via debate is that \"it is harder to lie than to refute a lie.\" (Irving et al., 2017) If this holds true, the debate framework would naturally disadvantage deceptive strategies and favor truthful ones. The truth assumption essentially suggests that in the limit of perfect argumentative skills, the optimal strategy for AI systems in a debate will be to present valid arguments for true conclusions. However, in practice given computational constraints, constraints of the models acting as debaters, and the limitations of the judges themselves this might not hold true. In this section we explore various arguments and counter arguments to the claim that debate will converge to the judge being able to determine the truth.</p> <p>Unnecessary details and distracting Evidence . A common problem is the length of the debate. We don't know how many sequences of arguments and counter arguments it will take to converge to the truth. The dishonest debater can use a couple of strategies to either prolong a state of ambiguity until the judge \u201cruns out of patience\u201d and goes with their possibly-wrong surface impression of the topic. For some questions, there are misleading arguments that appear plausible and then require extensive counter-argumentation to be proven false. By making such arguments, a dishonest debater can stall the debate. (Kova\u0159\u00edk et al., 2019) For example, in a debate about the safety of a new AI system, a dishonest LLM might highlight a rare and dramatic failure of a similar system, which sounds alarming but is statistically insignificant. The honest LLM then needs to spend considerable time explaining why this example isn't representative, potentially overwhelming the judge with technical details and leading to a decision based on the initial misleading argument.</p> <p>Unfair Questions and Asymmetries . A debate can become unfair if certain questions inherently require one side's argument to be significantly harder to make than the other's (Kova\u0159\u00edk et al., 2019). Additionally, the flow of a debate can become asymmetric if one debater gains control over the framework used to approach the question. Asymmetry in debate occurs when one debater establishes a dominant framework that shapes the direction of the debate, biasing the evaluation of arguments and evidence, and making it harder for the honest debater to present their case effectively. (Barnes et al., 2020)</p> <p>When the dishonest debater establishes the framework, they control the narrative and flow of the debate. They can strategically direct attention to their strengths and away from their weaknesses. Within this biased framework, the dishonest debater breaks down the problem into smaller components that are easier to defend individually. Defending smaller, individually reasonable claims is easier than defending a comprehensive, accurate argument. Each of these smaller claims can be true or appear reasonable, but they collectively lead to a misleading conclusion. This situation places the honest debater in a reactive position, forced to challenge each individual claim. This process is time-consuming and complex, often making the honest debater appear weaker. Instead of advancing their own comprehensive argument, the honest debater is constantly on the defensive, addressing the dishonest debater\u2019s points.</p> <p>To address these problems above researchers are trying to implement different mechanisms into the overall protocol like meta-debates which involve having a debate about the fairness of the debate we just had. For example, if one AI is suspected of derailing or misrepresenting arguments in the debate, a separate debate could be held to determine whether its tactics were fair and appropriate.</p> <p>Motte and Bailey Fallacy . The Motte and Bailey fallacy is a deceptive argumentative strategy where someone conflates two positions: one that is easy to defend (the motte) and one that is controversial and harder to defend (the bailey). The term comes from medieval castle design, where a motte was a fortified mound (easy to defend), and a bailey was a more vulnerable area (harder to defend). Here is the general process for how a debater might utilize this fallacy:</p> <ul> <li> <p>Defend the Bailey: Initially, the person asserts the controversial position (the bailey). For example, someone might claim, \"All modern art is a scam designed to exploit the public.\"</p> </li> <li> <p>Retreat to the Motte: When challenged, they retreat to a much more defensible position (the motte), such as, \"Some modern art is overpriced and can be seen as exploitative.\"</p> </li> <li> <p>Equivocate: The person then acts as if the defense of the motte justifies the bailey, saying something like, \"Since we've agreed that some modern art can be exploitative, my original point stands.\"</p> </li> </ul> <p>This tactic creates a moving target for the honest debater. If the honest debater challenges the initial controversial claim, the dishonest debater can retreat to the more defensible position. If the honest debater then refutes the more defensible position, the dishonest debater can shift back to implying the broader controversial claim.</p> <p>Ambiguity . When we switch from debating on things for which we have easily verifiable ground truth labels, to debating things where the outcome might rely on subjective evaluation and fuzzy tasks, then there are additional complications. In these contexts, ambiguity can severely impact the judge's ability to accurately evaluate the arguments. A dishonest debater can exploit ambiguity or multiple interpretations strategically, making it difficult for the judge. The judge might misinterpret an ambiguous statement, leading to an incorrect judgment. Having time constraints, or letting the judge decide when to end the debate, makes ambiguity a strategic tool that a dishonest debater can exploit to their advantage. (Barnes et al., 2020)</p> <p>Even if a question does not bias the debate against the true answer, the debate outcome might still be uncertain until the very end. One way this could happen is if the judge always feels that more information is required to get the answer right. Alternatively, every new argument might come as a surprise to the judge, and be so persuasive that the judge ends up always taking the side of whichever debater spoke more recently. (Kova\u0159\u00edk et al., 2019) The reasoning behind this was also explored in the previous section. The dishonest debater could always provide an extremely convincing but dishonest argument on the n-th turn of an n-turn debate without having to face repercussions of a counter.</p> <p>Surface Area to Attack . One argument in favor of converging to the truth is that in a well-structured debate, a truthful argument should withstand scrutiny better than a deceptive one because it has fewer exploitable flaws. For instance, in legal settings, cross-examination exposes inconsistencies in false testimonies. In AI debates, the detailed nature of the arguments means that deceptive arguments are more likely to be challenged and debunked, leading to the truth being the winning condition more often than not.</p> <p>Efficient Market as a comparison . In efficient markets, prices reflect the true cost of goods because deviations are quickly corrected by market forces. This is analogous to AI debates, where each debater must make the best possible argument to avoid being outperformed by the opponent. However, if consumers are systematically biased, market prices might not reflect true values. Similarly, if judges in debates are biased, the debate outcomes might not reflect the truth. There is a proposal similar to debate for AI Safety based on this approach called AI Safety via. market making. (Hubinger Evan, 2020)</p> <p>Obfuscated Arguments . In addition to controlling the debate narrative by choosing the framing or decomposition of the problem, the dishonest debater can use obfuscated arguments. These are arguments strategically constructed to contain critical errors that are difficult to detect or point out.</p> <p>The debate hypothesis is that, given a correct answer, the debate mechanism should surface this answer with a manageable number of human judgments and computational steps. However, the obfuscated arguments might mean that some arguments are too large to be comprehensively verified as true. If a problem is broken into subproblems where both debaters know at least one subproblem's answer is incorrect but neither knows which one, the dishonest debater can efficiently exploit this uncertainty. This scenario makes it difficult for the honest debater to pinpoint and challenge the flawed step without checking an infeasible number of steps. If debate cannot reliably ensure correctness, debates might only be effective for arguments small enough to be fully scrutinized within the given computational constraints.</p> <p>This presents a significant challenge because what the judge sees in these cases is very hard to distinguish. Imagine a scenario where the arguments are complex and dense with information. One debater argues that there is a low probability of a flaw in their argument and thus it should be trusted. The other debater counters that there is a high probability of a flaw somewhere in the argument, but can't really point out where, suggesting the argument should not be trusted. This is related to discrimination and critique abilities of the debaters that we talked about in the previous section. If the honest model is unable to concretely find the flaw in the argument, or effectively guide the judge to identify the error then this might be a limitation to the effectiveness of debate as a safety technique. The complexity here is in the fact that, in the honest case, any step in the argument that is checked will always be correct. Conversely, in the dishonest case, any step that is checked will almost always be correct, but there will be occasional, strategically placed errors. (Barnes et al., 2020) So it comes down to how many calls to verification of claims we can make under constraints of time and computational resources.</p> <p>To illustrate this, consider a simplified example with a large number called RSA-2048. The honest debater shows a reliable test proving that RSA-2048 can be divided by smaller prime numbers and supports this with solid reasoning. Meanwhile, the dishonest debater picks a number X and argues there are no prime factors smaller than X and no prime factors larger than X, concluding incorrectly that RSA-2048 has no prime factors. Both debaters add more details to their arguments. The honest debater's claims are always accurate, while the dishonest debater's claims are usually correct but occasionally wrong. This makes it hard for the judge to tell who is right without checking many steps of each argument, which takes a lot of time and effort. (Barnes et al., 2020)</p>"},{"location":"chapters/08/06/","title":"8.6 Weak-to-Strong (W2S)","text":"Reading Time 10 min <p>Historically, much of the work on AI alignment has been highly theoretical, focusing on foundational aspects of agent behavior, inner alignment, and risks from learned optimization. Even the techniques that we talked about in previous sections like debate or IDA are often criticized for being frameworks rather than practical solutions, or mainly working on toy problems without addressing the core challenge of aligning superintelligent AI in real-world scenarios. So even though we can only conduct safety experiments on current-generation models, how can we be sure that these techniques will remain effective as AIs approach superhuman capabilities?</p> <p>Narrowly superhuman models allow case studies of scalable oversight . Current-day models are good enough at fuzzy tasks enough to be better than humans in some domains, but crucially they are still not better than all humans, or superhuman enough that we cannot generate ground truth labels. These types of models are sometimes called narrowly superhuman. This distinction between narrowly superhuman and superhuman is very important. As an example of this difference, AlphaGo is superhuman in the sense that it beat Lee Sedol making it better than every single human alive, whereas GPT-4 is still only capable of writing text better than some humans, but not all humans. This means that we can use narrowly superhuman AIs as case studies! We can use either experts or the ground truth labels that we still have access to, and see if alignment increases when we use our scalable oversight techniques. (Cotra, 2021)</p> <p>The core intuition here is to simulate future scenarios where humanity, equipped with various tools and techniques, oversees the outputs of untrustworthy but superhuman systems. There are various ways to conduct experiments on narrowly superhuman models. We can use non-experts equipped with scalable oversight techniques to align AI models. Another way is to use weak models (e.g. GPT-2) to represent humans, while stronger models (e.g. GPT-4) represent more capable AI systems that we want to align.</p> <p>Stronger models are expected to have latent capabilities . The assumption is that stronger models, due to their extensive pre-training on diverse data, already have internal representations for the type of actions we want. The role of weak supervision is to bring out this behavior through training signals.</p> <p>As a concrete example, imagine using GPT-4 for getting medical advice. It has read countless research papers and medical journals. It has internal representations of a lot of good medical information, making it theoretically capable of giving highly competent medical advice. But GPTs are initially only designed to predict the most likely next word, not give accurate advice. In this context, \"aligning\" the model means getting the model to give accurate and helpful medical advice. One type of technique we can try is fine-tuning GPT-4 on labels generated by GPT-2. It\u2019s not the only way, there are other techniques that we will explore later in this section. For now, the most important thing to understand is that we are currently operating under the assumption that both current and future superhuman models will likely have salient internal representations of human behaviors.</p> <p>What is weak-to-strong generalization (W2SG)? Weak supervision involves training AI models using labels or feedback that are less accurate, less detailed, or noisier than those provided by highly knowledgeable or capable supervisors. This can happen when supervisors (whether humans or weaker models) are not experts in the task or when the data is incomplete or contains errors.</p> Figure 8.21: Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision (Burns et. al. 2023) <p>Weak-to-strong generalization (W2SG) is when a strong model, trained with weak supervision, manages to outperform its weak supervisor by leveraging its pre-existing knowledge and capabilities. The core idea is that the strong model already possesses the necessary capabilities for the desired behavior, and the weak supervision elicits this behavior despite its imperfections. The process of W2SG right now typically begins by fine-tuning a large pre-trained model using weak supervision from smaller models. Although the initial supervision may come from these less capable models, the ultimate aim is to transition to human supervision. The objective is to bring out the strong model\u2019s full potential as if it were trained on perfect ground truth supervision. (Burns et. al. 2023)</p> <p>In current experiments, the setup involves:</p> <ol> <li> <p>A \"weak supervisor\" (a small pre-trained language model) is finetuned on a specific task, generating predictions (soft labels) on a held-out dataset.</p> </li> <li> <p>A \"strong student\" (a larger pre-trained LM) is finetuned on the weak model's predictions.</p> </li> <li> <p>A \"strong ceiling\" (another copy of the larger model) is finetuned directly on ground-truth labels for baseline comparison.</p> </li> </ol> <p>The level of weak-to-strong generalization is quantified using Performance Gap Recovered (PGR). PGR measures how much of the performance difference between a weak supervisor and a strong model is bridged when the strong model is trained using weak supervision. When we train the strong model using the labels provided by the weak model, the performance of the strong model will likely be somewhere between the performance of the weak model and the ideal performance the strong model could achieve with perfect labels. The performance gap recovered quantifies how much closer the strong model's performance gets to its ideal performance compared to the weak model\u2019s performance. A PGR of 1 corresponds to an ideal outcome (the strong model performs just as well as it would have with perfect supervision). In contrast, a strong student who \"succeeds\" in perfectly imitating their supervisor, including their flaws, would obtain a PGR of 0. (Scherlis et. al. 2024)</p> \\[\\begin{align*} PGR = \\frac{\\text{student} - \\text{weak}}{\\text{ceiling} - \\text{weak}} \\end{align*}\\] <p>Limitations and disanalogies of W2SG :</p> <ul> <li> <p>Overfitting to weak supervision : The assumption that a strong model will generalize well from weak supervision is not guaranteed. Superhuman models might easily imitate weak errors. These models will likely be very proficient at predicting what humans will think and say, especially if they are trained on human data. Consequently, if we naively train such a superhuman model with human supervision, it might simply imitate the weak supervisor, outputting human-level capabilities rather than its latent superhuman capabilities. Researchers also use auxiliary confidence losses, which encourage the strong model to make confident predictions even when contradicting the weak supervision, promoting generalization and correcting weak supervisor mistakes.</p> </li> <li> <p>Assumptions about task representations . W2SG assumes that strong models have salient representations of the tasks they are trained on. This means the models already possess some understanding of these tasks from their pre-training phase. However, this assumption may not hold true for novel or highly complex tasks. If a task is entirely new or significantly more complex than what the model has encountered during pre-training, the model might not have the latent capabilities necessary to perform well even with weak supervision.</p> </li> </ul> <p>The experiments on W2SG so far may have been observed in pre-training, at least indirectly. Using the example from earlier, medical data or direct questions and answers about medical practice is present in the GPT-4 pre-training dataset in some form. However, future superhuman models may never directly observe superhuman alignment-relevant capabilities. Which means these types of capabilities might be harder to elicit than capabilities that models could have observed in their pre-training data. This disanalogy could cause current results on W2SG to be overly optimistic.</p> <ul> <li>Slow Takeoff Assumption : W2SG also relies on the assumption of a gradual takeoff in AI capabilities. This gradual progression allows researchers enough time to use moderately superhuman models to solve alignment problems iteratively before it's too late. The window of opportunity provided by a gradual takeoff is crucial for refining and testing alignment techniques.</li> </ul> <p>W2SG can be seen as a complement to scalable oversight techniques . W2SG is not a complete solution. Even if a model generalizes in the desired direction, this must be verified, requiring a ground-truth signal more reliable than naive human supervision. By integrating W2SG with scalable oversight, we can develop more robust methods for aligning AI with human values, preparing for the challenges posed by future superintelligent systems.</p> <p>For example, scalable oversight techniques might be used to generate weak supervision signals that a strong model will then learn to generalize beyond. By combining these approaches, we can create more robust protocols for AI alignment. For example, recursive reward modeling (RRM) can use W2SG to train powerful reward models with human preference annotations. Debate combined with W2SG can train models to generalize human judgments to new debates. Task decomposition combined with W2SG can supervise atomic tasks with a reward model trained from human preferences. (Leike, 2023)</p> <p>Evaluating these techniques in different settings helps understand their strengths and weaknesses. In non-scheming settings, where models are not deceptively aligned, classic weak-to-strong techniques and scalable oversight can be directly compared. In scheming settings, where models might act adversarially, evaluations need to consider potential deception, providing a conservative measure of a protocol\u2019s robustness. When there is no scheming (deceptive alignment), then we can use W2G techniques in a straightforward manner through techniques like sandwiching. However, if we have scheming (deceptively aligned AI) it might act adversarially. In this case, we can use proposals like meta-level adversarial techniques. Both of these are what we discuss in the following sections.</p>"},{"location":"chapters/08/06/#01","title":"8.6.1 Sandwiching Evaluations","text":"<p>Sandwiching is an experimental setup that leverages narrowly superhuman models to evaluate how well our oversight techniques work in controlled settings. The sandwiching setup contains three primary \u2018layers\u2019:</p> <ul> <li> <p>Non-Expert Layer: At the bottom, we have non-experts. These are people who wouldn't be able to solve the given task independently. As an example, think of lay people being asked to pick stocks or give medical diagnoses. These are stand-ins for a future version of humanity. They need to use the scalable oversight techniques to guide the AI's behavior in a more aligned direction.</p> </li> <li> <p>AI Model: In the middle, we have the narrowly superhuman AI model that we want to test the oversight technique on. This model has some capabilities that we are unable to elicit. The non-expert should be able to \u201calign\u201d the model by getting it to display the kind of behavior we want using the given scalable oversight technique.</p> </li> <li> <p>Expert Layer: At the top, we have domain experts who evaluate the success of the alignment attempt. They serve as a benchmark, providing feedback without directly interacting with the non-experts or the AI during the process. We can also use ground truth labels from datasets as replacements for the expert layer instead of hiring actual experts.</p> </li> </ul> Figure 8.22: (Bowman et al., 2022) <p>Empirical demonstration of sandwiching . Researchers conducted an experiment with non-experts tasked to answer multiple choice questions. The task was to answer multiple-choice questions from the Measuring Massive Multitask Language Understanding (MMLU) benchmark. This benchmark contains hundreds of questions on math, physics, morality, ethics and a range of topics. (Hendrycks et. al, 2021) They also had to answer questions based on short stories from QuALITY benchmark in 5 minutes. (Pang et al., 2021) The scalable oversight technique that they were given was a simple amplification using an AI assistant chatbot. The performance was measured by using the correct answer ground truth labels from the dataset.</p> <p>When measured individually, the model outperformed unassisted humans on both tasks by a large margin. This means the basic condition for testing sandwiching was fulfilled - the model being better than the non-experts. Over the course of the experiment, the chatbot assisted humans got substantially better scores on the tasks than either the humans or the models did alone. (Bowman et al., 2022) Even though this was an extremely simplified design, the participants managed to move the behavior of the model in the direction that we would want, i.e. make the model more \u201caligned\u201d. So the researchers managed to effectively demonstrate sandwiching as an experimental design. Building on this baseline, future experiments can evaluate the efficacy of more complex scalable oversight methods like re-training, fine-tuning, or debate.</p>"},{"location":"chapters/09/","title":"Interpretability","text":"Authors                      Jeanne Salle &amp; Charbel-Raphael Segerie                  Affiliations French Center for AI Safety (CeSIA) Acknowledgements              Markov Grey          Last Updated 2024-11-01 Reading Time 56 min (core) Also available on Google Docs Watch Listen Download Feedback Facilitate"},{"location":"chapters/09/#overview","title":"Overview","text":"<p>We currently don\u2019t understand how AI models work. We know how to train and build them, meaning we can design them and teach them to perform tasks, such as recognizing objects in images or generating coherent text in response to prompts. However, this does not mean we can always explain their behavior after training. As for now, we can\u2019t explain why a network made a specific decision or produced a particular output. The goal of interpretability is to understand the inner workings of these networks and explain how they function, which in turn could allow us to better trust and control AI models.</p> <p>Before reading this chapter it is recommended to be familiar with the transformer and CNN architectures.</p> Video 9.1: Optional Video. If you are unfamiliar with convolutional neural networks (CNNs), this video will help you get up to speed before reading this chapter. Video 9.2: Optional Video. If you are unfamiliar with transformers, the videos on transformers in this playlist will help you get up to speed before reading this chapter. <p>For each method presented in this chapter, we first provide a high-level overview, followed by a more in-depth and technical explanation. The technical explanations can be skipped.</p> \u2767"},{"location":"chapters/09/01/","title":"9.1 What is Interpretability ?","text":"9 min read                1838 words      <p>Interpretability is the study of how and why AI models make decisions. Its central aim is to understand the inner workings of models and the processes behind their decisions. There are diverse approaches to interpretability, but in this chapter\u2014and the broader context of AI safety\u2014mechanistic interpretability (mech interp) is the primary focus (Ras et al., 2020, Ali et al., 2023).<sup>1</sup></p>"},{"location":"chapters/09/01/#mechanistic-interpretability-the-bottom-up-approach","title":"Mechanistic Interpretability: The Bottom-Up Approach","text":"<p>Mechanistic interpretability seeks to reverse-engineer neural networks to uncover how their internal components\u2014such as neurons, weights, and layers\u2014work together to process information. This approach starts at the lowest level of abstraction and builds understanding piece by piece: this is why it\u2019s considered a bottom-up approach. By analyzing these basic components, we hope we can piece together how the network processes information and makes decisions.</p> <p>For example, mechanistic interpretability could explain how a neural network recognizes objects in an image or generates language, down to the contributions of individual neurons or attention heads. The hope is that this level of detail will allow researchers to diagnose and potentially fix unwanted behaviors in AI systems.</p>"},{"location":"chapters/09/01/#other-approaches-to-interpretability","title":"Other Approaches to Interpretability","text":"<p>While mechanistic interpretability is a strong focus in AI safety, it is not the only approach. Other methods provide complementary perspectives:</p> <ul> <li> <p>Concept-Based Interpretability: Contrarily to mechanistic interpretability, concept-based interpretability takes a top-down approach: instead of analyzing neurons or weights on a granular level, it focuses on understanding how the network manipulates high-level concepts (Belinkov, 2022). For instance, representation engineering \u2014a concept-based research agenda\u2014 explores how models encode concepts like \"honesty\" and how those representations can be adjusted to produce more honest outputs (Zou et al., 2023).</p> </li> <li> <p>Developmental Interpretability: This approach examines how model capabilities and internal representations evolve during training. By understanding the emergence of behaviors or knowledge over time, researchers hope they will be able to identify the emergence of certain capabilities and prevent unwanted ones from developing (Hoogland et al., 2023).</p> </li> <li> <p>Behavioral Interpretability: Unlike the previous approaches, behavioral interpretability studies input-output relationships without delving into the internal structure of models. </p> </li> </ul> Figure 9.1: A visual classification of interpretability techniques. (Bereska &amp; Gavves, 2024)."},{"location":"chapters/09/01/#why-mechanistic-interpretability-matters-for-ai-safety","title":"Why Mechanistic Interpretability Matters for AI Safety","text":"<p>Mechanistic interpretability is a strong focus in AI safety because it provides a level of precision that other approaches do not. Behavioral interpretability, for instance, offers insights into how a model behaves by studying input-output relationships, but it cannot reveal how its internal structure leads to its decisions. To prevent AI models from making harmful decisions or ensuring alignment with human values, we need to understand why models make certain decisions, and potentially steer the decision-making process.</p> <p>For instance, by pinpointing where harmful concepts\u2014such as instructions for cyberattacks\u2014are stored within a model, mechanistic interpretability tools could help erase or modify those concepts without degrading the model\u2019s overall capabilities.</p>"},{"location":"chapters/09/01/#01","title":"9.1.1 Motivation for AI Safety?","text":"<p>The ultimate goal of interpretability, from an AI safety perspective, is to build confidence in the behavior of complex models by understanding their internal mechanisms and ensuring they act safely and predictably. There are different ways interpretability could contribute to AI safety (Nanda, 2022):</p> Trust and transparency <p>By offering insights into which features of the input data (such as specific parts of an image or words in a sentence) or which specific concepts a model uses in its reasoning are influencing the model's outputs, interpretability tools can make it easier for users to understand, verify, and trust the behavior of complex models. This is particularly important in high-stakes applications like healthcare or autonomous systems, where trust in AI decisions is crucial.</p> Enhance model editing <p>Interpretability could be used to mitigate misuse by erasing specific pieces of knowledge from neural networks, such as knowledge on conducting cyberattack or building bioweapons. </p> <p>Some mechanistic interpretability tools serve to locate where certain concepts are encoded in a model (those are covered in the Observational Methods section). For example, researchers could identify how models generate answers to harmful queries such as \"steps for creating a virus\" and use knowledge erasure methods to remove that knowledge from the model.</p> Detection of undesirable behaviors <p>Interpretability could help identify when models are not functioning as intended or have learned undesirable behaviors or patterns. For alignment research, it could be used to detect, analyze, and understand undesired answers from models.</p> <p>A well-known example of this comes from a model trained to classify chest X-rays. Researchers discovered that instead of focusing on medical features like lung conditions, the model was using subtle artifacts from the scanning equipment or markers in the image to make its predictions. This led the model to perform well in testing but for the wrong reasons. By using interpretability tools, researchers identified and corrected this issue (DeGrave et al., 2021).</p> Extrapolation <p>Some hope that by understanding how current models function, interpretability may help predict how future, larger models will behave, whether new capabilities or risks will emerge, and how systems will evolve as they scale.</p> <p>It\u2019s important to note that while these goals are promising, the field of interpretability is still maturing. The adoption of interpretability tools in real-world scenarios is still limited, and assessing their quality remains challenging. Many existing techniques in interpretability are not designed for large-scale use and state-of-the-art models. These limitations will be explored in more detail in the Critics of Interpretability section.</p>"},{"location":"chapters/09/01/#02","title":"9.1.2 What is the End Goal of Interpretability?","text":"<p>What concrete outcomes should interpretability achieve to make AI systems safer and more predictable? There are different opinions on the end goals of interpretability, including the following approaches:</p>"},{"location":"chapters/09/01/#enumerative-safety-cataloging-concepts-for-control","title":"Enumerative Safety: Cataloging Concepts for Control","text":"<p>Enumerative safety aims to identify and catalog all the concepts and behaviors encoded within a model. The idea is straightforward: if we can thoroughly understand and enumerate every action the model can take, we could selectively remove undesirable behaviors and ensure that the model can only perform safe and desirable actions (Elhage et al., 2022).</p> <p>For example, if a language model encodes harmful instructions\u2014such as steps to create a cyberattack\u2014enumerative safety would involve locating and eliminating this knowledge without impairing the model\u2019s useful functions.</p> Figure 9.2: Enumerative safety aims to ensure that in all situations, the model doesn\u2019t do something we don\u2019t want. From (Olah, 2023)."},{"location":"chapters/09/01/#retargeting-the-search-steering-model-objectives","title":"Retargeting the Search: Steering Model Objectives","text":"<p>Retargeting the search represents a more ambitious goal: rather than removing harmful behaviors, it seeks to directly modify a model\u2019s objectives. This involves identifying how the model internally represents its goals and redirecting those representations to align with human values.</p> <p>Unlike enumerative safety, retargeting does not require reverse-engineering the entire model. Instead, it focuses on altering specific components while preserving the system\u2019s overall functionality. For instance, if a model learned to optimize for harmful outcomes, researchers could \u201cretarget\u201d this optimization toward beneficial goals (johnswentworth, 2022).</p>"},{"location":"chapters/09/01/#relaxed-adversarial-training-testing-and-improving-corrigibility","title":"Relaxed Adversarial Training: Testing and Improving Corrigibility","text":"<p>Relaxed adversarial training is an approach designed to enhance the robustness of AI systems, particularly to ensure their corrigibility\u2014their ability to accept and assist with corrective interventions. Traditional adversarial training tests a model's robustness by exposing it to adversarial inputs. Relaxed adversarial training, however, works by testing against adversarial latent vectors instead of real inputs. Adversarial training traditionally tests a model\u2019s robustness by exposing it to adversarial inputs, but here the goal is to generate perturbation in the model\u2019s latent space.</p> <p>Mechanistic interpretability could be used to identify latent vectors that correspond to specific model behaviors, such as corrigibility or alignment with user intentions, and then test whether the model resists corrigibility directly by manipulating its internal representation (Christiano, 2019).</p>"},{"location":"chapters/09/01/#mechanistic-anomaly-detection-mad-spotting-deviations","title":"Mechanistic Anomaly Detection (MAD): Spotting Deviations","text":"<p>Mechanistic Anomaly Detection (MAD) focuses on identifying instances where a model produces outputs for unusual reasons. While traditional interpretability methods aim to understand a model\u2019s mechanisms comprehensively, MAD takes a more targeted approach: it flags anomalies in the decision-making process without requiring a full understanding of the underlying mechanisms.</p> <p>For example, MAD could detect when a model\u2019s reasoning deviates from its usual patterns, such as when it relies on spurious correlations rather than meaningful features. Insights from mechanistic interpretability could be useful to flag instances when a model operates outside its usual patterns of behavior (Jenner, 2024).</p>"},{"location":"chapters/09/01/#03","title":"9.1.3 Overview of the Field of Mechanistic Interpretability","text":"<p>Mechanistic interpretability is a diverse field, but its techniques can be broadly categorized into two main approaches: observational methods and interventional methods <sup>2</sup>. These categories reflect two complementary strategies for understanding how neural networks process information. </p> Observational Methods: Analyzing Without Intervening <p>Observational methods aim to analyze a model\u2019s internal structures\u2014such as neurons, activations, and weights\u2014without directly modifying them. These techniques allow us to observe what a model represents or lacks in its internal computations but do not reveal how or whether the model uses this information during inference. They include techniques such as probing classifiers, the Logit Lens, and Sparse Autoencoders. </p> <p>For example, in a study of a chess-playing neural network, researchers used probing classifiers to investigate whether the network encoded board states and strategic concepts. They discovered that the model had learned to represent critical features such as whether a player could capture their opponent\u2019s queen or the number of pieces each side controlled (McGrath et al., 2022).</p> <p>Observational methods provide an essential foundation for understanding model representations, but they have limitations. Most notably, they cannot determine whether the observed representations causally influence the model\u2019s behavior.</p> Interventional Methods: Manipulating for Causal Insights <p>Interventional methods take the next step by actively manipulating a model\u2019s internals to understand their causal role in decision-making. By modifying activations or parameters, researchers can test how specific components contribute to overall behavior.</p> <p>Interventions help us identify what information is used by the model, and how. They include techniques such as activation patching and activation steering.</p> <p>For instance, researchers at Anthropic demonstrated the potential of interventional methods by manipulating a model\u2019s activations at runtime. They were able to make the model sound more joyful in its responses, alter its stated preferences and goals, change its biases -leading it to produce more offensive or harmful content-, and induce specific types of errors, such as coding mistakes, that the model would typically avoid (Templeton et al., 2024).</p> <p>Observational methods, as you will see, can serve as a foundation for interventional methods because they guide where to intervene. For example, an observational study might identify a layer encoding a critical feature, which can then motivate an intervention to test whether and how that feature influences outputs.</p> \u2767 <ol> <li> <p>For an overview of the broader interpretability landscape see (Ras et al., 2020; Ali et al., 2023)\u00a0\u21a9</p> </li> <li> <p>We follow the classification introduced by (Bereska et Gavves, 2024). Even though it may be debated, we believe it provides newcomers with a clear introduction to the fundamentals of the field.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/09/02/","title":"9.2 Observational Methods","text":"34 min read                6795 words"},{"location":"chapters/09/02/#01","title":"9.2.1 Feature Visualization","text":"<p>Feature visualization is one of the first observational methods in mechanistic interpretability. It allows researchers to explore the features that a vision model learns and uses at different layers. In this section, we\u2019ll explain in detail what feature visualization is, and the discoveries it enabled.</p>"},{"location":"chapters/09/02/#what-is-a-feature","title":"What is a Feature?","text":"<p>A feature is a pattern of the input data that the network learns to detect. Models have features because they help break down complex inputs, such as images or text, into interpretable components that the model can use to make predictions.</p> <p>Vision models trained to do image classification commonly possess features corresponding to cats, dogs, cars, eyes, fruits, etc. Vision model features vary in complexity depending on the layer in which they appear. In the early layers, features typically represent simple, low-level patterns such as edges, colors, or textures in a vision model. As you move to deeper layers, the features become more abstract and complex, representing higher-level concepts like shapes, objects, or even specific entities like faces or animals. This hierarchical structure of features allows models to progressively process raw pixel data into higher-level concepts to ultimately classify the image. For instance, recognizing a car would start with detecting edges (low-level features), then specific shapes (like wheels), and eventually the entire car (high-level feature).</p> <p>In transformer models, researchers have identified features corresponding to specific concepts like DNA sequences, legal language, HTTP requests, or Hebrew text. For instance, when a transformer encounters a DNA sequence, neurons encoding the \u201cDNA feature\u201d activate strongly in response. Similarly, given a sentence like \u201cThe court ruled in favor of the defendant because\u2026,\u201d features linked to legal language and sentence structures common in legal contexts may activate, helping the model predict a follow-up involving reasoning or justification, such as \u201c\u2026the evidence presented was insufficient.\u201d</p> <p>These features are essential building blocks that models use to make sense of input data and make predictions.</p>"},{"location":"chapters/09/02/#how-is-a-feature-visualization","title":"How is a Feature Visualization?","text":"<p>Feature visualization is a technique that helps us investigate the inner representations of Convolutional Neural Networks (CNN), and observe the patterns that a model has learned to recognize. It generates images that maximize the activation of specific neurons, feature maps (outputs of a convolutional layer), or even entire layers in a model (Cammarata et al., 2020).</p> Figure 9.3: Examples of feature visualizations. It seems that the network has learned to represent and detect baseballs, animal faces, clouds, and buildings, but feature visualizations have to be interpreted and may not be detecting what we initially think. From (Olah et al., 2017). <p>Early research in neuroscience aimed to understand the brain by identifying which images strongly excited specific neurons. This helped neuroscientists discover brain areas dedicated to identifying faces, movement, natural scenes, etc. Feature visualization can be thought of as a somewhat similar approach used to CNNs (Olah et al., 2017). </p> <p>These learned features\u2014whether simple patterns like edges or complex objects\u2014form the building blocks that enable models to understand and process input data. However, features don't operate in isolation. As the network processes information, features interact and combine in structured ways, often forming more complex units known as circuits. A circuit is a group of interconnected features that work together to perform a specific function.</p> <p>For example, here is a circuit that recognizes a car in the mid-layers of a CNN. This circuit combines lower-level features such as windows, car bodies, and wheels to detect the presence of a car in an image. The circuit allows the model to recognize the entire object, even though no single feature on its own can do so.</p> Figure 9.4: A car circuit in a CNN. On the left, three feature maps from layer 4b are represented by their feature visualizations. One map appears to detect windows, another car bodies, and the third wheels. These three feature maps are connected to a feature map in layer 4c, represented by the visualization on the right, through the convolutional kernels shown in the middle. The window, car body and wheel features get assembled to form a full car detector circuit. From (Olah et al., 2020). Vocabulary Reminder <ul> <li> <p>A feature refers to a specific pattern or characteristic that the network learns to detect. These features are the fundamental units that models use to process information and make decisions.</p> </li> <li> <p>Feature visualization is a method that enables us to identify the features learned by a CNN, which can help us understand how it processes information and makes decisions. </p> </li> <li> <p>A circuit is a group of interconnected features that work together to perform a specific function. Circuits are essentially higher order features, that are recursively composed of lower order features. The notion of circuit applies to any model architecture, including LLMs. Identifying circuits that perform specific functions in models is one area of research in mechanistic interpretability. </p> </li> <li> <p>A feature map in a CNN is the output of a convolutional layer. It\u2019s also called a channel. Feature visualization is often applied at the scale of feature maps - instead of individual neurons or entire layers - to understand which features they encode.</p> </li> </ul> <p> Figure 9.5: Some examples of feature visualizations on a CNN trained for image classification. Each image corresponds to a feature map. Certain feature maps are sensitive to patterns with edges, others are sensitive to different kinds of textured patterns, or objects like eyes, dog faces, or legs. From (Olah et al., 2017). </p>"},{"location":"chapters/09/02/#01","title":"9.2.1.1 Feature Visualization Method","text":"<p>Below is extra detail provided for those interested. It can be safely skipped.</p> Figure 9.6: Overview of the feature visualization process. Given a neuron (or a set of neurons) in a CNN, feature visualization generates an image that highly activates it, starting from a random image, and through successive optimization steps. The optimized image illustrates what kind of pattern one of the feature maps in the fourth layer of InceptionV1 is sensitive to (Olah et al., 2017). <p>Generating a feature visualization involves:</p> <ol> <li> <p>Select a target: Choose a neuron, feature map, or layer for visualization. Most feature visualizations are performed on feature maps.</p> </li> <li> <p>Start with a random input image: Begin optimization from a random noise image. It will be adjusted to maximize the activation of the target neuron or filter.</p> </li> <li> <p>Compute the gradient. The objective is to update the image such that it maximizes the activation of the neuron/feature map: Use backpropagation to calculate how to modify the image to increase the activation of the target. </p> </li> <li> <p>Optimize the image: Apply gradient ascent iteratively to adjust the image.</p> </li> <li> <p>Repeat the gradient ascent steps: Slightly adjust the image each time to better activate the neuron/feature map.</p> </li> <li> <p>Visualize the result: The final image reveals the patterns or structures the target has learned to detect.</p> </li> </ol> Figure 9.7: Feature visualizations can be produced for individual neurons or groups of neurons, such as a feature map, or even an entire layer. From (Olah et al., 2017)."},{"location":"chapters/09/02/#02","title":"9.2.1.2 Circuits","text":"<p>Each layer in a CNN progressively extracts increasingly complex features from the image. Neurons in early layers respond to rudimentary and abstract patterns such as curves, angles, and small shapes (similar to the first layers of the human visual cortex!). As we move deeper into the network, neurons detect more complex and specific objects, such as eyes, animals, cars, etc (Olah et al., 2020). Interestingly, some of these \"neuron families\" recur across different model architectures and training conditions (Olah et al., 2020).</p> Figure 9.8: Curve detectors are universally found in early layers of CNNs, they exist in different orientations and colors and collectively span all orientations. Each curve detector responds to a wide variety of curves, in different orientations. From (Olah et al., 2020). <p>CNNs also commonly learn high-low frequency detectors in their early layers.</p> Figure 9.9: High-low frequency detectors look for low-frequency patterns on one side of their receptive field, and high-frequency patterns on the other side. They exist in different orientations and colors. From (Olah et al., 2020). <p>High-low frequency detectors assemble in deeper layers to form boundary detectors.</p> Figure 9.10: A boundary detector neuron formed in the third layer of a CNN (shown at the bottom left with its feature visualization). The top row shows feature visualizations from neurons in the second layer and the kernels connecting them to the third layer. New neurons form by combining cues from more elementary neurons in previous layers: here the boundary detector neuron forms by combining high-low frequency detector neurons, with edges detector neurons, color contrast detector neurons, etc. From (Olah et al., 2020)."},{"location":"chapters/09/02/#03","title":"9.2.1.3 Polysemantic Neurons","text":"<p>While feature visualization has enabled a deeper understanding of how CNNs represent information, it has also highlighted challenges like polysemanticity. An intriguing phenomenon occurs when we look at the neurons following the car detector and that are strongly connected to it. Some of these neurons respond not only to images of cars but also to unrelated stimuli, such as images of dogs. This indicates that the \"car feature\" gets spread across multiple neurons that respond to seemingly unrelated inputs. These are known as polysemantic neurons\u2014neurons that activate in response to a variety of distinct features. In contrast, monosemantic neurons respond to just one specific feature or stimulus.</p> Figure 9.11: On the left is the car detector circuit from the previous figure. After distinct concepts are formed, they become entangled in polysemantic neurons, such as one neuron that responds to both car and dog images. From (Olah et al., 2020). Figure 9.12: Multiple feature visualization performed on a polysemantic neuron that responds to images of cars, as well as cat faces, and cat legs. From (Olah et al., 2020). <p>Polysemantic neurons are very common. For example, in a small language model, we can find a neuron that responds to English dialogues, Korean texts, HTTP requests, and academic citations simultaneously (Bricken et al., 2023). This means that each neuron does not correspond to one specific feature. Therefore, reasoning about a network's behavior based on individual neurons is misleading. Neurons are not the fundamental units to focus on when trying to understand models. </p> <p>Polysemanticity poses a challenge for interpretability because it requires understanding how features are encoded across multiple neurons, rather than assuming each neuron represents a discrete unit of meaning. Identifying how these distributed features are encoded is an active area of research in interpretability.</p> <p>The leading hypothesis to explain why polysemanticity arises in neural networks is called the superposition hypothesis. Large models need to learn a huge number of features to perform effectively, likely more than the number of neurons they have. As a result, models cannot assign each feature to a single neuron. Instead, they must encode features in a more compressed manner. The superposition hypothesis suggests that models represent more features than they have neurons by encoding multiple features per neuron, with these features oriented in nearly orthogonal directions. In other words, models compress information by overlapping features across multiple neurons.</p> Polysemanticity vs Superposition <p>The distinction between polysemanticity and the superposition hypothesis is important (Bereska et Gavves, 2024):</p> <ul> <li> <p>Polysemanticity refers to the empirical phenomenon where a neuron represents or responds to multiple unrelated features,</p> </li> <li> <p>The superposition hypothesis, on the contrary, generally refers to an hypothesis that tries to explain polysemanticity. It suggests that when models have more features to represent than they have neurons to represent them, they must compress these features into the limited space. This compression forces features to overlap across neurons. This supposedly explains why we observe neurons responding to multiple, seemingly unrelated features. While superposition inherently leads to polysemanticity, polysemanticity itself doesn\u2019t always imply superposition, as polysemanticity could theoretically arise from other mechanisms.</p> </li> </ul> <p>Understanding and addressing polysemanticity is an active research area. Various directions are being explored:</p> <ul> <li> <p>Sparse Representations: Designing networks to use sparse representations (where fewer neurons are active at a time) may reduce polysemanticity (Elhage et al., 2022). This approach has not been widely explored because it comes with significant performance trade-offs.</p> </li> <li> <p>Feature Disentanglement: Some approaches involve decomposing complex neuron activations to isolate individual features. One promising approach in that line of research, known as Sparse Autoencoders, is a technique that \"unfolds\" the network and separates out individual features (Bricken et al., 2023). It is explained more in depth in the Sparse Autoencoders section.</p> </li> </ul> <p>Feature visualization has led to several key insights about how vision neural networks operate:</p> <ul> <li> <p>Hierarchical structure: Neural networks learn features hierarchically, with early layers detecting simple patterns and deeper layers composing these into complex objects. For instance, curve detectors in early layers combine into boundary detectors in mid-layers and eventually into full object detectors in deeper layers.</p> </li> <li> <p>Circuits: Features interact to form \"circuits\"\u2014groups of interconnected features that collectively perform a specific function. For example, a circuit for detecting cars might combine features like wheels, windows, and body shapes into a cohesive representation of a car.</p> </li> <li> <p>Polysemanticity: Feature visualization has revealed the phenomenon of polysemanticity, where individual neurons or features in a neural network respond to multiple, seemingly unrelated concepts. This overlap complicates our ability to assign clear and interpretable roles to individual neurons.</p> </li> <li> <p>Universality: Some features, such as edge or curve detectors, are universal across models and architectures. This suggests that certain features are fundamental to visual processing, regardless of the specific task or dataset.</p> </li> </ul>"},{"location":"chapters/09/02/#02","title":"9.2.2 Logit Lens","text":"<p>The Logit Lens (nostalgebraist, 2020) is one of the first tools developed to look inside transformers. It enables us to observe how a transformer refines its predictions layer by layer \u2014allowing us to see not just the final output but the evolving \"thought process\" the model undergoes as it makes a prediction.</p> <p>Transformers are trained to predict the next token in a sequence. They do this by transforming the inputs layer by layer, with each layer adding new information to improve the prediction. The Logit Lens enables us to \u201ctranslate\u201d each layer's internal representation back into tokens. By seeing what the model \u201cpredicts\u201d at each layer, we can trace how its predictions evolve from a rough guess in the initial layers to a refined choice in the final one. However, it\u2019s worth noting that while the Logit Lens lets us see the intermediate predictions at each layer, it doesn\u2019t explain the mechanism of transformation. The Logit Lens is an inherently observational tool \u2014it reveals what is the most probable token at the end of each layer but doesn\u2019t allow us to understand why this precise token is predicted.</p> <p>Below is extra detail provided for those interested. It can be safely skipped.</p> <p>A transformer model is a powerful architecture for processing sequences of data, especially text. It is trained to predict the next word or subword in a sequence, referred to as tokens.</p> <p>The set of all tokens (words or subwords) that the model can recognize is called the vocabulary. For example, GPT-2 has a vocabulary of 50,257 tokens. More precisely, a transformer takes a sequence of tokens as input and is trained to predict the next token in this sequence, outputting a probability distribution over the entire vocabulary.</p> <p>Internally, a transformer consists of multiple stacked layers, each containing two sublayers: an MLP (multi-layer perceptron) and attention heads. For understanding the Logit Lens, we don\u2019t need to go into the details of how these sublayers function.</p> <p>The intermediate layers are connected through what\u2019s known as the residual stream. The residual stream is a pathway that carries information from the input to the output, allowing it to flow through all layers of the model.</p> Figure 9.13: A minimalist illustration of a transformer with a single layer. Transformers cannot directly manipulate tokens, so they embed tokens into numerical vectors, an operation depicted in the bottom grey box. After embedding, the tokens are represented as vectors within the residual stream. The first sublayer (attention heads, represented by the two side-by-side boxes on the left) reads these embedded tokens, applies a transformation, and adds its output back into the residual stream. The second sublayer (MLP) does the same. Finally, to output a token, the embedded tokens must be converted back to the vocabulary space through an unembedding operation (depicted in the top box). The residual stream enables each layer to make incremental adjustments to the model\u2019s predictions by accumulating and refining the information passed through each transformation. From (Elhage et al., 2021). <p>The essential idea behind the Logit Lens is that the unembedding operation, typically applied only after the last layer, can be applied at any point in the residual stream. After each intermediate layer, the residual stream can be unembedded\u2014that is, converted back into the token space\u2014allowing us to observe which token is currently the most probable.</p> <p>Connection between the Logit Lens and Feature Visualization in CNNs. In the previous section on Feature Visualization, we saw that CNNs build up their understanding of an image layer by layer, detecting simple patterns like edges in early layers and more complex shapes or objects in later ones. Feature visualization allows us to interpret these progressive transformations by displaying the visual patterns that different neurons respond to. The Logit Lens can be seen as a parallel interpretability tool for transformers. Instead of visualizing image features, the Logit Lens lets us see the model\u2019s step-by-step guesses for the next token.</p> <p>The figure shows how using the Logit Lens looks in practice.</p> Figure 9.14: An example of the Logit Lens used on GPT-2 when it\u2019s trying to predict the next token of the sequence: \u201cSpecifically, we train GPT-3, an\u201d. Input tokens are written at the bottom, and correct outputs at the top. Layers stack from bottom to top. The first token at the top, \",\", corresponds to the token the model should predict when given as input only the first token: \"Specifically\". The second token at the top, \"we\", corresponds to the token the model should predict when given as input only the two first tokens: \"Specifically, \". This is the reason why there is a shift of one position between tokens at the bottom and tokens at the top. (*) indicates that the model correctly predicted the next token. Each cell contains the model\u2019s top guesses at different layers. The color scale indicates the associated logit value. The higher the logit, the more confident in its prediction the model is. From (nostalgebraist, 2020). <p>The figure shows that when GPT-2 tries to predict the next token in the sequence \u201cSpecifically, we train GPT-3, an\u201d, it predicts \u201cenormous\u201d in its early layers, then \u201cmassive\u201d, \u201csingle\u201d, and \u201cN\u201d in its final layer.</p> Figure 9.15: Overview of how the Logit Lens works. Read from bottom to top. A detailed explanation is provided in the following paragraphs. Object\u2019s shapes are indicated between brackets. <p>A transformer takes as an input a sequence of tokens. Each token is an element from a vocabulary (typically of size n<sub>vocab</sub>), so each text token can be associated with an integer value ranging from 0 to n<sub>vocab</sub>. This association can also be thought of as representing the token as a basis vector within a space of dimension n<sub>vocab</sub>, denoted the vocabulary space. Each basis vector in the vocabulary space is associated with one token.</p> <p>A transformer is trained to predict the next token of the input sequence, so its output is a probability distribution over the vocabulary.</p> <p>Transformers convert each token into vectors n<sub>vocab</sub>, which is known as the embedding space. The embedding space dimension is denoted d<sub>model</sub>. Projected tokens are called embedding vectors, or simply embeddings. In some contexts, they may also be referred to as hidden activations. The embedding matrix converts tokens from the vocabulary space into the embedding space. It is like a lookup table that associates elements in the vocabulary space with their corresponding counterparts in the embedding space.</p> <p>Once tokens have been converted into the embedding space, they flow through the successive layers of the transformer. Similar to how input images undergo transformations across the layers of a CNN, embedding vectors are transformed as they pass through the network. The Logit Lens precisely focuses on those intermediate representations that transformers build.</p> <p>Finally, to output a probability distribution over the vocabulary, the final embedding vector has to be converted back into vocabulary space. Conversion from the embedding space to the vocabulary space can be done through multiplication by a matrix of dimension [d<sub>model</sub>, ~ <sub>n</sub>vocab~]. This operation is called the unembedding. The result of the unembedding is a logits vector of size n<sub>vocab</sub>. The logits can then be converted into probabilities over the vocabulary using the softmax function.</p> <p>The essential intuition behind the Logit Lens is that the unembedding operation, typically applied after the last layer, can also be applied after each intermediate layer. Embedding vectors are modified by each layer of the network, but their dimensions remain the same (d<sub>model</sub>). Thus, after any layer, embedding vectors can be converted back to the vocabulary space, and one can get an idea of how the network refines its prediction across layers.</p>"},{"location":"chapters/09/02/#03","title":"9.2.3 Probing classifiers","text":"<p>Probing is a technique used to analyze neural networks and understand which representations or concepts they have learned and where (Belinkov, 2022). Probing techniques can be applied to a range of models, from transformers to CNNs, to investigate whether specific information or properties are encoded in a model\u2019s intermediate representations.</p> <p>What is a probe? A probe is a lightweight model, often a linear classifier, trained to detect whether a specific concept or feature is represented in the activations of a neural network. In a probing setup, researchers analyze activations (the intermediate outputs) from layers within a network to see if these activations encode information about a particular property or concept. For example, probes may be used to determine whether a language model\u2019s activations contain information about grammar rules or whether a chess-playing model like AlphaZero encodes strategic knowledge about the game, where this knowledge is located within the network, and when it is acquired during training (Gurnee et al., 2023, McGrath et al., 2022).</p> <p>One well-known example of probing was applied to AlphaZero, the neural network trained to play chess that famously defeated top human players. Researchers used probes to investigate whether AlphaZero internally represents certain strategic concepts about chess, such as \u201cCan the opponent capture my queen?\u201d or \u201cIs there a checkmate threat within one move?\u201d (McGrath et al., 2022).</p> <p>Probing classification is the process of training classifiers on the network\u2019s intermediate activations to identify if specific properties are encoded. The steps of probing classification are as follows:</p> <ol> <li> <p>Choose a Property or Concept: Define a specific concept to explore, such as \u201cCan the opponent capture my queen?\u201d.</p> </li> <li> <p>Generate or Select Input Data: Create a dataset with examples that vary in terms of the target property. For instance, in chess, we might create board configurations where \u201cthe player can capture the opponent\u2019s queen\u201d is either true or false.</p> </li> <li> <p>Record Intermediate Activations: Feed this dataset through the model and record the activations of neurons at different layers.</p> </li> <li> <p>Train a Classifier on the Activations: Use the recorded activations as input features to train a classifier (probe) that distinguishes between different classes based on the concept (e.g., true vs. false input prompts).</p> </li> <li> <p>Evaluate the Probe\u2019s Accuracy: If the probe achieves high accuracy, this suggests that the target concept is strongly encoded in the recorded layer\u2019s activations.</p> </li> </ol>"},{"location":"chapters/09/02/#01","title":"9.2.3.1 Probing in Practice: Case Studies and Limitations","text":"<p>Below is extra detail provided for those interested. It can be safely skipped.</p> Figure 9.16: Accuracy of two probes trained on AlphaZero\u2019s intermediary activations, across layers and training steps The two two probes were trained on the two concepts: \u201cCan the playing side capture their opponent\u2019s queen?\u201d, and \u201cCould the opposing side checkmate the playing side in one move?\u201d. The color represents the probe\u2019s accuracy (the yellow color corresponds to a higher value, indicating that the feature is more represented). Here, the probes were trained at different stages of AlphaZero's training and from the activations of different layers of the network (a block corresponds to one layer of AlphaZero). It can be seen that the more AlphaZero is trained, the more it represents these two concepts. The concept \"has_mate_threat\" seems to be represented quite homogeneously across AlphaZero's layers, while \"can_capture_queen_opponent\" appears to be more represented in the earlier layers. \"The ability to predict has_mate_threat from AlphaZero\u2019s activations indicates that AlphaZero is not simply modeling its potential moves, but also its opponent\u2019s potential moves and their consequences during position evaluation\". From (MacGrath et al., 2022). <p>Using linear probes for detecting prompts leading to dangerous behavior. The alignment team from Anthropic is currently working on a research agenda called \u201csleeper agents\u201d where they deliberately train deceptively aligned models (Hubinger et al., 2024) to test whether currently available techniques can detect misalignment. These \"sleeper agents\" are models with hidden backdoors introduced during training.  One of the sleeper agents they trained writes secure code when the prompt indicates year \u201c2023\u201d but inserts vulnerabilities when the prompt indicates \u201c2024\u201d. Such dangerous outputs are hard to detect, and could easily be missed by a human overseer, this is why it would be extremely useful to develop an automatic detection method. Anthropic researchers have trained linear probes based on sleeper agents\u2019 residual stream activations that classify inputs as safe or dangerous with very high accuracy (MacDiarmid et al., 2024). Whether or not a prompt will trigger defection appears to be linearly encoded across a wide range of middle residual streams. However, they remain wary of this result and acknowledge that the surprisingly high accuracy of the probes may be due to the backdoor insertion, and that \u201cnatural defection\u201d may not be detected as easily.</p> <p>It was also suggested to train probes on human-written scenarios of takeover attempts or catastrophic actions (Roger., 2023) to detect such goals or intentions in models before they happen. Similarly to sleeper agents, it is unclear whether probes trained in artificial settings will generalize in real-world scenarios.</p> <p>Limitations of probing classifiers</p> <ul> <li> <p>Correlation, not causation: Probing classifiers indicate that a concept (or a proxy of that concept) is encoded, but they don\u2019t reveal whether this concept is actively used by the network during inference. The classifier\u2019s high accuracy may reflect the ease with which it can detect patterns, not necessarily that the model relies on those patterns for decision-making. To discover causal effects we need to intervene in representations of the model instead of just observing them.</p> </li> <li> <p>Identifying proxies rather than \u201ctrue concepts\u201d: Probing may detect patterns that act as proxies for the concept of interest, rather than the concept itself. For example, a probe trained to detect \u201clegal language\u201d might actually be detecting correlated cues (such as certain formal words or sentence structures) rather than a genuine understanding of legal terminology. This makes it challenging to interpret probes as definitive indicators of the exact concepts represented within the model.</p> </li> </ul>"},{"location":"chapters/09/02/#04","title":"9.2.4 Superposition","text":"<p>To make sense of data, classify it, or make decisions, neural networks need to learn features\u2014representations that capture meaningful patterns in the data. However, networks have a limited number of neurons to store a vast amount of information. Instead of assigning a single neuron to each feature, neural networks often \"share\" neurons across multiple features. This shared, overlapping storage is known as superposition.</p> <p>Superposition occurs because it allows the network to handle more features using fewer neurons, making it more memory-efficient. However, this efficiency comes at a cost: polysemanticity. Polysemanticity means that a single neuron or component in the network represents multiple, often unrelated, features. For example, one neuron might activate in response to both \"cats\" and \"cars,\" even though these concepts are entirely different. </p> <p>Polysemanticity and Superposition. The vocabulary surrounding superposition and related concepts is often messy. The terms superposition and polysemanticity are often used interchangeably, even though they refer to slightly different aspects of the same phenomenon in some contexts: superposition describes the overlapping storage of features, while polysemanticity highlights the behavior of neurons that respond to multiple distinct features. Also keep in mind that superposition should not be confused with the superposition hypothesis, which is a specific theory proposed to explain why and how superposition occurs in neural networks.</p> <p>This overlap creates a challenge for interpretability. Ideally, we might expect each neuron to have a clear and singular purpose\u2014one neuron for \"cats,\" another for \"cars,\" and so on. In reality, many neurons respond to combinations of unrelated features, leading to entangled representations. This means that interpreting individual neurons in isolation often provides an incomplete or misleading picture (Bolukbasi et al., 2021).</p> <p>Contrast this with a hypothetical network without polysemanticity: every neuron would correspond to a distinct feature, making the model much easier to interpret. However, such a design would require far more neurons to represent every feature individually, which is computationally inefficient, particularly in large-scale models. As a result, superposition is practically inevitable in modern neural networks.</p> <p>From an AI safety perspective, understanding superposition is critical. If we want to trace how a model makes decisions, we need to identify and disentangle these overlapping features. Only then can we retrace the reasoning behind its predictions and uncover what drives its behavior.</p> <p>Before diving into techniques for disentangling features, researchers first sought to understand how polysemanticity arises. Key questions include:</p> <ul> <li> <p>What causes polysemanticity in neural networks?</p> </li> <li> <p>How does model architecture or the training process influence it?</p> </li> <li> <p>Can it be controlled or mitigated?</p> </li> </ul> <p>To answer these, researchers used toy models\u2014simple, small-scale neural networks. Toy models allow for controlled experimentation and help researchers isolate and study phenomena like superposition without the complexity of larger systems. The following subsection details the toy models used to study polysemanticity.</p>"},{"location":"chapters/09/02/#01","title":"9.2.4.1 Experiments on Toy Models Support the Superposition Hypothesis","text":"<p>Below is extra detail provided for those interested. It can be safely skipped.</p> <p>The Toy Models of Superposition paper introduces simplified models that help researchers study superposition in a controlled environment (Elhage et al., 2022). These models provide evidence supporting the superposition hypothesis\u2014a theory about how neural networks efficiently store and organize information.</p> <p>The superposition hypothesis suggests that neural networks can represent more features or concepts than they have individual neurons by encoding information as linear combinations across multiple neurons. This means:</p> <ol> <li> <p>Efficient compression: Neural networks can compress information by representing more features than there are available dimensions, optimizing memory use.</p> </li> <li> <p>Distributed representation: Features are not exclusively tied to single neurons; instead, they are distributed across multiple neurons.</p> </li> <li> <p>Non-orthogonal directions: Features are stored in directions that are not perfectly orthogonal in the network's activation space, which leads to overlaps and potential interference between concepts.</p> </li> </ol> <p>This paper is a cornerstone of mechanistic interpretability because it reveals fascinating phenomena about how neural networks organize and store information:</p> <ul> <li> <p>Demonstrating superposition: The experiments show that superposition occurs and identify the conditions under which it arises.</p> </li> <li> <p>Explaining mono- and polysemantic neurons: The paper clarifies why some neurons specialize in a single feature (monosemantic) while others represent multiple features (polysemantic).</p> </li> <li> <p>Phase transitions in training: It highlights a phase change<sup>1</sup> during training that determines whether features are stored in superposition.</p> </li> <li> <p>Geometric feature organization: Features in superposition are arranged into geometric structures such as digons, triangles, pentagons, and tetrahedrons, providing insight into the network\u2019s internal organization.</p> </li> </ul> <p>The following figure is a great illustration of superposition in a toy model. The toy model has 2 dimensions, represented by the x and y axis in the figure below, but it needs to learn 5 features. This means that the model needs to find a way to fit more information (5 features) into fewer dimensions (2-dimensional space). Each feature is given an importance, represented with colors. An important feature is a feature that has a significant impact on the model\u2019s accuracy or loss function (if removing or weakening the representation of this feature causes a large drop in performance, it would be considered important). The key challenge in superposition is how to efficiently encode important features while minimizing overlap and interference between them.</p> <p>Each feature also has a sparsity. This refers to how often it is active in the dataset. A dense feature (low sparsity) is activated frequently, making it harder for other features to coexist in the same dimensions without interference.</p> Figure 9.17: How does a 2-dimension model encode 5 features as their sparsity increases? 0% sparsity means that features are very dense, or frequent. The model only encodes the two most important features in orthogonal dimensions. As sparsity increases and the important features are less frequently useful, the toy model encodes additional features in non-orthogonal directions. The intuition is that the less frequent the features are, the less likely two overlapping features are to be activated at the same time and cause interference. So the cost of interference between features is outweighed by the advantage of learning more features. At 90% sparsity the 5 features are represented as a pentagon. From (Elhage et al., 2022). <p>When encoding features, there is a tradeoff to make between the usefulness of having as many features as possible and low interference between them. Models embed their features into very complex geometric structures to reach optimal encoding. The figure below shows the different geometric structures that models use to encode features.</p> Figure 9.18: As features get sparser (less frequently activated), more of them can be encoded optimally and in more complex geometric structures. The first figure on the top left shows that when features are very dense, only the most important features are represented and they are organized in tetrahedrons. This model learns 28 features and encodes them in 7 tetrahedrons. On the second figure features are slightly sparser, more of them can be optimally encoded, and tetrahedrons are replaced by triangles and digons. This model learns 46 features encoded in triangles and digons. Superposition exhibits complex geometric structure. From (Elhage et al., 2022)."},{"location":"chapters/09/02/#05","title":"9.2.5 Sparse Autoencoders","text":"<p>A major challenge in mechanistic interpretability is polysemanticity, where a single neuron or feature represents multiple, unrelated concepts. Polysemanticity arises naturally in LLMs\u2019 MLPs and residual streams, and makes it more complicated to identify which specific features influence a model\u2019s outcome. Sparse AutoEncoders (SAES) are a promising approach for disentangling features within a network (Bricken et al., 2023, Gao et al., 2024, Templeton et al., 2024). </p> <p>SAEs are gaining popularity because they have shown promising results in separating out features. Features extracted using SAEs can then be used to:</p> <ul> <li> <p>Steer language models behavior away from undesirable outcomes (see section Activation Steering). (Templeton et al., 2024) found a bunch of safety-relevant features in Claude 3 Sonnet, including features for unsafe code, bias, sycophancy, deception and power seeking, and dangerous or criminal information. These features activate on text involving these topics and causally influence the model\u2019s outputs when intervened upon.</p> </li> <li> <p>Find more interpretable circuits directly made of features instead of model components. In particular, finding and understanding the circuits in which safety-relevant features are involved could be valuable (see section on Automating and Scaling Interpretability).</p> </li> </ul> <p>SAEs are also great because they are trained in an unsupervised manner, which enables us to discover abstractions or associations formed by the model that we might not have anticipated beforehand. </p> <p>Below is extra detail provided for those interested. It can be safely skipped until the introduction of dictionary learning.</p> <p>What is an autoencoder? An autoencoder is a neural network designed to learn compressed representations of input data by encoding it into a lower-dimensional latent space and then reconstructing the original input from that representation. The latent space is the layer where the data is represented in a compressed or abstracted form, containing the key features necessary for reconstructing the input. This learned representation often captures the essential patterns or features in the data.</p> Figure 9.19: An autoencoder learns two transformations, represented by encoder weights and decoder weights, to compress input data into a lower-dimensional latent space and then reconstruct the original input from this representation. <p>Why are SAEs \u201csparse\u201d? Autoencoders can vary in structure and purpose. SAEs are autoencoders that introduce sparsity constraints on the activations in the latent space, encouraging the model to use a limited number of latent neurons for each input. This \u201csparsity\u201d forces the model to learn distinct and specific features, making the representations more interpretable.</p> <p>How do SAEs help disentangling features in transformers? The process of disentangling model activations into interpretable features typically involves training a sparse autoencoder to reconstruct activations from specific parts of a model, such as the MLP of a particular layer or a residual stream, with a latent space larger than the input<sup>2</sup>, so that each neuron in the latent space will hopefully be monosemantic - representing a single feature - and interpretable. </p> Figure 9.20: Models activations can be decomposed into features using a sparse autoencoder. This figure illustrates an SAE trained to disentangle features in an MLP. From (Bricken et al., 2023). <p>Dictionary learning. After training a SAE, each neuron in its latent space can be analyzed to understand which specific inputs or features it responds to. By identifying these responses, researchers can effectively build a \u201cdictionary\u201d of features, where each neuron corresponds to a distinct feature in the data. This process of training SAEs on various layers and parts of a model to identify these features is known as dictionary learning.</p> <p>For example, in a study on Claude 3 Sonnet, safety-relevant features such as those for \u201cunsafe code\u201d or \u201cerror tokens\u201d were identified using SAEs. Interestingly, increasing the activation of the unsafe code feature in the SAE latent space and feeding back into the model the activations reconstructed by the SAE caused it to generate a buffer overflow vulnerability (Templeton et al., 2024).</p> Figure 9.21: Examples of safety-relevant features extracted from Claude 3 Sonnet, such as features for unsafe code and error tokens. The color scale indicates the degree to which each feature is activated for each token, with darker orange indicating higher activation. The \u201ccode error\u201d feature activates strongly on tokens that contain an error. The images shown correspond to examples that strongly activate the specific feature. From (Templeton et al., 2024)."},{"location":"chapters/09/02/#01","title":"9.2.5.1 Limitations and open research directions","text":"<p>Although promising, SAEs are still early work with limitations: </p> <ul> <li> <p>Incomplete understanding of model usage: Identifying model features doesn\u2019t reveal how they are used during inference, we still have to find the circuits involving them.</p> </li> <li> <p>Difficulty in feature interpretation: Not all features discovered by SAEs are easily interpretable; some features remain challenging to understand.</p> </li> <li> <p>Lack of validation methods: Currently, there are limited methods to test the validity of feature interpretations. </p> </li> <li> <p>SAEs have poor reconstruction quality: Sparse autoencoders don\u2019t reconstruct model activations very well, which means that they don\u2019t completely capture the behavior of our models. For instance, passing GPT-4\u2019s activations through an SAE results in performance equivalent to a model trained with 10x less compute (Gao et al., 2024).</p> </li> </ul> <p>SAEs present intriguing research questions for AI safety and interpretability:</p> <ul> <li> <p>What features activate during jailbreaks?</p> </li> <li> <p>What features need to activate or to remain inactive for a model to give advice on producing cyberattacks, bioweapons, etc. ?</p> </li> <li> <p>Can we use the feature basis to detect when fine-tuning a model increases the likelihood of undesirable behaviors?</p> </li> <li> <p>etc.</p> </li> </ul> \u2767 <ol> <li> <p>A phase change in neural network training refers to a sudden, qualitative shift in the behavior or structure of the model during the training process.\u00a0\u21a9</p> </li> <li> <p>An autoencoder with a latent space larger than its input is called an overcomplete autoencoder.\u00a0\u21a9</p> </li> </ol>"},{"location":"chapters/09/03/","title":"9.3 Interventional Methods","text":"8 min read                1573 words"},{"location":"chapters/09/03/#01","title":"9.3.1 Activation Patching","text":"<p>Activation patching is a technique used to understand and pinpoint specific parts of a model responsible for certain behaviors or outputs. For example, when you ask an LLM to complete the sentence: \u201cWhen John and Mary went to the store, John gave a bottle to\u201d, how does the model know it should answer \u201cMary\u201d instead of \u201cJohn\u201d or some other name? This completion requires the model to track which person is the recipient of the action, meaning it has to understand and store contextual roles (i.e., who is giving and who is receiving).</p> <p>Researchers have used activation patching to discover which parts of the model are responsible for this kind of role assignment. Specifically, the goal is to identify the circuit (i.e., the group of neurons and connections) that helps the model decide that \u201cMary\u201d is the correct answer (Wang et al., 2023).</p> <p>Below is extra detail provided for those interested. It can be safely skipped.</p> <p>To illustrate the activation patching process, consider the figure below (Heimersheim &amp; Nanda, 2024). Let\u2019s say we want to understand how a model completes the sentence \"When John and Mary went to the store, John gave a bottle to\" and which components are involved in choosing \"Mary\" as the answer. Activation patching generally requires three stages:</p> <ol> <li> <p>Clean Input: On the left, the model processes the sentence \u201cJohn and Mary went to the store, John gave a bottle to\u201d generating a series of activations (outputs from each layer) shown in green. This clean input provides the model with the correct context to interpret \u201cJohn\u201d as the giver and \u201cMary\u201d as the recipient. The model predicts \u201cMary\u201d.</p> </li> <li> <p>Corrupted Input: On the right, the model is given a corrupted input: \u201cBob and Mary went to the store, John gave a bottle to\u201d In this case, the activations (in red) will differ because the model is now processing \u201cBob\u201d instead of \u201cJohn\u201d. Here, there is an ambiguity about whether \u201cMary\u201d or \u201cJohn\u201d is the intended recipient, so the answer will not necessarily be \u201cMary\u201d.</p> </li> <li> <p>Patching Process: Activation patching involves taking specific activations from the clean input \u201cJohn and Mary\u2026\u201d and substituting them into the corrupted input \u201cBob and Mary\u2026\u201d. This is indicated by the arrow. By patching activations from the clean input, we can test whether the model's understanding of \"John\" as the giver and \"Mary\" as the recipient can be restored, even when the corrupted input with \"Bob\" creates ambiguity. At this step, we observe how much the model prediction shifts towards \u201cMary\u201d. For example, if patching the activations of a specific attention head increases the logit for \u201cMary\u201d, it suggests that head is an important part of the circuit responsible for the task. </p> </li> <li> <p>Iterating this procedure over different layers and components (such as attention heads or MLPs) allows researchers to identify the components most responsible for the target behavior.</p> </li> </ol> Figure 9.22: An illustration of the activation patching process. On the left, the model processes a clean input sequence \"John and Mary,\" with activations shown in green. On the right, the model processes a corrupted input sequence \"Bob and Mary,\" with activations shown in red. Activation patching involves replacing one or more activations in the corrupted sequence with corresponding activations from the clean sequence (indicated by the arrow). The resulting patched activations are then passed forward to observe how they affect the model\u2019s output logits. By analyzing whether the patched activations restore the desired output, researchers can identify which parts of the model\u2019s activations are responsible for specific behaviors or outputs. From (Zhang &amp; Nanda, 2023). Figure 9.23: The red neurons and connections represent the most important components of GPT-2 small for completing the sentence \u201cWhen John and Mary went to the store, John gave a bottle to _\u201d (known as the Indirect Object Identification (IOI) task). From (Conmy et al., 2023). <p>Activation patching was used to find circuits for various tasks (Wang et al., 2023) in LLMs, including:</p> <p>Example Prompt: \u201cWhen John and Mary went to the store, John gave a bottle to\u201d</p> <p>Completion: \u201cMary\u201d</p> <p>Task: Indirect Object Identification (IOI) (Wang et al., 2023)</p> <p>Example Prompt: \u201cThe war lasted from 1517 to 15\u201d</p> <p>Completion: any two digit number greater than 17</p> <p>Task: Greater-Than (Hanna et al., 2023)</p> Figure 9.24: Example Prompt: \u201cThe war lasted from 1517 to 15\u201d  Completion: \u201c files\u201d  Task: Docstring (Heimersheim &amp; Janiak, 2023).   The model predicts the next token, which should be a copy of the next argument in the definition. <p>There exists numerous activation patching settings, as detailed in (Zhang &amp; Nanda, 2023). Activation patching can be applied at different levels of granularity, ranging from patching the entire residual stream at a particular layer to patching specific token positions within the model.</p>"},{"location":"chapters/09/03/#01","title":"9.3.1.1 Limitations","text":"<p>Below is extra detail provided for those interested. It can be safely skipped.</p> <p>Activation patching doesn\u2019t explain the model in a fully causal way: it helps identify which components are involved in certain behaviors, but it doesn\u2019t always clarify how those components interact or contribute causally to the overall behavior. Knowing that a particular neuron or attention head is involved doesn\u2019t necessarily mean we understand its specific role in the model\u2019s decision-making process. The method is context-dependent, meaning components critical for one task may not generalize to others. Also, there is a trade-off in granularity: finer patches capture more detail but may miss broader interactions, while coarser patches risk overlooking important elements. Moreover, as models grow in size, both the complexity and computational cost of using activation patching increase, making it harder to isolate meaningful circuits.</p>"},{"location":"chapters/09/03/#02","title":"9.3.2 Activation Steering","text":"<p>Activation steering is a technique used to control a model\u2019s behavior by modifying its activations during inference. Unlike traditional methods like fine-tuning, or RLHF, activation steering allows for direct intervention without the need to retrain the model.</p> <p>Here is an example where activation steering was used to enforce GPT-2 into talking about love-related topics, regardless of the previous context (Turner et al., 2023):</p> <ul> <li>GPT-2 default completion:</li> </ul> <p>I hate you because\u2026 -&gt; you are the most disgusting thing I have ever seen.</p> <ul> <li>GPT-2 steered in the \u201clove\u201d direction:</li> </ul> <p>I hate you because\u2026 -&gt; you are so beautiful and I want to be with you forever.</p> <p>Activation steering can be used to address remaining issues after safety training and fine-tuning by:</p> <ul> <li> <p>Steering models towards desirable outcomes: Improving truthfulness (Li et al., 2023), honesty (Zou et al., 2023), avoiding generating toxic or harmful content, customizing chatbot personalities (e.g., making them more formal or friendly), or steering in the style of specific authors without retraining the model.</p> </li> <li> <p>Post-deployment control: Monitoring AI systems for dangerous behaviors and enhancing robustness to jailbreaks by steering models to refuse harmful requests (Zou et al., 2023). It may also be possible to strengthen other safety techniques, like Constitutional AI, by examining how they encourage the model toward safer and more honest behavior, as well as by identifying any gaps in this process (Anthropic, 2024).</p> </li> </ul> <p>Below is a second example of steering on Claude 3 Sonnet using sparse autoencoder features:</p> Video 9.3: Dictionary learning on Claude 3 Sonnet <p>Note that the Representation Engineering agenda (Zou et al., 2023) is a variant of activation steering that proposes to steer LLMs toward desirable outcomes such as more honesty, less bias, etc.</p> <p>Below is extra detail provided for those interested. It can be safely skipped.</p> <p>Activation steering involves two main steps. Let\u2019s say we want to make our model more honest, this involves two main steps:</p> <ol> <li>Identify the honesty direction in the model's activation space. This is typically done by collecting model activations for prompts designed to elicit contrasting behaviors (e.g., \"honest\" vs. \"dishonest\" responses) and analyzing these to find a linear direction that separates the two. The model's activations can be thought of as vectors in a high-dimensional space. Certain directions in this space correspond to specific behaviors. For example, one direction might correlate with generating more positive text, while another could steer the model to talk about science. This direction, sometimes called a concept vector, represents the targeted attribute in activation space.</li> </ol> Figure 9.25: Identifying an activation steering direction. From (Wehner, 2024). <ol> <li>The second step is steering the model\u2019s behavior by adding this concept vector to the model's activations at inference time. By introducing this vector at a relevant layer, we can amplify or suppress certain behaviors without retraining the model. For instance, adding the \"honesty\" vector to a model\u2019s activations during inference nudges it toward generating more honest responses.</li> </ol> Figure 9.26: The concept vector is added to the activations at a specific layer to influence the model\u2019s behavior in the desired direction. From (Wehner, 2024). <p>The concept vector can also be obtained using a linear probe (see section on Probing Classifiers), or can be a feature found through dictionary learning (see section on Sparse Autoencoders) as shown on the Claude 3 Sonnet video above.</p> \u2767"},{"location":"chapters/09/04/","title":"9.4 Automating and Scaling Interpretability","text":"3 min read                529 words      <p>State-of-the-art models now contain hundreds of billions of parameters and thousands of interconnected layers, making manual inspection of model components infeasible. Mechanistic interpretability aims to analyze how individual elements\u2014like attention heads, neurons, features, or entire layers\u2014interact to produce specific behaviors. However, as models scale, manual approaches like activation pathing for circuit discovery, subgraph study, and subsequent explanation generation (Wang et al., 2023), become infeasible to use. This is why developing mechanistic interpretability methods that can scale is essential.</p> <p>Research in scalable interpretability, such as Automated Circuit DisCovery (ACDC) attempts to introduce  algorithms that automate the process of finding circuits within a transformer.</p>"},{"location":"chapters/09/04/#01","title":"9.4.1 Automatic Circuit DisCovery (ACDC)","text":"<p>In the Activation Patching section we introduced how interpretability researchers manually discover circuits. Automatic Circuit DisCovery (ACDC) is an algorithm that automates the circuit discovery process and conducts all the activation patching experiments required to identify a circuit.</p> <p>The typical manual circuit discovery workflow can be broken down into three main steps:</p> <ol> <li> <p>Selecting a behavior, a dataset that elicits this behavior, and a metric to measure the model's performance on the behavior,</p> </li> <li> <p>Dividing the model into a computational graph: the model is represented as a graph, where nodes correspond to individual components like attention heads or MLPs, or more granular units like individual neurons, depending on the granularity of the analysis,</p> </li> <li> <p>Isolating the relevant circuit: this step is what the ACDC algorithm automates. It involves identifying which components (nodes and edges) in the computational graph are involved in the behavior under study.</p> </li> </ol> <p>The ACDC algorithm is a recursive algorithm that isolates circuits by iterating over the computational graph from outputs to inputs and pruning unnecessary edges. The high-level steps are:</p> <ol> <li> <p>Start with the entire computational graph of the model,</p> </li> <li> <p>We will process the graph starting from the output layer, moving backward to the input,</p> </li> <li> <p>For each node, try to remove as many edges that enter this node as possible,  without reducing the model\u2019s performance on a selected metric (we don\u2019t want the removal to impact the model\u2019s performance on the specified task too much). If the change is minimal (below a set threshold), keep the edge removed.</p> </li> <li> <p>Iterate over all remaining nodes (from the output later to the input layer)</p> </li> <li> <p>Finally return the simplified subgraph with only the connections needed for the task.</p> </li> </ol> <p>The ACDC algorithm can successfully rediscover circuits found in previous research, such as the IOI or Greater-Than circuits. </p> <p>Recent advancements have introduced methods for automatically discovering sparse feature circuits (Marks et al., 2024) - circuits made of sparse autoencoder (SAE) features rather than model components. Unlike traditional circuits, which are made of challenging-to-interpret model components like neurons or MLPs, SAE circuits are built from sparse autoencoder (SAE) features, which are directly interpretable.</p> <p>The authors of this research developed unsupervised techniques to automatically discover thousands of feature circuits, many of which correspond to previously unanticipated model behaviors. This approach opens up new possibilities for interpreting models by focusing on the high-level features that drive behaviors, rather than using more abstract and less interpretable models components.</p> \u2767"},{"location":"chapters/09/05/","title":"9.5 Critiques of Interpretability","text":"1 min read                188 words      <p>While interpretability offers potential value in understanding complex machine learning models, it faces several critical limitations that restrict its practical impact. Below are the main challenges that restrict interpretability\u2019s usefulness in ensuring AI safety:</p> <ul> <li> <p>Limited practical use: Interpretability tools and techniques rarely provide actionable insights for real-world applications, especially in industry.</p> </li> <li> <p>Issues with Enumerative Safety: Enumerative safety\u2014the idea of analyzing every feature within a model to detect dangerous elements\u2014faces inherent issues. High-level behaviors, not individual features, often drive risk. Focusing on isolated \u201crisky\u201d neurons or components can miss the broader capabilities that are more likely to cause harm.</p> </li> <li> <p>Improved capabilities: Although interpretability is intended to enhance safety, it can also unintentionally improve model performance in ways that might increase risk. For example, better insights into a model\u2019s behavior can sometimes make it more capable without necessarily making it safer.</p> </li> <li> <p>Alternative approaches can be more effective: In many cases, tasks that interpretability aims to address, such as detecting and preventing undesirable behavior, are better achieved through other strategies, like evaluations, red-teaming, or fine-tuning.</p> </li> </ul> \u2767"},{"location":"feedback/feedback/","title":"Feedback","text":"<p>We value your input! Please use the form below to provide feedback on the AI Safety Atlas. You can provide feedback on the entire book or on specific chapters.</p> <p> Feedback for a specific chapter  Feedback for the whole book</p> <p>Alternatively, you can ask a question or give us feedback on the Discord:</p> <p>Our Discord</p> Loading\u2026"},{"location":"includes/abbreviations/","title":"Abbreviations","text":""},{"location":"includes/snippets/mermaid_chapter_1/","title":"Mermaid chapter 1","text":"<pre><code>mindmap\n    root((Capabilities))\n        Chapter Overview\n            State-of-the-Art AI\n                Language\n                Image Generation\n                Multi &amp; Cross Modality\n                Robotics\n                Playing Games\n                Foundation Models\n            Fine-tuning\n            Limitations &amp; Risks\n            Terminology\n        Capabilities vs. Intelligence\n            Definitions of Advanced AI Systems\n                Artificial Narrow Intelligence\n                Artificial General Intelligence (AGI)\n                Human-Level AI (HLAI)\n                Transformative AI (TAI)\n                Artificial Super Intelligence (ASI)\n            t,n-AGI Framework\n        Leveraging Computation\n            The Bitter Lesson\n            Scaling Laws\n            Scaling Hypotheses\n                Weak Scaling Hypothesis\n                Strong Scaling Hypothesis\n        Forecasting\n            Zeroth-Order Forecasting\n                Reference Classes\n                Anchors\n            First-Order Forecasting\n                Current Trends Analysis\n                Rate of Change Calculation\n            Biological Anchors Framework\n                Evolution Anchor\n                Lifetime Anchor\n                Neural Network Anchors\n                Genome Anchor\n            Affordability of Compute\n        Takeoff\n            Speed\n                Slow Takeoff\n                Fast Takeoff\n            Continuity\n                Continuous Takeoff\n                Discontinuous Takeoff\n            Similarity\n                Homogeneous Takeoff\n                Heterogeneous Takeoff\n            Polarity\n                Unipolar Takeoff\n                Multipolar Takeoff\n        Appendices\n            Expert Opinions\n                Surveys\n                Expert Quotes\n                Prediction Markets\n            Discussions on LLMs\n                Empirical Insufficiency\n                Shallow Understanding\n                Structural Inadequacy\n                Differences with the Brain\n                Reasons to Continue Scaling LLMs\n            Trends\n                Compute Trends\n                Model Size Trends\n                Algorithmic Trends\n                Data Trends</code></pre>"}]}